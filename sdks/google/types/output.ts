// *** WARNING: this file was generated by pulumi-language-nodejs. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "../types/input";
import * as outputs from "../types/output";

export interface AccessContextManagerAccessLevelBasic {
    /**
     * How the conditions list should be combined to determine if a request
     * is granted this AccessLevel. If AND is used, each Condition in
     * conditions must be satisfied for the AccessLevel to be applied. If
     * OR is used, at least one Condition in conditions must be satisfied
     * for the AccessLevel to be applied. Default value: "AND" Possible values: ["AND", "OR"]
     */
    combiningFunction?: string;
    /**
     * A set of requirements for the AccessLevel to be granted.
     */
    conditions: outputs.AccessContextManagerAccessLevelBasicCondition[];
}

export interface AccessContextManagerAccessLevelBasicCondition {
    /**
     * Device specific restrictions, all restrictions must hold for
     * the Condition to be true. If not specified, all devices are
     * allowed.
     */
    devicePolicy?: outputs.AccessContextManagerAccessLevelBasicConditionDevicePolicy;
    /**
     * A list of CIDR block IP subnetwork specification. May be IPv4
     * or IPv6.
     * Note that for a CIDR IP address block, the specified IP address
     * portion must be properly truncated (i.e. all the host bits must
     * be zero) or the input is considered malformed. For example,
     * "192.0.2.0/24" is accepted but "192.0.2.1/24" is not. Similarly,
     * for IPv6, "2001:db8::/32" is accepted whereas "2001:db8::1/32"
     * is not. The originating IP of a request must be in one of the
     * listed subnets in order for this Condition to be true.
     * If empty, all IP addresses are allowed.
     */
    ipSubnetworks?: string[];
    /**
     * An allowed list of members (users, service accounts).
     * Using groups is not supported yet.
     *
     * The signed-in user originating the request must be a part of one
     * of the provided members. If not specified, a request may come
     * from any user (logged in/not logged in, not present in any
     * groups, etc.).
     * Formats: 'user:{emailid}', 'serviceAccount:{emailid}'
     */
    members?: string[];
    /**
     * Whether to negate the Condition. If true, the Condition becomes
     * a NAND over its non-empty fields, each field must be false for
     * the Condition overall to be satisfied. Defaults to false.
     */
    negate?: boolean;
    /**
     * The request must originate from one of the provided
     * countries/regions.
     * Format: A valid ISO 3166-1 alpha-2 code.
     */
    regions?: string[];
    /**
     * A list of other access levels defined in the same Policy,
     * referenced by resource name. Referencing an AccessLevel which
     * does not exist is an error. All access levels listed must be
     * granted for the Condition to be true.
     * Format: accessPolicies/{policy_id}/accessLevels/{short_name}
     */
    requiredAccessLevels?: string[];
    /**
     * The request must originate from one of the provided VPC networks in Google Cloud. Cannot specify this field together with 'ip_subnetworks'.
     */
    vpcNetworkSources?: outputs.AccessContextManagerAccessLevelBasicConditionVpcNetworkSource[];
}

export interface AccessContextManagerAccessLevelBasicConditionDevicePolicy {
    /**
     * A list of allowed device management levels.
     * An empty list allows all management levels. Possible values: ["MANAGEMENT_UNSPECIFIED", "NONE", "BASIC", "COMPLETE"]
     */
    allowedDeviceManagementLevels?: string[];
    /**
     * A list of allowed encryptions statuses.
     * An empty list allows all statuses. Possible values: ["ENCRYPTION_UNSPECIFIED", "ENCRYPTION_UNSUPPORTED", "UNENCRYPTED", "ENCRYPTED"]
     */
    allowedEncryptionStatuses?: string[];
    /**
     * A list of allowed OS versions.
     * An empty list allows all types and all versions.
     */
    osConstraints?: outputs.AccessContextManagerAccessLevelBasicConditionDevicePolicyOsConstraint[];
    /**
     * Whether the device needs to be approved by the customer admin.
     */
    requireAdminApproval?: boolean;
    /**
     * Whether the device needs to be corp owned.
     */
    requireCorpOwned?: boolean;
    /**
     * Whether or not screenlock is required for the DevicePolicy
     * to be true. Defaults to false.
     */
    requireScreenLock?: boolean;
}

export interface AccessContextManagerAccessLevelBasicConditionDevicePolicyOsConstraint {
    /**
     * The minimum allowed OS version. If not set, any version
     * of this OS satisfies the constraint.
     * Format: "major.minor.patch" such as "10.5.301", "9.2.1".
     */
    minimumVersion?: string;
    /**
     * The operating system type of the device. Possible values: ["OS_UNSPECIFIED", "DESKTOP_MAC", "DESKTOP_WINDOWS", "DESKTOP_LINUX", "DESKTOP_CHROME_OS", "ANDROID", "IOS"]
     */
    osType: string;
    /**
     * If you specify DESKTOP_CHROME_OS for osType, you can optionally include requireVerifiedChromeOs to require Chrome Verified Access.
     */
    requireVerifiedChromeOs?: boolean;
}

export interface AccessContextManagerAccessLevelBasicConditionVpcNetworkSource {
    /**
     * Sub networks within a VPC network.
     */
    vpcSubnetwork?: outputs.AccessContextManagerAccessLevelBasicConditionVpcNetworkSourceVpcSubnetwork;
}

export interface AccessContextManagerAccessLevelBasicConditionVpcNetworkSourceVpcSubnetwork {
    /**
     * Required. Network name to be allowed by this Access Level. Networks of foreign organizations requires 'compute.network.get' permission to be granted to caller.
     */
    network: string;
    /**
     * CIDR block IP subnetwork specification. Must be IPv4.
     */
    vpcIpSubnetworks?: string[];
}

export interface AccessContextManagerAccessLevelConditionDevicePolicy {
    /**
     * A list of allowed device management levels.
     * An empty list allows all management levels. Possible values: ["MANAGEMENT_UNSPECIFIED", "NONE", "BASIC", "COMPLETE"]
     */
    allowedDeviceManagementLevels?: string[];
    /**
     * A list of allowed encryptions statuses.
     * An empty list allows all statuses. Possible values: ["ENCRYPTION_UNSPECIFIED", "ENCRYPTION_UNSUPPORTED", "UNENCRYPTED", "ENCRYPTED"]
     */
    allowedEncryptionStatuses?: string[];
    /**
     * A list of allowed OS versions.
     * An empty list allows all types and all versions.
     */
    osConstraints?: outputs.AccessContextManagerAccessLevelConditionDevicePolicyOsConstraint[];
    /**
     * Whether the device needs to be approved by the customer admin.
     */
    requireAdminApproval?: boolean;
    /**
     * Whether the device needs to be corp owned.
     */
    requireCorpOwned?: boolean;
    /**
     * Whether or not screenlock is required for the DevicePolicy
     * to be true. Defaults to false.
     */
    requireScreenLock?: boolean;
}

export interface AccessContextManagerAccessLevelConditionDevicePolicyOsConstraint {
    /**
     * The minimum allowed OS version. If not set, any version
     * of this OS satisfies the constraint.
     * Format: "major.minor.patch" such as "10.5.301", "9.2.1".
     */
    minimumVersion?: string;
    /**
     * The operating system type of the device. Possible values: ["OS_UNSPECIFIED", "DESKTOP_MAC", "DESKTOP_WINDOWS", "DESKTOP_LINUX", "DESKTOP_CHROME_OS", "ANDROID", "IOS"]
     */
    osType: string;
}

export interface AccessContextManagerAccessLevelConditionTimeouts {
    create?: string;
    delete?: string;
}

export interface AccessContextManagerAccessLevelConditionVpcNetworkSource {
    /**
     * Sub networks within a VPC network.
     */
    vpcSubnetwork?: outputs.AccessContextManagerAccessLevelConditionVpcNetworkSourceVpcSubnetwork;
}

export interface AccessContextManagerAccessLevelConditionVpcNetworkSourceVpcSubnetwork {
    /**
     * Required. Network name to be allowed by this Access Level. Networks of foreign organizations requires 'compute.network.get' permission to be granted to caller.
     */
    network: string;
    /**
     * CIDR block IP subnetwork specification. Must be IPv4.
     */
    vpcIpSubnetworks?: string[];
}

export interface AccessContextManagerAccessLevelCustom {
    /**
     * Represents a textual expression in the Common Expression Language (CEL) syntax. CEL is a C-like expression language.
     * This page details the objects and attributes that are used to the build the CEL expressions for
     * custom access levels - https://cloud.google.com/access-context-manager/docs/custom-access-level-spec.
     */
    expr: outputs.AccessContextManagerAccessLevelCustomExpr;
}

export interface AccessContextManagerAccessLevelCustomExpr {
    /**
     * Description of the expression
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression: string;
    /**
     * String indicating the location of the expression for error reporting, e.g. a file name and a position in the file
     */
    location?: string;
    /**
     * Title for the expression, i.e. a short string describing its purpose.
     */
    title?: string;
}

export interface AccessContextManagerAccessLevelTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface AccessContextManagerAccessLevelsAccessLevel {
    /**
     * A set of predefined conditions for the access level and a combining function.
     */
    basic?: outputs.AccessContextManagerAccessLevelsAccessLevelBasic;
    /**
     * Custom access level conditions are set using the Cloud Common Expression Language to represent the necessary conditions for the level to apply to a request.
     * See CEL spec at: https://github.com/google/cel-spec.
     */
    custom?: outputs.AccessContextManagerAccessLevelsAccessLevelCustom;
    /**
     * Description of the AccessLevel and its use. Does not affect behavior.
     */
    description?: string;
    /**
     * Resource name for the Access Level. The short_name component must begin
     * with a letter and only include alphanumeric and '_'.
     * Format: accessPolicies/{policy_id}/accessLevels/{short_name}
     */
    name: string;
    /**
     * Human readable title. Must be unique within the Policy.
     */
    title: string;
}

export interface AccessContextManagerAccessLevelsAccessLevelBasic {
    /**
     * How the conditions list should be combined to determine if a request
     * is granted this AccessLevel. If AND is used, each Condition in
     * conditions must be satisfied for the AccessLevel to be applied. If
     * OR is used, at least one Condition in conditions must be satisfied
     * for the AccessLevel to be applied. Default value: "AND" Possible values: ["AND", "OR"]
     */
    combiningFunction?: string;
    /**
     * A set of requirements for the AccessLevel to be granted.
     */
    conditions: outputs.AccessContextManagerAccessLevelsAccessLevelBasicCondition[];
}

export interface AccessContextManagerAccessLevelsAccessLevelBasicCondition {
    /**
     * Device specific restrictions, all restrictions must hold for
     * the Condition to be true. If not specified, all devices are
     * allowed.
     */
    devicePolicy?: outputs.AccessContextManagerAccessLevelsAccessLevelBasicConditionDevicePolicy;
    /**
     * A list of CIDR block IP subnetwork specification. May be IPv4
     * or IPv6.
     * Note that for a CIDR IP address block, the specified IP address
     * portion must be properly truncated (i.e. all the host bits must
     * be zero) or the input is considered malformed. For example,
     * "192.0.2.0/24" is accepted but "192.0.2.1/24" is not. Similarly,
     * for IPv6, "2001:db8::/32" is accepted whereas "2001:db8::1/32"
     * is not. The originating IP of a request must be in one of the
     * listed subnets in order for this Condition to be true.
     * If empty, all IP addresses are allowed.
     */
    ipSubnetworks?: string[];
    /**
     * An allowed list of members (users, service accounts).
     * Using groups is not supported yet.
     *
     * The signed-in user originating the request must be a part of one
     * of the provided members. If not specified, a request may come
     * from any user (logged in/not logged in, not present in any
     * groups, etc.).
     * Formats: 'user:{emailid}', 'serviceAccount:{emailid}'
     */
    members?: string[];
    /**
     * Whether to negate the Condition. If true, the Condition becomes
     * a NAND over its non-empty fields, each field must be false for
     * the Condition overall to be satisfied. Defaults to false.
     */
    negate?: boolean;
    /**
     * The request must originate from one of the provided
     * countries/regions.
     * Format: A valid ISO 3166-1 alpha-2 code.
     */
    regions?: string[];
    /**
     * A list of other access levels defined in the same Policy,
     * referenced by resource name. Referencing an AccessLevel which
     * does not exist is an error. All access levels listed must be
     * granted for the Condition to be true.
     * Format: accessPolicies/{policy_id}/accessLevels/{short_name}
     */
    requiredAccessLevels?: string[];
    /**
     * The request must originate from one of the provided VPC networks in Google Cloud. Cannot specify this field together with 'ip_subnetworks'.
     */
    vpcNetworkSources?: outputs.AccessContextManagerAccessLevelsAccessLevelBasicConditionVpcNetworkSource[];
}

export interface AccessContextManagerAccessLevelsAccessLevelBasicConditionDevicePolicy {
    /**
     * A list of allowed device management levels.
     * An empty list allows all management levels. Possible values: ["MANAGEMENT_UNSPECIFIED", "NONE", "BASIC", "COMPLETE"]
     */
    allowedDeviceManagementLevels?: string[];
    /**
     * A list of allowed encryptions statuses.
     * An empty list allows all statuses. Possible values: ["ENCRYPTION_UNSPECIFIED", "ENCRYPTION_UNSUPPORTED", "UNENCRYPTED", "ENCRYPTED"]
     */
    allowedEncryptionStatuses?: string[];
    /**
     * A list of allowed OS versions.
     * An empty list allows all types and all versions.
     */
    osConstraints?: outputs.AccessContextManagerAccessLevelsAccessLevelBasicConditionDevicePolicyOsConstraint[];
    /**
     * Whether the device needs to be approved by the customer admin.
     */
    requireAdminApproval?: boolean;
    /**
     * Whether the device needs to be corp owned.
     */
    requireCorpOwned?: boolean;
    /**
     * Whether or not screenlock is required for the DevicePolicy
     * to be true. Defaults to false.
     */
    requireScreenLock?: boolean;
}

export interface AccessContextManagerAccessLevelsAccessLevelBasicConditionDevicePolicyOsConstraint {
    /**
     * The minimum allowed OS version. If not set, any version
     * of this OS satisfies the constraint.
     * Format: "major.minor.patch" such as "10.5.301", "9.2.1".
     */
    minimumVersion?: string;
    /**
     * The operating system type of the device. Possible values: ["OS_UNSPECIFIED", "DESKTOP_MAC", "DESKTOP_WINDOWS", "DESKTOP_LINUX", "DESKTOP_CHROME_OS", "ANDROID", "IOS"]
     */
    osType: string;
}

export interface AccessContextManagerAccessLevelsAccessLevelBasicConditionVpcNetworkSource {
    /**
     * Sub networks within a VPC network.
     */
    vpcSubnetwork?: outputs.AccessContextManagerAccessLevelsAccessLevelBasicConditionVpcNetworkSourceVpcSubnetwork;
}

export interface AccessContextManagerAccessLevelsAccessLevelBasicConditionVpcNetworkSourceVpcSubnetwork {
    /**
     * Required. Network name to be allowed by this Access Level. Networks of foreign organizations requires 'compute.network.get' permission to be granted to caller.
     */
    network: string;
    /**
     * CIDR block IP subnetwork specification. Must be IPv4.
     */
    vpcIpSubnetworks?: string[];
}

export interface AccessContextManagerAccessLevelsAccessLevelCustom {
    /**
     * Represents a textual expression in the Common Expression Language (CEL) syntax. CEL is a C-like expression language.
     * This page details the objects and attributes that are used to the build the CEL expressions for
     * custom access levels - https://cloud.google.com/access-context-manager/docs/custom-access-level-spec.
     */
    expr: outputs.AccessContextManagerAccessLevelsAccessLevelCustomExpr;
}

export interface AccessContextManagerAccessLevelsAccessLevelCustomExpr {
    /**
     * Description of the expression
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression: string;
    /**
     * String indicating the location of the expression for error reporting, e.g. a file name and a position in the file
     */
    location?: string;
    /**
     * Title for the expression, i.e. a short string describing its purpose.
     */
    title?: string;
}

export interface AccessContextManagerAccessLevelsTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface AccessContextManagerAccessPolicyIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface AccessContextManagerAccessPolicyIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface AccessContextManagerAccessPolicyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface AccessContextManagerAuthorizedOrgsDescTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface AccessContextManagerEgressPolicyTimeouts {
    create?: string;
    delete?: string;
}

export interface AccessContextManagerGcpUserAccessBindingTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface AccessContextManagerIngressPolicyTimeouts {
    create?: string;
    delete?: string;
}

export interface AccessContextManagerServicePerimeterDryRunEgressPolicyEgressFrom {
    /**
     * A list of identities that are allowed access through this 'EgressPolicy'.
     * Should be in the format of email address. The email address should
     * represent individual user or service account only.
     */
    identities?: string[];
    /**
     * Specifies the type of identities that are allowed access to outside the
     * perimeter. If left unspecified, then members of 'identities' field will
     * be allowed access. Possible values: ["ANY_IDENTITY", "ANY_USER_ACCOUNT", "ANY_SERVICE_ACCOUNT"]
     */
    identityType?: string;
    /**
     * Whether to enforce traffic restrictions based on 'sources' field. If the 'sources' field is non-empty, then this field must be set to 'SOURCE_RESTRICTION_ENABLED'. Possible values: ["SOURCE_RESTRICTION_ENABLED", "SOURCE_RESTRICTION_DISABLED"]
     */
    sourceRestriction?: string;
    /**
     * Sources that this EgressPolicy authorizes access from.
     */
    sources?: outputs.AccessContextManagerServicePerimeterDryRunEgressPolicyEgressFromSource[];
}

export interface AccessContextManagerServicePerimeterDryRunEgressPolicyEgressFromSource {
    /**
     * An AccessLevel resource name that allows resources outside the ServicePerimeter to be accessed from the inside.
     */
    accessLevel?: string;
}

export interface AccessContextManagerServicePerimeterDryRunEgressPolicyEgressTo {
    /**
     * A list of external resources that are allowed to be accessed. A request
     * matches if it contains an external resource in this list (Example:
     * s3://bucket/path). Currently '*' is not allowed.
     */
    externalResources?: string[];
    /**
     * A list of 'ApiOperations' that this egress rule applies to. A request matches
     * if it contains an operation/service in this list.
     */
    operations?: outputs.AccessContextManagerServicePerimeterDryRunEgressPolicyEgressToOperation[];
    /**
     * A list of resources, currently only projects in the form
     * 'projects/<projectnumber>', that match this to stanza. A request matches
     * if it contains a resource in this list. If * is specified for resources,
     * then this 'EgressTo' rule will authorize access to all resources outside
     * the perimeter.
     */
    resources?: string[];
}

export interface AccessContextManagerServicePerimeterDryRunEgressPolicyEgressToOperation {
    /**
     * API methods or permissions to allow. Method or permission must belong
     * to the service specified by 'serviceName' field. A single MethodSelector
     * entry with '*' specified for the 'method' field will allow all methods
     * AND permissions for the service specified in 'serviceName'.
     */
    methodSelectors?: outputs.AccessContextManagerServicePerimeterDryRunEgressPolicyEgressToOperationMethodSelector[];
    /**
     * The name of the API whose methods or permissions the 'IngressPolicy' or
     * 'EgressPolicy' want to allow. A single 'ApiOperation' with serviceName
     * field set to '*' will allow all methods AND permissions for all services.
     */
    serviceName?: string;
}

export interface AccessContextManagerServicePerimeterDryRunEgressPolicyEgressToOperationMethodSelector {
    /**
     * Value for 'method' should be a valid method name for the corresponding
     * 'serviceName' in 'ApiOperation'. If '*' used as value for method,
     * then ALL methods and permissions are allowed.
     */
    method?: string;
    /**
     * Value for permission should be a valid Cloud IAM permission for the
     * corresponding 'serviceName' in 'ApiOperation'.
     */
    permission?: string;
}

export interface AccessContextManagerServicePerimeterDryRunEgressPolicyTimeouts {
    create?: string;
    delete?: string;
}

export interface AccessContextManagerServicePerimeterDryRunIngressPolicyIngressFrom {
    /**
     * A list of identities that are allowed access through this ingress policy.
     * Should be in the format of email address. The email address should represent
     * individual user or service account only.
     */
    identities?: string[];
    /**
     * Specifies the type of identities that are allowed access from outside the
     * perimeter. If left unspecified, then members of 'identities' field will be
     * allowed access. Possible values: ["ANY_IDENTITY", "ANY_USER_ACCOUNT", "ANY_SERVICE_ACCOUNT"]
     */
    identityType?: string;
    /**
     * Sources that this 'IngressPolicy' authorizes access from.
     */
    sources?: outputs.AccessContextManagerServicePerimeterDryRunIngressPolicyIngressFromSource[];
}

export interface AccessContextManagerServicePerimeterDryRunIngressPolicyIngressFromSource {
    /**
     * An 'AccessLevel' resource name that allow resources within the
     * 'ServicePerimeters' to be accessed from the internet. 'AccessLevels' listed
     * must be in the same policy as this 'ServicePerimeter'. Referencing a nonexistent
     * 'AccessLevel' will cause an error. If no 'AccessLevel' names are listed,
     * resources within the perimeter can only be accessed via Google Cloud calls
     * with request origins within the perimeter.
     * Example 'accessPolicies/MY_POLICY/accessLevels/MY_LEVEL.'
     * If * is specified, then all IngressSources will be allowed.
     */
    accessLevel?: string;
    /**
     * A Google Cloud resource that is allowed to ingress the perimeter.
     * Requests from these resources will be allowed to access perimeter data.
     * Currently only projects are allowed. Format 'projects/{project_number}'
     * The project may be in any Google Cloud organization, not just the
     * organization that the perimeter is defined in. '*' is not allowed, the case
     * of allowing all Google Cloud resources only is not supported.
     */
    resource?: string;
}

export interface AccessContextManagerServicePerimeterDryRunIngressPolicyIngressTo {
    /**
     * A list of 'ApiOperations' the sources specified in corresponding 'IngressFrom'
     * are allowed to perform in this 'ServicePerimeter'.
     */
    operations?: outputs.AccessContextManagerServicePerimeterDryRunIngressPolicyIngressToOperation[];
    /**
     * A list of resources, currently only projects in the form
     * 'projects/<projectnumber>', protected by this 'ServicePerimeter'
     * that are allowed to be accessed by sources defined in the
     * corresponding 'IngressFrom'. A request matches if it contains
     * a resource in this list. If '*' is specified for resources,
     * then this 'IngressTo' rule will authorize access to all
     * resources inside the perimeter, provided that the request
     * also matches the 'operations' field.
     */
    resources?: string[];
}

export interface AccessContextManagerServicePerimeterDryRunIngressPolicyIngressToOperation {
    /**
     * API methods or permissions to allow. Method or permission must belong to
     * the service specified by serviceName field. A single 'MethodSelector' entry
     * with '*' specified for the method field will allow all methods AND
     * permissions for the service specified in 'serviceName'.
     */
    methodSelectors?: outputs.AccessContextManagerServicePerimeterDryRunIngressPolicyIngressToOperationMethodSelector[];
    /**
     * The name of the API whose methods or permissions the 'IngressPolicy' or
     * 'EgressPolicy' want to allow. A single 'ApiOperation' with 'serviceName'
     * field set to '*' will allow all methods AND permissions for all services.
     */
    serviceName?: string;
}

export interface AccessContextManagerServicePerimeterDryRunIngressPolicyIngressToOperationMethodSelector {
    /**
     * Value for method should be a valid method name for the corresponding
     * serviceName in 'ApiOperation'. If '*' used as value for 'method', then
     * ALL methods and permissions are allowed.
     */
    method?: string;
    /**
     * Value for permission should be a valid Cloud IAM permission for the
     * corresponding 'serviceName' in 'ApiOperation'.
     */
    permission?: string;
}

export interface AccessContextManagerServicePerimeterDryRunIngressPolicyTimeouts {
    create?: string;
    delete?: string;
}

export interface AccessContextManagerServicePerimeterDryRunResourceTimeouts {
    create?: string;
    delete?: string;
}

export interface AccessContextManagerServicePerimeterEgressPolicyEgressFrom {
    /**
     * A list of identities that are allowed access through this 'EgressPolicy'.
     * Should be in the format of email address. The email address should
     * represent individual user or service account only.
     */
    identities?: string[];
    /**
     * Specifies the type of identities that are allowed access to outside the
     * perimeter. If left unspecified, then members of 'identities' field will
     * be allowed access. Possible values: ["ANY_IDENTITY", "ANY_USER_ACCOUNT", "ANY_SERVICE_ACCOUNT"]
     */
    identityType?: string;
    /**
     * Whether to enforce traffic restrictions based on 'sources' field. If the 'sources' field is non-empty, then this field must be set to 'SOURCE_RESTRICTION_ENABLED'. Possible values: ["SOURCE_RESTRICTION_UNSPECIFIED", "SOURCE_RESTRICTION_ENABLED", "SOURCE_RESTRICTION_DISABLED"]
     */
    sourceRestriction?: string;
    /**
     * Sources that this EgressPolicy authorizes access from.
     */
    sources?: outputs.AccessContextManagerServicePerimeterEgressPolicyEgressFromSource[];
}

export interface AccessContextManagerServicePerimeterEgressPolicyEgressFromSource {
    /**
     * An AccessLevel resource name that allows resources outside the ServicePerimeter to be accessed from the inside.
     */
    accessLevel?: string;
}

export interface AccessContextManagerServicePerimeterEgressPolicyEgressTo {
    /**
     * A list of external resources that are allowed to be accessed. A request
     * matches if it contains an external resource in this list (Example:
     * s3://bucket/path). Currently '*' is not allowed.
     */
    externalResources?: string[];
    /**
     * A list of 'ApiOperations' that this egress rule applies to. A request matches
     * if it contains an operation/service in this list.
     */
    operations?: outputs.AccessContextManagerServicePerimeterEgressPolicyEgressToOperation[];
    /**
     * A list of resources, currently only projects in the form
     * 'projects/<projectnumber>', that match this to stanza. A request matches
     * if it contains a resource in this list. If * is specified for resources,
     * then this 'EgressTo' rule will authorize access to all resources outside
     * the perimeter.
     */
    resources?: string[];
}

export interface AccessContextManagerServicePerimeterEgressPolicyEgressToOperation {
    /**
     * API methods or permissions to allow. Method or permission must belong
     * to the service specified by 'serviceName' field. A single MethodSelector
     * entry with '*' specified for the 'method' field will allow all methods
     * AND permissions for the service specified in 'serviceName'.
     */
    methodSelectors?: outputs.AccessContextManagerServicePerimeterEgressPolicyEgressToOperationMethodSelector[];
    /**
     * The name of the API whose methods or permissions the 'IngressPolicy' or
     * 'EgressPolicy' want to allow. A single 'ApiOperation' with serviceName
     * field set to '*' will allow all methods AND permissions for all services.
     */
    serviceName?: string;
}

export interface AccessContextManagerServicePerimeterEgressPolicyEgressToOperationMethodSelector {
    /**
     * Value for 'method' should be a valid method name for the corresponding
     * 'serviceName' in 'ApiOperation'. If '*' used as value for method,
     * then ALL methods and permissions are allowed.
     */
    method?: string;
    /**
     * Value for permission should be a valid Cloud IAM permission for the
     * corresponding 'serviceName' in 'ApiOperation'.
     */
    permission?: string;
}

export interface AccessContextManagerServicePerimeterEgressPolicyTimeouts {
    create?: string;
    delete?: string;
}

export interface AccessContextManagerServicePerimeterIngressPolicyIngressFrom {
    /**
     * A list of identities that are allowed access through this ingress policy.
     * Should be in the format of email address. The email address should represent
     * individual user or service account only.
     */
    identities?: string[];
    /**
     * Specifies the type of identities that are allowed access from outside the
     * perimeter. If left unspecified, then members of 'identities' field will be
     * allowed access. Possible values: ["ANY_IDENTITY", "ANY_USER_ACCOUNT", "ANY_SERVICE_ACCOUNT"]
     */
    identityType?: string;
    /**
     * Sources that this 'IngressPolicy' authorizes access from.
     */
    sources?: outputs.AccessContextManagerServicePerimeterIngressPolicyIngressFromSource[];
}

export interface AccessContextManagerServicePerimeterIngressPolicyIngressFromSource {
    /**
     * An 'AccessLevel' resource name that allow resources within the
     * 'ServicePerimeters' to be accessed from the internet. 'AccessLevels' listed
     * must be in the same policy as this 'ServicePerimeter'. Referencing a nonexistent
     * 'AccessLevel' will cause an error. If no 'AccessLevel' names are listed,
     * resources within the perimeter can only be accessed via Google Cloud calls
     * with request origins within the perimeter.
     * Example 'accessPolicies/MY_POLICY/accessLevels/MY_LEVEL.'
     * If * is specified, then all IngressSources will be allowed.
     */
    accessLevel?: string;
    /**
     * A Google Cloud resource that is allowed to ingress the perimeter.
     * Requests from these resources will be allowed to access perimeter data.
     * Currently only projects and VPCs are allowed.
     * Project format: 'projects/{projectNumber}'
     * VPC network format:
     * '//compute.googleapis.com/projects/{PROJECT_ID}/global/networks/{NAME}'.
     * The project may be in any Google Cloud organization, not just the
     * organization that the perimeter is defined in. '*' is not allowed, the case
     * of allowing all Google Cloud resources only is not supported.
     */
    resource?: string;
}

export interface AccessContextManagerServicePerimeterIngressPolicyIngressTo {
    /**
     * A list of 'ApiOperations' the sources specified in corresponding 'IngressFrom'
     * are allowed to perform in this 'ServicePerimeter'.
     */
    operations?: outputs.AccessContextManagerServicePerimeterIngressPolicyIngressToOperation[];
    /**
     * A list of resources, currently only projects in the form
     * 'projects/<projectnumber>', protected by this 'ServicePerimeter'
     * that are allowed to be accessed by sources defined in the
     * corresponding 'IngressFrom'. A request matches if it contains
     * a resource in this list. If '*' is specified for resources,
     * then this 'IngressTo' rule will authorize access to all
     * resources inside the perimeter, provided that the request
     * also matches the 'operations' field.
     */
    resources?: string[];
}

export interface AccessContextManagerServicePerimeterIngressPolicyIngressToOperation {
    /**
     * API methods or permissions to allow. Method or permission must belong to
     * the service specified by serviceName field. A single 'MethodSelector' entry
     * with '*' specified for the method field will allow all methods AND
     * permissions for the service specified in 'serviceName'.
     */
    methodSelectors?: outputs.AccessContextManagerServicePerimeterIngressPolicyIngressToOperationMethodSelector[];
    /**
     * The name of the API whose methods or permissions the 'IngressPolicy' or
     * 'EgressPolicy' want to allow. A single 'ApiOperation' with 'serviceName'
     * field set to '*' will allow all methods AND permissions for all services.
     */
    serviceName?: string;
}

export interface AccessContextManagerServicePerimeterIngressPolicyIngressToOperationMethodSelector {
    /**
     * Value for method should be a valid method name for the corresponding
     * serviceName in 'ApiOperation'. If '*' used as value for 'method', then
     * ALL methods and permissions are allowed.
     */
    method?: string;
    /**
     * Value for permission should be a valid Cloud IAM permission for the
     * corresponding 'serviceName' in 'ApiOperation'.
     */
    permission?: string;
}

export interface AccessContextManagerServicePerimeterIngressPolicyTimeouts {
    create?: string;
    delete?: string;
}

export interface AccessContextManagerServicePerimeterResourceTimeouts {
    create?: string;
    delete?: string;
}

export interface AccessContextManagerServicePerimeterSpec {
    /**
     * A list of AccessLevel resource names that allow resources within
     * the ServicePerimeter to be accessed from the internet.
     * AccessLevels listed must be in the same policy as this
     * ServicePerimeter. Referencing a nonexistent AccessLevel is a
     * syntax error. If no AccessLevel names are listed, resources within
     * the perimeter can only be accessed via GCP calls with request
     * origins within the perimeter. For Service Perimeter Bridge, must
     * be empty.
     *
     * Format: accessPolicies/{policy_id}/accessLevels/{access_level_name}
     */
    accessLevels?: string[];
    /**
     * List of EgressPolicies to apply to the perimeter. A perimeter may
     * have multiple EgressPolicies, each of which is evaluated separately.
     * Access is granted if any EgressPolicy grants it. Must be empty for
     * a perimeter bridge.
     */
    egressPolicies?: outputs.AccessContextManagerServicePerimeterSpecEgressPolicy[];
    /**
     * List of 'IngressPolicies' to apply to the perimeter. A perimeter may
     * have multiple 'IngressPolicies', each of which is evaluated
     * separately. Access is granted if any 'Ingress Policy' grants it.
     * Must be empty for a perimeter bridge.
     */
    ingressPolicies?: outputs.AccessContextManagerServicePerimeterSpecIngressPolicy[];
    /**
     * A list of GCP resources that are inside of the service perimeter.
     * Currently only projects are allowed.
     * Format: projects/{project_number}
     */
    resources?: string[];
    /**
     * GCP services that are subject to the Service Perimeter
     * restrictions. Must contain a list of services. For example, if
     * 'storage.googleapis.com' is specified, access to the storage
     * buckets inside the perimeter must meet the perimeter's access
     * restrictions.
     */
    restrictedServices?: string[];
    /**
     * Specifies how APIs are allowed to communicate within the Service
     * Perimeter.
     */
    vpcAccessibleServices?: outputs.AccessContextManagerServicePerimeterSpecVpcAccessibleServices;
}

export interface AccessContextManagerServicePerimeterSpecEgressPolicy {
    /**
     * Defines conditions on the source of a request causing this 'EgressPolicy' to apply.
     */
    egressFrom?: outputs.AccessContextManagerServicePerimeterSpecEgressPolicyEgressFrom;
    /**
     * Defines the conditions on the 'ApiOperation' and destination resources that
     * cause this 'EgressPolicy' to apply.
     */
    egressTo?: outputs.AccessContextManagerServicePerimeterSpecEgressPolicyEgressTo;
}

export interface AccessContextManagerServicePerimeterSpecEgressPolicyEgressFrom {
    /**
     * A list of identities that are allowed access through this 'EgressPolicy'.
     * Should be in the format of email address. The email address should
     * represent individual user or service account only.
     */
    identities?: string[];
    /**
     * Specifies the type of identities that are allowed access to outside the
     * perimeter. If left unspecified, then members of 'identities' field will
     * be allowed access. Possible values: ["IDENTITY_TYPE_UNSPECIFIED", "ANY_IDENTITY", "ANY_USER_ACCOUNT", "ANY_SERVICE_ACCOUNT"]
     */
    identityType?: string;
    /**
     * Whether to enforce traffic restrictions based on 'sources' field. If the 'sources' field is non-empty, then this field must be set to 'SOURCE_RESTRICTION_ENABLED'. Possible values: ["SOURCE_RESTRICTION_UNSPECIFIED", "SOURCE_RESTRICTION_ENABLED", "SOURCE_RESTRICTION_DISABLED"]
     */
    sourceRestriction?: string;
    /**
     * Sources that this EgressPolicy authorizes access from.
     */
    sources?: outputs.AccessContextManagerServicePerimeterSpecEgressPolicyEgressFromSource[];
}

export interface AccessContextManagerServicePerimeterSpecEgressPolicyEgressFromSource {
    /**
     * An AccessLevel resource name that allows resources outside the ServicePerimeter to be accessed from the inside.
     */
    accessLevel?: string;
}

export interface AccessContextManagerServicePerimeterSpecEgressPolicyEgressTo {
    /**
     * A list of external resources that are allowed to be accessed. A request
     * matches if it contains an external resource in this list (Example:
     * s3://bucket/path). Currently '*' is not allowed.
     */
    externalResources?: string[];
    /**
     * A list of 'ApiOperations' that this egress rule applies to. A request matches
     * if it contains an operation/service in this list.
     */
    operations?: outputs.AccessContextManagerServicePerimeterSpecEgressPolicyEgressToOperation[];
    /**
     * A list of resources, currently only projects in the form
     * 'projects/<projectnumber>', that match this to stanza. A request matches
     * if it contains a resource in this list. If * is specified for resources,
     * then this 'EgressTo' rule will authorize access to all resources outside
     * the perimeter.
     */
    resources?: string[];
}

export interface AccessContextManagerServicePerimeterSpecEgressPolicyEgressToOperation {
    /**
     * API methods or permissions to allow. Method or permission must belong
     * to the service specified by 'serviceName' field. A single MethodSelector
     * entry with '*' specified for the 'method' field will allow all methods
     * AND permissions for the service specified in 'serviceName'.
     */
    methodSelectors?: outputs.AccessContextManagerServicePerimeterSpecEgressPolicyEgressToOperationMethodSelector[];
    /**
     * The name of the API whose methods or permissions the 'IngressPolicy' or
     * 'EgressPolicy' want to allow. A single 'ApiOperation' with serviceName
     * field set to '*' will allow all methods AND permissions for all services.
     */
    serviceName?: string;
}

export interface AccessContextManagerServicePerimeterSpecEgressPolicyEgressToOperationMethodSelector {
    /**
     * Value for 'method' should be a valid method name for the corresponding
     * 'serviceName' in 'ApiOperation'. If '*' used as value for method,
     * then ALL methods and permissions are allowed.
     */
    method?: string;
    /**
     * Value for permission should be a valid Cloud IAM permission for the
     * corresponding 'serviceName' in 'ApiOperation'.
     */
    permission?: string;
}

export interface AccessContextManagerServicePerimeterSpecIngressPolicy {
    /**
     * Defines the conditions on the source of a request causing this 'IngressPolicy'
     * to apply.
     */
    ingressFrom?: outputs.AccessContextManagerServicePerimeterSpecIngressPolicyIngressFrom;
    /**
     * Defines the conditions on the 'ApiOperation' and request destination that cause
     * this 'IngressPolicy' to apply.
     */
    ingressTo?: outputs.AccessContextManagerServicePerimeterSpecIngressPolicyIngressTo;
}

export interface AccessContextManagerServicePerimeterSpecIngressPolicyIngressFrom {
    /**
     * A list of identities that are allowed access through this ingress policy.
     * Should be in the format of email address. The email address should represent
     * individual user or service account only.
     */
    identities?: string[];
    /**
     * Specifies the type of identities that are allowed access from outside the
     * perimeter. If left unspecified, then members of 'identities' field will be
     * allowed access. Possible values: ["IDENTITY_TYPE_UNSPECIFIED", "ANY_IDENTITY", "ANY_USER_ACCOUNT", "ANY_SERVICE_ACCOUNT"]
     */
    identityType?: string;
    /**
     * Sources that this 'IngressPolicy' authorizes access from.
     */
    sources?: outputs.AccessContextManagerServicePerimeterSpecIngressPolicyIngressFromSource[];
}

export interface AccessContextManagerServicePerimeterSpecIngressPolicyIngressFromSource {
    /**
     * An 'AccessLevel' resource name that allow resources within the
     * 'ServicePerimeters' to be accessed from the internet. 'AccessLevels' listed
     * must be in the same policy as this 'ServicePerimeter'. Referencing a nonexistent
     * 'AccessLevel' will cause an error. If no 'AccessLevel' names are listed,
     * resources within the perimeter can only be accessed via Google Cloud calls
     * with request origins within the perimeter.
     * Example 'accessPolicies/MY_POLICY/accessLevels/MY_LEVEL.'
     * If * is specified, then all IngressSources will be allowed.
     */
    accessLevel?: string;
    /**
     * A Google Cloud resource that is allowed to ingress the perimeter.
     * Requests from these resources will be allowed to access perimeter data.
     * Currently only projects are allowed. Format 'projects/{project_number}'
     * The project may be in any Google Cloud organization, not just the
     * organization that the perimeter is defined in. '*' is not allowed, the case
     * of allowing all Google Cloud resources only is not supported.
     */
    resource?: string;
}

export interface AccessContextManagerServicePerimeterSpecIngressPolicyIngressTo {
    /**
     * A list of 'ApiOperations' the sources specified in corresponding 'IngressFrom'
     * are allowed to perform in this 'ServicePerimeter'.
     */
    operations?: outputs.AccessContextManagerServicePerimeterSpecIngressPolicyIngressToOperation[];
    /**
     * A list of resources, currently only projects in the form
     * 'projects/<projectnumber>', protected by this 'ServicePerimeter'
     * that are allowed to be accessed by sources defined in the
     * corresponding 'IngressFrom'. A request matches if it contains
     * a resource in this list. If '*' is specified for resources,
     * then this 'IngressTo' rule will authorize access to all
     * resources inside the perimeter, provided that the request
     * also matches the 'operations' field.
     */
    resources?: string[];
}

export interface AccessContextManagerServicePerimeterSpecIngressPolicyIngressToOperation {
    /**
     * API methods or permissions to allow. Method or permission must belong to
     * the service specified by serviceName field. A single 'MethodSelector' entry
     * with '*' specified for the method field will allow all methods AND
     * permissions for the service specified in 'serviceName'.
     */
    methodSelectors?: outputs.AccessContextManagerServicePerimeterSpecIngressPolicyIngressToOperationMethodSelector[];
    /**
     * The name of the API whose methods or permissions the 'IngressPolicy' or
     * 'EgressPolicy' want to allow. A single 'ApiOperation' with 'serviceName'
     * field set to '*' will allow all methods AND permissions for all services.
     */
    serviceName?: string;
}

export interface AccessContextManagerServicePerimeterSpecIngressPolicyIngressToOperationMethodSelector {
    /**
     * Value for method should be a valid method name for the corresponding
     * serviceName in 'ApiOperation'. If '*' used as value for 'method', then
     * ALL methods and permissions are allowed.
     */
    method?: string;
    /**
     * Value for permission should be a valid Cloud IAM permission for the
     * corresponding 'serviceName' in 'ApiOperation'.
     */
    permission?: string;
}

export interface AccessContextManagerServicePerimeterSpecVpcAccessibleServices {
    /**
     * The list of APIs usable within the Service Perimeter.
     * Must be empty unless 'enableRestriction' is True.
     */
    allowedServices?: string[];
    /**
     * Whether to restrict API calls within the Service Perimeter to the
     * list of APIs specified in 'allowedServices'.
     */
    enableRestriction?: boolean;
}

export interface AccessContextManagerServicePerimeterStatus {
    /**
     * A list of AccessLevel resource names that allow resources within
     * the ServicePerimeter to be accessed from the internet.
     * AccessLevels listed must be in the same policy as this
     * ServicePerimeter. Referencing a nonexistent AccessLevel is a
     * syntax error. If no AccessLevel names are listed, resources within
     * the perimeter can only be accessed via GCP calls with request
     * origins within the perimeter. For Service Perimeter Bridge, must
     * be empty.
     *
     * Format: accessPolicies/{policy_id}/accessLevels/{access_level_name}
     */
    accessLevels?: string[];
    /**
     * List of EgressPolicies to apply to the perimeter. A perimeter may
     * have multiple EgressPolicies, each of which is evaluated separately.
     * Access is granted if any EgressPolicy grants it. Must be empty for
     * a perimeter bridge.
     */
    egressPolicies?: outputs.AccessContextManagerServicePerimeterStatusEgressPolicy[];
    /**
     * List of 'IngressPolicies' to apply to the perimeter. A perimeter may
     * have multiple 'IngressPolicies', each of which is evaluated
     * separately. Access is granted if any 'Ingress Policy' grants it.
     * Must be empty for a perimeter bridge.
     */
    ingressPolicies?: outputs.AccessContextManagerServicePerimeterStatusIngressPolicy[];
    /**
     * A list of GCP resources that are inside of the service perimeter.
     * Currently only projects are allowed.
     * Format: projects/{project_number}
     */
    resources?: string[];
    /**
     * GCP services that are subject to the Service Perimeter
     * restrictions. Must contain a list of services. For example, if
     * 'storage.googleapis.com' is specified, access to the storage
     * buckets inside the perimeter must meet the perimeter's access
     * restrictions.
     */
    restrictedServices?: string[];
    /**
     * Specifies how APIs are allowed to communicate within the Service
     * Perimeter.
     */
    vpcAccessibleServices?: outputs.AccessContextManagerServicePerimeterStatusVpcAccessibleServices;
}

export interface AccessContextManagerServicePerimeterStatusEgressPolicy {
    /**
     * Defines conditions on the source of a request causing this 'EgressPolicy' to apply.
     */
    egressFrom?: outputs.AccessContextManagerServicePerimeterStatusEgressPolicyEgressFrom;
    /**
     * Defines the conditions on the 'ApiOperation' and destination resources that
     * cause this 'EgressPolicy' to apply.
     */
    egressTo?: outputs.AccessContextManagerServicePerimeterStatusEgressPolicyEgressTo;
}

export interface AccessContextManagerServicePerimeterStatusEgressPolicyEgressFrom {
    /**
     * A list of identities that are allowed access through this 'EgressPolicy'.
     * Should be in the format of email address. The email address should
     * represent individual user or service account only.
     */
    identities?: string[];
    /**
     * Specifies the type of identities that are allowed access to outside the
     * perimeter. If left unspecified, then members of 'identities' field will
     * be allowed access. Possible values: ["IDENTITY_TYPE_UNSPECIFIED", "ANY_IDENTITY", "ANY_USER_ACCOUNT", "ANY_SERVICE_ACCOUNT"]
     */
    identityType?: string;
    /**
     * Whether to enforce traffic restrictions based on 'sources' field. If the 'sources' field is non-empty, then this field must be set to 'SOURCE_RESTRICTION_ENABLED'. Possible values: ["SOURCE_RESTRICTION_UNSPECIFIED", "SOURCE_RESTRICTION_ENABLED", "SOURCE_RESTRICTION_DISABLED"]
     */
    sourceRestriction?: string;
    /**
     * Sources that this EgressPolicy authorizes access from.
     */
    sources?: outputs.AccessContextManagerServicePerimeterStatusEgressPolicyEgressFromSource[];
}

export interface AccessContextManagerServicePerimeterStatusEgressPolicyEgressFromSource {
    /**
     * An AccessLevel resource name that allows resources outside the ServicePerimeter to be accessed from the inside.
     */
    accessLevel?: string;
}

export interface AccessContextManagerServicePerimeterStatusEgressPolicyEgressTo {
    /**
     * A list of external resources that are allowed to be accessed. A request
     * matches if it contains an external resource in this list (Example:
     * s3://bucket/path). Currently '*' is not allowed.
     */
    externalResources?: string[];
    /**
     * A list of 'ApiOperations' that this egress rule applies to. A request matches
     * if it contains an operation/service in this list.
     */
    operations?: outputs.AccessContextManagerServicePerimeterStatusEgressPolicyEgressToOperation[];
    /**
     * A list of resources, currently only projects in the form
     * 'projects/<projectnumber>', that match this to stanza. A request matches
     * if it contains a resource in this list. If * is specified for resources,
     * then this 'EgressTo' rule will authorize access to all resources outside
     * the perimeter.
     */
    resources?: string[];
}

export interface AccessContextManagerServicePerimeterStatusEgressPolicyEgressToOperation {
    /**
     * API methods or permissions to allow. Method or permission must belong
     * to the service specified by 'serviceName' field. A single MethodSelector
     * entry with '*' specified for the 'method' field will allow all methods
     * AND permissions for the service specified in 'serviceName'.
     */
    methodSelectors?: outputs.AccessContextManagerServicePerimeterStatusEgressPolicyEgressToOperationMethodSelector[];
    /**
     * The name of the API whose methods or permissions the 'IngressPolicy' or
     * 'EgressPolicy' want to allow. A single 'ApiOperation' with serviceName
     * field set to '*' will allow all methods AND permissions for all services.
     */
    serviceName?: string;
}

export interface AccessContextManagerServicePerimeterStatusEgressPolicyEgressToOperationMethodSelector {
    /**
     * Value for 'method' should be a valid method name for the corresponding
     * 'serviceName' in 'ApiOperation'. If '*' used as value for method,
     * then ALL methods and permissions are allowed.
     */
    method?: string;
    /**
     * Value for permission should be a valid Cloud IAM permission for the
     * corresponding 'serviceName' in 'ApiOperation'.
     */
    permission?: string;
}

export interface AccessContextManagerServicePerimeterStatusIngressPolicy {
    /**
     * Defines the conditions on the source of a request causing this 'IngressPolicy'
     * to apply.
     */
    ingressFrom?: outputs.AccessContextManagerServicePerimeterStatusIngressPolicyIngressFrom;
    /**
     * Defines the conditions on the 'ApiOperation' and request destination that cause
     * this 'IngressPolicy' to apply.
     */
    ingressTo?: outputs.AccessContextManagerServicePerimeterStatusIngressPolicyIngressTo;
}

export interface AccessContextManagerServicePerimeterStatusIngressPolicyIngressFrom {
    /**
     * A list of identities that are allowed access through this ingress policy.
     * Should be in the format of email address. The email address should represent
     * individual user or service account only.
     */
    identities?: string[];
    /**
     * Specifies the type of identities that are allowed access from outside the
     * perimeter. If left unspecified, then members of 'identities' field will be
     * allowed access. Possible values: ["IDENTITY_TYPE_UNSPECIFIED", "ANY_IDENTITY", "ANY_USER_ACCOUNT", "ANY_SERVICE_ACCOUNT"]
     */
    identityType?: string;
    /**
     * Sources that this 'IngressPolicy' authorizes access from.
     */
    sources?: outputs.AccessContextManagerServicePerimeterStatusIngressPolicyIngressFromSource[];
}

export interface AccessContextManagerServicePerimeterStatusIngressPolicyIngressFromSource {
    /**
     * An 'AccessLevel' resource name that allow resources within the
     * 'ServicePerimeters' to be accessed from the internet. 'AccessLevels' listed
     * must be in the same policy as this 'ServicePerimeter'. Referencing a nonexistent
     * 'AccessLevel' will cause an error. If no 'AccessLevel' names are listed,
     * resources within the perimeter can only be accessed via Google Cloud calls
     * with request origins within the perimeter.
     * Example 'accessPolicies/MY_POLICY/accessLevels/MY_LEVEL.'
     * If * is specified, then all IngressSources will be allowed.
     */
    accessLevel?: string;
    /**
     * A Google Cloud resource that is allowed to ingress the perimeter.
     * Requests from these resources will be allowed to access perimeter data.
     * Currently only projects and VPCs are allowed.
     * Project format: 'projects/{projectNumber}'
     * VPC network format:
     * '//compute.googleapis.com/projects/{PROJECT_ID}/global/networks/{NAME}'.
     * The project may be in any Google Cloud organization, not just the
     * organization that the perimeter is defined in. '*' is not allowed, the case
     * of allowing all Google Cloud resources only is not supported.
     */
    resource?: string;
}

export interface AccessContextManagerServicePerimeterStatusIngressPolicyIngressTo {
    /**
     * A list of 'ApiOperations' the sources specified in corresponding 'IngressFrom'
     * are allowed to perform in this 'ServicePerimeter'.
     */
    operations?: outputs.AccessContextManagerServicePerimeterStatusIngressPolicyIngressToOperation[];
    /**
     * A list of resources, currently only projects in the form
     * 'projects/<projectnumber>', protected by this 'ServicePerimeter'
     * that are allowed to be accessed by sources defined in the
     * corresponding 'IngressFrom'. A request matches if it contains
     * a resource in this list. If '*' is specified for resources,
     * then this 'IngressTo' rule will authorize access to all
     * resources inside the perimeter, provided that the request
     * also matches the 'operations' field.
     */
    resources?: string[];
}

export interface AccessContextManagerServicePerimeterStatusIngressPolicyIngressToOperation {
    /**
     * API methods or permissions to allow. Method or permission must belong to
     * the service specified by serviceName field. A single 'MethodSelector' entry
     * with '*' specified for the method field will allow all methods AND
     * permissions for the service specified in 'serviceName'.
     */
    methodSelectors?: outputs.AccessContextManagerServicePerimeterStatusIngressPolicyIngressToOperationMethodSelector[];
    /**
     * The name of the API whose methods or permissions the 'IngressPolicy' or
     * 'EgressPolicy' want to allow. A single 'ApiOperation' with 'serviceName'
     * field set to '*' will allow all methods AND permissions for all services.
     */
    serviceName?: string;
}

export interface AccessContextManagerServicePerimeterStatusIngressPolicyIngressToOperationMethodSelector {
    /**
     * Value for method should be a valid method name for the corresponding
     * serviceName in 'ApiOperation'. If '*' used as value for 'method', then
     * ALL methods and permissions are allowed.
     */
    method?: string;
    /**
     * Value for permission should be a valid Cloud IAM permission for the
     * corresponding 'serviceName' in 'ApiOperation'.
     */
    permission?: string;
}

export interface AccessContextManagerServicePerimeterStatusVpcAccessibleServices {
    /**
     * The list of APIs usable within the Service Perimeter.
     * Must be empty unless 'enableRestriction' is True.
     */
    allowedServices?: string[];
    /**
     * Whether to restrict API calls within the Service Perimeter to the
     * list of APIs specified in 'allowedServices'.
     */
    enableRestriction?: boolean;
}

export interface AccessContextManagerServicePerimeterTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface AccessContextManagerServicePerimetersServicePerimeter {
    /**
     * Time the AccessPolicy was created in UTC.
     */
    createTime: string;
    /**
     * Description of the ServicePerimeter and its use. Does not affect
     * behavior.
     */
    description?: string;
    /**
     * Resource name for the ServicePerimeter. The short_name component must
     * begin with a letter and only include alphanumeric and '_'.
     * Format: accessPolicies/{policy_id}/servicePerimeters/{short_name}
     */
    name: string;
    /**
     * Specifies the type of the Perimeter. There are two types: regular and
     * bridge. Regular Service Perimeter contains resources, access levels,
     * and restricted services. Every resource can be in at most
     * ONE regular Service Perimeter.
     *
     * In addition to being in a regular service perimeter, a resource can also
     * be in zero or more perimeter bridges. A perimeter bridge only contains
     * resources. Cross project operations are permitted if all effected
     * resources share some perimeter (whether bridge or regular). Perimeter
     * Bridge does not contain access levels or services: those are governed
     * entirely by the regular perimeter that resource is in.
     *
     * Perimeter Bridges are typically useful when building more complex
     * topologies with many independent perimeters that need to share some data
     * with a common perimeter, but should not be able to share data among
     * themselves. Default value: "PERIMETER_TYPE_REGULAR" Possible values: ["PERIMETER_TYPE_REGULAR", "PERIMETER_TYPE_BRIDGE"]
     */
    perimeterType?: string;
    /**
     * Proposed (or dry run) ServicePerimeter configuration.
     * This configuration allows to specify and test ServicePerimeter configuration
     * without enforcing actual access restrictions. Only allowed to be set when
     * the 'useExplicitDryRunSpec' flag is set.
     */
    spec?: outputs.AccessContextManagerServicePerimetersServicePerimeterSpec;
    /**
     * ServicePerimeter configuration. Specifies sets of resources,
     * restricted services and access levels that determine
     * perimeter content and boundaries.
     */
    status?: outputs.AccessContextManagerServicePerimetersServicePerimeterStatus;
    /**
     * Human readable title. Must be unique within the Policy.
     */
    title: string;
    /**
     * Time the AccessPolicy was updated in UTC.
     */
    updateTime: string;
    /**
     * Use explicit dry run spec flag. Ordinarily, a dry-run spec implicitly exists
     * for all Service Perimeters, and that spec is identical to the status for those
     * Service Perimeters. When this flag is set, it inhibits the generation of the
     * implicit spec, thereby allowing the user to explicitly provide a
     * configuration ("spec") to use in a dry-run version of the Service Perimeter.
     * This allows the user to test changes to the enforced config ("status") without
     * actually enforcing them. This testing is done through analyzing the differences
     * between currently enforced and suggested restrictions. useExplicitDryRunSpec must
     * bet set to True if any of the fields in the spec are set to non-default values.
     */
    useExplicitDryRunSpec?: boolean;
}

export interface AccessContextManagerServicePerimetersServicePerimeterSpec {
    /**
     * A list of AccessLevel resource names that allow resources within
     * the ServicePerimeter to be accessed from the internet.
     * AccessLevels listed must be in the same policy as this
     * ServicePerimeter. Referencing a nonexistent AccessLevel is a
     * syntax error. If no AccessLevel names are listed, resources within
     * the perimeter can only be accessed via GCP calls with request
     * origins within the perimeter. For Service Perimeter Bridge, must
     * be empty.
     *
     * Format: accessPolicies/{policy_id}/accessLevels/{access_level_name}
     */
    accessLevels?: string[];
    /**
     * List of EgressPolicies to apply to the perimeter. A perimeter may
     * have multiple EgressPolicies, each of which is evaluated separately.
     * Access is granted if any EgressPolicy grants it. Must be empty for
     * a perimeter bridge.
     */
    egressPolicies?: outputs.AccessContextManagerServicePerimetersServicePerimeterSpecEgressPolicy[];
    /**
     * List of 'IngressPolicies' to apply to the perimeter. A perimeter may
     * have multiple 'IngressPolicies', each of which is evaluated
     * separately. Access is granted if any 'Ingress Policy' grants it.
     * Must be empty for a perimeter bridge.
     */
    ingressPolicies?: outputs.AccessContextManagerServicePerimetersServicePerimeterSpecIngressPolicy[];
    /**
     * A list of GCP resources that are inside of the service perimeter.
     * Currently only projects are allowed.
     * Format: projects/{project_number}
     */
    resources?: string[];
    /**
     * GCP services that are subject to the Service Perimeter
     * restrictions. Must contain a list of services. For example, if
     * 'storage.googleapis.com' is specified, access to the storage
     * buckets inside the perimeter must meet the perimeter's access
     * restrictions.
     */
    restrictedServices?: string[];
    /**
     * Specifies how APIs are allowed to communicate within the Service
     * Perimeter.
     */
    vpcAccessibleServices?: outputs.AccessContextManagerServicePerimetersServicePerimeterSpecVpcAccessibleServices;
}

export interface AccessContextManagerServicePerimetersServicePerimeterSpecEgressPolicy {
    /**
     * Defines conditions on the source of a request causing this 'EgressPolicy' to apply.
     */
    egressFrom?: outputs.AccessContextManagerServicePerimetersServicePerimeterSpecEgressPolicyEgressFrom;
    /**
     * Defines the conditions on the 'ApiOperation' and destination resources that
     * cause this 'EgressPolicy' to apply.
     */
    egressTo?: outputs.AccessContextManagerServicePerimetersServicePerimeterSpecEgressPolicyEgressTo;
}

export interface AccessContextManagerServicePerimetersServicePerimeterSpecEgressPolicyEgressFrom {
    /**
     * A list of identities that are allowed access through this 'EgressPolicy'.
     * Should be in the format of email address. The email address should
     * represent individual user or service account only.
     */
    identities?: string[];
    /**
     * Specifies the type of identities that are allowed access to outside the
     * perimeter. If left unspecified, then members of 'identities' field will
     * be allowed access. Possible values: ["IDENTITY_TYPE_UNSPECIFIED", "ANY_IDENTITY", "ANY_USER_ACCOUNT", "ANY_SERVICE_ACCOUNT"]
     */
    identityType?: string;
    /**
     * Whether to enforce traffic restrictions based on 'sources' field. If the 'sources' field is non-empty, then this field must be set to 'SOURCE_RESTRICTION_ENABLED'. Possible values: ["SOURCE_RESTRICTION_UNSPECIFIED", "SOURCE_RESTRICTION_ENABLED", "SOURCE_RESTRICTION_DISABLED"]
     */
    sourceRestriction?: string;
    /**
     * Sources that this EgressPolicy authorizes access from.
     */
    sources?: outputs.AccessContextManagerServicePerimetersServicePerimeterSpecEgressPolicyEgressFromSource[];
}

export interface AccessContextManagerServicePerimetersServicePerimeterSpecEgressPolicyEgressFromSource {
    /**
     * An AccessLevel resource name that allows resources outside the ServicePerimeter to be accessed from the inside.
     */
    accessLevel?: string;
}

export interface AccessContextManagerServicePerimetersServicePerimeterSpecEgressPolicyEgressTo {
    /**
     * A list of external resources that are allowed to be accessed. A request
     * matches if it contains an external resource in this list (Example:
     * s3://bucket/path). Currently '*' is not allowed.
     */
    externalResources?: string[];
    /**
     * A list of 'ApiOperations' that this egress rule applies to. A request matches
     * if it contains an operation/service in this list.
     */
    operations?: outputs.AccessContextManagerServicePerimetersServicePerimeterSpecEgressPolicyEgressToOperation[];
    /**
     * A list of resources, currently only projects in the form
     * 'projects/<projectnumber>', that match this to stanza. A request matches
     * if it contains a resource in this list. If * is specified for resources,
     * then this 'EgressTo' rule will authorize access to all resources outside
     * the perimeter.
     */
    resources?: string[];
}

export interface AccessContextManagerServicePerimetersServicePerimeterSpecEgressPolicyEgressToOperation {
    /**
     * API methods or permissions to allow. Method or permission must belong
     * to the service specified by 'serviceName' field. A single MethodSelector
     * entry with '*' specified for the 'method' field will allow all methods
     * AND permissions for the service specified in 'serviceName'.
     */
    methodSelectors?: outputs.AccessContextManagerServicePerimetersServicePerimeterSpecEgressPolicyEgressToOperationMethodSelector[];
    /**
     * The name of the API whose methods or permissions the 'IngressPolicy' or
     * 'EgressPolicy' want to allow. A single 'ApiOperation' with serviceName
     * field set to '*' will allow all methods AND permissions for all services.
     */
    serviceName?: string;
}

export interface AccessContextManagerServicePerimetersServicePerimeterSpecEgressPolicyEgressToOperationMethodSelector {
    /**
     * Value for 'method' should be a valid method name for the corresponding
     * 'serviceName' in 'ApiOperation'. If '*' used as value for method,
     * then ALL methods and permissions are allowed.
     */
    method?: string;
    /**
     * Value for permission should be a valid Cloud IAM permission for the
     * corresponding 'serviceName' in 'ApiOperation'.
     */
    permission?: string;
}

export interface AccessContextManagerServicePerimetersServicePerimeterSpecIngressPolicy {
    /**
     * Defines the conditions on the source of a request causing this 'IngressPolicy'
     * to apply.
     */
    ingressFrom?: outputs.AccessContextManagerServicePerimetersServicePerimeterSpecIngressPolicyIngressFrom;
    /**
     * Defines the conditions on the 'ApiOperation' and request destination that cause
     * this 'IngressPolicy' to apply.
     */
    ingressTo?: outputs.AccessContextManagerServicePerimetersServicePerimeterSpecIngressPolicyIngressTo;
}

export interface AccessContextManagerServicePerimetersServicePerimeterSpecIngressPolicyIngressFrom {
    /**
     * A list of identities that are allowed access through this ingress policy.
     * Should be in the format of email address. The email address should represent
     * individual user or service account only.
     */
    identities?: string[];
    /**
     * Specifies the type of identities that are allowed access from outside the
     * perimeter. If left unspecified, then members of 'identities' field will be
     * allowed access. Possible values: ["IDENTITY_TYPE_UNSPECIFIED", "ANY_IDENTITY", "ANY_USER_ACCOUNT", "ANY_SERVICE_ACCOUNT"]
     */
    identityType?: string;
    /**
     * Sources that this 'IngressPolicy' authorizes access from.
     */
    sources?: outputs.AccessContextManagerServicePerimetersServicePerimeterSpecIngressPolicyIngressFromSource[];
}

export interface AccessContextManagerServicePerimetersServicePerimeterSpecIngressPolicyIngressFromSource {
    /**
     * An 'AccessLevel' resource name that allow resources within the
     * 'ServicePerimeters' to be accessed from the internet. 'AccessLevels' listed
     * must be in the same policy as this 'ServicePerimeter'. Referencing a nonexistent
     * 'AccessLevel' will cause an error. If no 'AccessLevel' names are listed,
     * resources within the perimeter can only be accessed via Google Cloud calls
     * with request origins within the perimeter.
     * Example 'accessPolicies/MY_POLICY/accessLevels/MY_LEVEL.'
     * If * is specified, then all IngressSources will be allowed.
     */
    accessLevel?: string;
    /**
     * A Google Cloud resource that is allowed to ingress the perimeter.
     * Requests from these resources will be allowed to access perimeter data.
     * Currently only projects are allowed. Format 'projects/{project_number}'
     * The project may be in any Google Cloud organization, not just the
     * organization that the perimeter is defined in. '*' is not allowed, the case
     * of allowing all Google Cloud resources only is not supported.
     */
    resource?: string;
}

export interface AccessContextManagerServicePerimetersServicePerimeterSpecIngressPolicyIngressTo {
    /**
     * A list of 'ApiOperations' the sources specified in corresponding 'IngressFrom'
     * are allowed to perform in this 'ServicePerimeter'.
     */
    operations?: outputs.AccessContextManagerServicePerimetersServicePerimeterSpecIngressPolicyIngressToOperation[];
    /**
     * A list of resources, currently only projects in the form
     * 'projects/<projectnumber>', protected by this 'ServicePerimeter'
     * that are allowed to be accessed by sources defined in the
     * corresponding 'IngressFrom'. A request matches if it contains
     * a resource in this list. If '*' is specified for resources,
     * then this 'IngressTo' rule will authorize access to all
     * resources inside the perimeter, provided that the request
     * also matches the 'operations' field.
     */
    resources?: string[];
}

export interface AccessContextManagerServicePerimetersServicePerimeterSpecIngressPolicyIngressToOperation {
    /**
     * API methods or permissions to allow. Method or permission must belong to
     * the service specified by serviceName field. A single 'MethodSelector' entry
     * with '*' specified for the method field will allow all methods AND
     * permissions for the service specified in 'serviceName'.
     */
    methodSelectors?: outputs.AccessContextManagerServicePerimetersServicePerimeterSpecIngressPolicyIngressToOperationMethodSelector[];
    /**
     * The name of the API whose methods or permissions the 'IngressPolicy' or
     * 'EgressPolicy' want to allow. A single 'ApiOperation' with 'serviceName'
     * field set to '*' will allow all methods AND permissions for all services.
     */
    serviceName?: string;
}

export interface AccessContextManagerServicePerimetersServicePerimeterSpecIngressPolicyIngressToOperationMethodSelector {
    /**
     * Value for method should be a valid method name for the corresponding
     * serviceName in 'ApiOperation'. If '*' used as value for 'method', then
     * ALL methods and permissions are allowed.
     */
    method?: string;
    /**
     * Value for permission should be a valid Cloud IAM permission for the
     * corresponding 'serviceName' in 'ApiOperation'.
     */
    permission?: string;
}

export interface AccessContextManagerServicePerimetersServicePerimeterSpecVpcAccessibleServices {
    /**
     * The list of APIs usable within the Service Perimeter.
     * Must be empty unless 'enableRestriction' is True.
     */
    allowedServices?: string[];
    /**
     * Whether to restrict API calls within the Service Perimeter to the
     * list of APIs specified in 'allowedServices'.
     */
    enableRestriction?: boolean;
}

export interface AccessContextManagerServicePerimetersServicePerimeterStatus {
    /**
     * A list of AccessLevel resource names that allow resources within
     * the ServicePerimeter to be accessed from the internet.
     * AccessLevels listed must be in the same policy as this
     * ServicePerimeter. Referencing a nonexistent AccessLevel is a
     * syntax error. If no AccessLevel names are listed, resources within
     * the perimeter can only be accessed via GCP calls with request
     * origins within the perimeter. For Service Perimeter Bridge, must
     * be empty.
     *
     * Format: accessPolicies/{policy_id}/accessLevels/{access_level_name}
     */
    accessLevels?: string[];
    /**
     * List of EgressPolicies to apply to the perimeter. A perimeter may
     * have multiple EgressPolicies, each of which is evaluated separately.
     * Access is granted if any EgressPolicy grants it. Must be empty for
     * a perimeter bridge.
     */
    egressPolicies?: outputs.AccessContextManagerServicePerimetersServicePerimeterStatusEgressPolicy[];
    /**
     * List of 'IngressPolicies' to apply to the perimeter. A perimeter may
     * have multiple 'IngressPolicies', each of which is evaluated
     * separately. Access is granted if any 'Ingress Policy' grants it.
     * Must be empty for a perimeter bridge.
     */
    ingressPolicies?: outputs.AccessContextManagerServicePerimetersServicePerimeterStatusIngressPolicy[];
    /**
     * A list of GCP resources that are inside of the service perimeter.
     * Currently only projects are allowed.
     * Format: projects/{project_number}
     */
    resources?: string[];
    /**
     * GCP services that are subject to the Service Perimeter
     * restrictions. Must contain a list of services. For example, if
     * 'storage.googleapis.com' is specified, access to the storage
     * buckets inside the perimeter must meet the perimeter's access
     * restrictions.
     */
    restrictedServices?: string[];
    /**
     * Specifies how APIs are allowed to communicate within the Service
     * Perimeter.
     */
    vpcAccessibleServices?: outputs.AccessContextManagerServicePerimetersServicePerimeterStatusVpcAccessibleServices;
}

export interface AccessContextManagerServicePerimetersServicePerimeterStatusEgressPolicy {
    /**
     * Defines conditions on the source of a request causing this 'EgressPolicy' to apply.
     */
    egressFrom?: outputs.AccessContextManagerServicePerimetersServicePerimeterStatusEgressPolicyEgressFrom;
    /**
     * Defines the conditions on the 'ApiOperation' and destination resources that
     * cause this 'EgressPolicy' to apply.
     */
    egressTo?: outputs.AccessContextManagerServicePerimetersServicePerimeterStatusEgressPolicyEgressTo;
}

export interface AccessContextManagerServicePerimetersServicePerimeterStatusEgressPolicyEgressFrom {
    /**
     * A list of identities that are allowed access through this 'EgressPolicy'.
     * Should be in the format of email address. The email address should
     * represent individual user or service account only.
     */
    identities?: string[];
    /**
     * Specifies the type of identities that are allowed access to outside the
     * perimeter. If left unspecified, then members of 'identities' field will
     * be allowed access. Possible values: ["IDENTITY_TYPE_UNSPECIFIED", "ANY_IDENTITY", "ANY_USER_ACCOUNT", "ANY_SERVICE_ACCOUNT"]
     */
    identityType?: string;
    /**
     * Whether to enforce traffic restrictions based on 'sources' field. If the 'sources' field is non-empty, then this field must be set to 'SOURCE_RESTRICTION_ENABLED'. Possible values: ["SOURCE_RESTRICTION_UNSPECIFIED", "SOURCE_RESTRICTION_ENABLED", "SOURCE_RESTRICTION_DISABLED"]
     */
    sourceRestriction?: string;
    /**
     * Sources that this EgressPolicy authorizes access from.
     */
    sources?: outputs.AccessContextManagerServicePerimetersServicePerimeterStatusEgressPolicyEgressFromSource[];
}

export interface AccessContextManagerServicePerimetersServicePerimeterStatusEgressPolicyEgressFromSource {
    /**
     * An AccessLevel resource name that allows resources outside the ServicePerimeter to be accessed from the inside.
     */
    accessLevel?: string;
}

export interface AccessContextManagerServicePerimetersServicePerimeterStatusEgressPolicyEgressTo {
    /**
     * A list of external resources that are allowed to be accessed. A request
     * matches if it contains an external resource in this list (Example:
     * s3://bucket/path). Currently '*' is not allowed.
     */
    externalResources?: string[];
    /**
     * A list of 'ApiOperations' that this egress rule applies to. A request matches
     * if it contains an operation/service in this list.
     */
    operations?: outputs.AccessContextManagerServicePerimetersServicePerimeterStatusEgressPolicyEgressToOperation[];
    /**
     * A list of resources, currently only projects in the form
     * 'projects/<projectnumber>', that match this to stanza. A request matches
     * if it contains a resource in this list. If * is specified for resources,
     * then this 'EgressTo' rule will authorize access to all resources outside
     * the perimeter.
     */
    resources?: string[];
}

export interface AccessContextManagerServicePerimetersServicePerimeterStatusEgressPolicyEgressToOperation {
    /**
     * API methods or permissions to allow. Method or permission must belong
     * to the service specified by 'serviceName' field. A single MethodSelector
     * entry with '*' specified for the 'method' field will allow all methods
     * AND permissions for the service specified in 'serviceName'.
     */
    methodSelectors?: outputs.AccessContextManagerServicePerimetersServicePerimeterStatusEgressPolicyEgressToOperationMethodSelector[];
    /**
     * The name of the API whose methods or permissions the 'IngressPolicy' or
     * 'EgressPolicy' want to allow. A single 'ApiOperation' with serviceName
     * field set to '*' will allow all methods AND permissions for all services.
     */
    serviceName?: string;
}

export interface AccessContextManagerServicePerimetersServicePerimeterStatusEgressPolicyEgressToOperationMethodSelector {
    /**
     * Value for 'method' should be a valid method name for the corresponding
     * 'serviceName' in 'ApiOperation'. If '*' used as value for method,
     * then ALL methods and permissions are allowed.
     */
    method?: string;
    /**
     * Value for permission should be a valid Cloud IAM permission for the
     * corresponding 'serviceName' in 'ApiOperation'.
     */
    permission?: string;
}

export interface AccessContextManagerServicePerimetersServicePerimeterStatusIngressPolicy {
    /**
     * Defines the conditions on the source of a request causing this 'IngressPolicy'
     * to apply.
     */
    ingressFrom?: outputs.AccessContextManagerServicePerimetersServicePerimeterStatusIngressPolicyIngressFrom;
    /**
     * Defines the conditions on the 'ApiOperation' and request destination that cause
     * this 'IngressPolicy' to apply.
     */
    ingressTo?: outputs.AccessContextManagerServicePerimetersServicePerimeterStatusIngressPolicyIngressTo;
}

export interface AccessContextManagerServicePerimetersServicePerimeterStatusIngressPolicyIngressFrom {
    /**
     * A list of identities that are allowed access through this ingress policy.
     * Should be in the format of email address. The email address should represent
     * individual user or service account only.
     */
    identities?: string[];
    /**
     * Specifies the type of identities that are allowed access from outside the
     * perimeter. If left unspecified, then members of 'identities' field will be
     * allowed access. Possible values: ["IDENTITY_TYPE_UNSPECIFIED", "ANY_IDENTITY", "ANY_USER_ACCOUNT", "ANY_SERVICE_ACCOUNT"]
     */
    identityType?: string;
    /**
     * Sources that this 'IngressPolicy' authorizes access from.
     */
    sources?: outputs.AccessContextManagerServicePerimetersServicePerimeterStatusIngressPolicyIngressFromSource[];
}

export interface AccessContextManagerServicePerimetersServicePerimeterStatusIngressPolicyIngressFromSource {
    /**
     * An 'AccessLevel' resource name that allow resources within the
     * 'ServicePerimeters' to be accessed from the internet. 'AccessLevels' listed
     * must be in the same policy as this 'ServicePerimeter'. Referencing a nonexistent
     * 'AccessLevel' will cause an error. If no 'AccessLevel' names are listed,
     * resources within the perimeter can only be accessed via Google Cloud calls
     * with request origins within the perimeter.
     * Example 'accessPolicies/MY_POLICY/accessLevels/MY_LEVEL.'
     * If * is specified, then all IngressSources will be allowed.
     */
    accessLevel?: string;
    /**
     * A Google Cloud resource that is allowed to ingress the perimeter.
     * Requests from these resources will be allowed to access perimeter data.
     * Currently only projects are allowed. Format 'projects/{project_number}'
     * The project may be in any Google Cloud organization, not just the
     * organization that the perimeter is defined in. '*' is not allowed, the case
     * of allowing all Google Cloud resources only is not supported.
     */
    resource?: string;
}

export interface AccessContextManagerServicePerimetersServicePerimeterStatusIngressPolicyIngressTo {
    /**
     * A list of 'ApiOperations' the sources specified in corresponding 'IngressFrom'
     * are allowed to perform in this 'ServicePerimeter'.
     */
    operations?: outputs.AccessContextManagerServicePerimetersServicePerimeterStatusIngressPolicyIngressToOperation[];
    /**
     * A list of resources, currently only projects in the form
     * 'projects/<projectnumber>', protected by this 'ServicePerimeter'
     * that are allowed to be accessed by sources defined in the
     * corresponding 'IngressFrom'. A request matches if it contains
     * a resource in this list. If '*' is specified for resources,
     * then this 'IngressTo' rule will authorize access to all
     * resources inside the perimeter, provided that the request
     * also matches the 'operations' field.
     */
    resources?: string[];
}

export interface AccessContextManagerServicePerimetersServicePerimeterStatusIngressPolicyIngressToOperation {
    /**
     * API methods or permissions to allow. Method or permission must belong to
     * the service specified by serviceName field. A single 'MethodSelector' entry
     * with '*' specified for the method field will allow all methods AND
     * permissions for the service specified in 'serviceName'.
     */
    methodSelectors?: outputs.AccessContextManagerServicePerimetersServicePerimeterStatusIngressPolicyIngressToOperationMethodSelector[];
    /**
     * The name of the API whose methods or permissions the 'IngressPolicy' or
     * 'EgressPolicy' want to allow. A single 'ApiOperation' with 'serviceName'
     * field set to '*' will allow all methods AND permissions for all services.
     */
    serviceName?: string;
}

export interface AccessContextManagerServicePerimetersServicePerimeterStatusIngressPolicyIngressToOperationMethodSelector {
    /**
     * Value for method should be a valid method name for the corresponding
     * serviceName in 'ApiOperation'. If '*' used as value for 'method', then
     * ALL methods and permissions are allowed.
     */
    method?: string;
    /**
     * Value for permission should be a valid Cloud IAM permission for the
     * corresponding 'serviceName' in 'ApiOperation'.
     */
    permission?: string;
}

export interface AccessContextManagerServicePerimetersServicePerimeterStatusVpcAccessibleServices {
    /**
     * The list of APIs usable within the Service Perimeter.
     * Must be empty unless 'enableRestriction' is True.
     */
    allowedServices?: string[];
    /**
     * Whether to restrict API calls within the Service Perimeter to the
     * list of APIs specified in 'allowedServices'.
     */
    enableRestriction?: boolean;
}

export interface AccessContextManagerServicePerimetersTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ActiveDirectoryDomainTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ActiveDirectoryDomainTrustTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface AlloydbBackupEncryptionConfig {
    /**
     * The fully-qualified resource name of the KMS key. Each Cloud KMS key is regionalized and has the following format: projects/[PROJECT]/locations/[REGION]/keyRings/[RING]/cryptoKeys/[KEY_NAME].
     */
    kmsKeyName?: string;
}

export interface AlloydbBackupEncryptionInfo {
    encryptionType: string;
    kmsKeyVersions: string[];
}

export interface AlloydbBackupExpiryQuantity {
    retentionCount: number;
    totalRetentionCount: number;
}

export interface AlloydbBackupTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface AlloydbClusterAutomatedBackupPolicy {
    /**
     * The length of the time window during which a backup can be taken. If a backup does not succeed within this time window, it will be canceled and considered failed.
     *
     * The backup window must be at least 5 minutes long. There is no upper bound on the window. If not set, it will default to 1 hour.
     *
     * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
     */
    backupWindow: string;
    /**
     * Whether automated backups are enabled.
     */
    enabled: boolean;
    /**
     * EncryptionConfig describes the encryption config of a cluster or a backup that is encrypted with a CMEK (customer-managed encryption key).
     */
    encryptionConfig?: outputs.AlloydbClusterAutomatedBackupPolicyEncryptionConfig;
    /**
     * Labels to apply to backups created using this configuration.
     */
    labels?: {[key: string]: string};
    /**
     * The location where the backup will be stored. Currently, the only supported option is to store the backup in the same region as the cluster.
     */
    location: string;
    /**
     * Quantity-based Backup retention policy to retain recent backups. Conflicts with 'time_based_retention', both can't be set together.
     */
    quantityBasedRetention?: outputs.AlloydbClusterAutomatedBackupPolicyQuantityBasedRetention;
    /**
     * Time-based Backup retention policy. Conflicts with 'quantity_based_retention', both can't be set together.
     */
    timeBasedRetention?: outputs.AlloydbClusterAutomatedBackupPolicyTimeBasedRetention;
    /**
     * Weekly schedule for the Backup.
     */
    weeklySchedule?: outputs.AlloydbClusterAutomatedBackupPolicyWeeklySchedule;
}

export interface AlloydbClusterAutomatedBackupPolicyEncryptionConfig {
    /**
     * The fully-qualified resource name of the KMS key. Each Cloud KMS key is regionalized and has the following format: projects/[PROJECT]/locations/[REGION]/keyRings/[RING]/cryptoKeys/[KEY_NAME].
     */
    kmsKeyName?: string;
}

export interface AlloydbClusterAutomatedBackupPolicyQuantityBasedRetention {
    /**
     * The number of backups to retain.
     */
    count?: number;
}

export interface AlloydbClusterAutomatedBackupPolicyTimeBasedRetention {
    /**
     * The retention period.
     * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
     */
    retentionPeriod?: string;
}

export interface AlloydbClusterAutomatedBackupPolicyWeeklySchedule {
    /**
     * The days of the week to perform a backup. At least one day of the week must be provided. Possible values: ["MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    daysOfWeeks?: string[];
    /**
     * The times during the day to start a backup. At least one start time must be provided. The start times are assumed to be in UTC and to be an exact hour (e.g., 04:00:00).
     */
    startTimes: outputs.AlloydbClusterAutomatedBackupPolicyWeeklyScheduleStartTime[];
}

export interface AlloydbClusterAutomatedBackupPolicyWeeklyScheduleStartTime {
    /**
     * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
     */
    hours?: number;
    /**
     * Minutes of hour of day. Currently, only the value 0 is supported.
     */
    minutes?: number;
    /**
     * Fractions of seconds in nanoseconds. Currently, only the value 0 is supported.
     */
    nanos?: number;
    /**
     * Seconds of minutes of the time. Currently, only the value 0 is supported.
     */
    seconds?: number;
}

export interface AlloydbClusterBackupSource {
    backupName: string;
}

export interface AlloydbClusterContinuousBackupConfig {
    /**
     * Whether continuous backup recovery is enabled. If not set, defaults to true.
     */
    enabled?: boolean;
    /**
     * EncryptionConfig describes the encryption config of a cluster or a backup that is encrypted with a CMEK (customer-managed encryption key).
     */
    encryptionConfig?: outputs.AlloydbClusterContinuousBackupConfigEncryptionConfig;
    /**
     * The numbers of days that are eligible to restore from using PITR. To support the entire recovery window, backups and logs are retained for one day more than the recovery window.
     *
     * If not set, defaults to 14 days.
     */
    recoveryWindowDays: number;
}

export interface AlloydbClusterContinuousBackupConfigEncryptionConfig {
    /**
     * The fully-qualified resource name of the KMS key. Each Cloud KMS key is regionalized and has the following format: projects/[PROJECT]/locations/[REGION]/keyRings/[RING]/cryptoKeys/[KEY_NAME].
     */
    kmsKeyName?: string;
}

export interface AlloydbClusterContinuousBackupInfo {
    earliestRestorableTime: string;
    enabledTime: string;
    encryptionInfos: outputs.AlloydbClusterContinuousBackupInfoEncryptionInfo[];
    schedules: string[];
}

export interface AlloydbClusterContinuousBackupInfoEncryptionInfo {
    encryptionType: string;
    kmsKeyVersions: string[];
}

export interface AlloydbClusterEncryptionConfig {
    /**
     * The fully-qualified resource name of the KMS key. Each Cloud KMS key is regionalized and has the following format: projects/[PROJECT]/locations/[REGION]/keyRings/[RING]/cryptoKeys/[KEY_NAME].
     */
    kmsKeyName?: string;
}

export interface AlloydbClusterEncryptionInfo {
    encryptionType: string;
    kmsKeyVersions: string[];
}

export interface AlloydbClusterInitialUser {
    /**
     * The initial password for the user.
     */
    password: string;
    /**
     * The database username.
     */
    user?: string;
}

export interface AlloydbClusterMaintenanceUpdatePolicy {
    /**
     * Preferred windows to perform maintenance. Currently limited to 1.
     */
    maintenanceWindows?: outputs.AlloydbClusterMaintenanceUpdatePolicyMaintenanceWindow[];
}

export interface AlloydbClusterMaintenanceUpdatePolicyMaintenanceWindow {
    /**
     * Preferred day of the week for maintenance, e.g. MONDAY, TUESDAY, etc. Possible values: ["MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    day: string;
    /**
     * Preferred time to start the maintenance operation on the specified day. Maintenance will start within 1 hour of this time.
     */
    startTime: outputs.AlloydbClusterMaintenanceUpdatePolicyMaintenanceWindowStartTime;
}

export interface AlloydbClusterMaintenanceUpdatePolicyMaintenanceWindowStartTime {
    /**
     * Hours of day in 24 hour format. Should be from 0 to 23.
     */
    hours: number;
    /**
     * Minutes of hour of day. Currently, only the value 0 is supported.
     */
    minutes?: number;
    /**
     * Fractions of seconds in nanoseconds. Currently, only the value 0 is supported.
     */
    nanos?: number;
    /**
     * Seconds of minutes of the time. Currently, only the value 0 is supported.
     */
    seconds?: number;
}

export interface AlloydbClusterMigrationSource {
    hostPort: string;
    referenceId: string;
    sourceType: string;
}

export interface AlloydbClusterNetworkConfig {
    /**
     * The name of the allocated IP range for the private IP AlloyDB cluster. For example: "google-managed-services-default".
     * If set, the instance IPs for this cluster will be created in the allocated range.
     */
    allocatedIpRange?: string;
    /**
     * The resource link for the VPC network in which cluster resources are created and from which they are accessible via Private IP. The network must belong to the same project as the cluster.
     * It is specified in the form: "projects/{projectNumber}/global/networks/{network_id}".
     */
    network?: string;
}

export interface AlloydbClusterPscConfig {
    /**
     * Create an instance that allows connections from Private Service Connect endpoints to the instance.
     */
    pscEnabled?: boolean;
}

export interface AlloydbClusterRestoreBackupSource {
    /**
     * The name of the backup that this cluster is restored from.
     */
    backupName: string;
}

export interface AlloydbClusterRestoreContinuousBackupSource {
    /**
     * The name of the source cluster that this cluster is restored from.
     */
    cluster: string;
    /**
     * The point in time that this cluster is restored to, in RFC 3339 format.
     */
    pointInTime: string;
}

export interface AlloydbClusterSecondaryConfig {
    /**
     * Name of the primary cluster must be in the format
     * 'projects/{project}/locations/{location}/clusters/{cluster_id}'
     */
    primaryClusterName: string;
}

export interface AlloydbClusterTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface AlloydbClusterTrialMetadata {
    endTime: string;
    graceEndTime: string;
    startTime: string;
    upgradeTime: string;
}

export interface AlloydbInstanceClientConnectionConfig {
    /**
     * Configuration to enforce connectors only (ex: AuthProxy) connections to the database.
     */
    requireConnectors?: boolean;
    /**
     * SSL config option for this instance.
     */
    sslConfig?: outputs.AlloydbInstanceClientConnectionConfigSslConfig;
}

export interface AlloydbInstanceClientConnectionConfigSslConfig {
    /**
     * SSL mode. Specifies client-server SSL/TLS connection behavior. Possible values: ["ENCRYPTED_ONLY", "ALLOW_UNENCRYPTED_AND_ENCRYPTED"]
     */
    sslMode: string;
}

export interface AlloydbInstanceMachineConfig {
    /**
     * The number of CPU's in the VM instance.
     */
    cpuCount: number;
}

export interface AlloydbInstanceNetworkConfig {
    /**
     * A list of external networks authorized to access this instance. This
     * field is only allowed to be set when 'enable_public_ip' is set to
     * true.
     */
    authorizedExternalNetworks?: outputs.AlloydbInstanceNetworkConfigAuthorizedExternalNetwork[];
    /**
     * Enabling public ip for the instance. If a user wishes to disable this,
     * please also clear the list of the authorized external networks set on
     * the same instance.
     */
    enablePublicIp?: boolean;
}

export interface AlloydbInstanceNetworkConfigAuthorizedExternalNetwork {
    /**
     * CIDR range for one authorized network of the instance.
     */
    cidrRange?: string;
}

export interface AlloydbInstancePscInstanceConfig {
    /**
     * List of consumer projects that are allowed to create PSC endpoints to service-attachments to this instance.
     * These should be specified as project numbers only.
     */
    allowedConsumerProjects?: string[];
    /**
     * The DNS name of the instance for PSC connectivity.
     * Name convention: <uid>.<uid>.<region>.alloydb-psc.goog
     */
    pscDnsName: string;
    /**
     * The service attachment created when Private Service Connect (PSC) is enabled for the instance.
     * The name of the resource will be in the format of
     * 'projects/<alloydb-tenant-project-number>/regions/<region-name>/serviceAttachments/<service-attachment-name>'
     */
    serviceAttachmentLink: string;
}

export interface AlloydbInstanceQueryInsightsConfig {
    /**
     * Number of query execution plans captured by Insights per minute for all queries combined. The default value is 5. Any integer between 0 and 20 is considered valid.
     */
    queryPlansPerMinute?: number;
    /**
     * Query string length. The default value is 1024. Any integer between 256 and 4500 is considered valid.
     */
    queryStringLength?: number;
    /**
     * Record application tags for an instance. This flag is turned "on" by default.
     */
    recordApplicationTags?: boolean;
    /**
     * Record client address for an instance. Client address is PII information. This flag is turned "on" by default.
     */
    recordClientAddress?: boolean;
}

export interface AlloydbInstanceReadPoolConfig {
    /**
     * Read capacity, i.e. number of nodes in a read pool instance.
     */
    nodeCount?: number;
}

export interface AlloydbInstanceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface AlloydbUserTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ApigeeAddonsConfigAddonsConfig {
    /**
     * Configuration for the Monetization add-on.
     */
    advancedApiOpsConfig?: outputs.ApigeeAddonsConfigAddonsConfigAdvancedApiOpsConfig;
    /**
     * Configuration for the Monetization add-on.
     */
    apiSecurityConfig?: outputs.ApigeeAddonsConfigAddonsConfigApiSecurityConfig;
    /**
     * Configuration for the Monetization add-on.
     */
    connectorsPlatformConfig?: outputs.ApigeeAddonsConfigAddonsConfigConnectorsPlatformConfig;
    /**
     * Configuration for the Monetization add-on.
     */
    integrationConfig?: outputs.ApigeeAddonsConfigAddonsConfigIntegrationConfig;
    /**
     * Configuration for the Monetization add-on.
     */
    monetizationConfig?: outputs.ApigeeAddonsConfigAddonsConfigMonetizationConfig;
}

export interface ApigeeAddonsConfigAddonsConfigAdvancedApiOpsConfig {
    /**
     * Flag that specifies whether the Advanced API Ops add-on is enabled.
     */
    enabled?: boolean;
}

export interface ApigeeAddonsConfigAddonsConfigApiSecurityConfig {
    /**
     * Flag that specifies whether the Advanced API Ops add-on is enabled.
     */
    enabled?: boolean;
    /**
     * Flag that specifies whether the Advanced API Ops add-on is enabled.
     */
    expiresAt: string;
}

export interface ApigeeAddonsConfigAddonsConfigConnectorsPlatformConfig {
    /**
     * Flag that specifies whether the Advanced API Ops add-on is enabled.
     */
    enabled?: boolean;
    /**
     * Flag that specifies whether the Advanced API Ops add-on is enabled.
     */
    expiresAt: string;
}

export interface ApigeeAddonsConfigAddonsConfigIntegrationConfig {
    /**
     * Flag that specifies whether the Advanced API Ops add-on is enabled.
     */
    enabled?: boolean;
}

export interface ApigeeAddonsConfigAddonsConfigMonetizationConfig {
    /**
     * Flag that specifies whether the Advanced API Ops add-on is enabled.
     */
    enabled?: boolean;
}

export interface ApigeeAddonsConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ApigeeEndpointAttachmentTimeouts {
    create?: string;
    delete?: string;
}

export interface ApigeeEnvKeystoreTimeouts {
    create?: string;
    delete?: string;
}

export interface ApigeeEnvReferencesTimeouts {
    create?: string;
    delete?: string;
}

export interface ApigeeEnvgroupAttachmentTimeouts {
    create?: string;
    delete?: string;
}

export interface ApigeeEnvgroupTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ApigeeEnvironmentIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ApigeeEnvironmentIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ApigeeEnvironmentKeyvaluemapsEntriesTimeouts {
    create?: string;
    delete?: string;
}

export interface ApigeeEnvironmentKeyvaluemapsTimeouts {
    create?: string;
    delete?: string;
}

export interface ApigeeEnvironmentNodeConfig {
    /**
     * The current total number of gateway nodes that each environment currently has across
     * all instances.
     */
    currentAggregateNodeCount: string;
    /**
     * The maximum total number of gateway nodes that the is reserved for all instances that
     * has the specified environment. If not specified, the default is determined by the
     * recommended maximum number of nodes for that gateway.
     */
    maxNodeCount?: string;
    /**
     * The minimum total number of gateway nodes that the is reserved for all instances that
     * has the specified environment. If not specified, the default is determined by the
     * recommended minimum number of nodes for that gateway.
     */
    minNodeCount?: string;
}

export interface ApigeeEnvironmentTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ApigeeFlowhookTimeouts {
    create?: string;
    delete?: string;
}

export interface ApigeeInstanceAttachmentTimeouts {
    create?: string;
    delete?: string;
}

export interface ApigeeInstanceTimeouts {
    create?: string;
    delete?: string;
}

export interface ApigeeKeystoresAliasesKeyCertFileCertsInfo {
    /**
     * List of all properties in the object.
     */
    certInfos?: outputs.ApigeeKeystoresAliasesKeyCertFileCertsInfoCertInfo[];
}

export interface ApigeeKeystoresAliasesKeyCertFileCertsInfoCertInfo {
    /**
     * X.509 basic constraints extension.
     */
    basicConstraints: string;
    /**
     * X.509 notAfter validity period in milliseconds since epoch.
     */
    expiryDate: string;
    /**
     * Flag that specifies whether the certificate is valid. 
     * Flag is set to Yes if the certificate is valid, No if expired, or Not yet if not yet valid.
     */
    isValid: string;
    /**
     * X.509 issuer.
     */
    issuer: string;
    /**
     * Public key component of the X.509 subject public key info.
     */
    publicKey: string;
    /**
     * X.509 serial number.
     */
    serialNumber: string;
    /**
     * X.509 signatureAlgorithm.
     */
    sigAlgName: string;
    /**
     * X.509 subject.
     */
    subject: string;
    /**
     * X.509 subject alternative names (SANs) extension.
     */
    subjectAlternativeNames: string[];
    /**
     * X.509 notBefore validity period in milliseconds since epoch.
     */
    validFrom: string;
    /**
     * X.509 version.
     */
    version: number;
}

export interface ApigeeKeystoresAliasesKeyCertFileTimeouts {
    create?: string;
    delete?: string;
    read?: string;
    update?: string;
}

export interface ApigeeKeystoresAliasesPkcs12CertsInfo {
    certInfos: outputs.ApigeeKeystoresAliasesPkcs12CertsInfoCertInfo[];
}

export interface ApigeeKeystoresAliasesPkcs12CertsInfoCertInfo {
    basicConstraints: string;
    expiryDate: string;
    isValid: string;
    issuer: string;
    publicKey: string;
    serialNumber: string;
    sigAlgName: string;
    subject: string;
    subjectAlternativeNames: string[];
    validFrom: string;
    version: number;
}

export interface ApigeeKeystoresAliasesPkcs12Timeouts {
    create?: string;
    delete?: string;
}

export interface ApigeeKeystoresAliasesSelfSignedCertCertsInfo {
    certInfos: outputs.ApigeeKeystoresAliasesSelfSignedCertCertsInfoCertInfo[];
}

export interface ApigeeKeystoresAliasesSelfSignedCertCertsInfoCertInfo {
    basicConstraints: string;
    expiryDate: string;
    isValid: string;
    issuer: string;
    publicKey: string;
    serialNumber: string;
    sigAlgName: string;
    subject: string;
    subjectAlternativeNames: string[];
    validFrom: string;
    version: number;
}

export interface ApigeeKeystoresAliasesSelfSignedCertSubject {
    /**
     * Common name of the organization. Maximum length is 64 characters.
     */
    commonName?: string;
    /**
     * Two-letter country code. Example, IN for India, US for United States of America.
     */
    countryCode?: string;
    /**
     * Email address. Max 255 characters.
     */
    email?: string;
    /**
     * City or town name. Maximum length is 128 characters.
     */
    locality?: string;
    /**
     * Organization name. Maximum length is 64 characters.
     */
    org?: string;
    /**
     * Organization team name. Maximum length is 64 characters.
     */
    orgUnit?: string;
    /**
     * State or district name. Maximum length is 128 characters.
     */
    state?: string;
}

export interface ApigeeKeystoresAliasesSelfSignedCertSubjectAlternativeDnsNames {
    /**
     * Subject Alternative Name
     */
    subjectAlternativeName?: string;
}

export interface ApigeeKeystoresAliasesSelfSignedCertTimeouts {
    create?: string;
    delete?: string;
}

export interface ApigeeNatAddressTimeouts {
    create?: string;
    delete?: string;
}

export interface ApigeeOrganizationProperties {
    /**
     * List of all properties in the object.
     */
    properties?: outputs.ApigeeOrganizationPropertiesProperty[];
}

export interface ApigeeOrganizationPropertiesProperty {
    /**
     * Name of the property.
     */
    name?: string;
    /**
     * Value of the property.
     */
    value?: string;
}

export interface ApigeeOrganizationTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ApigeeSharedflowDeploymentTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ApigeeSharedflowMetaData {
    createdAt: string;
    lastModifiedAt: string;
    subType: string;
}

export interface ApigeeSharedflowTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ApigeeSyncAuthorizationTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ApigeeTargetServerSSlInfo {
    /**
     * The SSL/TLS cipher suites to be used. For programmable proxies, it must be one of the cipher suite names listed in: http://docs.oracle.com/javase/8/docs/technotes/guides/security/StandardNames.html#ciphersuites. For configurable proxies, it must follow the configuration specified in: https://commondatastorage.googleapis.com/chromium-boringssl-docs/ssl.h.html#Cipher-suite-configuration. This setting has no effect for configurable proxies when negotiating TLS 1.3.
     */
    ciphers?: string[];
    /**
     * Enables two-way TLS.
     */
    clientAuthEnabled?: boolean;
    /**
     * The TLS Common Name of the certificate.
     */
    commonName?: outputs.ApigeeTargetServerSSlInfoCommonName;
    /**
     * Enables TLS. If false, neither one-way nor two-way TLS will be enabled.
     */
    enabled: boolean;
    /**
     * If true, Edge ignores TLS certificate errors. Valid when configuring TLS for target servers and target endpoints, and when configuring virtual hosts that use 2-way TLS. When used with a target endpoint/target server, if the backend system uses SNI and returns a cert with a subject Distinguished Name (DN) that does not match the hostname, there is no way to ignore the error and the connection fails.
     */
    ignoreValidationErrors?: boolean;
    /**
     * Required if clientAuthEnabled is true. The resource ID for the alias containing the private key and cert.
     */
    keyAlias?: string;
    /**
     * Required if clientAuthEnabled is true. The resource ID of the keystore.
     */
    keyStore?: string;
    /**
     * The TLS versioins to be used.
     */
    protocols?: string[];
    /**
     * The resource ID of the truststore.
     */
    trustStore?: string;
}

export interface ApigeeTargetServerSSlInfoCommonName {
    /**
     * The TLS Common Name string of the certificate.
     */
    value?: string;
    /**
     * Indicates whether the cert should be matched against as a wildcard cert.
     */
    wildcardMatch?: boolean;
}

export interface ApigeeTargetServerTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ApikeysKeyRestrictions {
    /**
     * The Android apps that are allowed to use the key.
     */
    androidKeyRestrictions?: outputs.ApikeysKeyRestrictionsAndroidKeyRestrictions;
    /**
     * A restriction for a specific service and optionally one or more specific methods. Requests are allowed if they match any of these restrictions. If no restrictions are specified, all targets are allowed.
     */
    apiTargets?: outputs.ApikeysKeyRestrictionsApiTarget[];
    /**
     * The HTTP referrers (websites) that are allowed to use the key.
     */
    browserKeyRestrictions?: outputs.ApikeysKeyRestrictionsBrowserKeyRestrictions;
    /**
     * The iOS apps that are allowed to use the key.
     */
    iosKeyRestrictions?: outputs.ApikeysKeyRestrictionsIosKeyRestrictions;
    /**
     * The IP addresses of callers that are allowed to use the key.
     */
    serverKeyRestrictions?: outputs.ApikeysKeyRestrictionsServerKeyRestrictions;
}

export interface ApikeysKeyRestrictionsAndroidKeyRestrictions {
    /**
     * A list of Android applications that are allowed to make API calls with this key.
     */
    allowedApplications: outputs.ApikeysKeyRestrictionsAndroidKeyRestrictionsAllowedApplication[];
}

export interface ApikeysKeyRestrictionsAndroidKeyRestrictionsAllowedApplication {
    /**
     * The package name of the application.
     */
    packageName: string;
    /**
     * The SHA1 fingerprint of the application. For example, both sha1 formats are acceptable : DA:39:A3:EE:5E:6B:4B:0D:32:55:BF:EF:95:60:18:90:AF:D8:07:09 or DA39A3EE5E6B4B0D3255BFEF95601890AFD80709. Output format is the latter.
     */
    sha1Fingerprint: string;
}

export interface ApikeysKeyRestrictionsApiTarget {
    /**
     * Optional. List of one or more methods that can be called. If empty, all methods for the service are allowed. A wildcard (*) can be used as the last symbol. Valid examples: `google.cloud.translate.v2.TranslateService.GetSupportedLanguage` `TranslateText` `Get*` `translate.googleapis.com.Get*`
     */
    methods?: string[];
    /**
     * The service for this restriction. It should be the canonical service name, for example: `translate.googleapis.com`. You can use `gcloud services list` to get a list of services that are enabled in the project.
     */
    service: string;
}

export interface ApikeysKeyRestrictionsBrowserKeyRestrictions {
    /**
     * A list of regular expressions for the referrer URLs that are allowed to make API calls with this key.
     */
    allowedReferrers: string[];
}

export interface ApikeysKeyRestrictionsIosKeyRestrictions {
    /**
     * A list of bundle IDs that are allowed when making API calls with this key.
     */
    allowedBundleIds: string[];
}

export interface ApikeysKeyRestrictionsServerKeyRestrictions {
    /**
     * A list of the caller IP addresses that are allowed to make API calls with this key.
     */
    allowedIps: string[];
}

export interface ApikeysKeyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface AppEngineApplicationFeatureSettings {
    splitHealthChecks: boolean;
}

export interface AppEngineApplicationIap {
    /**
     * Adapted for use with the app
     */
    enabled?: boolean;
    /**
     * OAuth2 client ID to use for the authentication flow.
     */
    oauth2ClientId: string;
    /**
     * OAuth2 client secret to use for the authentication flow. The SHA-256 hash of the value is returned in the oauth2ClientSecretSha256 field.
     */
    oauth2ClientSecret: string;
    /**
     * Hex-encoded SHA-256 hash of the client secret.
     */
    oauth2ClientSecretSha256: string;
}

export interface AppEngineApplicationTimeouts {
    create?: string;
    update?: string;
}

export interface AppEngineApplicationUrlDispatchRule {
    domain: string;
    path: string;
    service: string;
}

export interface AppEngineApplicationUrlDispatchRulesDispatchRule {
    /**
     * Domain name to match against. The wildcard "*" is supported if specified before a period: "*.".
     * Defaults to matching all domains: "*".
     */
    domain?: string;
    /**
     * Pathname within the host. Must start with a "/". A single "*" can be included at the end of the path.
     * The sum of the lengths of the domain and path may not exceed 100 characters.
     */
    path: string;
    /**
     * Pathname within the host. Must start with a "/". A single "*" can be included at the end of the path.
     * The sum of the lengths of the domain and path may not exceed 100 characters.
     */
    service: string;
}

export interface AppEngineApplicationUrlDispatchRulesTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface AppEngineDomainMappingResourceRecord {
    name: string;
    rrdata: string;
    type: string;
}

export interface AppEngineDomainMappingSslSettings {
    /**
     * ID of the AuthorizedCertificate resource configuring SSL for the application. Clearing this field will
     * remove SSL support.
     * By default, a managed certificate is automatically created for every domain mapping. To omit SSL support
     * or to configure SSL manually, specify 'SslManagementType.MANUAL' on a 'CREATE' or 'UPDATE' request. You must be
     * authorized to administer the 'AuthorizedCertificate' resource to manually map it to a DomainMapping resource.
     * Example: 12345.
     */
    certificateId: string;
    /**
     * ID of the managed 'AuthorizedCertificate' resource currently being provisioned, if applicable. Until the new
     * managed certificate has been successfully provisioned, the previous SSL state will be preserved. Once the
     * provisioning process completes, the 'certificateId' field will reflect the new managed certificate and this
     * field will be left empty. To remove SSL support while there is still a pending managed certificate, clear the
     * 'certificateId' field with an update request.
     */
    pendingManagedCertificateId: string;
    /**
     * SSL management type for this domain. If 'AUTOMATIC', a managed certificate is automatically provisioned.
     * If 'MANUAL', 'certificateId' must be manually specified in order to configure SSL for this domain. Possible values: ["AUTOMATIC", "MANUAL"]
     */
    sslManagementType: string;
}

export interface AppEngineDomainMappingTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface AppEngineFirewallRuleTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface AppEngineFlexibleAppVersionApiConfig {
    /**
     * Action to take when users access resources that require authentication. Default value: "AUTH_FAIL_ACTION_REDIRECT" Possible values: ["AUTH_FAIL_ACTION_REDIRECT", "AUTH_FAIL_ACTION_UNAUTHORIZED"]
     */
    authFailAction?: string;
    /**
     * Level of login required to access this resource. Default value: "LOGIN_OPTIONAL" Possible values: ["LOGIN_OPTIONAL", "LOGIN_ADMIN", "LOGIN_REQUIRED"]
     */
    login?: string;
    /**
     * Path to the script from the application root directory.
     */
    script: string;
    /**
     * Security (HTTPS) enforcement for this URL. Possible values: ["SECURE_DEFAULT", "SECURE_NEVER", "SECURE_OPTIONAL", "SECURE_ALWAYS"]
     */
    securityLevel?: string;
    /**
     * URL to serve the endpoint at.
     */
    url?: string;
}

export interface AppEngineFlexibleAppVersionAutomaticScaling {
    /**
     * The time period that the Autoscaler should wait before it starts collecting information from a new instance.
     * This prevents the autoscaler from collecting information when the instance is initializing,
     * during which the collected usage would not be reliable. Default: 120s
     */
    coolDownPeriod?: string;
    /**
     * Target scaling by CPU usage.
     */
    cpuUtilization: outputs.AppEngineFlexibleAppVersionAutomaticScalingCpuUtilization;
    /**
     * Target scaling by disk usage.
     */
    diskUtilization?: outputs.AppEngineFlexibleAppVersionAutomaticScalingDiskUtilization;
    /**
     * Number of concurrent requests an automatic scaling instance can accept before the scheduler spawns a new instance.
     *
     * Defaults to a runtime-specific value.
     */
    maxConcurrentRequests: number;
    /**
     * Maximum number of idle instances that should be maintained for this version.
     */
    maxIdleInstances?: number;
    /**
     * Maximum amount of time that a request should wait in the pending queue before starting a new instance to handle it.
     */
    maxPendingLatency?: string;
    /**
     * Maximum number of instances that should be started to handle requests for this version. Default: 20
     */
    maxTotalInstances?: number;
    /**
     * Minimum number of idle instances that should be maintained for this version. Only applicable for the default version of a service.
     */
    minIdleInstances?: number;
    /**
     * Minimum amount of time a request should wait in the pending queue before starting a new instance to handle it.
     */
    minPendingLatency?: string;
    /**
     * Minimum number of running instances that should be maintained for this version. Default: 2
     */
    minTotalInstances?: number;
    /**
     * Target scaling by network usage.
     */
    networkUtilization?: outputs.AppEngineFlexibleAppVersionAutomaticScalingNetworkUtilization;
    /**
     * Target scaling by request utilization.
     */
    requestUtilization?: outputs.AppEngineFlexibleAppVersionAutomaticScalingRequestUtilization;
}

export interface AppEngineFlexibleAppVersionAutomaticScalingCpuUtilization {
    /**
     * Period of time over which CPU utilization is calculated.
     */
    aggregationWindowLength?: string;
    /**
     * Target CPU utilization ratio to maintain when scaling. Must be between 0 and 1.
     */
    targetUtilization: number;
}

export interface AppEngineFlexibleAppVersionAutomaticScalingDiskUtilization {
    /**
     * Target bytes read per second.
     */
    targetReadBytesPerSecond?: number;
    /**
     * Target ops read per seconds.
     */
    targetReadOpsPerSecond?: number;
    /**
     * Target bytes written per second.
     */
    targetWriteBytesPerSecond?: number;
    /**
     * Target ops written per second.
     */
    targetWriteOpsPerSecond?: number;
}

export interface AppEngineFlexibleAppVersionAutomaticScalingNetworkUtilization {
    /**
     * Target bytes received per second.
     */
    targetReceivedBytesPerSecond?: number;
    /**
     * Target packets received per second.
     */
    targetReceivedPacketsPerSecond?: number;
    /**
     * Target bytes sent per second.
     */
    targetSentBytesPerSecond?: number;
    /**
     * Target packets sent per second.
     */
    targetSentPacketsPerSecond?: number;
}

export interface AppEngineFlexibleAppVersionAutomaticScalingRequestUtilization {
    /**
     * Target number of concurrent requests.
     */
    targetConcurrentRequests?: number;
    /**
     * Target requests per second.
     */
    targetRequestCountPerSecond?: string;
}

export interface AppEngineFlexibleAppVersionDeployment {
    /**
     * Options for the build operations performed as a part of the version deployment. Only applicable when creating a version using source code directly.
     */
    cloudBuildOptions?: outputs.AppEngineFlexibleAppVersionDeploymentCloudBuildOptions;
    /**
     * The Docker image for the container that runs the version.
     */
    container?: outputs.AppEngineFlexibleAppVersionDeploymentContainer;
    /**
     * Manifest of the files stored in Google Cloud Storage that are included as part of this version.
     * All files must be readable using the credentials supplied with this call.
     */
    files?: outputs.AppEngineFlexibleAppVersionDeploymentFile[];
    /**
     * Zip File
     */
    zip?: outputs.AppEngineFlexibleAppVersionDeploymentZip;
}

export interface AppEngineFlexibleAppVersionDeploymentCloudBuildOptions {
    /**
     * Path to the yaml file used in deployment, used to determine runtime configuration details.
     */
    appYamlPath: string;
    /**
     * The Cloud Build timeout used as part of any dependent builds performed by version creation. Defaults to 10 minutes.
     *
     * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
     */
    cloudBuildTimeout?: string;
}

export interface AppEngineFlexibleAppVersionDeploymentContainer {
    /**
     * URI to the hosted container image in Google Container Registry. The URI must be fully qualified and include a tag or digest.
     * Examples: "gcr.io/my-project/image:tag" or "gcr.io/my-project/image@digest"
     */
    image: string;
}

export interface AppEngineFlexibleAppVersionDeploymentFile {
    name: string;
    /**
     * SHA1 checksum of the file
     */
    sha1Sum?: string;
    /**
     * Source URL
     */
    sourceUrl: string;
}

export interface AppEngineFlexibleAppVersionDeploymentZip {
    /**
     * files count
     */
    filesCount?: number;
    /**
     * Source URL
     */
    sourceUrl: string;
}

export interface AppEngineFlexibleAppVersionEndpointsApiService {
    /**
     * Endpoints service configuration ID as specified by the Service Management API. For example "2016-09-19r1".
     *
     * By default, the rollout strategy for Endpoints is "FIXED". This means that Endpoints starts up with a particular configuration ID.
     * When a new configuration is rolled out, Endpoints must be given the new configuration ID. The configId field is used to give the configuration ID
     * and is required in this case.
     *
     * Endpoints also has a rollout strategy called "MANAGED". When using this, Endpoints fetches the latest configuration and does not need
     * the configuration ID. In this case, configId must be omitted.
     */
    configId?: string;
    /**
     * Enable or disable trace sampling. By default, this is set to false for enabled.
     */
    disableTraceSampling?: boolean;
    /**
     * Endpoints service name which is the name of the "service" resource in the Service Management API.
     * For example "myapi.endpoints.myproject.cloud.goog"
     */
    name: string;
    /**
     * Endpoints rollout strategy. If FIXED, configId must be specified. If MANAGED, configId must be omitted. Default value: "FIXED" Possible values: ["FIXED", "MANAGED"]
     */
    rolloutStrategy?: string;
}

export interface AppEngineFlexibleAppVersionEntrypoint {
    /**
     * The format should be a shell command that can be fed to bash -c.
     */
    shell: string;
}

export interface AppEngineFlexibleAppVersionFlexibleRuntimeSettings {
    /**
     * Operating System of the application runtime.
     */
    operatingSystem?: string;
    /**
     * The runtime version of an App Engine flexible application.
     */
    runtimeVersion?: string;
}

export interface AppEngineFlexibleAppVersionHandler {
    /**
     * Actions to take when the user is not logged in. Possible values: ["AUTH_FAIL_ACTION_REDIRECT", "AUTH_FAIL_ACTION_UNAUTHORIZED"]
     */
    authFailAction?: string;
    /**
     * Methods to restrict access to a URL based on login status. Possible values: ["LOGIN_OPTIONAL", "LOGIN_ADMIN", "LOGIN_REQUIRED"]
     */
    login?: string;
    /**
     * 30x code to use when performing redirects for the secure field. Possible values: ["REDIRECT_HTTP_RESPONSE_CODE_301", "REDIRECT_HTTP_RESPONSE_CODE_302", "REDIRECT_HTTP_RESPONSE_CODE_303", "REDIRECT_HTTP_RESPONSE_CODE_307"]
     */
    redirectHttpResponseCode?: string;
    /**
     * Executes a script to handle the requests that match this URL pattern.
     * Only the auto value is supported for Node.js in the App Engine standard environment, for example "script:" "auto".
     */
    script?: outputs.AppEngineFlexibleAppVersionHandlerScript;
    /**
     * Security (HTTPS) enforcement for this URL. Possible values: ["SECURE_DEFAULT", "SECURE_NEVER", "SECURE_OPTIONAL", "SECURE_ALWAYS"]
     */
    securityLevel?: string;
    /**
     * Files served directly to the user for a given URL, such as images, CSS stylesheets, or JavaScript source files.
     * Static file handlers describe which files in the application directory are static files, and which URLs serve them.
     */
    staticFiles?: outputs.AppEngineFlexibleAppVersionHandlerStaticFiles;
    /**
     * URL prefix. Uses regular expression syntax, which means regexp special characters must be escaped, but should not contain groupings.
     * All URLs that begin with this prefix are handled by this handler, using the portion of the URL after the prefix as part of the file path.
     */
    urlRegex?: string;
}

export interface AppEngineFlexibleAppVersionHandlerScript {
    /**
     * Path to the script from the application root directory.
     */
    scriptPath: string;
}

export interface AppEngineFlexibleAppVersionHandlerStaticFiles {
    /**
     * Whether files should also be uploaded as code data. By default, files declared in static file handlers are
     * uploaded as static data and are only served to end users; they cannot be read by the application. If enabled,
     * uploads are charged against both your code and static data storage resource quotas.
     */
    applicationReadable?: boolean;
    /**
     * Time a static file served by this handler should be cached by web proxies and browsers.
     * A duration in seconds with up to nine fractional digits, terminated by 's'. Example "3.5s".
     * Default is '0s'
     */
    expiration?: string;
    /**
     * HTTP headers to use for all responses from these URLs.
     * An object containing a list of "key:value" value pairs.".
     */
    httpHeaders?: {[key: string]: string};
    /**
     * MIME type used to serve all files served by this handler.
     * Defaults to file-specific MIME types, which are derived from each file's filename extension.
     */
    mimeType?: string;
    /**
     * Path to the static files matched by the URL pattern, from the application root directory.
     * The path can refer to text matched in groupings in the URL pattern.
     */
    path?: string;
    /**
     * Whether this handler should match the request if the file referenced by the handler does not exist.
     */
    requireMatchingFile?: boolean;
    /**
     * Regular expression that matches the file paths for all files that should be referenced by this handler.
     */
    uploadPathRegex?: string;
}

export interface AppEngineFlexibleAppVersionLivenessCheck {
    /**
     * Interval between health checks.
     */
    checkInterval?: string;
    /**
     * Number of consecutive failed checks required before considering the VM unhealthy. Default: 4.
     */
    failureThreshold?: number;
    /**
     * Host header to send when performing a HTTP Readiness check. Example: "myapp.appspot.com"
     */
    host?: string;
    /**
     * The initial delay before starting to execute the checks. Default: "300s"
     */
    initialDelay?: string;
    /**
     * The request path.
     */
    path: string;
    /**
     * Number of consecutive successful checks required before considering the VM healthy. Default: 2.
     */
    successThreshold?: number;
    /**
     * Time before the check is considered failed. Default: "4s"
     */
    timeout?: string;
}

export interface AppEngineFlexibleAppVersionManualScaling {
    /**
     * Number of instances to assign to the service at the start.
     *
     * **Note:** When managing the number of instances at runtime through the App Engine Admin API or the (now deprecated) Python 2
     * Modules API set_num_instances() you must use 'lifecycle.ignore_changes = ["manual_scaling"[0].instances]' to prevent drift detection.
     */
    instances: number;
}

export interface AppEngineFlexibleAppVersionNetwork {
    /**
     * List of ports, or port pairs, to forward from the virtual machine to the application container.
     */
    forwardedPorts?: string[];
    /**
     * Tag to apply to the instance during creation.
     */
    instanceTag?: string;
    /**
     * Google Compute Engine network where the virtual machines are created. Specify the short name, not the resource path.
     */
    name: string;
    /**
     * Enable session affinity.
     */
    sessionAffinity?: boolean;
    /**
     * Google Cloud Platform sub-network where the virtual machines are created. Specify the short name, not the resource path.
     *
     * If the network that the instance is being created in is a Legacy network, then the IP address is allocated from the IPv4Range.
     * If the network that the instance is being created in is an auto Subnet Mode Network, then only network name should be specified (not the subnetworkName) and the IP address is created from the IPCidrRange of the subnetwork that exists in that zone for that network.
     * If the network that the instance is being created in is a custom Subnet Mode Network, then the subnetworkName must be specified and the IP address is created from the IPCidrRange of the subnetwork.
     * If specified, the subnetwork must exist in the same region as the App Engine flexible environment application.
     */
    subnetwork?: string;
}

export interface AppEngineFlexibleAppVersionReadinessCheck {
    /**
     * A maximum time limit on application initialization, measured from moment the application successfully
     * replies to a healthcheck until it is ready to serve traffic. Default: "300s"
     */
    appStartTimeout?: string;
    /**
     * Interval between health checks.  Default: "5s".
     */
    checkInterval?: string;
    /**
     * Number of consecutive failed checks required before removing traffic. Default: 2.
     */
    failureThreshold?: number;
    /**
     * Host header to send when performing a HTTP Readiness check. Example: "myapp.appspot.com"
     */
    host?: string;
    /**
     * The request path.
     */
    path: string;
    /**
     * Number of consecutive successful checks required before receiving traffic. Default: 2.
     */
    successThreshold?: number;
    /**
     * Time before the check is considered failed. Default: "4s"
     */
    timeout?: string;
}

export interface AppEngineFlexibleAppVersionResources {
    /**
     * Number of CPU cores needed.
     */
    cpu?: number;
    /**
     * Disk size (GB) needed.
     */
    diskGb?: number;
    /**
     * Memory (GB) needed.
     */
    memoryGb?: number;
    /**
     * List of ports, or port pairs, to forward from the virtual machine to the application container.
     */
    volumes?: outputs.AppEngineFlexibleAppVersionResourcesVolume[];
}

export interface AppEngineFlexibleAppVersionResourcesVolume {
    /**
     * Unique name for the volume.
     */
    name: string;
    /**
     * Volume size in gigabytes.
     */
    sizeGb: number;
    /**
     * Underlying volume type, e.g. 'tmpfs'.
     */
    volumeType: string;
}

export interface AppEngineFlexibleAppVersionTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface AppEngineFlexibleAppVersionVpcAccessConnector {
    /**
     * Full Serverless VPC Access Connector name e.g. /projects/my-project/locations/us-central1/connectors/c1.
     */
    name: string;
}

export interface AppEngineServiceNetworkSettingsNetworkSettings {
    /**
     * The ingress settings for version or service. Default value: "INGRESS_TRAFFIC_ALLOWED_UNSPECIFIED" Possible values: ["INGRESS_TRAFFIC_ALLOWED_UNSPECIFIED", "INGRESS_TRAFFIC_ALLOWED_ALL", "INGRESS_TRAFFIC_ALLOWED_INTERNAL_ONLY", "INGRESS_TRAFFIC_ALLOWED_INTERNAL_AND_LB"]
     */
    ingressTrafficAllowed?: string;
}

export interface AppEngineServiceNetworkSettingsTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface AppEngineServiceSplitTrafficSplit {
    /**
     * Mapping from version IDs within the service to fractional (0.000, 1] allocations of traffic for that version. Each version can be specified only once, but some versions in the service may not have any traffic allocation. Services that have traffic allocated cannot be deleted until either the service is deleted or their traffic allocation is removed. Allocations must sum to 1. Up to two decimal place precision is supported for IP-based splits and up to three decimal places is supported for cookie-based splits.
     */
    allocations: {[key: string]: string};
    /**
     * Mechanism used to determine which version a request is sent to. The traffic selection algorithm will be stable for either type until allocations are changed. Possible values: ["UNSPECIFIED", "COOKIE", "IP", "RANDOM"]
     */
    shardBy?: string;
}

export interface AppEngineServiceSplitTrafficTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface AppEngineStandardAppVersionAutomaticScaling {
    /**
     * Number of concurrent requests an automatic scaling instance can accept before the scheduler spawns a new instance.
     *
     * Defaults to a runtime-specific value.
     */
    maxConcurrentRequests?: number;
    /**
     * Maximum number of idle instances that should be maintained for this version.
     */
    maxIdleInstances?: number;
    /**
     * Maximum amount of time that a request should wait in the pending queue before starting a new instance to handle it.
     * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
     */
    maxPendingLatency?: string;
    /**
     * Minimum number of idle instances that should be maintained for this version. Only applicable for the default version of a service.
     */
    minIdleInstances?: number;
    /**
     * Minimum amount of time a request should wait in the pending queue before starting a new instance to handle it.
     * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
     */
    minPendingLatency?: string;
    /**
     * Scheduler settings for standard environment.
     */
    standardSchedulerSettings?: outputs.AppEngineStandardAppVersionAutomaticScalingStandardSchedulerSettings;
}

export interface AppEngineStandardAppVersionAutomaticScalingStandardSchedulerSettings {
    /**
     * Maximum number of instances to run for this version. Set to zero to disable maxInstances configuration.
     */
    maxInstances?: number;
    /**
     * Minimum number of instances to run for this version. Set to zero to disable minInstances configuration.
     */
    minInstances?: number;
    /**
     * Target CPU utilization ratio to maintain when scaling. Should be a value in the range [0.50, 0.95], zero, or a negative value.
     */
    targetCpuUtilization?: number;
    /**
     * Target throughput utilization ratio to maintain when scaling. Should be a value in the range [0.50, 0.95], zero, or a negative value.
     */
    targetThroughputUtilization?: number;
}

export interface AppEngineStandardAppVersionBasicScaling {
    /**
     * Duration of time after the last request that an instance must wait before the instance is shut down.
     * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s". Defaults to 900s.
     */
    idleTimeout?: string;
    /**
     * Maximum number of instances to create for this version. Must be in the range [1.0, 200.0].
     */
    maxInstances: number;
}

export interface AppEngineStandardAppVersionDeployment {
    /**
     * Manifest of the files stored in Google Cloud Storage that are included as part of this version.
     * All files must be readable using the credentials supplied with this call.
     */
    files?: outputs.AppEngineStandardAppVersionDeploymentFile[];
    /**
     * Zip File
     */
    zip?: outputs.AppEngineStandardAppVersionDeploymentZip;
}

export interface AppEngineStandardAppVersionDeploymentFile {
    name: string;
    /**
     * SHA1 checksum of the file
     */
    sha1Sum?: string;
    /**
     * Source URL
     */
    sourceUrl: string;
}

export interface AppEngineStandardAppVersionDeploymentZip {
    /**
     * files count
     */
    filesCount?: number;
    /**
     * Source URL
     */
    sourceUrl: string;
}

export interface AppEngineStandardAppVersionEntrypoint {
    /**
     * The format should be a shell command that can be fed to bash -c.
     */
    shell: string;
}

export interface AppEngineStandardAppVersionHandler {
    /**
     * Actions to take when the user is not logged in. Possible values: ["AUTH_FAIL_ACTION_REDIRECT", "AUTH_FAIL_ACTION_UNAUTHORIZED"]
     */
    authFailAction?: string;
    /**
     * Methods to restrict access to a URL based on login status. Possible values: ["LOGIN_OPTIONAL", "LOGIN_ADMIN", "LOGIN_REQUIRED"]
     */
    login?: string;
    /**
     * 30x code to use when performing redirects for the secure field. Possible values: ["REDIRECT_HTTP_RESPONSE_CODE_301", "REDIRECT_HTTP_RESPONSE_CODE_302", "REDIRECT_HTTP_RESPONSE_CODE_303", "REDIRECT_HTTP_RESPONSE_CODE_307"]
     */
    redirectHttpResponseCode?: string;
    /**
     * Executes a script to handle the requests that match this URL pattern.
     * Only the auto value is supported for Node.js in the App Engine standard environment, for example "script:" "auto".
     */
    script?: outputs.AppEngineStandardAppVersionHandlerScript;
    /**
     * Security (HTTPS) enforcement for this URL. Possible values: ["SECURE_DEFAULT", "SECURE_NEVER", "SECURE_OPTIONAL", "SECURE_ALWAYS"]
     */
    securityLevel?: string;
    /**
     * Files served directly to the user for a given URL, such as images, CSS stylesheets, or JavaScript source files. Static file handlers describe which files in the application directory are static files, and which URLs serve them.
     */
    staticFiles?: outputs.AppEngineStandardAppVersionHandlerStaticFiles;
    /**
     * URL prefix. Uses regular expression syntax, which means regexp special characters must be escaped, but should not contain groupings.
     * All URLs that begin with this prefix are handled by this handler, using the portion of the URL after the prefix as part of the file path.
     */
    urlRegex?: string;
}

export interface AppEngineStandardAppVersionHandlerScript {
    /**
     * Path to the script from the application root directory.
     */
    scriptPath: string;
}

export interface AppEngineStandardAppVersionHandlerStaticFiles {
    /**
     * Whether files should also be uploaded as code data. By default, files declared in static file handlers are uploaded as
     * static data and are only served to end users; they cannot be read by the application. If enabled, uploads are charged
     * against both your code and static data storage resource quotas.
     */
    applicationReadable?: boolean;
    /**
     * Time a static file served by this handler should be cached by web proxies and browsers.
     * A duration in seconds with up to nine fractional digits, terminated by 's'. Example "3.5s".
     */
    expiration?: string;
    /**
     * HTTP headers to use for all responses from these URLs.
     * An object containing a list of "key:value" value pairs.".
     */
    httpHeaders?: {[key: string]: string};
    /**
     * MIME type used to serve all files served by this handler.
     * Defaults to file-specific MIME types, which are derived from each file's filename extension.
     */
    mimeType?: string;
    /**
     * Path to the static files matched by the URL pattern, from the application root directory. The path can refer to text matched in groupings in the URL pattern.
     */
    path?: string;
    /**
     * Whether this handler should match the request if the file referenced by the handler does not exist.
     */
    requireMatchingFile?: boolean;
    /**
     * Regular expression that matches the file paths for all files that should be referenced by this handler.
     */
    uploadPathRegex?: string;
}

export interface AppEngineStandardAppVersionLibrary {
    /**
     * Name of the library. Example "django".
     */
    name?: string;
    /**
     * Version of the library to select, or "latest".
     */
    version?: string;
}

export interface AppEngineStandardAppVersionManualScaling {
    /**
     * Number of instances to assign to the service at the start.
     *
     * **Note:** When managing the number of instances at runtime through the App Engine Admin API or the (now deprecated) Python 2
     * Modules API set_num_instances() you must use 'lifecycle.ignore_changes = ["manual_scaling"[0].instances]' to prevent drift detection.
     */
    instances: number;
}

export interface AppEngineStandardAppVersionTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface AppEngineStandardAppVersionVpcAccessConnector {
    /**
     * The egress setting for the connector, controlling what traffic is diverted through it.
     */
    egressSetting?: string;
    /**
     * Full Serverless VPC Access Connector name e.g. /projects/my-project/locations/us-central1/connectors/c1.
     */
    name: string;
}

export interface ApphubApplicationAttributes {
    /**
     * Optional. Business team that ensures user needs are met and value is delivered
     */
    businessOwners?: outputs.ApphubApplicationAttributesBusinessOwner[];
    /**
     * Criticality of the Application, Service, or Workload
     */
    criticality?: outputs.ApphubApplicationAttributesCriticality;
    /**
     * Optional. Developer team that owns development and coding.
     */
    developerOwners?: outputs.ApphubApplicationAttributesDeveloperOwner[];
    /**
     * Environment of the Application, Service, or Workload
     */
    environment?: outputs.ApphubApplicationAttributesEnvironment;
    /**
     * Optional. Operator team that ensures runtime and operations.
     */
    operatorOwners?: outputs.ApphubApplicationAttributesOperatorOwner[];
}

export interface ApphubApplicationAttributesBusinessOwner {
    /**
     * Optional. Contact's name.
     */
    displayName?: string;
    /**
     * Required. Email address of the contacts.
     */
    email: string;
}

export interface ApphubApplicationAttributesCriticality {
    /**
     * Criticality type. Possible values: ["MISSION_CRITICAL", "HIGH", "MEDIUM", "LOW"]
     */
    type: string;
}

export interface ApphubApplicationAttributesDeveloperOwner {
    /**
     * Optional. Contact's name.
     */
    displayName?: string;
    /**
     * Required. Email address of the contacts.
     */
    email: string;
}

export interface ApphubApplicationAttributesEnvironment {
    /**
     * Environment type. Possible values: ["PRODUCTION", "STAGING", "TEST", "DEVELOPMENT"]
     */
    type: string;
}

export interface ApphubApplicationAttributesOperatorOwner {
    /**
     * Optional. Contact's name.
     */
    displayName?: string;
    /**
     * Required. Email address of the contacts.
     */
    email: string;
}

export interface ApphubApplicationScope {
    /**
     * Required. Scope Type. 
     *  Possible values:
     * REGIONAL Possible values: ["REGIONAL"]
     */
    type: string;
}

export interface ApphubApplicationTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ApphubServiceAttributes {
    /**
     * Business team that ensures user needs are met and value is delivered
     */
    businessOwners?: outputs.ApphubServiceAttributesBusinessOwner[];
    /**
     * Criticality of the Application, Service, or Workload
     */
    criticality?: outputs.ApphubServiceAttributesCriticality;
    /**
     * Developer team that owns development and coding.
     */
    developerOwners?: outputs.ApphubServiceAttributesDeveloperOwner[];
    /**
     * Environment of the Application, Service, or Workload
     */
    environment?: outputs.ApphubServiceAttributesEnvironment;
    /**
     * Operator team that ensures runtime and operations.
     */
    operatorOwners?: outputs.ApphubServiceAttributesOperatorOwner[];
}

export interface ApphubServiceAttributesBusinessOwner {
    /**
     * Contact's name.
     */
    displayName?: string;
    /**
     * Required. Email address of the contacts.
     */
    email: string;
}

export interface ApphubServiceAttributesCriticality {
    /**
     * Criticality type. Possible values: ["MISSION_CRITICAL", "HIGH", "MEDIUM", "LOW"]
     */
    type: string;
}

export interface ApphubServiceAttributesDeveloperOwner {
    /**
     * Contact's name.
     */
    displayName?: string;
    /**
     * Required. Email address of the contacts.
     */
    email: string;
}

export interface ApphubServiceAttributesEnvironment {
    /**
     * Environment type. Possible values: ["PRODUCTION", "STAGING", "TEST", "DEVELOPMENT"]
     */
    type: string;
}

export interface ApphubServiceAttributesOperatorOwner {
    /**
     * Contact's name.
     */
    displayName?: string;
    /**
     * Required. Email address of the contacts.
     */
    email: string;
}

export interface ApphubServiceProjectAttachmentTimeouts {
    create?: string;
    delete?: string;
}

export interface ApphubServiceServiceProperty {
    gcpProject: string;
    location: string;
    zone: string;
}

export interface ApphubServiceServiceReference {
    uri: string;
}

export interface ApphubServiceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ApphubWorkloadAttributes {
    /**
     * Business team that ensures user needs are met and value is delivered
     */
    businessOwners?: outputs.ApphubWorkloadAttributesBusinessOwner[];
    /**
     * Criticality of the Application, Service, or Workload
     */
    criticality?: outputs.ApphubWorkloadAttributesCriticality;
    /**
     * Developer team that owns development and coding.
     */
    developerOwners?: outputs.ApphubWorkloadAttributesDeveloperOwner[];
    /**
     * Environment of the Application, Service, or Workload
     */
    environment?: outputs.ApphubWorkloadAttributesEnvironment;
    /**
     * Operator team that ensures runtime and operations.
     */
    operatorOwners?: outputs.ApphubWorkloadAttributesOperatorOwner[];
}

export interface ApphubWorkloadAttributesBusinessOwner {
    /**
     * Contact's name.
     */
    displayName?: string;
    /**
     * Email address of the contacts.
     */
    email: string;
}

export interface ApphubWorkloadAttributesCriticality {
    /**
     * Criticality type. Possible values: ["MISSION_CRITICAL", "HIGH", "MEDIUM", "LOW"]
     */
    type: string;
}

export interface ApphubWorkloadAttributesDeveloperOwner {
    /**
     * Contact's name.
     */
    displayName?: string;
    /**
     * Email address of the contacts.
     */
    email: string;
}

export interface ApphubWorkloadAttributesEnvironment {
    /**
     * Environment type. Possible values: ["PRODUCTION", "STAGING", "TEST", "DEVELOPMENT"]
     */
    type: string;
}

export interface ApphubWorkloadAttributesOperatorOwner {
    /**
     * Contact's name.
     */
    displayName?: string;
    /**
     * Email address of the contacts.
     */
    email: string;
}

export interface ApphubWorkloadTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ApphubWorkloadWorkloadProperty {
    gcpProject: string;
    location: string;
    zone: string;
}

export interface ApphubWorkloadWorkloadReference {
    uri: string;
}

export interface ArtifactRegistryRepositoryCleanupPolicy {
    /**
     * Policy action. Possible values: ["DELETE", "KEEP"]
     */
    action?: string;
    /**
     * Policy condition for matching versions.
     */
    condition?: outputs.ArtifactRegistryRepositoryCleanupPolicyCondition;
    id: string;
    /**
     * Policy condition for retaining a minimum number of versions. May only be
     * specified with a Keep action.
     */
    mostRecentVersions?: outputs.ArtifactRegistryRepositoryCleanupPolicyMostRecentVersions;
}

export interface ArtifactRegistryRepositoryCleanupPolicyCondition {
    /**
     * Match versions newer than a duration.
     */
    newerThan?: string;
    /**
     * Match versions older than a duration.
     */
    olderThan?: string;
    /**
     * Match versions by package prefix. Applied on any prefix match.
     */
    packageNamePrefixes?: string[];
    /**
     * Match versions by tag prefix. Applied on any prefix match.
     */
    tagPrefixes?: string[];
    /**
     * Match versions by tag status. Default value: "ANY" Possible values: ["TAGGED", "UNTAGGED", "ANY"]
     */
    tagState?: string;
    /**
     * Match versions by version name prefix. Applied on any prefix match.
     */
    versionNamePrefixes?: string[];
}

export interface ArtifactRegistryRepositoryCleanupPolicyMostRecentVersions {
    /**
     * Minimum number of versions to keep.
     */
    keepCount?: number;
    /**
     * Match versions by package prefix. Applied on any prefix match.
     */
    packageNamePrefixes?: string[];
}

export interface ArtifactRegistryRepositoryDockerConfig {
    /**
     * The repository which enabled this flag prevents all tags from being modified, moved or deleted. This does not prevent tags from being created.
     */
    immutableTags?: boolean;
}

export interface ArtifactRegistryRepositoryIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ArtifactRegistryRepositoryIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ArtifactRegistryRepositoryMavenConfig {
    /**
     * The repository with this flag will allow publishing the same
     * snapshot versions.
     */
    allowSnapshotOverwrites?: boolean;
    /**
     * Version policy defines the versions that the registry will accept. Default value: "VERSION_POLICY_UNSPECIFIED" Possible values: ["VERSION_POLICY_UNSPECIFIED", "RELEASE", "SNAPSHOT"]
     */
    versionPolicy?: string;
}

export interface ArtifactRegistryRepositoryRemoteRepositoryConfig {
    /**
     * Specific settings for an Apt remote repository.
     */
    aptRepository?: outputs.ArtifactRegistryRepositoryRemoteRepositoryConfigAptRepository;
    /**
     * The description of the remote source.
     */
    description?: string;
    /**
     * If true, the remote repository upstream and upstream credentials will
     * not be validated.
     */
    disableUpstreamValidation?: boolean;
    /**
     * Specific settings for a Docker remote repository.
     */
    dockerRepository?: outputs.ArtifactRegistryRepositoryRemoteRepositoryConfigDockerRepository;
    /**
     * Specific settings for a Maven remote repository.
     */
    mavenRepository?: outputs.ArtifactRegistryRepositoryRemoteRepositoryConfigMavenRepository;
    /**
     * Specific settings for an Npm remote repository.
     */
    npmRepository?: outputs.ArtifactRegistryRepositoryRemoteRepositoryConfigNpmRepository;
    /**
     * Specific settings for a Python remote repository.
     */
    pythonRepository?: outputs.ArtifactRegistryRepositoryRemoteRepositoryConfigPythonRepository;
    /**
     * The credentials used to access the remote repository.
     */
    upstreamCredentials?: outputs.ArtifactRegistryRepositoryRemoteRepositoryConfigUpstreamCredentials;
    /**
     * Specific settings for an Yum remote repository.
     */
    yumRepository?: outputs.ArtifactRegistryRepositoryRemoteRepositoryConfigYumRepository;
}

export interface ArtifactRegistryRepositoryRemoteRepositoryConfigAptRepository {
    /**
     * One of the publicly available Apt repositories supported by Artifact Registry.
     */
    publicRepository?: outputs.ArtifactRegistryRepositoryRemoteRepositoryConfigAptRepositoryPublicRepository;
}

export interface ArtifactRegistryRepositoryRemoteRepositoryConfigAptRepositoryPublicRepository {
    /**
     * A common public repository base for Apt, e.g. '"debian/dists/buster"' Possible values: ["DEBIAN", "UBUNTU"]
     */
    repositoryBase: string;
    /**
     * Specific repository from the base.
     */
    repositoryPath: string;
}

export interface ArtifactRegistryRepositoryRemoteRepositoryConfigDockerRepository {
    /**
     * Settings for a remote repository with a custom uri.
     */
    customRepository?: outputs.ArtifactRegistryRepositoryRemoteRepositoryConfigDockerRepositoryCustomRepository;
    /**
     * Address of the remote repository. Default value: "DOCKER_HUB" Possible values: ["DOCKER_HUB"]
     */
    publicRepository?: string;
}

export interface ArtifactRegistryRepositoryRemoteRepositoryConfigDockerRepositoryCustomRepository {
    /**
     * Specific uri to the registry, e.g. '"https://registry-1.docker.io"'
     */
    uri?: string;
}

export interface ArtifactRegistryRepositoryRemoteRepositoryConfigMavenRepository {
    /**
     * Settings for a remote repository with a custom uri.
     */
    customRepository?: outputs.ArtifactRegistryRepositoryRemoteRepositoryConfigMavenRepositoryCustomRepository;
    /**
     * Address of the remote repository. Default value: "MAVEN_CENTRAL" Possible values: ["MAVEN_CENTRAL"]
     */
    publicRepository?: string;
}

export interface ArtifactRegistryRepositoryRemoteRepositoryConfigMavenRepositoryCustomRepository {
    /**
     * Specific uri to the registry, e.g. '"https://repo.maven.apache.org/maven2"'
     */
    uri?: string;
}

export interface ArtifactRegistryRepositoryRemoteRepositoryConfigNpmRepository {
    /**
     * Settings for a remote repository with a custom uri.
     */
    customRepository?: outputs.ArtifactRegistryRepositoryRemoteRepositoryConfigNpmRepositoryCustomRepository;
    /**
     * Address of the remote repository. Default value: "NPMJS" Possible values: ["NPMJS"]
     */
    publicRepository?: string;
}

export interface ArtifactRegistryRepositoryRemoteRepositoryConfigNpmRepositoryCustomRepository {
    /**
     * Specific uri to the registry, e.g. '"https://registry.npmjs.org"'
     */
    uri?: string;
}

export interface ArtifactRegistryRepositoryRemoteRepositoryConfigPythonRepository {
    /**
     * Settings for a remote repository with a custom uri.
     */
    customRepository?: outputs.ArtifactRegistryRepositoryRemoteRepositoryConfigPythonRepositoryCustomRepository;
    /**
     * Address of the remote repository. Default value: "PYPI" Possible values: ["PYPI"]
     */
    publicRepository?: string;
}

export interface ArtifactRegistryRepositoryRemoteRepositoryConfigPythonRepositoryCustomRepository {
    /**
     * Specific uri to the registry, e.g. '"https://pypi.io"'
     */
    uri?: string;
}

export interface ArtifactRegistryRepositoryRemoteRepositoryConfigUpstreamCredentials {
    /**
     * Use username and password to access the remote repository.
     */
    usernamePasswordCredentials?: outputs.ArtifactRegistryRepositoryRemoteRepositoryConfigUpstreamCredentialsUsernamePasswordCredentials;
}

export interface ArtifactRegistryRepositoryRemoteRepositoryConfigUpstreamCredentialsUsernamePasswordCredentials {
    /**
     * The Secret Manager key version that holds the password to access the
     * remote repository. Must be in the format of
     * 'projects/{project}/secrets/{secret}/versions/{version}'.
     */
    passwordSecretVersion?: string;
    /**
     * The username to access the remote repository.
     */
    username?: string;
}

export interface ArtifactRegistryRepositoryRemoteRepositoryConfigYumRepository {
    /**
     * One of the publicly available Yum repositories supported by Artifact Registry.
     */
    publicRepository?: outputs.ArtifactRegistryRepositoryRemoteRepositoryConfigYumRepositoryPublicRepository;
}

export interface ArtifactRegistryRepositoryRemoteRepositoryConfigYumRepositoryPublicRepository {
    /**
     * A common public repository base for Yum. Possible values: ["CENTOS", "CENTOS_DEBUG", "CENTOS_VAULT", "CENTOS_STREAM", "ROCKY", "EPEL"]
     */
    repositoryBase: string;
    /**
     * Specific repository from the base, e.g. '"pub/rocky/9/BaseOS/x86_64/os"'
     */
    repositoryPath: string;
}

export interface ArtifactRegistryRepositoryTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ArtifactRegistryRepositoryVirtualRepositoryConfig {
    /**
     * Policies that configure the upstream artifacts distributed by the Virtual
     * Repository. Upstream policies cannot be set on a standard repository.
     */
    upstreamPolicies?: outputs.ArtifactRegistryRepositoryVirtualRepositoryConfigUpstreamPolicy[];
}

export interface ArtifactRegistryRepositoryVirtualRepositoryConfigUpstreamPolicy {
    /**
     * The user-provided ID of the upstream policy.
     */
    id?: string;
    /**
     * Entries with a greater priority value take precedence in the pull order.
     */
    priority?: number;
    /**
     * A reference to the repository resource, for example:
     * "projects/p1/locations/us-central1/repository/repo1".
     */
    repository?: string;
}

export interface AssuredWorkloadsWorkloadComplianceStatus {
    acknowledgedViolationCounts: number[];
    activeViolationCounts: number[];
}

export interface AssuredWorkloadsWorkloadEkmProvisioningResponse {
    ekmProvisioningErrorDomain: string;
    ekmProvisioningErrorMapping: string;
    ekmProvisioningState: string;
}

export interface AssuredWorkloadsWorkloadKmsSettings {
    /**
     * Required. Input only. Immutable. The time at which the Key Management Service will automatically create a new version of the crypto key and mark it as the primary.
     */
    nextRotationTime: string;
    /**
     * Required. Input only. Immutable. will be advanced by this period when the Key Management Service automatically rotates a key. Must be at least 24 hours and at most 876,000 hours.
     */
    rotationPeriod: string;
}

export interface AssuredWorkloadsWorkloadPartnerPermissions {
    /**
     * Optional. Allow partner to view violation alerts.
     */
    assuredWorkloadsMonitoring?: boolean;
    /**
     * Allow the partner to view inspectability logs and monitoring violations.
     */
    dataLogsViewer?: boolean;
    /**
     * Optional. Allow partner to view access approval logs.
     */
    serviceAccessApprover?: boolean;
}

export interface AssuredWorkloadsWorkloadResource {
    resourceId: number;
    resourceType: string;
}

export interface AssuredWorkloadsWorkloadResourceSetting {
    /**
     * User-assigned resource display name. If not empty it will be used to create a resource with the specified name.
     */
    displayName?: string;
    /**
     * Resource identifier. For a project this represents projectId. If the project is already taken, the workload creation will fail. For KeyRing, this represents the keyring_id. For a folder, don't set this value as folder_id is assigned by Google.
     */
    resourceId?: string;
    /**
     * Indicates the type of resource. This field should be specified to correspond the id to the right project type (CONSUMER_PROJECT or ENCRYPTION_KEYS_PROJECT) Possible values: RESOURCE_TYPE_UNSPECIFIED, CONSUMER_PROJECT, ENCRYPTION_KEYS_PROJECT, KEYRING, CONSUMER_FOLDER
     */
    resourceType?: string;
}

export interface AssuredWorkloadsWorkloadSaaEnrollmentResponse {
    setupErrors: string[];
    setupStatus: string;
}

export interface AssuredWorkloadsWorkloadTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface BeyondcorpAppConnectionApplicationEndpoint {
    /**
     * Hostname or IP address of the remote application endpoint.
     */
    host: string;
    /**
     * Port of the remote application endpoint.
     */
    port: number;
}

export interface BeyondcorpAppConnectionGateway {
    /**
     * AppGateway name in following format: projects/{project_id}/locations/{locationId}/appgateways/{gateway_id}.
     */
    appGateway: string;
    /**
     * Ingress port reserved on the gateways for this AppConnection, if not specified or zero, the default port is 19443.
     */
    ingressPort: number;
    /**
     * The type of hosting used by the gateway. Refer to
     * https://cloud.google.com/beyondcorp/docs/reference/rest/v1/projects.locations.appConnections#Type_1
     * for a list of possible values.
     */
    type?: string;
    /**
     * Server-defined URI for this resource.
     */
    uri: string;
}

export interface BeyondcorpAppConnectionTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface BeyondcorpAppConnectorPrincipalInfo {
    /**
     * ServiceAccount represents a GCP service account.
     */
    serviceAccount: outputs.BeyondcorpAppConnectorPrincipalInfoServiceAccount;
}

export interface BeyondcorpAppConnectorPrincipalInfoServiceAccount {
    /**
     * Email address of the service account.
     */
    email: string;
}

export interface BeyondcorpAppConnectorTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface BeyondcorpAppGatewayAllocatedConnection {
    ingressPort: number;
    pscUri: string;
}

export interface BeyondcorpAppGatewayTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface BiglakeCatalogTimeouts {
    create?: string;
    delete?: string;
}

export interface BiglakeDatabaseHiveOptions {
    /**
     * Cloud Storage folder URI where the database data is stored, starting with "gs://".
     */
    locationUri?: string;
    /**
     * Stores user supplied Hive database parameters. An object containing a
     * list of"key": value pairs.
     * Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
     */
    parameters?: {[key: string]: string};
}

export interface BiglakeDatabaseTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface BiglakeTableHiveOptions {
    /**
     * Stores user supplied Hive table parameters. An object containing a
     * list of "key": value pairs.
     * Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
     */
    parameters?: {[key: string]: string};
    /**
     * Stores physical storage information on the data.
     */
    storageDescriptor?: outputs.BiglakeTableHiveOptionsStorageDescriptor;
    /**
     * Hive table type. For example, MANAGED_TABLE, EXTERNAL_TABLE.
     */
    tableType?: string;
}

export interface BiglakeTableHiveOptionsStorageDescriptor {
    /**
     * The fully qualified Java class name of the input format.
     */
    inputFormat?: string;
    /**
     * Cloud Storage folder URI where the table data is stored, starting with "gs://".
     */
    locationUri?: string;
    /**
     * The fully qualified Java class name of the output format.
     */
    outputFormat?: string;
}

export interface BiglakeTableTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface BigqueryAnalyticsHubDataExchangeIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface BigqueryAnalyticsHubDataExchangeIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface BigqueryAnalyticsHubDataExchangeSharingEnvironmentConfig {
    /**
     * Data Clean Room (DCR), used for privacy-safe and secured data sharing.
     */
    dcrExchangeConfig?: outputs.BigqueryAnalyticsHubDataExchangeSharingEnvironmentConfigDcrExchangeConfig;
    /**
     * Default Analytics Hub data exchange, used for secured data sharing.
     */
    defaultExchangeConfig?: outputs.BigqueryAnalyticsHubDataExchangeSharingEnvironmentConfigDefaultExchangeConfig;
}

export interface BigqueryAnalyticsHubDataExchangeSharingEnvironmentConfigDcrExchangeConfig {
}

export interface BigqueryAnalyticsHubDataExchangeSharingEnvironmentConfigDefaultExchangeConfig {
}

export interface BigqueryAnalyticsHubDataExchangeTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface BigqueryAnalyticsHubListingBigqueryDataset {
    /**
     * Resource name of the dataset source for this listing. e.g. projects/myproject/datasets/123
     */
    dataset: string;
    /**
     * Resource in this dataset that is selectively shared. This field is required for data clean room exchanges.
     */
    selectedResources?: outputs.BigqueryAnalyticsHubListingBigqueryDatasetSelectedResource[];
}

export interface BigqueryAnalyticsHubListingBigqueryDatasetSelectedResource {
    /**
     * Format: For table: projects/{projectId}/datasets/{datasetId}/tables/{tableId} Example:"projects/test_project/datasets/test_dataset/tables/test_table"
     */
    table?: string;
}

export interface BigqueryAnalyticsHubListingDataProvider {
    /**
     * Name of the data provider.
     */
    name: string;
    /**
     * Email or URL of the data provider.
     */
    primaryContact?: string;
}

export interface BigqueryAnalyticsHubListingIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface BigqueryAnalyticsHubListingIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface BigqueryAnalyticsHubListingPublisher {
    /**
     * Name of the listing publisher.
     */
    name: string;
    /**
     * Email or URL of the listing publisher.
     */
    primaryContact?: string;
}

export interface BigqueryAnalyticsHubListingRestrictedExportConfig {
    /**
     * If true, enable restricted export.
     */
    enabled?: boolean;
    /**
     * If true, restrict direct table access(read api/tabledata.list) on linked table.
     */
    restrictDirectTableAccess: boolean;
    /**
     * If true, restrict export of query result derived from restricted linked dataset table.
     */
    restrictQueryResult?: boolean;
}

export interface BigqueryAnalyticsHubListingTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface BigqueryBiReservationPreferredTable {
    /**
     * The ID of the dataset in the above project.
     */
    datasetId?: string;
    /**
     * The assigned project ID of the project.
     */
    projectId?: string;
    /**
     * The ID of the table in the above dataset.
     */
    tableId?: string;
}

export interface BigqueryBiReservationTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface BigqueryCapacityCommitmentTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface BigqueryConnectionAws {
    /**
     * Authentication using Google owned service account to assume into customer's AWS IAM Role.
     */
    accessRole: outputs.BigqueryConnectionAwsAccessRole;
}

export interface BigqueryConnectionAwsAccessRole {
    /**
     * The user’s AWS IAM Role that trusts the Google-owned AWS IAM user Connection.
     */
    iamRoleId: string;
    /**
     * A unique Google-owned and Google-generated identity for the Connection. This identity will be used to access the user's AWS IAM Role.
     */
    identity: string;
}

export interface BigqueryConnectionAzure {
    /**
     * The name of the Azure Active Directory Application.
     */
    application: string;
    /**
     * The client id of the Azure Active Directory Application.
     */
    clientId: string;
    /**
     * The id of customer's directory that host the data.
     */
    customerTenantId: string;
    /**
     * The Azure Application (client) ID where the federated credentials will be hosted.
     */
    federatedApplicationClientId?: string;
    /**
     * A unique Google-owned and Google-generated identity for the Connection. This identity will be used to access the user's Azure Active Directory Application.
     */
    identity: string;
    /**
     * The object id of the Azure Active Directory Application.
     */
    objectId: string;
    /**
     * The URL user will be redirected to after granting consent during connection setup.
     */
    redirectUri: string;
}

export interface BigqueryConnectionCloudResource {
    /**
     * The account ID of the service created for the purpose of this connection.
     */
    serviceAccountId: string;
}

export interface BigqueryConnectionCloudSpanner {
    /**
     * Cloud Spanner database in the form 'project/instance/database'.
     */
    database: string;
    /**
     * Cloud Spanner database role for fine-grained access control. The Cloud Spanner admin should have provisioned the database role with appropriate permissions, such as 'SELECT' and 'INSERT'. Other users should only use roles provided by their Cloud Spanner admins. The database role name must start with a letter, and can only contain letters, numbers, and underscores. For more details, see https://cloud.google.com/spanner/docs/fgac-about.
     */
    databaseRole?: string;
    /**
     * Allows setting max parallelism per query when executing on Spanner independent compute resources. If unspecified, default values of parallelism are chosen that are dependent on the Cloud Spanner instance configuration. 'useParallelism' and 'useDataBoost' must be set when setting max parallelism.
     */
    maxParallelism?: number;
    /**
     * If set, the request will be executed via Spanner independent compute resources. 'use_parallelism' must be set when using data boost.
     */
    useDataBoost?: boolean;
    /**
     * If parallelism should be used when reading from Cloud Spanner.
     */
    useParallelism?: boolean;
    /**
     * If the serverless analytics service should be used to read data from Cloud Spanner. 'useParallelism' must be set when using serverless analytics.
     *
     * @deprecated Deprecated
     */
    useServerlessAnalytics?: boolean;
}

export interface BigqueryConnectionCloudSql {
    /**
     * Cloud SQL properties.
     */
    credential: outputs.BigqueryConnectionCloudSqlCredential;
    /**
     * Database name.
     */
    database: string;
    /**
     * Cloud SQL instance ID in the form project:location:instance.
     */
    instanceId: string;
    /**
     * When the connection is used in the context of an operation in BigQuery, this service account will serve as the identity being used for connecting to the CloudSQL instance specified in this connection.
     */
    serviceAccountId: string;
    /**
     * Type of the Cloud SQL database. Possible values: ["DATABASE_TYPE_UNSPECIFIED", "POSTGRES", "MYSQL"]
     */
    type: string;
}

export interface BigqueryConnectionCloudSqlCredential {
    /**
     * Password for database.
     */
    password: string;
    /**
     * Username for database.
     */
    username: string;
}

export interface BigqueryConnectionIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface BigqueryConnectionIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface BigqueryConnectionSpark {
    /**
     * Dataproc Metastore Service configuration for the connection.
     */
    metastoreServiceConfig?: outputs.BigqueryConnectionSparkMetastoreServiceConfig;
    /**
     * The account ID of the service created for the purpose of this connection.
     */
    serviceAccountId: string;
    /**
     * Spark History Server configuration for the connection.
     */
    sparkHistoryServerConfig?: outputs.BigqueryConnectionSparkSparkHistoryServerConfig;
}

export interface BigqueryConnectionSparkMetastoreServiceConfig {
    /**
     * Resource name of an existing Dataproc Metastore service in the form of projects/[projectId]/locations/[region]/services/[serviceId].
     */
    metastoreService?: string;
}

export interface BigqueryConnectionSparkSparkHistoryServerConfig {
    /**
     * Resource name of an existing Dataproc Cluster to act as a Spark History Server for the connection if the form of projects/[projectId]/regions/[region]/clusters/[cluster_name].
     */
    dataprocCluster?: string;
}

export interface BigqueryConnectionTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface BigqueryDataTransferConfigEmailPreferences {
    /**
     * If true, email notifications will be sent on transfer run failures.
     */
    enableFailureEmail: boolean;
}

export interface BigqueryDataTransferConfigEncryptionConfiguration {
    /**
     * The name of the KMS key used for encrypting BigQuery data.
     */
    kmsKeyName: string;
}

export interface BigqueryDataTransferConfigScheduleOptions {
    /**
     * If true, automatic scheduling of data transfer runs for this
     * configuration will be disabled. The runs can be started on ad-hoc
     * basis using transferConfigs.startManualRuns API. When automatic
     * scheduling is disabled, the TransferConfig.schedule field will
     * be ignored.
     */
    disableAutoScheduling?: boolean;
    /**
     * Defines time to stop scheduling transfer runs. A transfer run cannot be
     * scheduled at or after the end time. The end time can be changed at any
     * moment. The time when a data transfer can be triggered manually is not
     * limited by this option.
     */
    endTime?: string;
    /**
     * Specifies time to start scheduling transfer runs. The first run will be
     * scheduled at or after the start time according to a recurrence pattern
     * defined in the schedule string. The start time can be changed at any
     * moment. The time when a data transfer can be triggered manually is not
     * limited by this option.
     */
    startTime?: string;
}

export interface BigqueryDataTransferConfigSensitiveParams {
    /**
     * The Secret Access Key of the AWS account transferring data from.
     */
    secretAccessKey: string;
}

export interface BigqueryDataTransferConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface BigqueryDatapolicyDataPolicyDataMaskingPolicy {
    /**
     * The available masking rules. Learn more here: https://cloud.google.com/bigquery/docs/column-data-masking-intro#masking_options. Possible values: ["SHA256", "ALWAYS_NULL", "DEFAULT_MASKING_VALUE", "LAST_FOUR_CHARACTERS", "FIRST_FOUR_CHARACTERS", "EMAIL_MASK", "DATE_YEAR_MASK"]
     */
    predefinedExpression?: string;
    /**
     * The name of the BigQuery routine that contains the custom masking routine, in the format of projects/{projectNumber}/datasets/{dataset_id}/routines/{routine_id}.
     */
    routine?: string;
}

export interface BigqueryDatapolicyDataPolicyIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface BigqueryDatapolicyDataPolicyIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface BigqueryDatapolicyDataPolicyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface BigqueryDatasetAccess {
    /**
     * Grants all resources of particular types in a particular dataset read access to the current dataset.
     */
    dataset?: outputs.BigqueryDatasetAccessDataset;
    /**
     * A domain to grant access to. Any users signed in with the
     * domain specified will be granted the specified access
     */
    domain?: string;
    /**
     * An email address of a Google Group to grant access to.
     */
    groupByEmail?: string;
    /**
     * Some other type of member that appears in the IAM Policy but isn't a user,
     * group, domain, or special group. For example: 'allUsers'
     */
    iamMember?: string;
    /**
     * Describes the rights granted to the user specified by the other
     * member of the access object. Basic, predefined, and custom roles
     * are supported. Predefined roles that have equivalent basic roles
     * are swapped by the API to their basic counterparts. See
     * [official docs](https://cloud.google.com/bigquery/docs/access-control).
     */
    role?: string;
    /**
     * A routine from a different dataset to grant access to. Queries
     * executed against that routine will have read access to tables in
     * this dataset. The role field is not required when this field is
     * set. If that routine is updated by any user, access to the routine
     * needs to be granted again via an update operation.
     */
    routine?: outputs.BigqueryDatasetAccessRoutine;
    /**
     * A special group to grant access to. Possible values include:
     * * 'projectOwners': Owners of the enclosing project.
     * * 'projectReaders': Readers of the enclosing project.
     * * 'projectWriters': Writers of the enclosing project.
     * * 'allAuthenticatedUsers': All authenticated BigQuery users.
     */
    specialGroup?: string;
    /**
     * An email address of a user to grant access to. For example:
     * fred@example.com
     */
    userByEmail?: string;
    /**
     * A view from a different dataset to grant access to. Queries
     * executed against that view will have read access to tables in
     * this dataset. The role field is not required when this field is
     * set. If that view is updated by any user, access to the view
     * needs to be granted again via an update operation.
     */
    view?: outputs.BigqueryDatasetAccessView;
}

export interface BigqueryDatasetAccessDataset {
    /**
     * The dataset this entry applies to
     */
    dataset: outputs.BigqueryDatasetAccessDatasetDataset;
    /**
     * Which resources in the dataset this entry applies to. Currently, only views are supported,
     * but additional target types may be added in the future. Possible values: VIEWS
     */
    targetTypes: string[];
}

export interface BigqueryDatasetAccessDatasetDataset {
    /**
     * The ID of the dataset containing this table.
     */
    datasetId: string;
    /**
     * The ID of the project containing this table.
     */
    projectId: string;
}

export interface BigqueryDatasetAccessRoutine {
    /**
     * The ID of the dataset containing this table.
     */
    datasetId: string;
    /**
     * The ID of the project containing this table.
     */
    projectId: string;
    /**
     * The ID of the routine. The ID must contain only letters (a-z,
     * A-Z), numbers (0-9), or underscores (_). The maximum length
     * is 256 characters.
     */
    routineId: string;
}

export interface BigqueryDatasetAccessTimeouts {
    create?: string;
    delete?: string;
}

export interface BigqueryDatasetAccessView {
    /**
     * The ID of the dataset containing this table.
     */
    datasetId: string;
    /**
     * The ID of the project containing this table.
     */
    projectId: string;
    /**
     * The ID of the table. The ID must contain only letters (a-z,
     * A-Z), numbers (0-9), or underscores (_). The maximum length
     * is 1,024 characters.
     */
    tableId: string;
}

export interface BigqueryDatasetDefaultEncryptionConfiguration {
    /**
     * Describes the Cloud KMS encryption key that will be used to protect destination
     * BigQuery table. The BigQuery Service Account associated with your project requires
     * access to this encryption key.
     */
    kmsKeyName: string;
}

export interface BigqueryDatasetExternalDatasetReference {
    /**
     * The connection id that is used to access the externalSource.
     * Format: projects/{projectId}/locations/{locationId}/connections/{connectionId}
     */
    connection: string;
    /**
     * External source that backs this dataset.
     */
    externalSource: string;
}

export interface BigqueryDatasetIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface BigqueryDatasetIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface BigqueryDatasetTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface BigqueryJobCopy {
    /**
     * Specifies whether the job is allowed to create new tables. The following values are supported:
     * CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
     * CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
     * Creation, truncation and append actions occur as one atomic update upon job completion Default value: "CREATE_IF_NEEDED" Possible values: ["CREATE_IF_NEEDED", "CREATE_NEVER"]
     */
    createDisposition?: string;
    /**
     * Custom encryption configuration (e.g., Cloud KMS keys)
     */
    destinationEncryptionConfiguration?: outputs.BigqueryJobCopyDestinationEncryptionConfiguration;
    /**
     * The destination table.
     */
    destinationTable?: outputs.BigqueryJobCopyDestinationTable;
    /**
     * Source tables to copy.
     */
    sourceTables: outputs.BigqueryJobCopySourceTable[];
    /**
     * Specifies the action that occurs if the destination table already exists. The following values are supported:
     * WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
     * WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
     * WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
     * Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
     * Creation, truncation and append actions occur as one atomic update upon job completion. Default value: "WRITE_EMPTY" Possible values: ["WRITE_TRUNCATE", "WRITE_APPEND", "WRITE_EMPTY"]
     */
    writeDisposition?: string;
}

export interface BigqueryJobCopyDestinationEncryptionConfiguration {
    /**
     * Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
     * The BigQuery Service Account associated with your project requires access to this encryption key.
     */
    kmsKeyName: string;
    /**
     * Describes the Cloud KMS encryption key version used to protect destination BigQuery table.
     */
    kmsKeyVersion: string;
}

export interface BigqueryJobCopyDestinationTable {
    /**
     * The ID of the dataset containing this table.
     */
    datasetId: string;
    /**
     * The ID of the project containing this table.
     */
    projectId: string;
    /**
     * The table. Can be specified '{{table_id}}' if 'project_id' and 'dataset_id' are also set,
     * or of the form 'projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}}' if not.
     */
    tableId: string;
}

export interface BigqueryJobCopySourceTable {
    /**
     * The ID of the dataset containing this table.
     */
    datasetId: string;
    /**
     * The ID of the project containing this table.
     */
    projectId: string;
    /**
     * The table. Can be specified '{{table_id}}' if 'project_id' and 'dataset_id' are also set,
     * or of the form 'projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}}' if not.
     */
    tableId: string;
}

export interface BigqueryJobExtract {
    /**
     * The compression type to use for exported files. Possible values include GZIP, DEFLATE, SNAPPY, and NONE.
     * The default value is NONE. DEFLATE and SNAPPY are only supported for Avro.
     */
    compression?: string;
    /**
     * The exported file format. Possible values include CSV, NEWLINE_DELIMITED_JSON and AVRO for tables and SAVED_MODEL for models.
     * The default value for tables is CSV. Tables with nested or repeated fields cannot be exported as CSV.
     * The default value for models is SAVED_MODEL.
     */
    destinationFormat: string;
    /**
     * A list of fully-qualified Google Cloud Storage URIs where the extracted table should be written.
     */
    destinationUris: string[];
    /**
     * When extracting data in CSV format, this defines the delimiter to use between fields in the exported data.
     * Default is ','
     */
    fieldDelimiter: string;
    /**
     * Whether to print out a header row in the results. Default is true.
     */
    printHeader?: boolean;
    /**
     * A reference to the model being exported.
     */
    sourceModel?: outputs.BigqueryJobExtractSourceModel;
    /**
     * A reference to the table being exported.
     */
    sourceTable?: outputs.BigqueryJobExtractSourceTable;
    /**
     * Whether to use logical types when extracting to AVRO format.
     */
    useAvroLogicalTypes?: boolean;
}

export interface BigqueryJobExtractSourceModel {
    /**
     * The ID of the dataset containing this model.
     */
    datasetId: string;
    /**
     * The ID of the model.
     */
    modelId: string;
    /**
     * The ID of the project containing this model.
     */
    projectId: string;
}

export interface BigqueryJobExtractSourceTable {
    /**
     * The ID of the dataset containing this table.
     */
    datasetId: string;
    /**
     * The ID of the project containing this table.
     */
    projectId: string;
    /**
     * The table. Can be specified '{{table_id}}' if 'project_id' and 'dataset_id' are also set,
     * or of the form 'projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}}' if not.
     */
    tableId: string;
}

export interface BigqueryJobLoad {
    /**
     * Accept rows that are missing trailing optional columns. The missing values are treated as nulls.
     * If false, records with missing trailing columns are treated as bad records, and if there are too many bad records,
     * an invalid error is returned in the job result. The default value is false. Only applicable to CSV, ignored for other formats.
     */
    allowJaggedRows?: boolean;
    /**
     * Indicates if BigQuery should allow quoted data sections that contain newline characters in a CSV file.
     * The default value is false.
     */
    allowQuotedNewlines?: boolean;
    /**
     * Indicates if we should automatically infer the options and schema for CSV and JSON sources.
     */
    autodetect?: boolean;
    /**
     * Specifies whether the job is allowed to create new tables. The following values are supported:
     * CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
     * CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
     * Creation, truncation and append actions occur as one atomic update upon job completion Default value: "CREATE_IF_NEEDED" Possible values: ["CREATE_IF_NEEDED", "CREATE_NEVER"]
     */
    createDisposition?: string;
    /**
     * Custom encryption configuration (e.g., Cloud KMS keys)
     */
    destinationEncryptionConfiguration?: outputs.BigqueryJobLoadDestinationEncryptionConfiguration;
    /**
     * The destination table to load the data into.
     */
    destinationTable: outputs.BigqueryJobLoadDestinationTable;
    /**
     * The character encoding of the data. The supported values are UTF-8 or ISO-8859-1.
     * The default value is UTF-8. BigQuery decodes the data after the raw, binary data
     * has been split using the values of the quote and fieldDelimiter properties.
     */
    encoding?: string;
    /**
     * The separator for fields in a CSV file. The separator can be any ISO-8859-1 single-byte character.
     * To use a character in the range 128-255, you must encode the character as UTF8. BigQuery converts
     * the string to ISO-8859-1 encoding, and then uses the first byte of the encoded string to split the
     * data in its raw, binary state. BigQuery also supports the escape sequence "\t" to specify a tab separator.
     * The default value is a comma (',').
     */
    fieldDelimiter: string;
    /**
     * Indicates if BigQuery should allow extra values that are not represented in the table schema.
     * If true, the extra values are ignored. If false, records with extra columns are treated as bad records,
     * and if there are too many bad records, an invalid error is returned in the job result.
     * The default value is false. The sourceFormat property determines what BigQuery treats as an extra value:
     * CSV: Trailing columns
     * JSON: Named values that don't match any column names
     */
    ignoreUnknownValues?: boolean;
    /**
     * If sourceFormat is set to newline-delimited JSON, indicates whether it should be processed as a JSON variant such as GeoJSON.
     * For a sourceFormat other than JSON, omit this field. If the sourceFormat is newline-delimited JSON: - for newline-delimited
     * GeoJSON: set to GEOJSON.
     */
    jsonExtension?: string;
    /**
     * The maximum number of bad records that BigQuery can ignore when running the job. If the number of bad records exceeds this value,
     * an invalid error is returned in the job result. The default value is 0, which requires that all records are valid.
     */
    maxBadRecords?: number;
    /**
     * Specifies a string that represents a null value in a CSV file. For example, if you specify "\N", BigQuery interprets "\N" as a null value
     * when loading a CSV file. The default value is the empty string. If you set this property to a custom value, BigQuery throws an error if an
     * empty string is present for all data types except for STRING and BYTE. For STRING and BYTE columns, BigQuery interprets the empty string as
     * an empty value.
     */
    nullMarker?: string;
    /**
     * Parquet Options for load and make external tables.
     */
    parquetOptions?: outputs.BigqueryJobLoadParquetOptions;
    /**
     * If sourceFormat is set to "DATASTORE_BACKUP", indicates which entity properties to load into BigQuery from a Cloud Datastore backup.
     * Property names are case sensitive and must be top-level properties. If no properties are specified, BigQuery loads all properties.
     * If any named property isn't found in the Cloud Datastore backup, an invalid error is returned in the job result.
     */
    projectionFields?: string[];
    /**
     * The value that is used to quote data sections in a CSV file. BigQuery converts the string to ISO-8859-1 encoding,
     * and then uses the first byte of the encoded string to split the data in its raw, binary state.
     * The default value is a double-quote ('"'). If your data does not contain quoted sections, set the property value to an empty string.
     * If your data contains quoted newline characters, you must also set the allowQuotedNewlines property to true.
     */
    quote: string;
    /**
     * Allows the schema of the destination table to be updated as a side effect of the load job if a schema is autodetected or
     * supplied in the job configuration. Schema update options are supported in two cases: when writeDisposition is WRITE_APPEND;
     * when writeDisposition is WRITE_TRUNCATE and the destination table is a partition of a table, specified by partition decorators.
     * For normal tables, WRITE_TRUNCATE will always overwrite the schema. One or more of the following values are specified:
     * ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.
     * ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original schema to nullable.
     */
    schemaUpdateOptions?: string[];
    /**
     * The number of rows at the top of a CSV file that BigQuery will skip when loading the data.
     * The default value is 0. This property is useful if you have header rows in the file that should be skipped.
     * When autodetect is on, the behavior is the following:
     * skipLeadingRows unspecified - Autodetect tries to detect headers in the first row. If they are not detected,
     * the row is read as data. Otherwise data is read starting from the second row.
     * skipLeadingRows is 0 - Instructs autodetect that there are no headers and data should be read starting from the first row.
     * skipLeadingRows = N > 0 - Autodetect skips N-1 rows and tries to detect headers in row N. If headers are not detected,
     * row N is just skipped. Otherwise row N is used to extract column names for the detected schema.
     */
    skipLeadingRows?: number;
    /**
     * The format of the data files. For CSV files, specify "CSV". For datastore backups, specify "DATASTORE_BACKUP".
     * For newline-delimited JSON, specify "NEWLINE_DELIMITED_JSON". For Avro, specify "AVRO". For parquet, specify "PARQUET".
     * For orc, specify "ORC". [Beta] For Bigtable, specify "BIGTABLE".
     * The default value is CSV.
     */
    sourceFormat?: string;
    /**
     * The fully-qualified URIs that point to your data in Google Cloud.
     * For Google Cloud Storage URIs: Each URI can contain one '\*' wildcard character
     * and it must come after the 'bucket' name. Size limits related to load jobs apply
     * to external data sources. For Google Cloud Bigtable URIs: Exactly one URI can be
     * specified and it has be a fully specified and valid HTTPS URL for a Google Cloud Bigtable table.
     * For Google Cloud Datastore backups: Exactly one URI can be specified. Also, the '\*' wildcard character is not allowed.
     */
    sourceUris: string[];
    /**
     * Time-based partitioning specification for the destination table.
     */
    timePartitioning?: outputs.BigqueryJobLoadTimePartitioning;
    /**
     * Specifies the action that occurs if the destination table already exists. The following values are supported:
     * WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
     * WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
     * WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
     * Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
     * Creation, truncation and append actions occur as one atomic update upon job completion. Default value: "WRITE_EMPTY" Possible values: ["WRITE_TRUNCATE", "WRITE_APPEND", "WRITE_EMPTY"]
     */
    writeDisposition?: string;
}

export interface BigqueryJobLoadDestinationEncryptionConfiguration {
    /**
     * Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
     * The BigQuery Service Account associated with your project requires access to this encryption key.
     */
    kmsKeyName: string;
    /**
     * Describes the Cloud KMS encryption key version used to protect destination BigQuery table.
     */
    kmsKeyVersion: string;
}

export interface BigqueryJobLoadDestinationTable {
    /**
     * The ID of the dataset containing this table.
     */
    datasetId: string;
    /**
     * The ID of the project containing this table.
     */
    projectId: string;
    /**
     * The table. Can be specified '{{table_id}}' if 'project_id' and 'dataset_id' are also set,
     * or of the form 'projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}}' if not.
     */
    tableId: string;
}

export interface BigqueryJobLoadParquetOptions {
    /**
     * If sourceFormat is set to PARQUET, indicates whether to use schema inference specifically for Parquet LIST logical type.
     */
    enableListInference?: boolean;
    /**
     * If sourceFormat is set to PARQUET, indicates whether to infer Parquet ENUM logical type as STRING instead of BYTES by default.
     */
    enumAsString?: boolean;
}

export interface BigqueryJobLoadTimePartitioning {
    /**
     * Number of milliseconds for which to keep the storage for a partition. A wrapper is used here because 0 is an invalid value.
     */
    expirationMs?: string;
    /**
     * If not set, the table is partitioned by pseudo column '_PARTITIONTIME'; if set, the table is partitioned by this field.
     * The field must be a top-level TIMESTAMP or DATE field. Its mode must be NULLABLE or REQUIRED.
     * A wrapper is used here because an empty string is an invalid value.
     */
    field?: string;
    /**
     * The only type supported is DAY, which will generate one partition per day. Providing an empty string used to cause an error,
     * but in OnePlatform the field will be treated as unset.
     */
    type: string;
}

export interface BigqueryJobQuery {
    /**
     * If true and query uses legacy SQL dialect, allows the query to produce arbitrarily large result tables at a slight cost in performance.
     * Requires destinationTable to be set. For standard SQL queries, this flag is ignored and large results are always allowed.
     * However, you must still set destinationTable when result size exceeds the allowed maximum response size.
     */
    allowLargeResults?: boolean;
    /**
     * Specifies whether the job is allowed to create new tables. The following values are supported:
     * CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table.
     * CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result.
     * Creation, truncation and append actions occur as one atomic update upon job completion Default value: "CREATE_IF_NEEDED" Possible values: ["CREATE_IF_NEEDED", "CREATE_NEVER"]
     */
    createDisposition?: string;
    /**
     * Specifies the default dataset to use for unqualified table names in the query. Note that this does not alter behavior of unqualified dataset names.
     */
    defaultDataset?: outputs.BigqueryJobQueryDefaultDataset;
    /**
     * Custom encryption configuration (e.g., Cloud KMS keys)
     */
    destinationEncryptionConfiguration?: outputs.BigqueryJobQueryDestinationEncryptionConfiguration;
    /**
     * Describes the table where the query results should be stored.
     * This property must be set for large results that exceed the maximum response size.
     * For queries that produce anonymous (cached) results, this field will be populated by BigQuery.
     */
    destinationTable?: outputs.BigqueryJobQueryDestinationTable;
    /**
     * If true and query uses legacy SQL dialect, flattens all nested and repeated fields in the query results.
     * allowLargeResults must be true if this is set to false. For standard SQL queries, this flag is ignored and results are never flattened.
     */
    flattenResults?: boolean;
    /**
     * Limits the billing tier for this job. Queries that have resource usage beyond this tier will fail (without incurring a charge).
     * If unspecified, this will be set to your project default.
     */
    maximumBillingTier?: number;
    /**
     * Limits the bytes billed for this job. Queries that will have bytes billed beyond this limit will fail (without incurring a charge).
     * If unspecified, this will be set to your project default.
     */
    maximumBytesBilled?: string;
    /**
     * Standard SQL only. Set to POSITIONAL to use positional (?) query parameters or to NAMED to use named (@myparam) query parameters in this query.
     */
    parameterMode?: string;
    /**
     * Specifies a priority for the query. Default value: "INTERACTIVE" Possible values: ["INTERACTIVE", "BATCH"]
     */
    priority?: string;
    /**
     * SQL query text to execute. The useLegacySql field can be used to indicate whether the query uses legacy SQL or standard SQL.
     * *NOTE*: queries containing [DML language](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-manipulation-language)
     * ('DELETE', 'UPDATE', 'MERGE', 'INSERT') must specify 'create_disposition = ""' and 'write_disposition = ""'.
     */
    query: string;
    /**
     * Allows the schema of the destination table to be updated as a side effect of the query job.
     * Schema update options are supported in two cases: when writeDisposition is WRITE_APPEND;
     * when writeDisposition is WRITE_TRUNCATE and the destination table is a partition of a table,
     * specified by partition decorators. For normal tables, WRITE_TRUNCATE will always overwrite the schema.
     * One or more of the following values are specified:
     * ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema.
     * ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original schema to nullable.
     */
    schemaUpdateOptions?: string[];
    /**
     * Options controlling the execution of scripts.
     */
    scriptOptions?: outputs.BigqueryJobQueryScriptOptions;
    /**
     * Specifies whether to use BigQuery's legacy SQL dialect for this query. The default value is true.
     * If set to false, the query will use BigQuery's standard SQL.
     */
    useLegacySql?: boolean;
    /**
     * Whether to look for the result in the query cache. The query cache is a best-effort cache that will be flushed whenever
     * tables in the query are modified. Moreover, the query cache is only available when a query does not have a destination table specified.
     * The default value is true.
     */
    useQueryCache?: boolean;
    /**
     * Describes user-defined function resources used in the query.
     */
    userDefinedFunctionResources?: outputs.BigqueryJobQueryUserDefinedFunctionResource[];
    /**
     * Specifies the action that occurs if the destination table already exists. The following values are supported:
     * WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the table data and uses the schema from the query result.
     * WRITE_APPEND: If the table already exists, BigQuery appends the data to the table.
     * WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result.
     * Each action is atomic and only occurs if BigQuery is able to complete the job successfully.
     * Creation, truncation and append actions occur as one atomic update upon job completion. Default value: "WRITE_EMPTY" Possible values: ["WRITE_TRUNCATE", "WRITE_APPEND", "WRITE_EMPTY"]
     */
    writeDisposition?: string;
}

export interface BigqueryJobQueryDefaultDataset {
    /**
     * The dataset. Can be specified '{{dataset_id}}' if 'project_id' is also set,
     * or of the form 'projects/{{project}}/datasets/{{dataset_id}}' if not.
     */
    datasetId: string;
    /**
     * The ID of the project containing this table.
     */
    projectId: string;
}

export interface BigqueryJobQueryDestinationEncryptionConfiguration {
    /**
     * Describes the Cloud KMS encryption key that will be used to protect destination BigQuery table.
     * The BigQuery Service Account associated with your project requires access to this encryption key.
     */
    kmsKeyName: string;
    /**
     * Describes the Cloud KMS encryption key version used to protect destination BigQuery table.
     */
    kmsKeyVersion: string;
}

export interface BigqueryJobQueryDestinationTable {
    /**
     * The ID of the dataset containing this table.
     */
    datasetId: string;
    /**
     * The ID of the project containing this table.
     */
    projectId: string;
    /**
     * The table. Can be specified '{{table_id}}' if 'project_id' and 'dataset_id' are also set,
     * or of the form 'projects/{{project}}/datasets/{{dataset_id}}/tables/{{table_id}}' if not.
     */
    tableId: string;
}

export interface BigqueryJobQueryScriptOptions {
    /**
     * Determines which statement in the script represents the "key result",
     * used to populate the schema and query results of the script job. Possible values: ["LAST", "FIRST_SELECT"]
     */
    keyResultStatement?: string;
    /**
     * Limit on the number of bytes billed per statement. Exceeding this budget results in an error.
     */
    statementByteBudget?: string;
    /**
     * Timeout period for each statement in a script.
     */
    statementTimeoutMs?: string;
}

export interface BigqueryJobQueryUserDefinedFunctionResource {
    /**
     * An inline resource that contains code for a user-defined function (UDF).
     * Providing a inline code resource is equivalent to providing a URI for a file containing the same code.
     */
    inlineCode?: string;
    /**
     * A code resource to load from a Google Cloud Storage URI (gs://bucket/path).
     */
    resourceUri?: string;
}

export interface BigqueryJobStatus {
    errorResults: outputs.BigqueryJobStatusErrorResult[];
    errors: outputs.BigqueryJobStatusError[];
    state: string;
}

export interface BigqueryJobStatusError {
    location: string;
    message: string;
    reason: string;
}

export interface BigqueryJobStatusErrorResult {
    location: string;
    message: string;
    reason: string;
}

export interface BigqueryJobTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface BigqueryReservationAssignmentTimeouts {
    create?: string;
    delete?: string;
}

export interface BigqueryReservationAutoscale {
    /**
     * The slot capacity added to this reservation when autoscale happens. Will be between [0, max_slots].
     */
    currentSlots: number;
    /**
     * Number of slots to be scaled when needed.
     */
    maxSlots?: number;
}

export interface BigqueryReservationTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface BigqueryRoutineArgument {
    /**
     * Defaults to FIXED_TYPE. Default value: "FIXED_TYPE" Possible values: ["FIXED_TYPE", "ANY_TYPE"]
     */
    argumentKind?: string;
    /**
     * A JSON schema for the data type. Required unless argumentKind = ANY_TYPE.
     * ~>**NOTE**: Because this field expects a JSON string, any changes to the string
     * will create a diff, even if the JSON itself hasn't changed. If the API returns
     * a different value for the same schema, e.g. it switched the order of values
     * or replaced STRUCT field type with RECORD field type, we currently cannot
     * suppress the recurring diff this causes. As a workaround, we recommend using
     * the schema as returned by the API.
     */
    dataType?: string;
    /**
     * Specifies whether the argument is input or output. Can be set for procedures only. Possible values: ["IN", "OUT", "INOUT"]
     */
    mode?: string;
    /**
     * The name of this argument. Can be absent for function return argument.
     */
    name?: string;
}

export interface BigqueryRoutineRemoteFunctionOptions {
    /**
     * Fully qualified name of the user-provided connection object which holds
     * the authentication information to send requests to the remote service.
     * Format: "projects/{projectId}/locations/{locationId}/connections/{connectionId}"
     */
    connection?: string;
    /**
     * Endpoint of the user-provided remote service, e.g.
     * 'https://us-east1-my_gcf_project.cloudfunctions.net/remote_add'
     */
    endpoint?: string;
    /**
     * Max number of rows in each batch sent to the remote service. If absent or if 0,
     * BigQuery dynamically decides the number of rows in a batch.
     */
    maxBatchingRows?: string;
    /**
     * User-defined context as a set of key/value pairs, which will be sent as function
     * invocation context together with batched arguments in the requests to the remote
     * service. The total number of bytes of keys and values must be less than 8KB.
     *
     * An object containing a list of "key": value pairs. Example:
     * '{ "name": "wrench", "mass": "1.3kg", "count": "3" }'.
     */
    userDefinedContext: {[key: string]: string};
}

export interface BigqueryRoutineSparkOptions {
    /**
     * Archive files to be extracted into the working directory of each executor. For more information about Apache Spark, see Apache Spark.
     */
    archiveUris: string[];
    /**
     * Fully qualified name of the user-provided Spark connection object.
     * Format: "projects/{projectId}/locations/{locationId}/connections/{connectionId}"
     */
    connection?: string;
    /**
     * Custom container image for the runtime environment.
     */
    containerImage?: string;
    /**
     * Files to be placed in the working directory of each executor. For more information about Apache Spark, see Apache Spark.
     */
    fileUris: string[];
    /**
     * JARs to include on the driver and executor CLASSPATH. For more information about Apache Spark, see Apache Spark.
     */
    jarUris: string[];
    /**
     * The fully qualified name of a class in jarUris, for example, com.example.wordcount.
     * Exactly one of mainClass and main_jar_uri field should be set for Java/Scala language type.
     */
    mainClass?: string;
    /**
     * The main file/jar URI of the Spark application.
     * Exactly one of the definitionBody field and the mainFileUri field must be set for Python.
     * Exactly one of mainClass and mainFileUri field should be set for Java/Scala language type.
     */
    mainFileUri?: string;
    /**
     * Configuration properties as a set of key/value pairs, which will be passed on to the Spark application.
     * For more information, see Apache Spark and the procedure option list.
     * An object containing a list of "key": value pairs. Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
     */
    properties: {[key: string]: string};
    /**
     * Python files to be placed on the PYTHONPATH for PySpark application. Supported file types: .py, .egg, and .zip. For more information about Apache Spark, see Apache Spark.
     */
    pyFileUris: string[];
    /**
     * Runtime version. If not specified, the default runtime version is used.
     */
    runtimeVersion?: string;
}

export interface BigqueryRoutineTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface BigqueryTableEncryptionConfiguration {
    /**
     * The self link or full name of a key which should be used to encrypt this table. Note that the default bigquery service account will need to have encrypt/decrypt permissions on this key - you may want to see the google.getBigqueryDefaultServiceAccount datasource and the google.KmsCryptoKeyIamBinding resource.
     */
    kmsKeyName: string;
    /**
     * The self link or full name of the kms key version used to encrypt this table.
     */
    kmsKeyVersion: string;
}

export interface BigqueryTableExternalDataConfiguration {
    /**
     * Let BigQuery try to autodetect the schema and format of the table.
     */
    autodetect: boolean;
    /**
     * Additional options if source_format is set to "AVRO"
     */
    avroOptions?: outputs.BigqueryTableExternalDataConfigurationAvroOptions;
    /**
     * Additional options if sourceFormat is set to BIGTABLE.
     */
    bigtableOptions?: outputs.BigqueryTableExternalDataConfigurationBigtableOptions;
    /**
     * The compression type of the data source. Valid values are "NONE" or "GZIP".
     */
    compression?: string;
    /**
     * The connection specifying the credentials to be used to read external storage, such as Azure Blob, Cloud Storage, or S3. The connectionId can have the form "{{project}}.{{location}}.{{connection_id}}" or "projects/{{project}}/locations/{{location}}/connections/{{connection_id}}".
     */
    connectionId?: string;
    /**
     * Additional properties to set if source_format is set to "CSV".
     */
    csvOptions?: outputs.BigqueryTableExternalDataConfigurationCsvOptions;
    /**
     * Specifies how source URIs are interpreted for constructing the file set to load.  By default source URIs are expanded against the underlying storage.  Other options include specifying manifest files. Only applicable to object storage systems.
     */
    fileSetSpecType?: string;
    /**
     * Additional options if source_format is set to "GOOGLE_SHEETS".
     */
    googleSheetsOptions?: outputs.BigqueryTableExternalDataConfigurationGoogleSheetsOptions;
    /**
     * When set, configures hive partitioning support. Not all storage formats support hive partitioning -- requesting hive partitioning on an unsupported format will lead to an error, as will providing an invalid specification.
     */
    hivePartitioningOptions?: outputs.BigqueryTableExternalDataConfigurationHivePartitioningOptions;
    /**
     * Indicates if BigQuery should allow extra values that are not represented in the table schema. If true, the extra values are ignored. If false, records with extra columns are treated as bad records, and if there are too many bad records, an invalid error is returned in the job result. The default value is false.
     */
    ignoreUnknownValues?: boolean;
    /**
     * Load option to be used together with sourceFormat newline-delimited JSON to indicate that a variant of JSON is being loaded. To load newline-delimited GeoJSON, specify GEOJSON (and sourceFormat must be set to NEWLINE_DELIMITED_JSON).
     */
    jsonExtension?: string;
    /**
     * Additional properties to set if sourceFormat is set to JSON.
     */
    jsonOptions?: outputs.BigqueryTableExternalDataConfigurationJsonOptions;
    /**
     * The maximum number of bad records that BigQuery can ignore when reading data.
     */
    maxBadRecords?: number;
    /**
     * Metadata Cache Mode for the table. Set this to enable caching of metadata from external data source.
     */
    metadataCacheMode?: string;
    /**
     * Object Metadata is used to create Object Tables. Object Tables contain a listing of objects (with their metadata) found at the sourceUris. If ObjectMetadata is set, sourceFormat should be omitted.
     */
    objectMetadata?: string;
    /**
     * Additional properties to set if sourceFormat is set to PARQUET.
     */
    parquetOptions?: outputs.BigqueryTableExternalDataConfigurationParquetOptions;
    /**
     * When creating an external table, the user can provide a reference file with the table schema. This is enabled for the following formats: AVRO, PARQUET, ORC.
     */
    referenceFileSchemaUri?: string;
    /**
     * A JSON schema for the external table. Schema is required for CSV and JSON formats and is disallowed for Google Cloud Bigtable, Cloud Datastore backups, and Avro formats when using external tables.
     */
    schema: string;
    /**
     * Please see sourceFormat under ExternalDataConfiguration in Bigquery's public API documentation (https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#externaldataconfiguration) for supported formats. To use "GOOGLE_SHEETS" the scopes must include "googleapis.com/auth/drive.readonly".
     */
    sourceFormat?: string;
    /**
     * A list of the fully-qualified URIs that point to your data in Google Cloud.
     */
    sourceUris: string[];
}

export interface BigqueryTableExternalDataConfigurationAvroOptions {
    /**
     * If sourceFormat is set to "AVRO", indicates whether to interpret logical types as the corresponding BigQuery data type (for example, TIMESTAMP), instead of using the raw type (for example, INTEGER).
     */
    useAvroLogicalTypes: boolean;
}

export interface BigqueryTableExternalDataConfigurationBigtableOptions {
    /**
     * A list of column families to expose in the table schema along with their types. This list restricts the column families that can be referenced in queries and specifies their value types. You can use this list to do type conversions - see the 'type' field for more details. If you leave this list empty, all column families are present in the table schema and their values are read as BYTES. During a query only the column families referenced in that query are read from Bigtable.
     */
    columnFamilies?: outputs.BigqueryTableExternalDataConfigurationBigtableOptionsColumnFamily[];
    /**
     * If field is true, then the column families that are not specified in columnFamilies list are not exposed in the table schema. Otherwise, they are read with BYTES type values. The default value is false.
     */
    ignoreUnspecifiedColumnFamilies?: boolean;
    /**
     * If field is true, then each column family will be read as a single JSON column. Otherwise they are read as a repeated cell structure containing timestamp/value tuples. The default value is false.
     */
    outputColumnFamiliesAsJson?: boolean;
    /**
     * If field is true, then the rowkey column families will be read and converted to string. Otherwise they are read with BYTES type values and users need to manually cast them with CAST if necessary. The default value is false.
     */
    readRowkeyAsString?: boolean;
}

export interface BigqueryTableExternalDataConfigurationBigtableOptionsColumnFamily {
    /**
     * A List of columns that should be exposed as individual fields as opposed to a list of (column name, value) pairs. All columns whose qualifier matches a qualifier in this list can be accessed as Other columns can be accessed as a list through column field
     */
    columns?: outputs.BigqueryTableExternalDataConfigurationBigtableOptionsColumnFamilyColumn[];
    /**
     * The encoding of the values when the type is not STRING. Acceptable encoding values are: TEXT - indicates values are alphanumeric text strings. BINARY - indicates values are encoded using HBase Bytes.toBytes family of functions. This can be overridden for a specific column by listing that column in 'columns' and specifying an encoding for it.
     */
    encoding?: string;
    /**
     * Identifier of the column family.
     */
    familyId?: string;
    /**
     * If this is set only the latest version of value are exposed for all columns in this column family. This can be overridden for a specific column by listing that column in 'columns' and specifying a different setting for that column.
     */
    onlyReadLatest?: boolean;
    /**
     * The type to convert the value in cells of this column family. The values are expected to be encoded using HBase Bytes.toBytes function when using the BINARY encoding value. Following BigQuery types are allowed (case-sensitive): "BYTES", "STRING", "INTEGER", "FLOAT", "BOOLEAN", "JSON". Default type is BYTES. This can be overridden for a specific column by listing that column in 'columns' and specifying a type for it.
     */
    type?: string;
}

export interface BigqueryTableExternalDataConfigurationBigtableOptionsColumnFamilyColumn {
    /**
     * The encoding of the values when the type is not STRING. Acceptable encoding values are: TEXT - indicates values are alphanumeric text strings. BINARY - indicates values are encoded using HBase Bytes.toBytes family of functions. 'encoding' can also be set at the column family level. However, the setting at this level takes precedence if 'encoding' is set at both levels.
     */
    encoding?: string;
    /**
     * If the qualifier is not a valid BigQuery field identifier i.e. does not match [a-zA-Z][a-zA-Z0-9_]*, a valid identifier must be provided as the column field name and is used as field name in queries.
     */
    fieldName?: string;
    /**
     * If this is set, only the latest version of value in this column are exposed. 'onlyReadLatest' can also be set at the column family level. However, the setting at this level takes precedence if 'onlyReadLatest' is set at both levels.
     */
    onlyReadLatest?: boolean;
    /**
     * Qualifier of the column. Columns in the parent column family that has this exact qualifier are exposed as . field. If the qualifier is valid UTF-8 string, it can be specified in the qualifierString field. Otherwise, a base-64 encoded value must be set to qualifierEncoded. The column field name is the same as the column qualifier. However, if the qualifier is not a valid BigQuery field identifier i.e. does not match [a-zA-Z][a-zA-Z0-9_]*, a valid identifier must be provided as fieldName.
     */
    qualifierEncoded?: string;
    /**
     * Qualifier string.
     */
    qualifierString?: string;
    /**
     * The type to convert the value in cells of this column. The values are expected to be encoded using HBase Bytes.toBytes function when using the BINARY encoding value. Following BigQuery types are allowed (case-sensitive): "BYTES", "STRING", "INTEGER", "FLOAT", "BOOLEAN", "JSON", Default type is "BYTES". 'type' can also be set at the column family level. However, the setting at this level takes precedence if 'type' is set at both levels.
     */
    type?: string;
}

export interface BigqueryTableExternalDataConfigurationCsvOptions {
    /**
     * Indicates if BigQuery should accept rows that are missing trailing optional columns.
     */
    allowJaggedRows?: boolean;
    /**
     * Indicates if BigQuery should allow quoted data sections that contain newline characters in a CSV file. The default value is false.
     */
    allowQuotedNewlines?: boolean;
    /**
     * The character encoding of the data. The supported values are UTF-8 or ISO-8859-1.
     */
    encoding?: string;
    /**
     * The separator for fields in a CSV file.
     */
    fieldDelimiter?: string;
    quote: string;
    /**
     * The number of rows at the top of a CSV file that BigQuery will skip when reading the data.
     */
    skipLeadingRows?: number;
}

export interface BigqueryTableExternalDataConfigurationGoogleSheetsOptions {
    /**
     * Range of a sheet to query from. Only used when non-empty. At least one of range or skip_leading_rows must be set. Typical format: "sheet_name!top_left_cell_id:bottom_right_cell_id" For example: "sheet1!A1:B20
     */
    range?: string;
    /**
     * The number of rows at the top of the sheet that BigQuery will skip when reading the data. At least one of range or skip_leading_rows must be set.
     */
    skipLeadingRows?: number;
}

export interface BigqueryTableExternalDataConfigurationHivePartitioningOptions {
    /**
     * When set, what mode of hive partitioning to use when reading data.
     */
    mode?: string;
    /**
     * If set to true, queries over this table require a partition filter that can be used for partition elimination to be specified.
     */
    requirePartitionFilter?: boolean;
    /**
     * When hive partition detection is requested, a common for all source uris must be required. The prefix must end immediately before the partition key encoding begins.
     */
    sourceUriPrefix?: string;
}

export interface BigqueryTableExternalDataConfigurationJsonOptions {
    /**
     * The character encoding of the data. The supported values are UTF-8, UTF-16BE, UTF-16LE, UTF-32BE, and UTF-32LE. The default value is UTF-8.
     */
    encoding?: string;
}

export interface BigqueryTableExternalDataConfigurationParquetOptions {
    /**
     * Indicates whether to use schema inference specifically for Parquet LIST logical type.
     */
    enableListInference?: boolean;
    /**
     * Indicates whether to infer Parquet ENUM logical type as STRING instead of BYTES by default.
     */
    enumAsString?: boolean;
}

export interface BigqueryTableIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface BigqueryTableIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface BigqueryTableMaterializedView {
    /**
     * Allow non incremental materialized view definition. The default value is false.
     */
    allowNonIncrementalDefinition?: boolean;
    /**
     * Specifies if BigQuery should automatically refresh materialized view when the base table is updated. The default is true.
     */
    enableRefresh?: boolean;
    /**
     * A query whose result is persisted.
     */
    query: string;
    /**
     * Specifies maximum frequency at which this materialized view will be refreshed. The default is 1800000.
     */
    refreshIntervalMs?: number;
}

export interface BigqueryTableRangePartitioning {
    /**
     * The field used to determine how to create a range-based partition.
     */
    field: string;
    /**
     * Information required to partition based on ranges. Structure is documented below.
     */
    range: outputs.BigqueryTableRangePartitioningRange;
}

export interface BigqueryTableRangePartitioningRange {
    /**
     * End of the range partitioning, exclusive.
     */
    end: number;
    /**
     * The width of each range within the partition.
     */
    interval: number;
    /**
     * Start of the range partitioning, inclusive.
     */
    start: number;
}

export interface BigqueryTableTableConstraints {
    /**
     * Present only if the table has a foreign key. The foreign key is not enforced.
     */
    foreignKeys?: outputs.BigqueryTableTableConstraintsForeignKey[];
    /**
     * Represents a primary key constraint on a table's columns. Present only if the table has a primary key. The primary key is not enforced.
     */
    primaryKey?: outputs.BigqueryTableTableConstraintsPrimaryKey;
}

export interface BigqueryTableTableConstraintsForeignKey {
    /**
     * The pair of the foreign key column and primary key column.
     */
    columnReferences: outputs.BigqueryTableTableConstraintsForeignKeyColumnReferences;
    /**
     * Set only if the foreign key constraint is named.
     */
    name?: string;
    /**
     * The table that holds the primary key and is referenced by this foreign key.
     */
    referencedTable: outputs.BigqueryTableTableConstraintsForeignKeyReferencedTable;
}

export interface BigqueryTableTableConstraintsForeignKeyColumnReferences {
    /**
     * The column in the primary key that are referenced by the referencingColumn.
     */
    referencedColumn: string;
    /**
     * The column that composes the foreign key.
     */
    referencingColumn: string;
}

export interface BigqueryTableTableConstraintsForeignKeyReferencedTable {
    /**
     * The ID of the dataset containing this table.
     */
    datasetId: string;
    /**
     * The ID of the project containing this table.
     */
    projectId: string;
    /**
     * The ID of the table. The ID must contain only letters (a-z, A-Z), numbers (0-9), or underscores (_). The maximum length is 1,024 characters. Certain operations allow suffixing of the table ID with a partition decorator, such as sample_table$20190123.
     */
    tableId: string;
}

export interface BigqueryTableTableConstraintsPrimaryKey {
    /**
     * The columns that are composed of the primary key constraint.
     */
    columns: string[];
}

export interface BigqueryTableTableReplicationInfo {
    /**
     * The interval at which the source materialized view is polled for updates. The default is 300000.
     */
    replicationIntervalMs?: number;
    /**
     * The ID of the source dataset.
     */
    sourceDatasetId: string;
    /**
     * The ID of the source project.
     */
    sourceProjectId: string;
    /**
     * The ID of the source materialized view.
     */
    sourceTableId: string;
}

export interface BigqueryTableTimePartitioning {
    /**
     * Number of milliseconds for which to keep the storage for a partition.
     */
    expirationMs: number;
    /**
     * The field used to determine how to create a time-based partition. If time-based partitioning is enabled without this value, the table is partitioned based on the load time.
     */
    field?: string;
    /**
     * If set to true, queries over this table require a partition filter that can be used for partition elimination to be specified.
     *
     * @deprecated Deprecated
     */
    requirePartitionFilter?: boolean;
    /**
     * The supported types are DAY, HOUR, MONTH, and YEAR, which will generate one partition per day, hour, month, and year, respectively.
     */
    type: string;
}

export interface BigqueryTableView {
    /**
     * A query that BigQuery executes when the view is referenced.
     */
    query: string;
    /**
     * Specifies whether to use BigQuery's legacy SQL for this view. The default value is true. If set to false, the view will use BigQuery's standard SQL
     */
    useLegacySql?: boolean;
}

export interface BigtableAppProfileDataBoostIsolationReadOnly {
    /**
     * The Compute Billing Owner for this Data Boost App Profile. Possible values: ["HOST_PAYS"]
     */
    computeBillingOwner: string;
}

export interface BigtableAppProfileSingleClusterRouting {
    /**
     * If true, CheckAndMutateRow and ReadModifyWriteRow requests are allowed by this app profile.
     * It is unsafe to send these requests to the same table/row/column in multiple clusters.
     */
    allowTransactionalWrites?: boolean;
    /**
     * The cluster to which read/write requests should be routed.
     */
    clusterId: string;
}

export interface BigtableAppProfileStandardIsolation {
    /**
     * The priority of requests sent using this app profile. Possible values: ["PRIORITY_LOW", "PRIORITY_MEDIUM", "PRIORITY_HIGH"]
     */
    priority: string;
}

export interface BigtableAppProfileTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface BigtableAuthorizedViewSubsetView {
    /**
     * Subsets of column families to be included in the authorized view.
     */
    familySubsets?: outputs.BigtableAuthorizedViewSubsetViewFamilySubset[];
    /**
     * Base64-encoded row prefixes to be included in the authorized view. To provide access to all rows, include the empty string as a prefix ("").
     */
    rowPrefixes?: string[];
}

export interface BigtableAuthorizedViewSubsetViewFamilySubset {
    /**
     * Name of the column family to be included in the authorized view.
     */
    familyName: string;
    /**
     * Base64-encoded prefixes for qualifiers of the column family to be included in the authorized view. Every qualifier starting with one of these prefixes is included in the authorized view. To provide access to all qualifiers, include the empty string as a prefix ("").
     */
    qualifierPrefixes?: string[];
    /**
     * Base64-encoded individual exact column qualifiers of the column family to be included in the authorized view.
     */
    qualifiers?: string[];
}

export interface BigtableAuthorizedViewTimeouts {
    create?: string;
    update?: string;
}

export interface BigtableGcPolicyMaxAge {
    /**
     * Number of days before applying GC policy.
     *
     * @deprecated Deprecated
     */
    days: number;
    /**
     * Duration before applying GC policy
     */
    duration: string;
}

export interface BigtableGcPolicyMaxVersion {
    /**
     * Number of version before applying the GC policy.
     */
    number: number;
}

export interface BigtableGcPolicyTimeouts {
    create?: string;
    delete?: string;
}

export interface BigtableInstanceCluster {
    /**
     * A list of Autoscaling configurations. Only one element is used and allowed.
     */
    autoscalingConfig?: outputs.BigtableInstanceClusterAutoscalingConfig;
    /**
     * The ID of the Cloud Bigtable cluster. Must be 6-30 characters and must only contain hyphens, lowercase letters and numbers.
     */
    clusterId: string;
    /**
     * Describes the Cloud KMS encryption key that will be used to protect the destination Bigtable cluster. The requirements for this key are: 1) The Cloud Bigtable service account associated with the project that contains this cluster must be granted the cloudkms.cryptoKeyEncrypterDecrypter role on the CMEK key. 2) Only regional keys can be used and the region of the CMEK key must match the region of the cluster. 3) All clusters within an instance must use the same CMEK key. Values are of the form projects/{project}/locations/{location}/keyRings/{keyring}/cryptoKeys/{key}
     */
    kmsKeyName: string;
    /**
     * The number of nodes in the cluster. If no value is set, Cloud Bigtable automatically allocates nodes based on your data footprint and optimized for 50% storage utilization.
     */
    numNodes: number;
    /**
     * The state of the cluster
     */
    state: string;
    /**
     * The storage type to use. One of "SSD" or "HDD". Defaults to "SSD".
     */
    storageType?: string;
    /**
     * The zone to create the Cloud Bigtable cluster in. Each cluster must have a different zone in the same region. Zones that support Bigtable instances are noted on the Cloud Bigtable locations page.
     */
    zone: string;
}

export interface BigtableInstanceClusterAutoscalingConfig {
    /**
     * The target CPU utilization for autoscaling. Value must be between 10 and 80.
     */
    cpuTarget: number;
    /**
     * The maximum number of nodes for autoscaling.
     */
    maxNodes: number;
    /**
     * The minimum number of nodes for autoscaling.
     */
    minNodes: number;
    /**
     * The target storage utilization for autoscaling, in GB, for each node in a cluster. This number is limited between 2560 (2.5TiB) and 5120 (5TiB) for a SSD cluster and between 8192 (8TiB) and 16384 (16 TiB) for an HDD cluster. If not set, whatever is already set for the cluster will not change, or if the cluster is just being created, it will use the default value of 2560 for SSD clusters and 8192 for HDD clusters.
     */
    storageTarget: number;
}

export interface BigtableInstanceIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface BigtableInstanceIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface BigtableInstanceTimeouts {
    create?: string;
    read?: string;
    update?: string;
}

export interface BigtableTableAutomatedBackupPolicy {
    /**
     * How frequently automated backups should occur.
     */
    frequency: string;
    /**
     * How long the automated backups should be retained.
     */
    retentionPeriod: string;
}

export interface BigtableTableColumnFamily {
    /**
     * The name of the column family.
     */
    family: string;
}

export interface BigtableTableIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface BigtableTableIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface BigtableTableTimeouts {
    create?: string;
    update?: string;
}

export interface BillingAccountIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface BillingAccountIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface BillingBudgetAllUpdatesRule {
    /**
     * Boolean. When set to true, disables default notifications sent
     * when a threshold is exceeded. Default recipients are
     * those with Billing Account Administrators and Billing
     * Account Users IAM roles for the target account.
     */
    disableDefaultIamRecipients?: boolean;
    /**
     * When set to true, and when the budget has a single project configured,
     * notifications will be sent to project level recipients of that project.
     * This field will be ignored if the budget has multiple or no project configured.
     *
     * Currently, project level recipients are the users with Owner role on a cloud project.
     */
    enableProjectLevelRecipients?: boolean;
    /**
     * The full resource name of a monitoring notification
     * channel in the form
     * projects/{project_id}/notificationChannels/{channel_id}.
     * A maximum of 5 channels are allowed.
     */
    monitoringNotificationChannels?: string[];
    /**
     * The name of the Cloud Pub/Sub topic where budget related
     * messages will be published, in the form
     * projects/{project_id}/topics/{topic_id}. Updates are sent
     * at regular intervals to the topic.
     */
    pubsubTopic?: string;
    /**
     * The schema version of the notification. Only "1.0" is
     * accepted. It represents the JSON schema as defined in
     * https://cloud.google.com/billing/docs/how-to/budgets#notification_format.
     */
    schemaVersion?: string;
}

export interface BillingBudgetAmount {
    /**
     * Configures a budget amount that is automatically set to 100% of
     * last period's spend.
     * Boolean. Set value to true to use. Do not set to false, instead
     * use the 'specified_amount' block.
     */
    lastPeriodAmount?: boolean;
    /**
     * A specified amount to use as the budget. currencyCode is
     * optional. If specified, it must match the currency of the
     * billing account. The currencyCode is provided on output.
     */
    specifiedAmount?: outputs.BillingBudgetAmountSpecifiedAmount;
}

export interface BillingBudgetAmountSpecifiedAmount {
    /**
     * The 3-letter currency code defined in ISO 4217.
     */
    currencyCode: string;
    /**
     * Number of nano (10^-9) units of the amount.
     * The value must be between -999,999,999 and +999,999,999
     * inclusive. If units is positive, nanos must be positive or
     * zero. If units is zero, nanos can be positive, zero, or
     * negative. If units is negative, nanos must be negative or
     * zero. For example $-1.75 is represented as units=-1 and
     * nanos=-750,000,000.
     */
    nanos?: number;
    /**
     * The whole units of the amount. For example if currencyCode
     * is "USD", then 1 unit is one US dollar.
     */
    units?: string;
}

export interface BillingBudgetBudgetFilter {
    /**
     * A CalendarPeriod represents the abstract concept of a recurring time period that has a
     * canonical start. Grammatically, "the start of the current CalendarPeriod".
     * All calendar times begin at 12 AM US and Canadian Pacific Time (UTC-8).
     *
     * Exactly one of 'calendar_period', 'custom_period' must be provided. Possible values: ["MONTH", "QUARTER", "YEAR", "CALENDAR_PERIOD_UNSPECIFIED"]
     */
    calendarPeriod?: string;
    /**
     * Optional. If creditTypesTreatment is INCLUDE_SPECIFIED_CREDITS,
     * this is a list of credit types to be subtracted from gross cost to determine the spend for threshold calculations. See a list of acceptable credit type values.
     * If creditTypesTreatment is not INCLUDE_SPECIFIED_CREDITS, this field must be empty.
     *
     * **Note:** If the field has a value in the config and needs to be removed, the field has to be an emtpy array in the config.
     */
    creditTypes: string[];
    /**
     * Specifies how credits should be treated when determining spend
     * for threshold calculations. Default value: "INCLUDE_ALL_CREDITS" Possible values: ["INCLUDE_ALL_CREDITS", "EXCLUDE_ALL_CREDITS", "INCLUDE_SPECIFIED_CREDITS"]
     */
    creditTypesTreatment?: string;
    /**
     * Specifies to track usage from any start date (required) to any end date (optional).
     * This time period is static, it does not recur.
     *
     * Exactly one of 'calendar_period', 'custom_period' must be provided.
     */
    customPeriod?: outputs.BillingBudgetBudgetFilterCustomPeriod;
    /**
     * A single label and value pair specifying that usage from only
     * this set of labeled resources should be included in the budget.
     */
    labels: {[key: string]: string};
    /**
     * A set of projects of the form projects/{project_number},
     * specifying that usage from only this set of projects should be
     * included in the budget. If omitted, the report will include
     * all usage for the billing account, regardless of which project
     * the usage occurred on.
     */
    projects?: string[];
    /**
     * A set of folder and organization names of the form folders/{folderId} or organizations/{organizationId},
     * specifying that usage from only this set of folders and organizations should be included in the budget.
     * If omitted, the budget includes all usage that the billing account pays for. If the folder or organization
     * contains projects that are paid for by a different Cloud Billing account, the budget doesn't apply to those projects.
     */
    resourceAncestors?: string[];
    /**
     * A set of services of the form services/{service_id},
     * specifying that usage from only this set of services should be
     * included in the budget. If omitted, the report will include
     * usage for all the services. The service names are available
     * through the Catalog API:
     * https://cloud.google.com/billing/v1/how-tos/catalog-api.
     */
    services: string[];
    /**
     * A set of subaccounts of the form billingAccounts/{account_id},
     * specifying that usage from only this set of subaccounts should
     * be included in the budget. If a subaccount is set to the name of
     * the parent account, usage from the parent account will be included.
     * If the field is omitted, the report will include usage from the parent
     * account and all subaccounts, if they exist.
     *
     * **Note:** If the field has a value in the config and needs to be removed, the field has to be an emtpy array in the config.
     */
    subaccounts: string[];
}

export interface BillingBudgetBudgetFilterCustomPeriod {
    /**
     * Optional. The end date of the time period. Budgets with elapsed end date won't be processed.
     * If unset, specifies to track all usage incurred since the startDate.
     */
    endDate?: outputs.BillingBudgetBudgetFilterCustomPeriodEndDate;
    /**
     * A start date is required. The start date must be after January 1, 2017.
     */
    startDate: outputs.BillingBudgetBudgetFilterCustomPeriodStartDate;
}

export interface BillingBudgetBudgetFilterCustomPeriodEndDate {
    /**
     * Day of a month. Must be from 1 to 31 and valid for the year and month.
     */
    day: number;
    /**
     * Month of a year. Must be from 1 to 12.
     */
    month: number;
    /**
     * Year of the date. Must be from 1 to 9999.
     */
    year: number;
}

export interface BillingBudgetBudgetFilterCustomPeriodStartDate {
    /**
     * Day of a month. Must be from 1 to 31 and valid for the year and month.
     */
    day: number;
    /**
     * Month of a year. Must be from 1 to 12.
     */
    month: number;
    /**
     * Year of the date. Must be from 1 to 9999.
     */
    year: number;
}

export interface BillingBudgetThresholdRule {
    /**
     * The type of basis used to determine if spend has passed
     * the threshold. Default value: "CURRENT_SPEND" Possible values: ["CURRENT_SPEND", "FORECASTED_SPEND"]
     */
    spendBasis?: string;
    /**
     * Send an alert when this threshold is exceeded. This is a
     * 1.0-based percentage, so 0.5 = 50%. Must be >= 0.
     */
    thresholdPercent: number;
}

export interface BillingBudgetTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface BillingProjectInfoTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface BinaryAuthorizationAttestorAttestationAuthorityNote {
    /**
     * This field will contain the service account email address that
     * this Attestor will use as the principal when querying Container
     * Analysis. Attestor administrators must grant this service account
     * the IAM role needed to read attestations from the noteReference in
     * Container Analysis (containeranalysis.notes.occurrences.viewer).
     * This email address is fixed for the lifetime of the Attestor, but
     * callers should not make any other assumptions about the service
     * account email; future versions may use an email based on a
     * different naming pattern.
     */
    delegationServiceAccountEmail: string;
    /**
     * The resource name of a ATTESTATION_AUTHORITY Note, created by the
     * user. If the Note is in a different project from the Attestor, it
     * should be specified in the format 'projects/*&#47;notes/*' (or the legacy
     * 'providers/*&#47;notes/*'). This field may not be updated.
     * An attestation by this attestor is stored as a Container Analysis
     * ATTESTATION_AUTHORITY Occurrence that names a container image
     * and that links to this Note.
     */
    noteReference: string;
    /**
     * Public keys that verify attestations signed by this attestor. This
     * field may be updated.
     * If this field is non-empty, one of the specified public keys must
     * verify that an attestation was signed by this attestor for the
     * image specified in the admission request.
     * If this field is empty, this attestor always returns that no valid
     * attestations exist.
     */
    publicKeys?: outputs.BinaryAuthorizationAttestorAttestationAuthorityNotePublicKey[];
}

export interface BinaryAuthorizationAttestorAttestationAuthorityNotePublicKey {
    /**
     * ASCII-armored representation of a PGP public key, as the
     * entire output by the command
     * 'gpg --export --armor foo@example.com' (either LF or CRLF
     * line endings). When using this field, id should be left
     * blank. The BinAuthz API handlers will calculate the ID
     * and fill it in automatically. BinAuthz computes this ID
     * as the OpenPGP RFC4880 V4 fingerprint, represented as
     * upper-case hex. If id is provided by the caller, it will
     * be overwritten by the API-calculated ID.
     */
    asciiArmoredPgpPublicKey?: string;
    /**
     * A descriptive comment. This field may be updated.
     */
    comment?: string;
    /**
     * The ID of this public key. Signatures verified by BinAuthz
     * must include the ID of the public key that can be used to
     * verify them, and that ID must match the contents of this
     * field exactly. Additional restrictions on this field can
     * be imposed based on which public key type is encapsulated.
     * See the documentation on publicKey cases below for details.
     */
    id: string;
    /**
     * A raw PKIX SubjectPublicKeyInfo format public key.
     *
     * NOTE: id may be explicitly provided by the caller when using this
     * type of public key, but it MUST be a valid RFC3986 URI. If id is left
     * blank, a default one will be computed based on the digest of the DER
     * encoding of the public key.
     */
    pkixPublicKey?: outputs.BinaryAuthorizationAttestorAttestationAuthorityNotePublicKeyPkixPublicKey;
}

export interface BinaryAuthorizationAttestorAttestationAuthorityNotePublicKeyPkixPublicKey {
    /**
     * A PEM-encoded public key, as described in
     * 'https://tools.ietf.org/html/rfc7468#section-13'
     */
    publicKeyPem?: string;
    /**
     * The signature algorithm used to verify a message against
     * a signature using this key. These signature algorithm must
     * match the structure and any object identifiers encoded in
     * publicKeyPem (i.e. this algorithm must match that of the
     * public key).
     */
    signatureAlgorithm?: string;
}

export interface BinaryAuthorizationAttestorIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface BinaryAuthorizationAttestorIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface BinaryAuthorizationAttestorTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface BinaryAuthorizationPolicyAdmissionWhitelistPattern {
    /**
     * An image name pattern to whitelist, in the form
     * 'registry/path/to/image'. This supports a trailing * as a
     * wildcard, but this is allowed only in text after the registry/
     * part.
     */
    namePattern: string;
}

export interface BinaryAuthorizationPolicyClusterAdmissionRule {
    cluster: string;
    /**
     * The action when a pod creation is denied by the admission rule. Possible values: ["ENFORCED_BLOCK_AND_AUDIT_LOG", "DRYRUN_AUDIT_LOG_ONLY"]
     */
    enforcementMode: string;
    /**
     * How this admission rule will be evaluated. Possible values: ["ALWAYS_ALLOW", "REQUIRE_ATTESTATION", "ALWAYS_DENY"]
     */
    evaluationMode: string;
    /**
     * The resource names of the attestors that must attest to a
     * container image. If the attestor is in a different project from the
     * policy, it should be specified in the format 'projects/*&#47;attestors/*'.
     * Each attestor must exist before a policy can reference it. To add an
     * attestor to a policy the principal issuing the policy change
     * request must be able to read the attestor resource.
     *
     * Note: this field must be non-empty when the evaluation_mode field
     * specifies REQUIRE_ATTESTATION, otherwise it must be empty.
     */
    requireAttestationsBies?: string[];
}

export interface BinaryAuthorizationPolicyDefaultAdmissionRule {
    /**
     * The action when a pod creation is denied by the admission rule. Possible values: ["ENFORCED_BLOCK_AND_AUDIT_LOG", "DRYRUN_AUDIT_LOG_ONLY"]
     */
    enforcementMode: string;
    /**
     * How this admission rule will be evaluated. Possible values: ["ALWAYS_ALLOW", "REQUIRE_ATTESTATION", "ALWAYS_DENY"]
     */
    evaluationMode: string;
    /**
     * The resource names of the attestors that must attest to a
     * container image. If the attestor is in a different project from the
     * policy, it should be specified in the format 'projects/*&#47;attestors/*'.
     * Each attestor must exist before a policy can reference it. To add an
     * attestor to a policy the principal issuing the policy change
     * request must be able to read the attestor resource.
     *
     * Note: this field must be non-empty when the evaluation_mode field
     * specifies REQUIRE_ATTESTATION, otherwise it must be empty.
     */
    requireAttestationsBies?: string[];
}

export interface BinaryAuthorizationPolicyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface BlockchainNodeEngineBlockchainNodesConnectionInfo {
    endpointInfos: outputs.BlockchainNodeEngineBlockchainNodesConnectionInfoEndpointInfo[];
    serviceAttachment: string;
}

export interface BlockchainNodeEngineBlockchainNodesConnectionInfoEndpointInfo {
    jsonRpcApiEndpoint: string;
    websocketsApiEndpoint: string;
}

export interface BlockchainNodeEngineBlockchainNodesEthereumDetails {
    /**
     * User-provided key-value pairs
     */
    additionalEndpoints: outputs.BlockchainNodeEngineBlockchainNodesEthereumDetailsAdditionalEndpoint[];
    /**
     * Enables JSON-RPC access to functions in the admin namespace. Defaults to false.
     */
    apiEnableAdmin?: boolean;
    /**
     * Enables JSON-RPC access to functions in the debug namespace. Defaults to false.
     */
    apiEnableDebug?: boolean;
    /**
     * The consensus client Possible values: ["CONSENSUS_CLIENT_UNSPECIFIED", "LIGHTHOUSE"]
     */
    consensusClient?: string;
    /**
     * The execution client Possible values: ["EXECUTION_CLIENT_UNSPECIFIED", "GETH", "ERIGON"]
     */
    executionClient?: string;
    /**
     * User-provided key-value pairs
     */
    gethDetails?: outputs.BlockchainNodeEngineBlockchainNodesEthereumDetailsGethDetails;
    /**
     * The Ethereum environment being accessed. Possible values: ["MAINNET", "TESTNET_GOERLI_PRATER", "TESTNET_SEPOLIA"]
     */
    network?: string;
    /**
     * The type of Ethereum node. Possible values: ["LIGHT", "FULL", "ARCHIVE"]
     */
    nodeType?: string;
    /**
     * Configuration for validator-related parameters on the beacon client, and for any managed validator client.
     */
    validatorConfig?: outputs.BlockchainNodeEngineBlockchainNodesEthereumDetailsValidatorConfig;
}

export interface BlockchainNodeEngineBlockchainNodesEthereumDetailsAdditionalEndpoint {
    beaconApiEndpoint: string;
    beaconPrometheusMetricsApiEndpoint: string;
    executionClientPrometheusMetricsApiEndpoint: string;
}

export interface BlockchainNodeEngineBlockchainNodesEthereumDetailsGethDetails {
    /**
     * Blockchain garbage collection modes. Only applicable when NodeType is FULL or ARCHIVE. Possible values: ["FULL", "ARCHIVE"]
     */
    garbageCollectionMode?: string;
}

export interface BlockchainNodeEngineBlockchainNodesEthereumDetailsValidatorConfig {
    /**
     * URLs for MEV-relay services to use for block building. When set, a managed MEV-boost service is configured on the beacon client.
     */
    mevRelayUrls?: string[];
}

export interface BlockchainNodeEngineBlockchainNodesTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface CertificateManagerCertificateIssuanceConfigCertificateAuthorityConfig {
    /**
     * Defines a CertificateAuthorityServiceConfig.
     */
    certificateAuthorityServiceConfig?: outputs.CertificateManagerCertificateIssuanceConfigCertificateAuthorityConfigCertificateAuthorityServiceConfig;
}

export interface CertificateManagerCertificateIssuanceConfigCertificateAuthorityConfigCertificateAuthorityServiceConfig {
    /**
     * A CA pool resource used to issue a certificate.
     * The CA pool string has a relative resource path following the form
     * "projects/{project}/locations/{location}/caPools/{caPool}".
     */
    caPool: string;
}

export interface CertificateManagerCertificateIssuanceConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface CertificateManagerCertificateManaged {
    /**
     * Detailed state of the latest authorization attempt for each domain
     * specified for this Managed Certificate.
     */
    authorizationAttemptInfos: outputs.CertificateManagerCertificateManagedAuthorizationAttemptInfo[];
    /**
     * Authorizations that will be used for performing domain authorization. Either issuanceConfig or dnsAuthorizations should be specificed, but not both.
     */
    dnsAuthorizations?: string[];
    /**
     * The domains for which a managed SSL certificate will be generated.
     * Wildcard domains are only supported with DNS challenge resolution
     */
    domains?: string[];
    /**
     * The resource name for a CertificateIssuanceConfig used to configure private PKI certificates in the format projects/*&#47;locations/*&#47;certificateIssuanceConfigs/*.
     * If this field is not set, the certificates will instead be publicly signed as documented at https://cloud.google.com/load-balancing/docs/ssl-certificates/google-managed-certs#caa.
     * Either issuanceConfig or dnsAuthorizations should be specificed, but not both.
     */
    issuanceConfig?: string;
    /**
     * Information about issues with provisioning this Managed Certificate.
     */
    provisioningIssues: outputs.CertificateManagerCertificateManagedProvisioningIssue[];
    /**
     * A state of this Managed Certificate.
     */
    state: string;
}

export interface CertificateManagerCertificateManagedAuthorizationAttemptInfo {
    details: string;
    domain: string;
    failureReason: string;
    state: string;
}

export interface CertificateManagerCertificateManagedProvisioningIssue {
    details: string;
    reason: string;
}

export interface CertificateManagerCertificateMapEntryTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface CertificateManagerCertificateMapGclbTarget {
    ipConfigs: outputs.CertificateManagerCertificateMapGclbTargetIpConfig[];
    targetHttpsProxy: string;
    targetSslProxy: string;
}

export interface CertificateManagerCertificateMapGclbTargetIpConfig {
    ipAddress: string;
    ports: number[];
}

export interface CertificateManagerCertificateMapTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface CertificateManagerCertificateSelfManaged {
    /**
     * The certificate chain in PEM-encoded form.
     *
     * Leaf certificate comes first, followed by intermediate ones if any.
     *
     * @deprecated Deprecated
     */
    certificatePem?: string;
    /**
     * The certificate chain in PEM-encoded form.
     *
     * Leaf certificate comes first, followed by intermediate ones if any.
     */
    pemCertificate?: string;
    /**
     * The private key of the leaf certificate in PEM-encoded form.
     */
    pemPrivateKey?: string;
    /**
     * The private key of the leaf certificate in PEM-encoded form.
     *
     * @deprecated Deprecated
     */
    privateKeyPem?: string;
}

export interface CertificateManagerCertificateTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface CertificateManagerDnsAuthorizationDnsResourceRecord {
    data: string;
    name: string;
    type: string;
}

export interface CertificateManagerDnsAuthorizationTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface CertificateManagerTrustConfigAllowlistedCertificate {
    /**
     * PEM certificate that is allowlisted. The certificate can be up to 5k bytes, and must be a parseable X.509 certificate.
     */
    pemCertificate: string;
}

export interface CertificateManagerTrustConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface CertificateManagerTrustConfigTrustStore {
    /**
     * Set of intermediate CA certificates used for the path building phase of chain validation.
     * The field is currently not supported if trust config is used for the workload certificate feature.
     */
    intermediateCas?: outputs.CertificateManagerTrustConfigTrustStoreIntermediateCa[];
    /**
     * List of Trust Anchors to be used while performing validation against a given TrustStore.
     */
    trustAnchors?: outputs.CertificateManagerTrustConfigTrustStoreTrustAnchor[];
}

export interface CertificateManagerTrustConfigTrustStoreIntermediateCa {
    /**
     * PEM intermediate certificate used for building up paths for validation.
     * Each certificate provided in PEM format may occupy up to 5kB.
     */
    pemCertificate?: string;
}

export interface CertificateManagerTrustConfigTrustStoreTrustAnchor {
    /**
     * PEM root certificate of the PKI used for validation.
     * Each certificate provided in PEM format may occupy up to 5kB.
     */
    pemCertificate?: string;
}

export interface CloudAssetFolderFeedCondition {
    /**
     * Description of the expression. This is a longer text which describes the expression,
     * e.g. when hovered over it in a UI.
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression: string;
    /**
     * String indicating the location of the expression for error reporting, e.g. a file
     * name and a position in the file.
     */
    location?: string;
    /**
     * Title for the expression, i.e. a short string describing its purpose.
     * This can be used e.g. in UIs which allow to enter the expression.
     */
    title?: string;
}

export interface CloudAssetFolderFeedFeedOutputConfig {
    /**
     * Destination on Cloud Pubsub.
     */
    pubsubDestination: outputs.CloudAssetFolderFeedFeedOutputConfigPubsubDestination;
}

export interface CloudAssetFolderFeedFeedOutputConfigPubsubDestination {
    /**
     * Destination on Cloud Pubsub topic.
     */
    topic: string;
}

export interface CloudAssetFolderFeedTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface CloudAssetOrganizationFeedCondition {
    /**
     * Description of the expression. This is a longer text which describes the expression,
     * e.g. when hovered over it in a UI.
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression: string;
    /**
     * String indicating the location of the expression for error reporting, e.g. a file
     * name and a position in the file.
     */
    location?: string;
    /**
     * Title for the expression, i.e. a short string describing its purpose.
     * This can be used e.g. in UIs which allow to enter the expression.
     */
    title?: string;
}

export interface CloudAssetOrganizationFeedFeedOutputConfig {
    /**
     * Destination on Cloud Pubsub.
     */
    pubsubDestination: outputs.CloudAssetOrganizationFeedFeedOutputConfigPubsubDestination;
}

export interface CloudAssetOrganizationFeedFeedOutputConfigPubsubDestination {
    /**
     * Destination on Cloud Pubsub topic.
     */
    topic: string;
}

export interface CloudAssetOrganizationFeedTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface CloudAssetProjectFeedCondition {
    /**
     * Description of the expression. This is a longer text which describes the expression,
     * e.g. when hovered over it in a UI.
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression: string;
    /**
     * String indicating the location of the expression for error reporting, e.g. a file
     * name and a position in the file.
     */
    location?: string;
    /**
     * Title for the expression, i.e. a short string describing its purpose.
     * This can be used e.g. in UIs which allow to enter the expression.
     */
    title?: string;
}

export interface CloudAssetProjectFeedFeedOutputConfig {
    /**
     * Destination on Cloud Pubsub.
     */
    pubsubDestination: outputs.CloudAssetProjectFeedFeedOutputConfigPubsubDestination;
}

export interface CloudAssetProjectFeedFeedOutputConfigPubsubDestination {
    /**
     * Destination on Cloud Pubsub topic.
     */
    topic: string;
}

export interface CloudAssetProjectFeedTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface CloudIdentityGroupAdditionalGroupKey {
    id: string;
    namespace: string;
}

export interface CloudIdentityGroupGroupKey {
    /**
     * The ID of the entity.
     *
     * For Google-managed entities, the id must be the email address of an existing
     * group or user.
     *
     * For external-identity-mapped entities, the id must be a string conforming
     * to the Identity Source's requirements.
     *
     * Must be unique within a namespace.
     */
    id: string;
    /**
     * The namespace in which the entity exists.
     *
     * If not specified, the EntityKey represents a Google-managed entity
     * such as a Google user or a Google Group.
     *
     * If specified, the EntityKey represents an external-identity-mapped group.
     * The namespace must correspond to an identity source created in Admin Console
     * and must be in the form of 'identitysources/{identity_source_id}'.
     */
    namespace?: string;
}

export interface CloudIdentityGroupMembershipPreferredMemberKey {
    /**
     * The ID of the entity.
     *
     * For Google-managed entities, the id must be the email address of an existing
     * group or user.
     *
     * For external-identity-mapped entities, the id must be a string conforming
     * to the Identity Source's requirements.
     *
     * Must be unique within a namespace.
     */
    id: string;
    /**
     * The namespace in which the entity exists.
     *
     * If not specified, the EntityKey represents a Google-managed entity
     * such as a Google user or a Google Group.
     *
     * If specified, the EntityKey represents an external-identity-mapped group.
     * The namespace must correspond to an identity source created in Admin Console
     * and must be in the form of 'identitysources/{identity_source_id}'.
     */
    namespace?: string;
}

export interface CloudIdentityGroupMembershipRole {
    /**
     * The MembershipRole expiry details, only supported for MEMBER role.
     * Other roles cannot be accompanied with MEMBER role having expiry.
     */
    expiryDetail?: outputs.CloudIdentityGroupMembershipRoleExpiryDetail;
    /**
     * The name of the MembershipRole. Must be one of OWNER, MANAGER, MEMBER. Possible values: ["OWNER", "MANAGER", "MEMBER"]
     */
    name: string;
}

export interface CloudIdentityGroupMembershipRoleExpiryDetail {
    /**
     * The time at which the MembershipRole will expire.
     *
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond
     * resolution and up to nine fractional digits.
     *
     * Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
     */
    expireTime: string;
}

export interface CloudIdentityGroupMembershipTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface CloudIdentityGroupTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface CloudIdsEndpointTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface CloudQuotasQuotaPreferenceQuotaConfig {
    /**
     * The annotations map for clients to store small amounts of arbitrary data. Do not put PII or other sensitive information here. See https://google.aip.dev/128#annotations.
     *
     * An object containing a list of "key: value" pairs. Example: '{ "name": "wrench", "mass": "1.3kg", "count": "3" }'.
     */
    annotations?: {[key: string]: string};
    /**
     * Granted quota value.
     */
    grantedValue: string;
    /**
     * The preferred value. Must be greater than or equal to -1. If set to -1, it means the value is "unlimited".
     */
    preferredValue: string;
    /**
     * The origin of the quota preference request.
     */
    requestOrigin: string;
    /**
     * Optional details about the state of this quota preference.
     */
    stateDetail: string;
    /**
     * The trace id that the Google Cloud uses to provision the requested quota. This trace id may be used by the client to contact Cloud support to track the state of a quota preference request. The trace id is only produced for increase requests and is unique for each request. The quota decrease requests do not have a trace id.
     */
    traceId: string;
}

export interface CloudQuotasQuotaPreferenceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface CloudRunDomainMappingMetadata {
    annotations?: {[key: string]: string};
    effectiveAnnotations: {[key: string]: string};
    effectiveLabels: {[key: string]: string};
    /**
     * A sequence number representing a specific generation of the desired state.
     */
    generation: number;
    /**
     * Map of string keys and values that can be used to organize and categorize
     * (scope and select) objects. May match selectors of replication controllers
     * and routes.
     * More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels
     *
     * **Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
     * Please refer to the field 'effective_labels' for all of the labels present on the resource.
     */
    labels?: {[key: string]: string};
    /**
     * In Cloud Run the namespace must be equal to either the
     * project ID or project number.
     */
    namespace: string;
    /**
     * An opaque value that represents the internal version of this object that
     * can be used by clients to determine when objects have changed. May be used
     * for optimistic concurrency, change detection, and the watch operation on a
     * resource or set of resources. They may only be valid for a
     * particular resource or set of resources.
     *
     * More info:
     * https://git.k8s.io/community/contributors/devel/api-conventions.md#concurrency-control-and-consistency
     */
    resourceVersion: string;
    /**
     * SelfLink is a URL representing this object.
     */
    selfLink: string;
    /**
     * The combination of labels configured directly on the resource
     *  and default labels configured on the provider.
     */
    terraformLabels: {[key: string]: string};
    /**
     * UID is a unique id generated by the server on successful creation of a resource and is not
     * allowed to change on PUT operations.
     *
     * More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids
     */
    uid: string;
}

export interface CloudRunDomainMappingSpec {
    /**
     * The mode of the certificate. Default value: "AUTOMATIC" Possible values: ["NONE", "AUTOMATIC"]
     */
    certificateMode?: string;
    /**
     * If set, the mapping will override any mapping set before this spec was set.
     * It is recommended that the user leaves this empty to receive an error
     * warning about a potential conflict and only set it once the respective UI
     * has given such a warning.
     */
    forceOverride?: boolean;
    /**
     * The name of the Cloud Run Service that this DomainMapping applies to.
     * The route must exist.
     */
    routeName: string;
}

export interface CloudRunDomainMappingStatus {
    conditions: outputs.CloudRunDomainMappingStatusCondition[];
    mappedRouteName: string;
    observedGeneration: number;
    resourceRecords: outputs.CloudRunDomainMappingStatusResourceRecord[];
}

export interface CloudRunDomainMappingStatusCondition {
    message: string;
    reason: string;
    status: string;
    type: string;
}

export interface CloudRunDomainMappingStatusResourceRecord {
    name: string;
    rrdata: string;
    type: string;
}

export interface CloudRunDomainMappingTimeouts {
    create?: string;
    delete?: string;
}

export interface CloudRunServiceIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface CloudRunServiceIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface CloudRunServiceMetadata {
    annotations?: {[key: string]: string};
    effectiveAnnotations: {[key: string]: string};
    effectiveLabels: {[key: string]: string};
    /**
     * A sequence number representing a specific generation of the desired state.
     */
    generation: number;
    /**
     * Map of string keys and values that can be used to organize and categorize
     * (scope and select) objects. May match selectors of replication controllers
     * and routes.
     *
     * **Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
     * Please refer to the field 'effective_labels' for all of the labels present on the resource.
     */
    labels?: {[key: string]: string};
    /**
     * In Cloud Run the namespace must be equal to either the
     * project ID or project number.
     */
    namespace: string;
    /**
     * An opaque value that represents the internal version of this object that
     * can be used by clients to determine when objects have changed. May be used
     * for optimistic concurrency, change detection, and the watch operation on a
     * resource or set of resources. They may only be valid for a
     * particular resource or set of resources.
     */
    resourceVersion: string;
    /**
     * SelfLink is a URL representing this object.
     */
    selfLink: string;
    /**
     * The combination of labels configured directly on the resource
     *  and default labels configured on the provider.
     */
    terraformLabels: {[key: string]: string};
    /**
     * UID is a unique id generated by the server on successful creation of a resource and is not
     * allowed to change on PUT operations.
     */
    uid: string;
}

export interface CloudRunServiceStatus {
    conditions: outputs.CloudRunServiceStatusCondition[];
    latestCreatedRevisionName: string;
    latestReadyRevisionName: string;
    observedGeneration: number;
    traffics: outputs.CloudRunServiceStatusTraffic[];
    url: string;
}

export interface CloudRunServiceStatusCondition {
    message: string;
    reason: string;
    status: string;
    type: string;
}

export interface CloudRunServiceStatusTraffic {
    latestRevision: boolean;
    percent: number;
    revisionName: string;
    tag: string;
    url: string;
}

export interface CloudRunServiceTemplate {
    /**
     * Optional metadata for this Revision, including labels and annotations.
     * Name will be generated by the Configuration. To set minimum instances
     * for this revision, use the "autoscaling.knative.dev/minScale" annotation
     * key. To set maximum instances for this revision, use the
     * "autoscaling.knative.dev/maxScale" annotation key. To set Cloud SQL
     * connections for the revision, use the "run.googleapis.com/cloudsql-instances"
     * annotation key.
     */
    metadata?: outputs.CloudRunServiceTemplateMetadata;
    /**
     * RevisionSpec holds the desired state of the Revision (from the client).
     */
    spec?: outputs.CloudRunServiceTemplateSpec;
}

export interface CloudRunServiceTemplateMetadata {
    annotations: {[key: string]: string};
    /**
     * A sequence number representing a specific generation of the desired state.
     */
    generation: number;
    /**
     * Map of string keys and values that can be used to organize and categorize
     * (scope and select) objects.
     */
    labels: {[key: string]: string};
    /**
     * Name must be unique within a Google Cloud project and region.
     * Is required when creating resources. Name is primarily intended
     * for creation idempotence and configuration definition. Cannot be updated.
     */
    name: string;
    /**
     * In Cloud Run the namespace must be equal to either the
     * project ID or project number. It will default to the resource's project.
     */
    namespace: string;
    /**
     * An opaque value that represents the internal version of this object that
     * can be used by clients to determine when objects have changed. May be used
     * for optimistic concurrency, change detection, and the watch operation on a
     * resource or set of resources. They may only be valid for a
     * particular resource or set of resources.
     */
    resourceVersion: string;
    /**
     * SelfLink is a URL representing this object.
     */
    selfLink: string;
    /**
     * UID is a unique id generated by the server on successful creation of a resource and is not
     * allowed to change on PUT operations.
     */
    uid: string;
}

export interface CloudRunServiceTemplateSpec {
    /**
     * ContainerConcurrency specifies the maximum allowed in-flight (concurrent)
     * requests per container of the Revision. If not specified or 0, defaults to 80 when
     * requested CPU >= 1 and defaults to 1 when requested CPU < 1.
     */
    containerConcurrency: number;
    /**
     * Containers defines the unit of execution for this Revision.
     */
    containers?: outputs.CloudRunServiceTemplateSpecContainer[];
    /**
     * Email address of the IAM service account associated with the revision of the
     * service. The service account represents the identity of the running revision,
     * and determines what permissions the revision has. If not provided, the revision
     * will use the project's default service account.
     */
    serviceAccountName: string;
    /**
     * ServingState holds a value describing the state the resources
     * are in for this Revision.
     * It is expected
     * that the system will manipulate this based on routability and load.
     *
     * @deprecated Deprecated
     */
    servingState: string;
    /**
     * TimeoutSeconds holds the max duration the instance is allowed for responding to a request.
     */
    timeoutSeconds: number;
    /**
     * Volume represents a named volume in a container.
     */
    volumes?: outputs.CloudRunServiceTemplateSpecVolume[];
}

export interface CloudRunServiceTemplateSpecContainer {
    /**
     * Arguments to the entrypoint.
     * The docker image's CMD is used if this is not provided.
     */
    args?: string[];
    /**
     * Entrypoint array. Not executed within a shell.
     * The docker image's ENTRYPOINT is used if this is not provided.
     */
    commands?: string[];
    /**
     * List of sources to populate environment variables in the container.
     * All invalid keys will be reported as an event when the container is starting.
     * When a key exists in multiple sources, the value associated with the last source will
     * take precedence. Values defined by an Env with a duplicate key will take
     * precedence.
     *
     * @deprecated Deprecated
     */
    envFroms?: outputs.CloudRunServiceTemplateSpecContainerEnvFrom[];
    /**
     * List of environment variables to set in the container.
     */
    envs?: outputs.CloudRunServiceTemplateSpecContainerEnv[];
    /**
     * Docker image name. This is most often a reference to a container located
     * in the container registry, such as gcr.io/cloudrun/hello
     */
    image: string;
    /**
     * Periodic probe of container liveness. Container will be restarted if the probe fails.
     */
    livenessProbe?: outputs.CloudRunServiceTemplateSpecContainerLivenessProbe;
    /**
     * Name of the container
     */
    name: string;
    /**
     * List of open ports in the container.
     */
    ports?: outputs.CloudRunServiceTemplateSpecContainerPort[];
    /**
     * Compute Resources required by this container. Used to set values such as max memory
     */
    resources?: outputs.CloudRunServiceTemplateSpecContainerResources;
    /**
     * Startup probe of application within the container.
     * All other probes are disabled if a startup probe is provided, until it
     * succeeds. Container will not be added to service endpoints if the probe fails.
     */
    startupProbe?: outputs.CloudRunServiceTemplateSpecContainerStartupProbe;
    /**
     * Volume to mount into the container's filesystem.
     * Only supports SecretVolumeSources.
     */
    volumeMounts?: outputs.CloudRunServiceTemplateSpecContainerVolumeMount[];
    /**
     * Container's working directory.
     * If not specified, the container runtime's default will be used, which
     * might be configured in the container image.
     *
     * @deprecated Deprecated
     */
    workingDir?: string;
}

export interface CloudRunServiceTemplateSpecContainerEnv {
    /**
     * Name of the environment variable.
     */
    name?: string;
    /**
     * Defaults to "".
     */
    value?: string;
    /**
     * Source for the environment variable's value. Only supports secret_key_ref.
     */
    valueFrom?: outputs.CloudRunServiceTemplateSpecContainerEnvValueFrom;
}

export interface CloudRunServiceTemplateSpecContainerEnvFrom {
    /**
     * The ConfigMap to select from.
     */
    configMapRef?: outputs.CloudRunServiceTemplateSpecContainerEnvFromConfigMapRef;
    /**
     * An optional identifier to prepend to each key in the ConfigMap.
     */
    prefix?: string;
    /**
     * The Secret to select from.
     */
    secretRef?: outputs.CloudRunServiceTemplateSpecContainerEnvFromSecretRef;
}

export interface CloudRunServiceTemplateSpecContainerEnvFromConfigMapRef {
    /**
     * The ConfigMap to select from.
     */
    localObjectReference?: outputs.CloudRunServiceTemplateSpecContainerEnvFromConfigMapRefLocalObjectReference;
    /**
     * Specify whether the ConfigMap must be defined
     */
    optional?: boolean;
}

export interface CloudRunServiceTemplateSpecContainerEnvFromConfigMapRefLocalObjectReference {
    /**
     * Name of the referent.
     */
    name: string;
}

export interface CloudRunServiceTemplateSpecContainerEnvFromSecretRef {
    /**
     * The Secret to select from.
     */
    localObjectReference?: outputs.CloudRunServiceTemplateSpecContainerEnvFromSecretRefLocalObjectReference;
    /**
     * Specify whether the Secret must be defined
     */
    optional?: boolean;
}

export interface CloudRunServiceTemplateSpecContainerEnvFromSecretRefLocalObjectReference {
    /**
     * Name of the referent.
     */
    name: string;
}

export interface CloudRunServiceTemplateSpecContainerEnvValueFrom {
    /**
     * Selects a key (version) of a secret in Secret Manager.
     */
    secretKeyRef: outputs.CloudRunServiceTemplateSpecContainerEnvValueFromSecretKeyRef;
}

export interface CloudRunServiceTemplateSpecContainerEnvValueFromSecretKeyRef {
    /**
     * A Cloud Secret Manager secret version. Must be 'latest' for the latest
     * version or an integer for a specific version.
     */
    key: string;
    /**
     * The name of the secret in Cloud Secret Manager. By default, the secret is assumed to be in the same project.
     * If the secret is in another project, you must define an alias.
     * An alias definition has the form: :projects/{project-id|project-number}/secrets/.
     * If multiple alias definitions are needed, they must be separated by commas.
     * The alias definitions must be set on the run.googleapis.com/secrets annotation.
     */
    name: string;
}

export interface CloudRunServiceTemplateSpecContainerLivenessProbe {
    /**
     * Minimum consecutive failures for the probe to be considered failed after
     * having succeeded. Defaults to 3. Minimum value is 1.
     */
    failureThreshold?: number;
    /**
     * GRPC specifies an action involving a GRPC port.
     */
    grpc?: outputs.CloudRunServiceTemplateSpecContainerLivenessProbeGrpc;
    /**
     * HttpGet specifies the http request to perform.
     */
    httpGet?: outputs.CloudRunServiceTemplateSpecContainerLivenessProbeHttpGet;
    /**
     * Number of seconds after the container has started before the probe is
     * initiated.
     * Defaults to 0 seconds. Minimum value is 0. Maximum value is 3600.
     */
    initialDelaySeconds?: number;
    /**
     * How often (in seconds) to perform the probe.
     * Default to 10 seconds. Minimum value is 1. Maximum value is 3600.
     */
    periodSeconds?: number;
    /**
     * Number of seconds after which the probe times out.
     * Defaults to 1 second. Minimum value is 1. Maximum value is 3600.
     * Must be smaller than period_seconds.
     */
    timeoutSeconds?: number;
}

export interface CloudRunServiceTemplateSpecContainerLivenessProbeGrpc {
    /**
     * Port number to access on the container. Number must be in the range 1 to 65535.
     * If not specified, defaults to the same value as container.ports[0].containerPort.
     */
    port: number;
    /**
     * The name of the service to place in the gRPC HealthCheckRequest
     * (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
     * If this is not specified, the default behavior is defined by gRPC.
     */
    service?: string;
}

export interface CloudRunServiceTemplateSpecContainerLivenessProbeHttpGet {
    /**
     * Custom headers to set in the request. HTTP allows repeated headers.
     */
    httpHeaders?: outputs.CloudRunServiceTemplateSpecContainerLivenessProbeHttpGetHttpHeader[];
    /**
     * Path to access on the HTTP server. If set, it should not be empty string.
     */
    path?: string;
    /**
     * Port number to access on the container. Number must be in the range 1 to 65535.
     * If not specified, defaults to the same value as container.ports[0].containerPort.
     */
    port: number;
}

export interface CloudRunServiceTemplateSpecContainerLivenessProbeHttpGetHttpHeader {
    /**
     * The header field name.
     */
    name: string;
    /**
     * The header field value.
     */
    value?: string;
}

export interface CloudRunServiceTemplateSpecContainerPort {
    /**
     * Port number the container listens on. This must be a valid port number (between 1 and 65535). Defaults to "8080".
     */
    containerPort?: number;
    /**
     * If specified, used to specify which protocol to use. Allowed values are "http1" (HTTP/1) and "h2c" (HTTP/2 end-to-end). Defaults to "http1".
     */
    name: string;
    /**
     * Protocol for port. Must be "TCP". Defaults to "TCP".
     */
    protocol?: string;
}

export interface CloudRunServiceTemplateSpecContainerResources {
    /**
     * Limits describes the maximum amount of compute resources allowed.
     * The values of the map is string form of the 'quantity' k8s type:
     * https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apimachinery/pkg/api/resource/quantity.go
     */
    limits: {[key: string]: string};
    /**
     * Requests describes the minimum amount of compute resources required.
     * If Requests is omitted for a container, it defaults to Limits if that is
     * explicitly specified, otherwise to an implementation-defined value.
     * The values of the map is string form of the 'quantity' k8s type:
     * https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apimachinery/pkg/api/resource/quantity.go
     */
    requests?: {[key: string]: string};
}

export interface CloudRunServiceTemplateSpecContainerStartupProbe {
    /**
     * Minimum consecutive failures for the probe to be considered failed after
     * having succeeded. Defaults to 3. Minimum value is 1.
     */
    failureThreshold?: number;
    /**
     * GRPC specifies an action involving a GRPC port.
     */
    grpc?: outputs.CloudRunServiceTemplateSpecContainerStartupProbeGrpc;
    /**
     * HttpGet specifies the http request to perform.
     */
    httpGet?: outputs.CloudRunServiceTemplateSpecContainerStartupProbeHttpGet;
    /**
     * Number of seconds after the container has started before the probe is
     * initiated.
     * Defaults to 0 seconds. Minimum value is 0. Maximum value is 240.
     */
    initialDelaySeconds?: number;
    /**
     * How often (in seconds) to perform the probe.
     * Default to 10 seconds. Minimum value is 1. Maximum value is 240.
     */
    periodSeconds?: number;
    /**
     * TcpSocket specifies an action involving a TCP port.
     */
    tcpSocket?: outputs.CloudRunServiceTemplateSpecContainerStartupProbeTcpSocket;
    /**
     * Number of seconds after which the probe times out.
     * Defaults to 1 second. Minimum value is 1. Maximum value is 3600.
     * Must be smaller than periodSeconds.
     */
    timeoutSeconds?: number;
}

export interface CloudRunServiceTemplateSpecContainerStartupProbeGrpc {
    /**
     * Port number to access on the container. Number must be in the range 1 to 65535.
     * If not specified, defaults to the same value as container.ports[0].containerPort.
     */
    port: number;
    /**
     * The name of the service to place in the gRPC HealthCheckRequest
     * (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
     * If this is not specified, the default behavior is defined by gRPC.
     */
    service?: string;
}

export interface CloudRunServiceTemplateSpecContainerStartupProbeHttpGet {
    /**
     * Custom headers to set in the request. HTTP allows repeated headers.
     */
    httpHeaders?: outputs.CloudRunServiceTemplateSpecContainerStartupProbeHttpGetHttpHeader[];
    /**
     * Path to access on the HTTP server. If set, it should not be empty string.
     */
    path?: string;
    /**
     * Port number to access on the container. Number must be in the range 1 to 65535.
     * If not specified, defaults to the same value as container.ports[0].containerPort.
     */
    port: number;
}

export interface CloudRunServiceTemplateSpecContainerStartupProbeHttpGetHttpHeader {
    /**
     * The header field name.
     */
    name: string;
    /**
     * The header field value.
     */
    value?: string;
}

export interface CloudRunServiceTemplateSpecContainerStartupProbeTcpSocket {
    /**
     * Port number to access on the container. Number must be in the range 1 to 65535.
     * If not specified, defaults to the same value as container.ports[0].containerPort.
     */
    port: number;
}

export interface CloudRunServiceTemplateSpecContainerVolumeMount {
    /**
     * Path within the container at which the volume should be mounted.  Must
     * not contain ':'.
     */
    mountPath: string;
    /**
     * This must match the Name of a Volume.
     */
    name: string;
}

export interface CloudRunServiceTemplateSpecVolume {
    /**
     * Volume's name.
     */
    name: string;
    /**
     * The secret's value will be presented as the content of a file whose
     * name is defined in the item path. If no items are defined, the name of
     * the file is the secret_name.
     */
    secret?: outputs.CloudRunServiceTemplateSpecVolumeSecret;
}

export interface CloudRunServiceTemplateSpecVolumeSecret {
    /**
     * Mode bits to use on created files by default. Must be a value between 0000
     * and 0777. Defaults to 0644. Directories within the path are not affected by
     * this setting. This might be in conflict with other options that affect the
     * file mode, like fsGroup, and the result can be other mode bits set.
     */
    defaultMode?: number;
    /**
     * If unspecified, the volume will expose a file whose name is the
     * secret_name.
     * If specified, the key will be used as the version to fetch from Cloud
     * Secret Manager and the path will be the name of the file exposed in the
     * volume. When items are defined, they must specify a key and a path.
     */
    items?: outputs.CloudRunServiceTemplateSpecVolumeSecretItem[];
    /**
     * The name of the secret in Cloud Secret Manager. By default, the secret
     * is assumed to be in the same project.
     * If the secret is in another project, you must define an alias.
     * An alias definition has the form:
     * {alias}:projects/{project-id|project-number}/secrets/{secret-name}.
     * If multiple alias definitions are needed, they must be separated by
     * commas.
     * The alias definitions must be set on the run.googleapis.com/secrets
     * annotation.
     */
    secretName: string;
}

export interface CloudRunServiceTemplateSpecVolumeSecretItem {
    /**
     * The Cloud Secret Manager secret version.
     * Can be 'latest' for the latest value or an integer for a specific version.
     */
    key: string;
    /**
     * Mode bits to use on this file, must be a value between 0000 and 0777. If
     * not specified, the volume defaultMode will be used. This might be in
     * conflict with other options that affect the file mode, like fsGroup, and
     * the result can be other mode bits set.
     */
    mode?: number;
    /**
     * The relative path of the file to map the key to.
     * May not be an absolute path.
     * May not contain the path element '..'.
     * May not start with the string '..'.
     */
    path: string;
}

export interface CloudRunServiceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface CloudRunServiceTraffic {
    /**
     * LatestRevision may be optionally provided to indicate that the latest ready
     * Revision of the Configuration should be used for this traffic target. When
     * provided LatestRevision must be true if RevisionName is empty; it must be
     * false when RevisionName is non-empty.
     */
    latestRevision?: boolean;
    /**
     * Percent specifies percent of the traffic to this Revision or Configuration.
     */
    percent: number;
    /**
     * RevisionName of a specific revision to which to send this portion of traffic.
     */
    revisionName?: string;
    /**
     * Tag is optionally used to expose a dedicated url for referencing this target exclusively.
     */
    tag?: string;
    /**
     * URL displays the URL for accessing tagged traffic targets. URL is displayed in status,
     * and is disallowed on spec. URL must contain a scheme (e.g. http://) and a hostname,
     * but may not contain anything else (e.g. basic auth, url path, etc.)
     */
    url: string;
}

export interface CloudRunV2JobBinaryAuthorization {
    /**
     * If present, indicates to use Breakglass using this justification. If useDefault is False, then it must be empty. For more information on breakglass, see https://cloud.google.com/binary-authorization/docs/using-breakglass
     */
    breakglassJustification?: string;
    /**
     * The path to a binary authorization policy. Format: projects/{project}/platforms/cloudRun/{policy-name}
     */
    policy?: string;
    /**
     * If True, indicates to use the default project's binary authorization policy. If False, binary authorization will be disabled.
     */
    useDefault?: boolean;
}

export interface CloudRunV2JobCondition {
    executionReason: string;
    lastTransitionTime: string;
    message: string;
    reason: string;
    revisionReason: string;
    severity: string;
    state: string;
    type: string;
}

export interface CloudRunV2JobIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface CloudRunV2JobIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface CloudRunV2JobLatestCreatedExecution {
    completionTime: string;
    createTime: string;
    name: string;
}

export interface CloudRunV2JobTemplate {
    /**
     * Unstructured key value map that may be set by external tools to store and arbitrary metadata. They are not queryable and should be preserved when modifying objects.
     *
     * Cloud Run API v2 does not support annotations with 'run.googleapis.com', 'cloud.googleapis.com', 'serving.knative.dev', or 'autoscaling.knative.dev' namespaces, and they will be rejected.
     * All system annotations in v1 now have a corresponding field in v2 ExecutionTemplate.
     *
     * This field follows Kubernetes annotations' namespacing, limits, and rules.
     */
    annotations?: {[key: string]: string};
    /**
     * Unstructured key value map that can be used to organize and categorize objects. User-provided labels are shared with Google's billing system, so they can be used to filter,
     * or break down billing charges by team, component, environment, state, etc. For more information, visit https://cloud.google.com/resource-manager/docs/creating-managing-labels or
     * https://cloud.google.com/run/docs/configuring/labels.
     *
     * Cloud Run API v2 does not support labels with 'run.googleapis.com', 'cloud.googleapis.com', 'serving.knative.dev', or 'autoscaling.knative.dev' namespaces, and they will be rejected.
     * All system labels in v1 now have a corresponding field in v2 ExecutionTemplate.
     */
    labels?: {[key: string]: string};
    /**
     * Specifies the maximum desired number of tasks the execution should run at given time. Must be <= taskCount. When the job is run, if this field is 0 or unset, the maximum possible value will be used for that execution. The actual number of tasks running in steady state will be less than this number when there are fewer tasks waiting to be completed remaining, i.e. when the work left to do is less than max parallelism.
     */
    parallelism: number;
    /**
     * Specifies the desired number of tasks the execution should run. Setting to 1 means that parallelism is limited to 1 and the success of that task signals the success of the execution. More info: https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/
     */
    taskCount: number;
    /**
     * Describes the task(s) that will be created when executing an execution
     */
    template: outputs.CloudRunV2JobTemplateTemplate;
}

export interface CloudRunV2JobTemplateTemplate {
    /**
     * Holds the single container that defines the unit of execution for this task.
     */
    containers?: outputs.CloudRunV2JobTemplateTemplateContainer[];
    /**
     * A reference to a customer managed encryption key (CMEK) to use to encrypt this container image. For more information, go to https://cloud.google.com/run/docs/securing/using-cmek
     */
    encryptionKey?: string;
    /**
     * The execution environment being used to host this Task. Possible values: ["EXECUTION_ENVIRONMENT_GEN1", "EXECUTION_ENVIRONMENT_GEN2"]
     */
    executionEnvironment: string;
    /**
     * Number of retries allowed per Task, before marking this Task failed.
     */
    maxRetries?: number;
    /**
     * Email address of the IAM service account associated with the Task of a Job. The service account represents the identity of the running task, and determines what permissions the task has. If not provided, the task will use the project's default service account.
     */
    serviceAccount: string;
    /**
     * Max allowed time duration the Task may be active before the system will actively try to mark it failed and kill associated containers. This applies per attempt of a task, meaning each retry can run for the full timeout.
     *
     * A duration in seconds with up to nine fractional digits, ending with 's'. Example: "3.5s".
     */
    timeout: string;
    /**
     * A list of Volumes to make available to containers.
     */
    volumes?: outputs.CloudRunV2JobTemplateTemplateVolume[];
    /**
     * VPC Access configuration to use for this Task. For more information, visit https://cloud.google.com/run/docs/configuring/connecting-vpc.
     */
    vpcAccess?: outputs.CloudRunV2JobTemplateTemplateVpcAccess;
}

export interface CloudRunV2JobTemplateTemplateContainer {
    /**
     * Arguments to the entrypoint. The docker image's CMD is used if this is not provided. Variable references are not supported in Cloud Run.
     */
    args?: string[];
    /**
     * Entrypoint array. Not executed within a shell. The docker image's ENTRYPOINT is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
     */
    commands?: string[];
    /**
     * List of environment variables to set in the container.
     */
    envs?: outputs.CloudRunV2JobTemplateTemplateContainerEnv[];
    /**
     * URL of the Container image in Google Container Registry or Google Artifact Registry. More info: https://kubernetes.io/docs/concepts/containers/images
     */
    image: string;
    /**
     * Name of the container specified as a DNS_LABEL.
     */
    name?: string;
    /**
     * List of ports to expose from the container. Only a single port can be specified. The specified ports must be listening on all interfaces (0.0.0.0) within the container to be accessible.
     *
     * If omitted, a port number will be chosen and passed to the container through the PORT environment variable for the container to listen on
     */
    ports?: outputs.CloudRunV2JobTemplateTemplateContainerPort[];
    /**
     * Compute Resource requirements by this container. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#resources
     */
    resources?: outputs.CloudRunV2JobTemplateTemplateContainerResources;
    /**
     * Volume to mount into the container's filesystem.
     */
    volumeMounts?: outputs.CloudRunV2JobTemplateTemplateContainerVolumeMount[];
    /**
     * Container's working directory. If not specified, the container runtime's default will be used, which might be configured in the container image.
     */
    workingDir?: string;
}

export interface CloudRunV2JobTemplateTemplateContainerEnv {
    /**
     * Name of the environment variable. Must be a C_IDENTIFIER, and mnay not exceed 32768 characters.
     */
    name: string;
    /**
     * Literal value of the environment variable. Defaults to "" and the maximum allowed length is 32768 characters. Variable references are not supported in Cloud Run.
     */
    value?: string;
    /**
     * Source for the environment variable's value.
     */
    valueSource?: outputs.CloudRunV2JobTemplateTemplateContainerEnvValueSource;
}

export interface CloudRunV2JobTemplateTemplateContainerEnvValueSource {
    /**
     * Selects a secret and a specific version from Cloud Secret Manager.
     */
    secretKeyRef?: outputs.CloudRunV2JobTemplateTemplateContainerEnvValueSourceSecretKeyRef;
}

export interface CloudRunV2JobTemplateTemplateContainerEnvValueSourceSecretKeyRef {
    /**
     * The name of the secret in Cloud Secret Manager. Format: {secretName} if the secret is in the same project. projects/{project}/secrets/{secretName} if the secret is in a different project.
     */
    secret: string;
    /**
     * The Cloud Secret Manager secret version. Can be 'latest' for the latest value or an integer for a specific version.
     */
    version: string;
}

export interface CloudRunV2JobTemplateTemplateContainerPort {
    /**
     * Port number the container listens on. This must be a valid TCP port number, 0 < containerPort < 65536.
     */
    containerPort?: number;
    /**
     * If specified, used to specify which protocol to use. Allowed values are "http1" and "h2c".
     */
    name?: string;
}

export interface CloudRunV2JobTemplateTemplateContainerResources {
    /**
     * Only memory and CPU are supported. Use key 'cpu' for CPU limit and 'memory' for memory limit. Note: The only supported values for CPU are '1', '2', '4', and '8'. Setting 4 CPU requires at least 2Gi of memory. The values of the map is string form of the 'quantity' k8s type: https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apimachinery/pkg/api/resource/quantity.go
     */
    limits: {[key: string]: string};
}

export interface CloudRunV2JobTemplateTemplateContainerVolumeMount {
    /**
     * Path within the container at which the volume should be mounted. Must not contain ':'. For Cloud SQL volumes, it can be left empty, or must otherwise be /cloudsql. All instances defined in the Volume will be available as /cloudsql/[instance]. For more information on Cloud SQL volumes, visit https://cloud.google.com/sql/docs/mysql/connect-run
     */
    mountPath: string;
    /**
     * This must match the Name of a Volume.
     */
    name: string;
}

export interface CloudRunV2JobTemplateTemplateVolume {
    /**
     * For Cloud SQL volumes, contains the specific instances that should be mounted. Visit https://cloud.google.com/sql/docs/mysql/connect-run for more information on how to connect Cloud SQL and Cloud Run.
     */
    cloudSqlInstance?: outputs.CloudRunV2JobTemplateTemplateVolumeCloudSqlInstance;
    /**
     * Volume's name.
     */
    name: string;
    /**
     * Secret represents a secret that should populate this volume. More info: https://kubernetes.io/docs/concepts/storage/volumes#secret
     */
    secret?: outputs.CloudRunV2JobTemplateTemplateVolumeSecret;
}

export interface CloudRunV2JobTemplateTemplateVolumeCloudSqlInstance {
    /**
     * The Cloud SQL instance connection names, as can be found in https://console.cloud.google.com/sql/instances. Visit https://cloud.google.com/sql/docs/mysql/connect-run for more information on how to connect Cloud SQL and Cloud Run. Format: {project}:{location}:{instance}
     */
    instances?: string[];
}

export interface CloudRunV2JobTemplateTemplateVolumeSecret {
    /**
     * Integer representation of mode bits to use on created files by default. Must be a value between 0000 and 0777 (octal), defaulting to 0444. Directories within the path are not affected by this setting.
     */
    defaultMode?: number;
    /**
     * If unspecified, the volume will expose a file whose name is the secret, relative to VolumeMount.mount_path. If specified, the key will be used as the version to fetch from Cloud Secret Manager and the path will be the name of the file exposed in the volume. When items are defined, they must specify a path and a version.
     */
    items?: outputs.CloudRunV2JobTemplateTemplateVolumeSecretItem[];
    /**
     * The name of the secret in Cloud Secret Manager. Format: {secret} if the secret is in the same project. projects/{project}/secrets/{secret} if the secret is in a different project.
     */
    secret: string;
}

export interface CloudRunV2JobTemplateTemplateVolumeSecretItem {
    /**
     * Integer octal mode bits to use on this file, must be a value between 01 and 0777 (octal). If 0 or not set, the Volume's default mode will be used.
     */
    mode?: number;
    /**
     * The relative path of the secret in the container.
     */
    path: string;
    /**
     * The Cloud Secret Manager secret version. Can be 'latest' for the latest value or an integer for a specific version
     */
    version: string;
}

export interface CloudRunV2JobTemplateTemplateVpcAccess {
    /**
     * VPC Access connector name. Format: projects/{project}/locations/{location}/connectors/{connector}, where {project} can be project id or number.
     */
    connector?: string;
    /**
     * Traffic VPC egress settings. Possible values: ["ALL_TRAFFIC", "PRIVATE_RANGES_ONLY"]
     */
    egress: string;
    /**
     * Direct VPC egress settings. Currently only single network interface is supported.
     */
    networkInterfaces?: outputs.CloudRunV2JobTemplateTemplateVpcAccessNetworkInterface[];
}

export interface CloudRunV2JobTemplateTemplateVpcAccessNetworkInterface {
    /**
     * The VPC network that the Cloud Run resource will be able to send traffic to. At least one of network or subnetwork must be specified. If both
     * network and subnetwork are specified, the given VPC subnetwork must belong to the given VPC network. If network is not specified, it will be
     * looked up from the subnetwork.
     */
    network: string;
    /**
     * The VPC subnetwork that the Cloud Run resource will get IPs from. At least one of network or subnetwork must be specified. If both
     * network and subnetwork are specified, the given VPC subnetwork must belong to the given VPC network. If subnetwork is not specified, the
     * subnetwork with the same name with the network will be used.
     */
    subnetwork: string;
    /**
     * Network tags applied to this Cloud Run job.
     */
    tags?: string[];
}

export interface CloudRunV2JobTerminalCondition {
    executionReason: string;
    lastTransitionTime: string;
    message: string;
    reason: string;
    revisionReason: string;
    severity: string;
    state: string;
    type: string;
}

export interface CloudRunV2JobTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface CloudRunV2ServiceBinaryAuthorization {
    /**
     * If present, indicates to use Breakglass using this justification. If useDefault is False, then it must be empty. For more information on breakglass, see https://cloud.google.com/binary-authorization/docs/using-breakglass
     */
    breakglassJustification?: string;
    /**
     * The path to a binary authorization policy. Format: projects/{project}/platforms/cloudRun/{policy-name}
     */
    policy?: string;
    /**
     * If True, indicates to use the default project's binary authorization policy. If False, binary authorization will be disabled.
     */
    useDefault?: boolean;
}

export interface CloudRunV2ServiceCondition {
    executionReason: string;
    lastTransitionTime: string;
    message: string;
    reason: string;
    revisionReason: string;
    severity: string;
    state: string;
    type: string;
}

export interface CloudRunV2ServiceIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface CloudRunV2ServiceIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface CloudRunV2ServiceTemplate {
    /**
     * Unstructured key value map that may be set by external tools to store and arbitrary metadata. They are not queryable and should be preserved when modifying objects.
     *
     * Cloud Run API v2 does not support annotations with 'run.googleapis.com', 'cloud.googleapis.com', 'serving.knative.dev', or 'autoscaling.knative.dev' namespaces, and they will be rejected.
     * All system annotations in v1 now have a corresponding field in v2 RevisionTemplate.
     *
     * This field follows Kubernetes annotations' namespacing, limits, and rules.
     */
    annotations?: {[key: string]: string};
    /**
     * Holds the containers that define the unit of execution for this Service.
     */
    containers?: outputs.CloudRunV2ServiceTemplateContainer[];
    /**
     * A reference to a customer managed encryption key (CMEK) to use to encrypt this container image. For more information, go to https://cloud.google.com/run/docs/securing/using-cmek
     */
    encryptionKey?: string;
    /**
     * The sandbox environment to host this Revision. Possible values: ["EXECUTION_ENVIRONMENT_GEN1", "EXECUTION_ENVIRONMENT_GEN2"]
     */
    executionEnvironment?: string;
    /**
     * Unstructured key value map that can be used to organize and categorize objects. User-provided labels are shared with Google's billing system, so they can be used to filter, or break down billing charges by team, component, environment, state, etc.
     * For more information, visit https://cloud.google.com/resource-manager/docs/creating-managing-labels or https://cloud.google.com/run/docs/configuring/labels.
     *
     * Cloud Run API v2 does not support labels with 'run.googleapis.com', 'cloud.googleapis.com', 'serving.knative.dev', or 'autoscaling.knative.dev' namespaces, and they will be rejected.
     * All system labels in v1 now have a corresponding field in v2 RevisionTemplate.
     */
    labels?: {[key: string]: string};
    /**
     * Sets the maximum number of requests that each serving instance can receive.
     * If not specified or 0, defaults to 80 when requested CPU >= 1 and defaults to 1 when requested CPU < 1.
     */
    maxInstanceRequestConcurrency: number;
    /**
     * The unique name for the revision. If this field is omitted, it will be automatically generated based on the Service name.
     */
    revision?: string;
    /**
     * Scaling settings for this Revision.
     */
    scaling?: outputs.CloudRunV2ServiceTemplateScaling;
    /**
     * Email address of the IAM service account associated with the revision of the service. The service account represents the identity of the running revision, and determines what permissions the revision has. If not provided, the revision will use the project's default service account.
     */
    serviceAccount: string;
    /**
     * Enables session affinity. For more information, go to https://cloud.google.com/run/docs/configuring/session-affinity
     */
    sessionAffinity?: boolean;
    /**
     * Max allowed time for an instance to respond to a request.
     *
     * A duration in seconds with up to nine fractional digits, ending with 's'. Example: "3.5s".
     */
    timeout: string;
    /**
     * A list of Volumes to make available to containers.
     */
    volumes?: outputs.CloudRunV2ServiceTemplateVolume[];
    /**
     * VPC Access configuration to use for this Task. For more information, visit https://cloud.google.com/run/docs/configuring/connecting-vpc.
     */
    vpcAccess?: outputs.CloudRunV2ServiceTemplateVpcAccess;
}

export interface CloudRunV2ServiceTemplateContainer {
    /**
     * Arguments to the entrypoint. The docker image's CMD is used if this is not provided. Variable references are not supported in Cloud Run.
     */
    args?: string[];
    /**
     * Entrypoint array. Not executed within a shell. The docker image's ENTRYPOINT is used if this is not provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not. More info: https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell
     */
    commands?: string[];
    /**
     * Containers which should be started before this container. If specified the container will wait to start until all containers with the listed names are healthy.
     */
    dependsOns?: string[];
    /**
     * List of environment variables to set in the container.
     */
    envs?: outputs.CloudRunV2ServiceTemplateContainerEnv[];
    /**
     * URL of the Container image in Google Container Registry or Google Artifact Registry. More info: https://kubernetes.io/docs/concepts/containers/images
     */
    image: string;
    /**
     * Periodic probe of container liveness. Container will be restarted if the probe fails. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
     */
    livenessProbe?: outputs.CloudRunV2ServiceTemplateContainerLivenessProbe;
    /**
     * Name of the container specified as a DNS_LABEL.
     */
    name?: string;
    /**
     * List of ports to expose from the container. Only a single port can be specified. The specified ports must be listening on all interfaces (0.0.0.0) within the container to be accessible.
     *
     * If omitted, a port number will be chosen and passed to the container through the PORT environment variable for the container to listen on
     */
    ports?: outputs.CloudRunV2ServiceTemplateContainerPorts;
    /**
     * Compute Resource requirements by this container. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#resources
     */
    resources?: outputs.CloudRunV2ServiceTemplateContainerResources;
    /**
     * Startup probe of application within the container. All other probes are disabled if a startup probe is provided, until it succeeds. Container will not be added to service endpoints if the probe fails. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
     */
    startupProbe?: outputs.CloudRunV2ServiceTemplateContainerStartupProbe;
    /**
     * Volume to mount into the container's filesystem.
     */
    volumeMounts?: outputs.CloudRunV2ServiceTemplateContainerVolumeMount[];
    /**
     * Container's working directory. If not specified, the container runtime's default will be used, which might be configured in the container image.
     */
    workingDir?: string;
}

export interface CloudRunV2ServiceTemplateContainerEnv {
    /**
     * Name of the environment variable. Must be a C_IDENTIFIER, and may not exceed 32768 characters.
     */
    name: string;
    /**
     * Literal value of the environment variable. Defaults to "" and the maximum allowed length is 32768 characters. Variable references are not supported in Cloud Run.
     */
    value?: string;
    /**
     * Source for the environment variable's value.
     */
    valueSource?: outputs.CloudRunV2ServiceTemplateContainerEnvValueSource;
}

export interface CloudRunV2ServiceTemplateContainerEnvValueSource {
    /**
     * Selects a secret and a specific version from Cloud Secret Manager.
     */
    secretKeyRef?: outputs.CloudRunV2ServiceTemplateContainerEnvValueSourceSecretKeyRef;
}

export interface CloudRunV2ServiceTemplateContainerEnvValueSourceSecretKeyRef {
    /**
     * The name of the secret in Cloud Secret Manager. Format: {secretName} if the secret is in the same project. projects/{project}/secrets/{secretName} if the secret is in a different project.
     */
    secret: string;
    /**
     * The Cloud Secret Manager secret version. Can be 'latest' for the latest value or an integer for a specific version.
     */
    version?: string;
}

export interface CloudRunV2ServiceTemplateContainerLivenessProbe {
    /**
     * Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1.
     */
    failureThreshold?: number;
    /**
     * GRPC specifies an action involving a GRPC port.
     */
    grpc?: outputs.CloudRunV2ServiceTemplateContainerLivenessProbeGrpc;
    /**
     * HTTPGet specifies the http request to perform.
     */
    httpGet?: outputs.CloudRunV2ServiceTemplateContainerLivenessProbeHttpGet;
    /**
     * Number of seconds after the container has started before the probe is initiated. Defaults to 0 seconds. Minimum value is 0. Maximum value for liveness probe is 3600. Maximum value for startup probe is 240. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
     */
    initialDelaySeconds?: number;
    /**
     * How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1. Maximum value for liveness probe is 3600. Maximum value for startup probe is 240. Must be greater or equal than timeoutSeconds
     */
    periodSeconds?: number;
    /**
     * TCPSocketAction describes an action based on opening a socket
     */
    tcpSocket?: outputs.CloudRunV2ServiceTemplateContainerLivenessProbeTcpSocket;
    /**
     * Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. Maximum value is 3600. Must be smaller than periodSeconds. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
     */
    timeoutSeconds?: number;
}

export interface CloudRunV2ServiceTemplateContainerLivenessProbeGrpc {
    /**
     * Port number to access on the container. Number must be in the range 1 to 65535.
     * If not specified, defaults to the same value as container.ports[0].containerPort.
     */
    port: number;
    /**
     * The name of the service to place in the gRPC HealthCheckRequest
     * (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
     * If this is not specified, the default behavior is defined by gRPC.
     */
    service?: string;
}

export interface CloudRunV2ServiceTemplateContainerLivenessProbeHttpGet {
    /**
     * Custom headers to set in the request. HTTP allows repeated headers.
     */
    httpHeaders?: outputs.CloudRunV2ServiceTemplateContainerLivenessProbeHttpGetHttpHeader[];
    /**
     * Path to access on the HTTP server. Defaults to '/'.
     */
    path?: string;
    /**
     * Port number to access on the container. Number must be in the range 1 to 65535.
     * If not specified, defaults to the same value as container.ports[0].containerPort.
     */
    port: number;
}

export interface CloudRunV2ServiceTemplateContainerLivenessProbeHttpGetHttpHeader {
    /**
     * The header field name
     */
    name: string;
    /**
     * The header field value
     */
    value?: string;
}

export interface CloudRunV2ServiceTemplateContainerLivenessProbeTcpSocket {
    /**
     * Port number to access on the container. Must be in the range 1 to 65535.
     * If not specified, defaults to the exposed port of the container, which
     * is the value of container.ports[0].containerPort.
     */
    port: number;
}

export interface CloudRunV2ServiceTemplateContainerPorts {
    /**
     * Port number the container listens on. This must be a valid TCP port number, 0 < containerPort < 65536.
     */
    containerPort?: number;
    /**
     * If specified, used to specify which protocol to use. Allowed values are "http1" and "h2c".
     */
    name: string;
}

export interface CloudRunV2ServiceTemplateContainerResources {
    /**
     * Determines whether CPU is only allocated during requests. True by default if the parent 'resources' field is not set. However, if
     * 'resources' is set, this field must be explicitly set to true to preserve the default behavior.
     */
    cpuIdle?: boolean;
    /**
     * Only memory and CPU are supported. Use key 'cpu' for CPU limit and 'memory' for memory limit. Note: The only supported values for CPU are '1', '2', '4', and '8'. Setting 4 CPU requires at least 2Gi of memory. The values of the map is string form of the 'quantity' k8s type: https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apimachinery/pkg/api/resource/quantity.go
     */
    limits: {[key: string]: string};
    /**
     * Determines whether CPU should be boosted on startup of a new container instance above the requested CPU threshold, this can help reduce cold-start latency.
     */
    startupCpuBoost?: boolean;
}

export interface CloudRunV2ServiceTemplateContainerStartupProbe {
    /**
     * Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1.
     */
    failureThreshold?: number;
    /**
     * GRPC specifies an action involving a GRPC port.
     */
    grpc?: outputs.CloudRunV2ServiceTemplateContainerStartupProbeGrpc;
    /**
     * HTTPGet specifies the http request to perform. Exactly one of HTTPGet or TCPSocket must be specified.
     */
    httpGet?: outputs.CloudRunV2ServiceTemplateContainerStartupProbeHttpGet;
    /**
     * Number of seconds after the container has started before the probe is initiated. Defaults to 0 seconds. Minimum value is 0. Maximum value for liveness probe is 3600. Maximum value for startup probe is 240. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
     */
    initialDelaySeconds?: number;
    /**
     * How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1. Maximum value for liveness probe is 3600. Maximum value for startup probe is 240. Must be greater or equal than timeoutSeconds
     */
    periodSeconds?: number;
    /**
     * TCPSocket specifies an action involving a TCP port. Exactly one of HTTPGet or TCPSocket must be specified.
     */
    tcpSocket?: outputs.CloudRunV2ServiceTemplateContainerStartupProbeTcpSocket;
    /**
     * Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. Maximum value is 3600. Must be smaller than periodSeconds. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes
     */
    timeoutSeconds?: number;
}

export interface CloudRunV2ServiceTemplateContainerStartupProbeGrpc {
    /**
     * Port number to access on the container. Number must be in the range 1 to 65535.
     * If not specified, defaults to the same value as container.ports[0].containerPort.
     */
    port: number;
    /**
     * The name of the service to place in the gRPC HealthCheckRequest
     * (see https://github.com/grpc/grpc/blob/master/doc/health-checking.md).
     * If this is not specified, the default behavior is defined by gRPC.
     */
    service?: string;
}

export interface CloudRunV2ServiceTemplateContainerStartupProbeHttpGet {
    /**
     * Custom headers to set in the request. HTTP allows repeated headers.
     */
    httpHeaders?: outputs.CloudRunV2ServiceTemplateContainerStartupProbeHttpGetHttpHeader[];
    /**
     * Path to access on the HTTP server. Defaults to '/'.
     */
    path?: string;
    /**
     * Port number to access on the container. Must be in the range 1 to 65535.
     * If not specified, defaults to the same value as container.ports[0].containerPort.
     */
    port: number;
}

export interface CloudRunV2ServiceTemplateContainerStartupProbeHttpGetHttpHeader {
    /**
     * The header field name
     */
    name: string;
    /**
     * The header field value
     */
    value?: string;
}

export interface CloudRunV2ServiceTemplateContainerStartupProbeTcpSocket {
    /**
     * Port number to access on the container. Must be in the range 1 to 65535.
     * If not specified, defaults to the same value as container.ports[0].containerPort.
     */
    port: number;
}

export interface CloudRunV2ServiceTemplateContainerVolumeMount {
    /**
     * Path within the container at which the volume should be mounted. Must not contain ':'. For Cloud SQL volumes, it can be left empty, or must otherwise be /cloudsql. All instances defined in the Volume will be available as /cloudsql/[instance]. For more information on Cloud SQL volumes, visit https://cloud.google.com/sql/docs/mysql/connect-run
     */
    mountPath: string;
    /**
     * This must match the Name of a Volume.
     */
    name: string;
}

export interface CloudRunV2ServiceTemplateScaling {
    /**
     * Maximum number of serving instances that this resource should have.
     */
    maxInstanceCount?: number;
    /**
     * Minimum number of serving instances that this resource should have.
     */
    minInstanceCount?: number;
}

export interface CloudRunV2ServiceTemplateVolume {
    /**
     * For Cloud SQL volumes, contains the specific instances that should be mounted. Visit https://cloud.google.com/sql/docs/mysql/connect-run for more information on how to connect Cloud SQL and Cloud Run.
     */
    cloudSqlInstance?: outputs.CloudRunV2ServiceTemplateVolumeCloudSqlInstance;
    /**
     * Cloud Storage bucket mounted as a volume using GCSFuse. This feature is only supported in the gen2 execution environment and requires launch-stage to be set to ALPHA or BETA.
     */
    gcs?: outputs.CloudRunV2ServiceTemplateVolumeGcs;
    /**
     * Volume's name.
     */
    name: string;
    /**
     * Represents an NFS mount.
     */
    nfs?: outputs.CloudRunV2ServiceTemplateVolumeNfs;
    /**
     * Secret represents a secret that should populate this volume. More info: https://kubernetes.io/docs/concepts/storage/volumes#secret
     */
    secret?: outputs.CloudRunV2ServiceTemplateVolumeSecret;
}

export interface CloudRunV2ServiceTemplateVolumeCloudSqlInstance {
    /**
     * The Cloud SQL instance connection names, as can be found in https://console.cloud.google.com/sql/instances. Visit https://cloud.google.com/sql/docs/mysql/connect-run for more information on how to connect Cloud SQL and Cloud Run. Format: {project}:{location}:{instance}
     */
    instances?: string[];
}

export interface CloudRunV2ServiceTemplateVolumeGcs {
    /**
     * GCS Bucket name
     */
    bucket: string;
    /**
     * If true, mount the GCS bucket as read-only
     */
    readOnly?: boolean;
}

export interface CloudRunV2ServiceTemplateVolumeNfs {
    /**
     * Path that is exported by the NFS server.
     */
    path: string;
    /**
     * If true, mount the NFS volume as read only
     */
    readOnly?: boolean;
    /**
     * Hostname or IP address of the NFS server
     */
    server: string;
}

export interface CloudRunV2ServiceTemplateVolumeSecret {
    /**
     * Integer representation of mode bits to use on created files by default. Must be a value between 0000 and 0777 (octal), defaulting to 0444. Directories within the path are not affected by this setting.
     */
    defaultMode?: number;
    /**
     * If unspecified, the volume will expose a file whose name is the secret, relative to VolumeMount.mount_path. If specified, the key will be used as the version to fetch from Cloud Secret Manager and the path will be the name of the file exposed in the volume. When items are defined, they must specify a path and a version.
     */
    items?: outputs.CloudRunV2ServiceTemplateVolumeSecretItem[];
    /**
     * The name of the secret in Cloud Secret Manager. Format: {secret} if the secret is in the same project. projects/{project}/secrets/{secret} if the secret is in a different project.
     */
    secret: string;
}

export interface CloudRunV2ServiceTemplateVolumeSecretItem {
    /**
     * Integer octal mode bits to use on this file, must be a value between 01 and 0777 (octal). If 0 or not set, the Volume's default mode will be used.
     */
    mode?: number;
    /**
     * The relative path of the secret in the container.
     */
    path: string;
    /**
     * The Cloud Secret Manager secret version. Can be 'latest' for the latest value or an integer for a specific version
     */
    version?: string;
}

export interface CloudRunV2ServiceTemplateVpcAccess {
    /**
     * VPC Access connector name. Format: projects/{project}/locations/{location}/connectors/{connector}, where {project} can be project id or number.
     */
    connector?: string;
    /**
     * Traffic VPC egress settings. Possible values: ["ALL_TRAFFIC", "PRIVATE_RANGES_ONLY"]
     */
    egress: string;
    /**
     * Direct VPC egress settings. Currently only single network interface is supported.
     */
    networkInterfaces?: outputs.CloudRunV2ServiceTemplateVpcAccessNetworkInterface[];
}

export interface CloudRunV2ServiceTemplateVpcAccessNetworkInterface {
    /**
     * The VPC network that the Cloud Run resource will be able to send traffic to. At least one of network or subnetwork must be specified. If both
     * network and subnetwork are specified, the given VPC subnetwork must belong to the given VPC network. If network is not specified, it will be
     * looked up from the subnetwork.
     */
    network: string;
    /**
     * The VPC subnetwork that the Cloud Run resource will get IPs from. At least one of network or subnetwork must be specified. If both
     * network and subnetwork are specified, the given VPC subnetwork must belong to the given VPC network. If subnetwork is not specified, the
     * subnetwork with the same name with the network will be used.
     */
    subnetwork: string;
    /**
     * Network tags applied to this Cloud Run service.
     */
    tags?: string[];
}

export interface CloudRunV2ServiceTerminalCondition {
    executionReason: string;
    lastTransitionTime: string;
    message: string;
    reason: string;
    revisionReason: string;
    severity: string;
    state: string;
    type: string;
}

export interface CloudRunV2ServiceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface CloudRunV2ServiceTraffic {
    /**
     * Specifies percent of the traffic to this Revision. This defaults to zero if unspecified.
     */
    percent: number;
    /**
     * Revision to which to send this portion of traffic, if traffic allocation is by revision.
     */
    revision?: string;
    /**
     * Indicates a string to be part of the URI to exclusively reference this target.
     */
    tag?: string;
    /**
     * The allocation type for this traffic target. Possible values: ["TRAFFIC_TARGET_ALLOCATION_TYPE_LATEST", "TRAFFIC_TARGET_ALLOCATION_TYPE_REVISION"]
     */
    type?: string;
}

export interface CloudRunV2ServiceTrafficStatus {
    percent: number;
    revision: string;
    tag: string;
    type: string;
    uri: string;
}

export interface CloudSchedulerJobAppEngineHttpTarget {
    /**
     * App Engine Routing setting for the job.
     */
    appEngineRouting?: outputs.CloudSchedulerJobAppEngineHttpTargetAppEngineRouting;
    /**
     * HTTP request body.
     * A request body is allowed only if the HTTP method is POST or PUT.
     * It will result in invalid argument error to set a body on a job with an incompatible HttpMethod.
     *
     * A base64-encoded string.
     */
    body?: string;
    /**
     * HTTP request headers.
     * This map contains the header field names and values.
     * Headers can be set when the job is created.
     */
    headers?: {[key: string]: string};
    /**
     * Which HTTP method to use for the request.
     */
    httpMethod?: string;
    /**
     * The relative URI.
     * The relative URL must begin with "/" and must be a valid HTTP relative URL.
     * It can contain a path, query string arguments, and \# fragments.
     * If the relative URL is empty, then the root path "/" will be used.
     * No spaces are allowed, and the maximum length allowed is 2083 characters
     */
    relativeUri: string;
}

export interface CloudSchedulerJobAppEngineHttpTargetAppEngineRouting {
    /**
     * App instance.
     * By default, the job is sent to an instance which is available when the job is attempted.
     */
    instance?: string;
    /**
     * App service.
     * By default, the job is sent to the service which is the default service when the job is attempted.
     */
    service?: string;
    /**
     * App version.
     * By default, the job is sent to the version which is the default version when the job is attempted.
     */
    version?: string;
}

export interface CloudSchedulerJobHttpTarget {
    /**
     * HTTP request body.
     * A request body is allowed only if the HTTP method is POST, PUT, or PATCH.
     * It is an error to set body on a job with an incompatible HttpMethod.
     *
     * A base64-encoded string.
     */
    body?: string;
    /**
     * This map contains the header field names and values.
     * Repeated headers are not supported, but a header value can contain commas.
     */
    headers?: {[key: string]: string};
    /**
     * Which HTTP method to use for the request.
     */
    httpMethod?: string;
    /**
     * Contains information needed for generating an OAuth token.
     * This type of authorization should be used when sending requests to a GCP endpoint.
     */
    oauthToken?: outputs.CloudSchedulerJobHttpTargetOauthToken;
    /**
     * Contains information needed for generating an OpenID Connect token.
     * This type of authorization should be used when sending requests to third party endpoints or Cloud Run.
     */
    oidcToken?: outputs.CloudSchedulerJobHttpTargetOidcToken;
    /**
     * The full URI path that the request will be sent to.
     */
    uri: string;
}

export interface CloudSchedulerJobHttpTargetOauthToken {
    /**
     * OAuth scope to be used for generating OAuth access token. If not specified,
     * "https://www.googleapis.com/auth/cloud-platform" will be used.
     */
    scope?: string;
    /**
     * Service account email to be used for generating OAuth token.
     * The service account must be within the same project as the job.
     */
    serviceAccountEmail: string;
}

export interface CloudSchedulerJobHttpTargetOidcToken {
    /**
     * Audience to be used when generating OIDC token. If not specified,
     * the URI specified in target will be used.
     */
    audience?: string;
    /**
     * Service account email to be used for generating OAuth token.
     * The service account must be within the same project as the job.
     */
    serviceAccountEmail: string;
}

export interface CloudSchedulerJobPubsubTarget {
    /**
     * Attributes for PubsubMessage.
     * Pubsub message must contain either non-empty data, or at least one attribute.
     */
    attributes?: {[key: string]: string};
    /**
     * The message payload for PubsubMessage.
     * Pubsub message must contain either non-empty data, or at least one attribute.
     *
     *  A base64-encoded string.
     */
    data?: string;
    /**
     * The full resource name for the Cloud Pub/Sub topic to which
     * messages will be published when a job is delivered. ~>**NOTE:**
     * The topic name must be in the same format as required by PubSub's
     * PublishRequest.name, e.g. 'projects/my-project/topics/my-topic'.
     */
    topicName: string;
}

export interface CloudSchedulerJobRetryConfig {
    /**
     * The maximum amount of time to wait before retrying a job after it fails.
     * A duration in seconds with up to nine fractional digits, terminated by 's'.
     */
    maxBackoffDuration: string;
    /**
     * The time between retries will double maxDoublings times.
     * A job's retry interval starts at minBackoffDuration,
     * then doubles maxDoublings times, then increases linearly,
     * and finally retries retries at intervals of maxBackoffDuration up to retryCount times.
     */
    maxDoublings: number;
    /**
     * The time limit for retrying a failed job, measured from time when an execution was first attempted.
     * If specified with retryCount, the job will be retried until both limits are reached.
     * A duration in seconds with up to nine fractional digits, terminated by 's'.
     */
    maxRetryDuration: string;
    /**
     * The minimum amount of time to wait before retrying a job after it fails.
     * A duration in seconds with up to nine fractional digits, terminated by 's'.
     */
    minBackoffDuration: string;
    /**
     * The number of attempts that the system will make to run a
     * job using the exponential backoff procedure described by maxDoublings.
     * Values greater than 5 and negative values are not allowed.
     */
    retryCount: number;
}

export interface CloudSchedulerJobTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface CloudTasksQueueAppEngineRoutingOverride {
    /**
     * The host that the task is sent to.
     */
    host: string;
    /**
     * App instance.
     *
     * By default, the task is sent to an instance which is available when the task is attempted.
     */
    instance?: string;
    /**
     * App service.
     *
     * By default, the task is sent to the service which is the default service when the task is attempted.
     */
    service?: string;
    /**
     * App version.
     *
     * By default, the task is sent to the version which is the default version when the task is attempted.
     */
    version?: string;
}

export interface CloudTasksQueueHttpTarget {
    /**
     * HTTP target headers.
     *
     * This map contains the header field names and values.
     * Headers will be set when running the CreateTask and/or BufferTask.
     *
     * These headers represent a subset of the headers that will be configured for the task's HTTP request.
     * Some HTTP request headers will be ignored or replaced.
     *
     * Headers which can have multiple values (according to RFC2616) can be specified using comma-separated values.
     *
     * The size of the headers must be less than 80KB. Queue-level headers to override headers of all the tasks in the queue.
     */
    headerOverrides?: outputs.CloudTasksQueueHttpTargetHeaderOverride[];
    /**
     * The HTTP method to use for the request.
     *
     * When specified, it overrides HttpRequest for the task.
     * Note that if the value is set to GET the body of the task will be ignored at execution time. Possible values: ["HTTP_METHOD_UNSPECIFIED", "POST", "GET", "HEAD", "PUT", "DELETE", "PATCH", "OPTIONS"]
     */
    httpMethod: string;
    /**
     * If specified, an OAuth token is generated and attached as the Authorization header in the HTTP request.
     *
     * This type of authorization should generally be used only when calling Google APIs hosted on *.googleapis.com.
     * Note that both the service account email and the scope MUST be specified when using the queue-level authorization override.
     */
    oauthToken?: outputs.CloudTasksQueueHttpTargetOauthToken;
    /**
     * If specified, an OIDC token is generated and attached as an Authorization header in the HTTP request.
     *
     * This type of authorization can be used for many scenarios, including calling Cloud Run, or endpoints where you intend to validate the token yourself.
     * Note that both the service account email and the audience MUST be specified when using the queue-level authorization override.
     */
    oidcToken?: outputs.CloudTasksQueueHttpTargetOidcToken;
    /**
     * URI override.
     *
     * When specified, overrides the execution URI for all the tasks in the queue.
     */
    uriOverride?: outputs.CloudTasksQueueHttpTargetUriOverride;
}

export interface CloudTasksQueueHttpTargetHeaderOverride {
    /**
     * Header embodying a key and a value.
     */
    header: outputs.CloudTasksQueueHttpTargetHeaderOverrideHeader;
}

export interface CloudTasksQueueHttpTargetHeaderOverrideHeader {
    /**
     * The Key of the header.
     */
    key: string;
    /**
     * The Value of the header.
     */
    value: string;
}

export interface CloudTasksQueueHttpTargetOauthToken {
    /**
     * OAuth scope to be used for generating OAuth access token.
     * If not specified, "https://www.googleapis.com/auth/cloud-platform" will be used.
     */
    scope: string;
    /**
     * Service account email to be used for generating OAuth token.
     * The service account must be within the same project as the queue.
     * The caller must have iam.serviceAccounts.actAs permission for the service account.
     */
    serviceAccountEmail: string;
}

export interface CloudTasksQueueHttpTargetOidcToken {
    /**
     * Audience to be used when generating OIDC token. If not specified, the URI specified in target will be used.
     */
    audience: string;
    /**
     * Service account email to be used for generating OIDC token.
     * The service account must be within the same project as the queue.
     * The caller must have iam.serviceAccounts.actAs permission for the service account.
     */
    serviceAccountEmail: string;
}

export interface CloudTasksQueueHttpTargetUriOverride {
    /**
     * Host override.
     *
     * When specified, replaces the host part of the task URL.
     * For example, if the task URL is "https://www.google.com", and host value
     * is set to "example.net", the overridden URI will be changed to "https://example.net".
     * Host value cannot be an empty string (INVALID_ARGUMENT).
     */
    host?: string;
    /**
     * URI path.
     *
     * When specified, replaces the existing path of the task URL.
     * Setting the path value to an empty string clears the URI path segment.
     */
    pathOverride?: outputs.CloudTasksQueueHttpTargetUriOverridePathOverride;
    /**
     * Port override.
     *
     * When specified, replaces the port part of the task URI.
     * For instance, for a URI http://www.google.com/foo and port=123, the overridden URI becomes http://www.google.com:123/foo.
     * Note that the port value must be a positive integer.
     * Setting the port to 0 (Zero) clears the URI port.
     */
    port?: string;
    /**
     * URI query.
     *
     * When specified, replaces the query part of the task URI. Setting the query value to an empty string clears the URI query segment.
     */
    queryOverride?: outputs.CloudTasksQueueHttpTargetUriOverrideQueryOverride;
    /**
     * Scheme override.
     *
     * When specified, the task URI scheme is replaced by the provided value (HTTP or HTTPS). Possible values: ["HTTP", "HTTPS"]
     */
    scheme: string;
    /**
     * URI Override Enforce Mode
     *
     * When specified, determines the Target UriOverride mode. If not specified, it defaults to ALWAYS. Possible values: ["ALWAYS", "IF_NOT_EXISTS"]
     */
    uriOverrideEnforceMode: string;
}

export interface CloudTasksQueueHttpTargetUriOverridePathOverride {
    /**
     * The URI path (e.g., /users/1234). Default is an empty string.
     */
    path: string;
}

export interface CloudTasksQueueHttpTargetUriOverrideQueryOverride {
    /**
     * The query parameters (e.g., qparam1=123&qparam2=456). Default is an empty string.
     */
    queryParams: string;
}

export interface CloudTasksQueueIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface CloudTasksQueueIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface CloudTasksQueueRateLimits {
    /**
     * The max burst size.
     *
     * Max burst size limits how fast tasks in queue are processed when many tasks are
     * in the queue and the rate is high. This field allows the queue to have a high
     * rate so processing starts shortly after a task is enqueued, but still limits
     * resource usage when many tasks are enqueued in a short period of time.
     */
    maxBurstSize: number;
    /**
     * The maximum number of concurrent tasks that Cloud Tasks allows to
     * be dispatched for this queue. After this threshold has been
     * reached, Cloud Tasks stops dispatching tasks until the number of
     * concurrent requests decreases.
     */
    maxConcurrentDispatches: number;
    /**
     * The maximum rate at which tasks are dispatched from this queue.
     *
     * If unspecified when the queue is created, Cloud Tasks will pick the default.
     */
    maxDispatchesPerSecond: number;
}

export interface CloudTasksQueueRetryConfig {
    /**
     * Number of attempts per task.
     *
     * Cloud Tasks will attempt the task maxAttempts times (that is, if
     * the first attempt fails, then there will be maxAttempts - 1
     * retries). Must be >= -1.
     *
     * If unspecified when the queue is created, Cloud Tasks will pick
     * the default.
     *
     * -1 indicates unlimited attempts.
     */
    maxAttempts: number;
    /**
     * A task will be scheduled for retry between minBackoff and
     * maxBackoff duration after it fails, if the queue's RetryConfig
     * specifies that the task should be retried.
     */
    maxBackoff: string;
    /**
     * The time between retries will double maxDoublings times.
     *
     * A task's retry interval starts at minBackoff, then doubles maxDoublings times,
     * then increases linearly, and finally retries retries at intervals of maxBackoff
     * up to maxAttempts times.
     */
    maxDoublings: number;
    /**
     * If positive, maxRetryDuration specifies the time limit for
     * retrying a failed task, measured from when the task was first
     * attempted. Once maxRetryDuration time has passed and the task has
     * been attempted maxAttempts times, no further attempts will be
     * made and the task will be deleted.
     *
     * If zero, then the task age is unlimited.
     */
    maxRetryDuration: string;
    /**
     * A task will be scheduled for retry between minBackoff and
     * maxBackoff duration after it fails, if the queue's RetryConfig
     * specifies that the task should be retried.
     */
    minBackoff: string;
}

export interface CloudTasksQueueStackdriverLoggingConfig {
    /**
     * Specifies the fraction of operations to write to Stackdriver Logging.
     * This field may contain any value between 0.0 and 1.0, inclusive. 0.0 is the
     * default and means that no operations are logged.
     */
    samplingRatio: number;
}

export interface CloudTasksQueueTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface CloudbuildBitbucketServerConfigConnectedRepository {
    /**
     * Identifier for the project storing the repository.
     */
    projectKey: string;
    /**
     * Identifier for the repository.
     */
    repoSlug: string;
}

export interface CloudbuildBitbucketServerConfigSecrets {
    /**
     * The resource name for the admin access token's secret version.
     */
    adminAccessTokenVersionName: string;
    /**
     * The resource name for the read access token's secret version.
     */
    readAccessTokenVersionName: string;
    /**
     * Immutable. The resource name for the webhook secret's secret version. Once this field has been set, it cannot be changed.
     * Changing this field will result in deleting/ recreating the resource.
     */
    webhookSecretVersionName: string;
}

export interface CloudbuildBitbucketServerConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface CloudbuildTriggerApprovalConfig {
    /**
     * Whether or not approval is needed. If this is set on a build, it will become pending when run,
     * and will need to be explicitly approved to start.
     */
    approvalRequired?: boolean;
}

export interface CloudbuildTriggerBitbucketServerTriggerConfig {
    /**
     * The Bitbucket server config resource that this trigger config maps to.
     */
    bitbucketServerConfigResource: string;
    /**
     * Key of the project that the repo is in. For example: The key for https://mybitbucket.server/projects/TEST/repos/test-repo is "TEST".
     */
    projectKey: string;
    /**
     * Filter to match changes in pull requests.
     */
    pullRequest?: outputs.CloudbuildTriggerBitbucketServerTriggerConfigPullRequest;
    /**
     * Filter to match changes in refs like branches, tags.
     */
    push?: outputs.CloudbuildTriggerBitbucketServerTriggerConfigPush;
    /**
     * Slug of the repository. A repository slug is a URL-friendly version of a repository name, automatically generated by Bitbucket for use in the URL.
     * For example, if the repository name is 'test repo', in the URL it would become 'test-repo' as in https://mybitbucket.server/projects/TEST/repos/test-repo.
     */
    repoSlug: string;
}

export interface CloudbuildTriggerBitbucketServerTriggerConfigPullRequest {
    /**
     * Regex of branches to match.
     * The syntax of the regular expressions accepted is the syntax accepted by RE2 and described at https://github.com/google/re2/wiki/Syntax
     */
    branch: string;
    /**
     * Configure builds to run whether a repository owner or collaborator need to comment /gcbrun. Possible values: ["COMMENTS_DISABLED", "COMMENTS_ENABLED", "COMMENTS_ENABLED_FOR_EXTERNAL_CONTRIBUTORS_ONLY"]
     */
    commentControl?: string;
    /**
     * If true, branches that do NOT match the git_ref will trigger a build.
     */
    invertRegex?: boolean;
}

export interface CloudbuildTriggerBitbucketServerTriggerConfigPush {
    /**
     * Regex of branches to match.  Specify only one of branch or tag.
     */
    branch?: string;
    /**
     * When true, only trigger a build if the revision regex does NOT match the gitRef regex.
     */
    invertRegex?: boolean;
    /**
     * Regex of tags to match.  Specify only one of branch or tag.
     */
    tag?: string;
}

export interface CloudbuildTriggerBuild {
    /**
     * Artifacts produced by the build that should be uploaded upon successful completion of all build steps.
     */
    artifacts?: outputs.CloudbuildTriggerBuildArtifacts;
    /**
     * Secrets and secret environment variables.
     */
    availableSecrets?: outputs.CloudbuildTriggerBuildAvailableSecrets;
    /**
     * A list of images to be pushed upon the successful completion of all build steps.
     * The images are pushed using the builder service account's credentials.
     * The digests of the pushed images will be stored in the Build resource's results field.
     * If any of the images fail to be pushed, the build status is marked FAILURE.
     */
    images?: string[];
    /**
     * Google Cloud Storage bucket where logs should be written.
     * Logs file names will be of the format ${logsBucket}/log-${build_id}.txt.
     */
    logsBucket?: string;
    /**
     * Special options for this build.
     */
    options?: outputs.CloudbuildTriggerBuildOptions;
    /**
     * TTL in queue for this build. If provided and the build is enqueued longer than this value,
     * the build will expire and the build status will be EXPIRED.
     * The TTL starts ticking from createTime.
     * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
     */
    queueTtl?: string;
    /**
     * Secrets to decrypt using Cloud Key Management Service.
     */
    secrets?: outputs.CloudbuildTriggerBuildSecret[];
    /**
     * The location of the source files to build.
     *
     * One of 'storageSource' or 'repoSource' must be provided.
     */
    source?: outputs.CloudbuildTriggerBuildSource;
    /**
     * The operations to be performed on the workspace.
     */
    steps: outputs.CloudbuildTriggerBuildStep[];
    /**
     * Substitutions data for Build resource.
     */
    substitutions?: {[key: string]: string};
    /**
     * Tags for annotation of a Build. These are not docker tags.
     */
    tags?: string[];
    /**
     * Amount of time that this build should be allowed to run, to second granularity.
     * If this amount of time elapses, work on the build will cease and the build status will be TIMEOUT.
     * This timeout must be equal to or greater than the sum of the timeouts for build steps within the build.
     * The expected format is the number of seconds followed by s.
     * Default time is ten minutes (600s).
     */
    timeout?: string;
}

export interface CloudbuildTriggerBuildArtifacts {
    /**
     * A list of images to be pushed upon the successful completion of all build steps.
     *
     * The images will be pushed using the builder service account's credentials.
     *
     * The digests of the pushed images will be stored in the Build resource's results field.
     *
     * If any of the images fail to be pushed, the build is marked FAILURE.
     */
    images?: string[];
    /**
     * A Maven artifact to upload to Artifact Registry upon successful completion of all build steps.
     *
     * The location and generation of the uploaded objects will be stored in the Build resource's results field.
     *
     * If any objects fail to be pushed, the build is marked FAILURE.
     */
    mavenArtifacts?: outputs.CloudbuildTriggerBuildArtifactsMavenArtifact[];
    /**
     * Npm package to upload to Artifact Registry upon successful completion of all build steps.
     *
     * The location and generation of the uploaded objects will be stored in the Build resource's results field.
     *
     * If any objects fail to be pushed, the build is marked FAILURE.
     */
    npmPackages?: outputs.CloudbuildTriggerBuildArtifactsNpmPackage[];
    /**
     * A list of objects to be uploaded to Cloud Storage upon successful completion of all build steps.
     *
     * Files in the workspace matching specified paths globs will be uploaded to the
     * Cloud Storage location using the builder service account's credentials.
     *
     * The location and generation of the uploaded objects will be stored in the Build resource's results field.
     *
     * If any objects fail to be pushed, the build is marked FAILURE.
     */
    objects?: outputs.CloudbuildTriggerBuildArtifactsObjects;
    /**
     * Python package to upload to Artifact Registry upon successful completion of all build steps. A package can encapsulate multiple objects to be uploaded to a single repository.
     *
     * The location and generation of the uploaded objects will be stored in the Build resource's results field.
     *
     * If any objects fail to be pushed, the build is marked FAILURE.
     */
    pythonPackages?: outputs.CloudbuildTriggerBuildArtifactsPythonPackage[];
}

export interface CloudbuildTriggerBuildArtifactsMavenArtifact {
    /**
     * Maven artifactId value used when uploading the artifact to Artifact Registry.
     */
    artifactId?: string;
    /**
     * Maven groupId value used when uploading the artifact to Artifact Registry.
     */
    groupId?: string;
    /**
     * Path to an artifact in the build's workspace to be uploaded to Artifact Registry. This can be either an absolute path, e.g. /workspace/my-app/target/my-app-1.0.SNAPSHOT.jar or a relative path from /workspace, e.g. my-app/target/my-app-1.0.SNAPSHOT.jar.
     */
    path?: string;
    /**
     * Artifact Registry repository, in the form "https://$REGION-maven.pkg.dev/$PROJECT/$REPOSITORY"
     *
     * Artifact in the workspace specified by path will be uploaded to Artifact Registry with this location as a prefix.
     */
    repository?: string;
    /**
     * Maven version value used when uploading the artifact to Artifact Registry.
     */
    version?: string;
}

export interface CloudbuildTriggerBuildArtifactsNpmPackage {
    /**
     * Path to the package.json. e.g. workspace/path/to/package
     */
    packagePath?: string;
    /**
     * Artifact Registry repository, in the form "https://$REGION-npm.pkg.dev/$PROJECT/$REPOSITORY"
     *
     * Npm package in the workspace specified by path will be zipped and uploaded to Artifact Registry with this location as a prefix.
     */
    repository?: string;
}

export interface CloudbuildTriggerBuildArtifactsObjects {
    /**
     * Cloud Storage bucket and optional object path, in the form "gs://bucket/path/to/somewhere/".
     *
     * Files in the workspace matching any path pattern will be uploaded to Cloud Storage with
     * this location as a prefix.
     */
    location?: string;
    /**
     * Path globs used to match files in the build's workspace.
     */
    paths?: string[];
    /**
     * Output only. Stores timing information for pushing all artifact objects.
     */
    timings: outputs.CloudbuildTriggerBuildArtifactsObjectsTiming[];
}

export interface CloudbuildTriggerBuildArtifactsObjectsTiming {
    endTime: string;
    startTime: string;
}

export interface CloudbuildTriggerBuildArtifactsPythonPackage {
    /**
     * Path globs used to match files in the build's workspace. For Python/ Twine, this is usually dist/*, and sometimes additionally an .asc file.
     */
    paths?: string[];
    /**
     * Artifact Registry repository, in the form "https://$REGION-python.pkg.dev/$PROJECT/$REPOSITORY"
     *
     * Files in the workspace matching any path pattern will be uploaded to Artifact Registry with this location as a prefix.
     */
    repository?: string;
}

export interface CloudbuildTriggerBuildAvailableSecrets {
    /**
     * Pairs a secret environment variable with a SecretVersion in Secret Manager.
     */
    secretManagers: outputs.CloudbuildTriggerBuildAvailableSecretsSecretManager[];
}

export interface CloudbuildTriggerBuildAvailableSecretsSecretManager {
    /**
     * Environment variable name to associate with the secret. Secret environment
     * variables must be unique across all of a build's secrets, and must be used
     * by at least one build step.
     */
    env: string;
    /**
     * Resource name of the SecretVersion. In format: projects/*&#47;secrets/*&#47;versions/*
     */
    versionName: string;
}

export interface CloudbuildTriggerBuildOptions {
    /**
     * Requested disk size for the VM that runs the build. Note that this is NOT "disk free";
     * some of the space will be used by the operating system and build utilities.
     * Also note that this is the minimum disk size that will be allocated for the build --
     * the build may run with a larger disk than requested. At present, the maximum disk size
     * is 1000GB; builds that request more than the maximum are rejected with an error.
     */
    diskSizeGb?: number;
    /**
     * Option to specify whether or not to apply bash style string operations to the substitutions.
     *
     * NOTE this is always enabled for triggered builds and cannot be overridden in the build configuration file.
     */
    dynamicSubstitutions?: boolean;
    /**
     * A list of global environment variable definitions that will exist for all build steps
     * in this build. If a variable is defined in both globally and in a build step,
     * the variable will use the build step value.
     *
     * The elements are of the form "KEY=VALUE" for the environment variable "KEY" being given the value "VALUE".
     */
    envs?: string[];
    /**
     * Option to define build log streaming behavior to Google Cloud Storage. Possible values: ["STREAM_DEFAULT", "STREAM_ON", "STREAM_OFF"]
     */
    logStreamingOption?: string;
    /**
     * Option to specify the logging mode, which determines if and where build logs are stored. Possible values: ["LOGGING_UNSPECIFIED", "LEGACY", "GCS_ONLY", "STACKDRIVER_ONLY", "CLOUD_LOGGING_ONLY", "NONE"]
     */
    logging?: string;
    /**
     * Compute Engine machine type on which to run the build.
     */
    machineType?: string;
    /**
     * Requested verifiability options. Possible values: ["NOT_VERIFIED", "VERIFIED"]
     */
    requestedVerifyOption?: string;
    /**
     * A list of global environment variables, which are encrypted using a Cloud Key Management
     * Service crypto key. These values must be specified in the build's Secret. These variables
     * will be available to all build steps in this build.
     */
    secretEnvs?: string[];
    /**
     * Requested hash for SourceProvenance. Possible values: ["NONE", "SHA256", "MD5"]
     */
    sourceProvenanceHashes?: string[];
    /**
     * Option to specify behavior when there is an error in the substitution checks.
     *
     * NOTE this is always set to ALLOW_LOOSE for triggered builds and cannot be overridden
     * in the build configuration file. Possible values: ["MUST_MATCH", "ALLOW_LOOSE"]
     */
    substitutionOption?: string;
    /**
     * Global list of volumes to mount for ALL build steps
     *
     * Each volume is created as an empty volume prior to starting the build process.
     * Upon completion of the build, volumes and their contents are discarded. Global
     * volume names and paths cannot conflict with the volumes defined a build step.
     *
     * Using a global volume in a build with only one step is not valid as it is indicative
     * of a build request with an incorrect configuration.
     */
    volumes?: outputs.CloudbuildTriggerBuildOptionsVolume[];
    /**
     * Option to specify a WorkerPool for the build. Format projects/{project}/workerPools/{workerPool}
     *
     * This field is experimental.
     */
    workerPool?: string;
}

export interface CloudbuildTriggerBuildOptionsVolume {
    /**
     * Name of the volume to mount.
     *
     * Volume names must be unique per build step and must be valid names for Docker volumes.
     * Each named volume must be used by at least two build steps.
     */
    name?: string;
    /**
     * Path at which to mount the volume.
     *
     * Paths must be absolute and cannot conflict with other volume paths on the same
     * build step or with certain reserved volume paths.
     */
    path?: string;
}

export interface CloudbuildTriggerBuildSecret {
    /**
     * Cloud KMS key name to use to decrypt these envs.
     */
    kmsKeyName: string;
    /**
     * Map of environment variable name to its encrypted value.
     * Secret environment variables must be unique across all of a build's secrets,
     * and must be used by at least one build step. Values can be at most 64 KB in size.
     * There can be at most 100 secret values across all of a build's secrets.
     */
    secretEnv?: {[key: string]: string};
}

export interface CloudbuildTriggerBuildSource {
    /**
     * Location of the source in a Google Cloud Source Repository.
     */
    repoSource?: outputs.CloudbuildTriggerBuildSourceRepoSource;
    /**
     * Location of the source in an archive file in Google Cloud Storage.
     */
    storageSource?: outputs.CloudbuildTriggerBuildSourceStorageSource;
}

export interface CloudbuildTriggerBuildSourceRepoSource {
    /**
     * Regex matching branches to build. Exactly one a of branch name, tag, or commit SHA must be provided.
     * The syntax of the regular expressions accepted is the syntax accepted by RE2 and
     * described at https://github.com/google/re2/wiki/Syntax
     */
    branchName?: string;
    /**
     * Explicit commit SHA to build. Exactly one a of branch name, tag, or commit SHA must be provided.
     */
    commitSha?: string;
    /**
     * Directory, relative to the source root, in which to run the build.
     * This must be a relative path. If a step's dir is specified and is an absolute path,
     * this value is ignored for that step's execution.
     */
    dir?: string;
    /**
     * Only trigger a build if the revision regex does NOT match the revision regex.
     */
    invertRegex?: boolean;
    /**
     * ID of the project that owns the Cloud Source Repository.
     * If omitted, the project ID requesting the build is assumed.
     */
    projectId?: string;
    /**
     * Name of the Cloud Source Repository.
     */
    repoName: string;
    /**
     * Substitutions to use in a triggered build. Should only be used with triggers.run
     */
    substitutions?: {[key: string]: string};
    /**
     * Regex matching tags to build. Exactly one a of branch name, tag, or commit SHA must be provided.
     * The syntax of the regular expressions accepted is the syntax accepted by RE2 and
     * described at https://github.com/google/re2/wiki/Syntax
     */
    tagName?: string;
}

export interface CloudbuildTriggerBuildSourceStorageSource {
    /**
     * Google Cloud Storage bucket containing the source.
     */
    bucket: string;
    /**
     * Google Cloud Storage generation for the object.
     * If the generation is omitted, the latest generation will be used
     */
    generation?: string;
    /**
     * Google Cloud Storage object containing the source.
     * This object must be a gzipped archive file (.tar.gz) containing source to build.
     */
    object: string;
}

export interface CloudbuildTriggerBuildStep {
    /**
     * Allow this build step to fail without failing the entire build if and
     * only if the exit code is one of the specified codes.
     *
     * If 'allowFailure' is also specified, this field will take precedence.
     */
    allowExitCodes?: number[];
    /**
     * Allow this build step to fail without failing the entire build.
     * If false, the entire build will fail if this step fails. Otherwise, the
     * build will succeed, but this step will still have a failure status.
     * Error information will be reported in the 'failureDetail' field.
     *
     * 'allowExitCodes' takes precedence over this field.
     */
    allowFailure?: boolean;
    /**
     * A list of arguments that will be presented to the step when it is started.
     *
     * If the image used to run the step's container has an entrypoint, the args
     * are used as arguments to that entrypoint. If the image does not define an
     * entrypoint, the first element in args is used as the entrypoint, and the
     * remainder will be used as arguments.
     */
    args?: string[];
    /**
     * Working directory to use when running this step's container.
     *
     * If this value is a relative path, it is relative to the build's working
     * directory. If this value is absolute, it may be outside the build's working
     * directory, in which case the contents of the path may not be persisted
     * across build step executions, unless a 'volume' for that path is specified.
     *
     * If the build specifies a 'RepoSource' with 'dir' and a step with a
     * 'dir',
     * which specifies an absolute path, the 'RepoSource' 'dir' is ignored
     * for the step's execution.
     */
    dir?: string;
    /**
     * Entrypoint to be used instead of the build step image's
     * default entrypoint.
     * If unset, the image's default entrypoint is used
     */
    entrypoint?: string;
    /**
     * A list of environment variable definitions to be used when
     * running a step.
     *
     * The elements are of the form "KEY=VALUE" for the environment variable
     * "KEY" being given the value "VALUE".
     */
    envs?: string[];
    /**
     * Unique identifier for this build step, used in 'wait_for' to
     * reference this build step as a dependency.
     */
    id?: string;
    /**
     * The name of the container image that will run this particular build step.
     *
     * If the image is available in the host's Docker daemon's cache, it will be
     * run directly. If not, the host will attempt to pull the image first, using
     * the builder service account's credentials if necessary.
     *
     * The Docker daemon's cache will already have the latest versions of all of
     * the officially supported build steps (see https://github.com/GoogleCloudPlatform/cloud-builders
     * for images and examples).
     * The Docker daemon will also have cached many of the layers for some popular
     * images, like "ubuntu", "debian", but they will be refreshed at the time
     * you attempt to use them.
     *
     * If you built an image in a previous build step, it will be stored in the
     * host's Docker daemon's cache and is available to use as the name for a
     * later build step.
     */
    name: string;
    /**
     * A shell script to be executed in the step.
     * When script is provided, the user cannot specify the entrypoint or args.
     */
    script?: string;
    /**
     * A list of environment variables which are encrypted using
     * a Cloud Key
     * Management Service crypto key. These values must be specified in
     * the build's 'Secret'.
     */
    secretEnvs?: string[];
    /**
     * Time limit for executing this build step. If not defined,
     * the step has no
     * time limit and will be allowed to continue to run until either it
     * completes or the build itself times out.
     */
    timeout?: string;
    /**
     * Output only. Stores timing information for executing this
     * build step.
     */
    timing?: string;
    /**
     * List of volumes to mount into the build step.
     *
     * Each volume is created as an empty volume prior to execution of the
     * build step. Upon completion of the build, volumes and their contents
     * are discarded.
     *
     * Using a named volume in only one step is not valid as it is
     * indicative of a build request with an incorrect configuration.
     */
    volumes?: outputs.CloudbuildTriggerBuildStepVolume[];
    /**
     * The ID(s) of the step(s) that this build step depends on.
     *
     * This build step will not start until all the build steps in 'wait_for'
     * have completed successfully. If 'wait_for' is empty, this build step
     * will start when all previous build steps in the 'Build.Steps' list
     * have completed successfully.
     */
    waitFors?: string[];
}

export interface CloudbuildTriggerBuildStepVolume {
    /**
     * Name of the volume to mount.
     *
     * Volume names must be unique per build step and must be valid names for
     * Docker volumes. Each named volume must be used by at least two build steps.
     */
    name: string;
    /**
     * Path at which to mount the volume.
     *
     * Paths must be absolute and cannot conflict with other volume paths on
     * the same build step or with certain reserved volume paths.
     */
    path: string;
}

export interface CloudbuildTriggerGitFileSource {
    /**
     * The full resource name of the bitbucket server config.
     * Format: projects/{project}/locations/{location}/bitbucketServerConfigs/{id}.
     */
    bitbucketServerConfig?: string;
    /**
     * The full resource name of the github enterprise config.
     * Format: projects/{project}/locations/{location}/githubEnterpriseConfigs/{id}. projects/{project}/githubEnterpriseConfigs/{id}.
     */
    githubEnterpriseConfig?: string;
    /**
     * The path of the file, with the repo root as the root of the path.
     */
    path: string;
    /**
     * The type of the repo, since it may not be explicit from the repo field (e.g from a URL).
     * Values can be UNKNOWN, CLOUD_SOURCE_REPOSITORIES, GITHUB, BITBUCKET_SERVER Possible values: ["UNKNOWN", "CLOUD_SOURCE_REPOSITORIES", "GITHUB", "BITBUCKET_SERVER"]
     */
    repoType: string;
    /**
     * The fully qualified resource name of the Repo API repository. The fully qualified resource name of the Repo API repository.
     * If unspecified, the repo from which the trigger invocation originated is assumed to be the repo from which to read the specified path.
     */
    repository?: string;
    /**
     * The branch, tag, arbitrary ref, or SHA version of the repo to use when resolving the
     * filename (optional). This field respects the same syntax/resolution as described here: https://git-scm.com/docs/gitrevisions
     * If unspecified, the revision from which the trigger invocation originated is assumed to be the revision from which to read the specified path.
     */
    revision?: string;
    /**
     * The URI of the repo (optional). If unspecified, the repo from which the trigger
     * invocation originated is assumed to be the repo from which to read the specified path.
     */
    uri?: string;
}

export interface CloudbuildTriggerGithub {
    /**
     * The resource name of the github enterprise config that should be applied to this installation.
     * For example: "projects/{$projectId}/locations/{$locationId}/githubEnterpriseConfigs/{$configId}"
     */
    enterpriseConfigResourceName?: string;
    /**
     * Name of the repository. For example: The name for
     * https://github.com/googlecloudplatform/cloud-builders is "cloud-builders".
     */
    name?: string;
    /**
     * Owner of the repository. For example: The owner for
     * https://github.com/googlecloudplatform/cloud-builders is "googlecloudplatform".
     */
    owner?: string;
    /**
     * filter to match changes in pull requests. Specify only one of 'pull_request' or 'push'.
     */
    pullRequest?: outputs.CloudbuildTriggerGithubPullRequest;
    /**
     * filter to match changes in refs, like branches or tags. Specify only one of 'pull_request' or 'push'.
     */
    push?: outputs.CloudbuildTriggerGithubPush;
}

export interface CloudbuildTriggerGithubPullRequest {
    /**
     * Regex of branches to match.
     */
    branch: string;
    /**
     * Whether to block builds on a "/gcbrun" comment from a repository owner or collaborator. Possible values: ["COMMENTS_DISABLED", "COMMENTS_ENABLED", "COMMENTS_ENABLED_FOR_EXTERNAL_CONTRIBUTORS_ONLY"]
     */
    commentControl?: string;
    /**
     * If true, branches that do NOT match the git_ref will trigger a build.
     */
    invertRegex?: boolean;
}

export interface CloudbuildTriggerGithubPush {
    /**
     * Regex of branches to match.  Specify only one of branch or tag.
     */
    branch?: string;
    /**
     * When true, only trigger a build if the revision regex does NOT match the git_ref regex.
     */
    invertRegex?: boolean;
    /**
     * Regex of tags to match.  Specify only one of branch or tag.
     */
    tag?: string;
}

export interface CloudbuildTriggerPubsubConfig {
    /**
     * Service account that will make the push request.
     */
    serviceAccountEmail?: string;
    /**
     * Potential issues with the underlying Pub/Sub subscription configuration.
     * Only populated on get requests.
     */
    state: string;
    /**
     * Output only. Name of the subscription.
     */
    subscription: string;
    /**
     * The name of the topic from which this subscription is receiving messages.
     */
    topic: string;
}

export interface CloudbuildTriggerRepositoryEventConfig {
    /**
     * Contains filter properties for matching Pull Requests.
     */
    pullRequest?: outputs.CloudbuildTriggerRepositoryEventConfigPullRequest;
    /**
     * Contains filter properties for matching git pushes.
     */
    push?: outputs.CloudbuildTriggerRepositoryEventConfigPush;
    /**
     * The resource name of the Repo API resource.
     */
    repository?: string;
}

export interface CloudbuildTriggerRepositoryEventConfigPullRequest {
    /**
     * Regex of branches to match.
     *
     * The syntax of the regular expressions accepted is the syntax accepted by
     * RE2 and described at https://github.com/google/re2/wiki/Syntax
     */
    branch?: string;
    /**
     * Configure builds to run whether a repository owner or collaborator need to comment '/gcbrun'. Possible values: ["COMMENTS_DISABLED", "COMMENTS_ENABLED", "COMMENTS_ENABLED_FOR_EXTERNAL_CONTRIBUTORS_ONLY"]
     */
    commentControl?: string;
    /**
     * If true, branches that do NOT match the git_ref will trigger a build.
     */
    invertRegex?: boolean;
}

export interface CloudbuildTriggerRepositoryEventConfigPush {
    /**
     * Regex of branches to match.
     *
     * The syntax of the regular expressions accepted is the syntax accepted by
     * RE2 and described at https://github.com/google/re2/wiki/Syntax
     */
    branch?: string;
    /**
     * If true, only trigger a build if the revision regex does NOT match the git_ref regex.
     */
    invertRegex?: boolean;
    /**
     * Regex of tags to match.
     *
     * The syntax of the regular expressions accepted is the syntax accepted by
     * RE2 and described at https://github.com/google/re2/wiki/Syntax
     */
    tag?: string;
}

export interface CloudbuildTriggerSourceToBuild {
    /**
     * The full resource name of the bitbucket server config.
     * Format: projects/{project}/locations/{location}/bitbucketServerConfigs/{id}.
     */
    bitbucketServerConfig?: string;
    /**
     * The full resource name of the github enterprise config.
     * Format: projects/{project}/locations/{location}/githubEnterpriseConfigs/{id}. projects/{project}/githubEnterpriseConfigs/{id}.
     */
    githubEnterpriseConfig?: string;
    /**
     * The branch or tag to use. Must start with "refs/" (required).
     */
    ref: string;
    /**
     * The type of the repo, since it may not be explicit from the repo field (e.g from a URL).
     * Values can be UNKNOWN, CLOUD_SOURCE_REPOSITORIES, GITHUB, BITBUCKET_SERVER Possible values: ["UNKNOWN", "CLOUD_SOURCE_REPOSITORIES", "GITHUB", "BITBUCKET_SERVER"]
     */
    repoType: string;
    /**
     * The qualified resource name of the Repo API repository.
     * Either uri or repository can be specified and is required.
     */
    repository?: string;
    /**
     * The URI of the repo.
     */
    uri?: string;
}

export interface CloudbuildTriggerTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface CloudbuildTriggerTriggerTemplate {
    /**
     * Name of the branch to build. Exactly one a of branch name, tag, or commit SHA must be provided.
     * This field is a regular expression.
     */
    branchName?: string;
    /**
     * Explicit commit SHA to build. Exactly one of a branch name, tag, or commit SHA must be provided.
     */
    commitSha?: string;
    /**
     * Directory, relative to the source root, in which to run the build.
     *
     * This must be a relative path. If a step's dir is specified and
     * is an absolute path, this value is ignored for that step's
     * execution.
     */
    dir?: string;
    /**
     * Only trigger a build if the revision regex does NOT match the revision regex.
     */
    invertRegex?: boolean;
    /**
     * ID of the project that owns the Cloud Source Repository. If
     * omitted, the project ID requesting the build is assumed.
     */
    projectId: string;
    /**
     * Name of the Cloud Source Repository. If omitted, the name "default" is assumed.
     */
    repoName?: string;
    /**
     * Name of the tag to build. Exactly one of a branch name, tag, or commit SHA must be provided.
     * This field is a regular expression.
     */
    tagName?: string;
}

export interface CloudbuildTriggerWebhookConfig {
    /**
     * Resource name for the secret required as a URL parameter.
     */
    secret: string;
    /**
     * Potential issues with the underlying Pub/Sub subscription configuration.
     * Only populated on get requests.
     */
    state: string;
}

export interface CloudbuildWorkerPoolNetworkConfig {
    /**
     * Required. Immutable. The network definition that the workers are peered to. If this section is left empty, the workers will be peered to `WorkerPool.project_id` on the service producer network. Must be in the format `projects/{project}/global/networks/{network}`, where `{project}` is a project number, such as `12345`, and `{network}` is the name of a VPC network in the project. See [Understanding network configuration options](https://cloud.google.com/cloud-build/docs/custom-workers/set-up-custom-worker-pool-environment#understanding_the_network_configuration_options)
     */
    peeredNetwork: string;
    /**
     * Optional. Immutable. Subnet IP range within the peered network. This is specified in CIDR notation with a slash and the subnet prefix size. You can optionally specify an IP address before the subnet prefix value. e.g. `192.168.0.0/29` would specify an IP range starting at 192.168.0.0 with a prefix size of 29 bits. `/16` would specify a prefix size of 16 bits, with an automatically determined IP within the peered VPC. If unspecified, a value of `/24` will be used.
     */
    peeredNetworkIpRange?: string;
}

export interface CloudbuildWorkerPoolTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface CloudbuildWorkerPoolWorkerConfig {
    /**
     * Size of the disk attached to the worker, in GB. See [Worker pool config file](https://cloud.google.com/cloud-build/docs/custom-workers/worker-pool-config-file). Specify a value of up to 1000. If `0` is specified, Cloud Build will use a standard disk size.
     */
    diskSizeGb?: number;
    /**
     * Machine type of a worker, such as `n1-standard-1`. See [Worker pool config file](https://cloud.google.com/cloud-build/docs/custom-workers/worker-pool-config-file). If left blank, Cloud Build will use `n1-standard-1`.
     */
    machineType?: string;
    /**
     * If true, workers are created without any public address, which prevents network egress to public IPs.
     */
    noExternalIp: boolean;
}

export interface Cloudbuildv2ConnectionBitbucketCloudConfig {
    /**
     * Required. An access token with the 'webhook', 'repository', 'repository:admin' and 'pullrequest' scope access. It can be either a workspace, project or repository access token. It's recommended to use a system account to generate these credentials.
     */
    authorizerCredential: outputs.Cloudbuildv2ConnectionBitbucketCloudConfigAuthorizerCredential;
    /**
     * Required. An access token with the 'repository' access. It can be either a workspace, project or repository access token. It's recommended to use a system account to generate the credentials.
     */
    readAuthorizerCredential: outputs.Cloudbuildv2ConnectionBitbucketCloudConfigReadAuthorizerCredential;
    /**
     * Required. Immutable. SecretManager resource containing the webhook secret used to verify webhook events, formatted as 'projects/*&#47;secrets/*&#47;versions/*'.
     */
    webhookSecretSecretVersion: string;
    /**
     * The Bitbucket Cloud Workspace ID to be connected to Google Cloud Platform.
     */
    workspace: string;
}

export interface Cloudbuildv2ConnectionBitbucketCloudConfigAuthorizerCredential {
    /**
     * Required. A SecretManager resource containing the user token that authorizes the Cloud Build connection. Format: 'projects/*&#47;secrets/*&#47;versions/*'.
     */
    userTokenSecretVersion: string;
    /**
     * Output only. The username associated to this token.
     */
    username: string;
}

export interface Cloudbuildv2ConnectionBitbucketCloudConfigReadAuthorizerCredential {
    /**
     * Required. A SecretManager resource containing the user token that authorizes the Cloud Build connection. Format: 'projects/*&#47;secrets/*&#47;versions/*'.
     */
    userTokenSecretVersion: string;
    /**
     * Output only. The username associated to this token.
     */
    username: string;
}

export interface Cloudbuildv2ConnectionBitbucketDataCenterConfig {
    /**
     * Required. A http access token with the 'REPO_ADMIN' scope access.
     */
    authorizerCredential: outputs.Cloudbuildv2ConnectionBitbucketDataCenterConfigAuthorizerCredential;
    /**
     * The URI of the Bitbucket Data Center host this connection is for.
     */
    hostUri: string;
    /**
     * Required. A http access token with the 'REPO_READ' access.
     */
    readAuthorizerCredential: outputs.Cloudbuildv2ConnectionBitbucketDataCenterConfigReadAuthorizerCredential;
    /**
     * Output only. Version of the Bitbucket Data Center running on the 'host_uri'.
     */
    serverVersion: string;
    /**
     * Configuration for using Service Directory to privately connect to a Bitbucket Data Center. This should only be set if the Bitbucket Data Center is hosted on-premises and not reachable by public internet. If this field is left empty, calls to the Bitbucket Data Center will be made over the public internet.
     */
    serviceDirectoryConfig?: outputs.Cloudbuildv2ConnectionBitbucketDataCenterConfigServiceDirectoryConfig;
    /**
     * SSL certificate to use for requests to the Bitbucket Data Center.
     */
    sslCa?: string;
    /**
     * Required. Immutable. SecretManager resource containing the webhook secret used to verify webhook events, formatted as 'projects/*&#47;secrets/*&#47;versions/*'.
     */
    webhookSecretSecretVersion: string;
}

export interface Cloudbuildv2ConnectionBitbucketDataCenterConfigAuthorizerCredential {
    /**
     * Required. A SecretManager resource containing the user token that authorizes the Cloud Build connection. Format: 'projects/*&#47;secrets/*&#47;versions/*'.
     */
    userTokenSecretVersion: string;
    /**
     * Output only. The username associated to this token.
     */
    username: string;
}

export interface Cloudbuildv2ConnectionBitbucketDataCenterConfigReadAuthorizerCredential {
    /**
     * Required. A SecretManager resource containing the user token that authorizes the Cloud Build connection. Format: 'projects/*&#47;secrets/*&#47;versions/*'.
     */
    userTokenSecretVersion: string;
    /**
     * Output only. The username associated to this token.
     */
    username: string;
}

export interface Cloudbuildv2ConnectionBitbucketDataCenterConfigServiceDirectoryConfig {
    /**
     * Required. The Service Directory service name. Format: projects/{project}/locations/{location}/namespaces/{namespace}/services/{service}.
     */
    service: string;
}

export interface Cloudbuildv2ConnectionGithubConfig {
    /**
     * GitHub App installation id.
     */
    appInstallationId?: number;
    /**
     * OAuth credential of the account that authorized the Cloud Build GitHub App. It is recommended to use a robot account instead of a human user account. The OAuth token must be tied to the Cloud Build GitHub App.
     */
    authorizerCredential?: outputs.Cloudbuildv2ConnectionGithubConfigAuthorizerCredential;
}

export interface Cloudbuildv2ConnectionGithubConfigAuthorizerCredential {
    /**
     * A SecretManager resource containing the OAuth token that authorizes the Cloud Build connection. Format: 'projects/*&#47;secrets/*&#47;versions/*'.
     */
    oauthTokenSecretVersion?: string;
    /**
     * Output only. The username associated to this token.
     */
    username: string;
}

export interface Cloudbuildv2ConnectionGithubEnterpriseConfig {
    /**
     * Id of the GitHub App created from the manifest.
     */
    appId?: number;
    /**
     * ID of the installation of the GitHub App.
     */
    appInstallationId?: number;
    /**
     * The URL-friendly name of the GitHub App.
     */
    appSlug?: string;
    /**
     * Required. The URI of the GitHub Enterprise host this connection is for.
     */
    hostUri: string;
    /**
     * SecretManager resource containing the private key of the GitHub App, formatted as 'projects/*&#47;secrets/*&#47;versions/*'.
     */
    privateKeySecretVersion?: string;
    /**
     * Configuration for using Service Directory to privately connect to a GitHub Enterprise server. This should only be set if the GitHub Enterprise server is hosted on-premises and not reachable by public internet. If this field is left empty, calls to the GitHub Enterprise server will be made over the public internet.
     */
    serviceDirectoryConfig?: outputs.Cloudbuildv2ConnectionGithubEnterpriseConfigServiceDirectoryConfig;
    /**
     * SSL certificate to use for requests to GitHub Enterprise.
     */
    sslCa?: string;
    /**
     * SecretManager resource containing the webhook secret of the GitHub App, formatted as 'projects/*&#47;secrets/*&#47;versions/*'.
     */
    webhookSecretSecretVersion?: string;
}

export interface Cloudbuildv2ConnectionGithubEnterpriseConfigServiceDirectoryConfig {
    /**
     * Required. The Service Directory service name. Format: projects/{project}/locations/{location}/namespaces/{namespace}/services/{service}.
     */
    service: string;
}

export interface Cloudbuildv2ConnectionGitlabConfig {
    /**
     * Required. A GitLab personal access token with the 'api' scope access.
     */
    authorizerCredential: outputs.Cloudbuildv2ConnectionGitlabConfigAuthorizerCredential;
    /**
     * The URI of the GitLab Enterprise host this connection is for. If not specified, the default value is https://gitlab.com.
     */
    hostUri: string;
    /**
     * Required. A GitLab personal access token with the minimum 'read_api' scope access.
     */
    readAuthorizerCredential: outputs.Cloudbuildv2ConnectionGitlabConfigReadAuthorizerCredential;
    /**
     * Output only. Version of the GitLab Enterprise server running on the 'host_uri'.
     */
    serverVersion: string;
    /**
     * Configuration for using Service Directory to privately connect to a GitLab Enterprise server. This should only be set if the GitLab Enterprise server is hosted on-premises and not reachable by public internet. If this field is left empty, calls to the GitLab Enterprise server will be made over the public internet.
     */
    serviceDirectoryConfig?: outputs.Cloudbuildv2ConnectionGitlabConfigServiceDirectoryConfig;
    /**
     * SSL certificate to use for requests to GitLab Enterprise.
     */
    sslCa?: string;
    /**
     * Required. Immutable. SecretManager resource containing the webhook secret of a GitLab Enterprise project, formatted as 'projects/*&#47;secrets/*&#47;versions/*'.
     */
    webhookSecretSecretVersion: string;
}

export interface Cloudbuildv2ConnectionGitlabConfigAuthorizerCredential {
    /**
     * Required. A SecretManager resource containing the user token that authorizes the Cloud Build connection. Format: 'projects/*&#47;secrets/*&#47;versions/*'.
     */
    userTokenSecretVersion: string;
    /**
     * Output only. The username associated to this token.
     */
    username: string;
}

export interface Cloudbuildv2ConnectionGitlabConfigReadAuthorizerCredential {
    /**
     * Required. A SecretManager resource containing the user token that authorizes the Cloud Build connection. Format: 'projects/*&#47;secrets/*&#47;versions/*'.
     */
    userTokenSecretVersion: string;
    /**
     * Output only. The username associated to this token.
     */
    username: string;
}

export interface Cloudbuildv2ConnectionGitlabConfigServiceDirectoryConfig {
    /**
     * Required. The Service Directory service name. Format: projects/{project}/locations/{location}/namespaces/{namespace}/services/{service}.
     */
    service: string;
}

export interface Cloudbuildv2ConnectionIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface Cloudbuildv2ConnectionIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface Cloudbuildv2ConnectionInstallationState {
    actionUri: string;
    message: string;
    stage: string;
}

export interface Cloudbuildv2ConnectionTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface Cloudbuildv2RepositoryTimeouts {
    create?: string;
    delete?: string;
}

export interface ClouddeployAutomationRule {
    /**
     * Optional. The 'AdvanceRolloutRule' will automatically advance a successful Rollout.
     */
    advanceRolloutRule?: outputs.ClouddeployAutomationRuleAdvanceRolloutRule;
    /**
     * Optional. 'PromoteReleaseRule' will automatically promote a release from the current target to a specified target.
     */
    promoteReleaseRule?: outputs.ClouddeployAutomationRulePromoteReleaseRule;
}

export interface ClouddeployAutomationRuleAdvanceRolloutRule {
    /**
     * Required. ID of the rule. This id must be unique in the 'Automation' resource to which this rule belongs. The format is 'a-z{0,62}'.
     */
    id: string;
    /**
     * Optional. Proceeds only after phase name matched any one in the list. This value must consist of lower-case letters, numbers, and hyphens, start with a letter and end with a letter or a number, and have a max length of 63 characters. In other words, it must match the following regex: '^a-z?$'.
     */
    sourcePhases?: string[];
    /**
     * Optional. How long to wait after a rollout is finished.
     */
    wait?: string;
}

export interface ClouddeployAutomationRulePromoteReleaseRule {
    /**
     * Optional. The starting phase of the rollout created by this operation. Default to the first phase.
     */
    destinationPhase?: string;
    /**
     * Optional. The ID of the stage in the pipeline to which this 'Release' is deploying. If unspecified, default it to the next stage in the promotion flow. The value of this field could be one of the following: * The last segment of a target name. It only needs the ID to determine if the target is one of the stages in the promotion sequence defined in the pipeline. * "@next", the next target in the promotion sequence.
     */
    destinationTargetId?: string;
    /**
     * Required. ID of the rule. This id must be unique in the 'Automation' resource to which this rule belongs. The format is 'a-z{0,62}'.
     */
    id: string;
    /**
     * Optional. How long the release need to be paused until being promoted to the next target.
     */
    wait?: string;
}

export interface ClouddeployAutomationSelector {
    /**
     * Contains attributes about a target.
     */
    targets: outputs.ClouddeployAutomationSelectorTarget[];
}

export interface ClouddeployAutomationSelectorTarget {
    /**
     * ID of the 'Target'. The value of this field could be one of the following: * The last segment of a target name. It only needs the ID to determine which target is being referred to * "*", all targets in a location.
     */
    id?: string;
    /**
     * Target labels.
     */
    labels: {[key: string]: string};
}

export interface ClouddeployAutomationTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ClouddeployCustomTargetTypeCustomActions {
    /**
     * The Skaffold custom action responsible for deploy operations.
     */
    deployAction: string;
    /**
     * List of Skaffold modules Cloud Deploy will include in the Skaffold Config as required before performing diagnose.
     */
    includeSkaffoldModules?: outputs.ClouddeployCustomTargetTypeCustomActionsIncludeSkaffoldModule[];
    /**
     * The Skaffold custom action responsible for render operations. If not provided then Cloud Deploy will perform the render operations via 'skaffold render'.
     */
    renderAction?: string;
}

export interface ClouddeployCustomTargetTypeCustomActionsIncludeSkaffoldModule {
    /**
     * The Skaffold Config modules to use from the specified source.
     */
    configs?: string[];
    /**
     * Remote git repository containing the Skaffold Config modules.
     */
    git?: outputs.ClouddeployCustomTargetTypeCustomActionsIncludeSkaffoldModuleGit;
    /**
     * Cloud Build 2nd gen repository containing the Skaffold Config modules.
     */
    googleCloudBuildRepo?: outputs.ClouddeployCustomTargetTypeCustomActionsIncludeSkaffoldModuleGoogleCloudBuildRepo;
    /**
     * Cloud Storage bucket containing Skaffold Config modules.
     */
    googleCloudStorage?: outputs.ClouddeployCustomTargetTypeCustomActionsIncludeSkaffoldModuleGoogleCloudStorage;
}

export interface ClouddeployCustomTargetTypeCustomActionsIncludeSkaffoldModuleGit {
    /**
     * Relative path from the repository root to the Skaffold file.
     */
    path?: string;
    /**
     * Git ref the package should be cloned from.
     */
    ref?: string;
    /**
     * Git repository the package should be cloned from.
     */
    repo: string;
}

export interface ClouddeployCustomTargetTypeCustomActionsIncludeSkaffoldModuleGoogleCloudBuildRepo {
    /**
     * Relative path from the repository root to the Skaffold file.
     */
    path?: string;
    /**
     * Branch or tag to use when cloning the repository.
     */
    ref?: string;
    /**
     * Cloud Build 2nd gen repository in the format of 'projects/<project>/locations/<location>/connections/<connection>/repositories/<repository>'.
     */
    repository: string;
}

export interface ClouddeployCustomTargetTypeCustomActionsIncludeSkaffoldModuleGoogleCloudStorage {
    /**
     * Relative path from the source to the Skaffold file.
     */
    path?: string;
    /**
     * Cloud Storage source paths to copy recursively. For example, providing 'gs://my-bucket/dir/configs/*' will result in Skaffold copying all files within the 'dir/configs' directory in the bucket 'my-bucket'.
     */
    source: string;
}

export interface ClouddeployCustomTargetTypeIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ClouddeployCustomTargetTypeIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ClouddeployCustomTargetTypeTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ClouddeployDeliveryPipelineCondition {
    pipelineReadyConditions: outputs.ClouddeployDeliveryPipelineConditionPipelineReadyCondition[];
    targetsPresentConditions: outputs.ClouddeployDeliveryPipelineConditionTargetsPresentCondition[];
    targetsTypeConditions: outputs.ClouddeployDeliveryPipelineConditionTargetsTypeCondition[];
}

export interface ClouddeployDeliveryPipelineConditionPipelineReadyCondition {
    status: boolean;
    updateTime: string;
}

export interface ClouddeployDeliveryPipelineConditionTargetsPresentCondition {
    missingTargets: string[];
    status: boolean;
    updateTime: string;
}

export interface ClouddeployDeliveryPipelineConditionTargetsTypeCondition {
    errorDetails: string;
    status: boolean;
}

export interface ClouddeployDeliveryPipelineIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ClouddeployDeliveryPipelineIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ClouddeployDeliveryPipelineSerialPipeline {
    /**
     * Each stage specifies configuration for a `Target`. The ordering of this list defines the promotion flow.
     */
    stages?: outputs.ClouddeployDeliveryPipelineSerialPipelineStage[];
}

export interface ClouddeployDeliveryPipelineSerialPipelineStage {
    /**
     * Optional. The deploy parameters to use for the target in this stage.
     */
    deployParameters?: outputs.ClouddeployDeliveryPipelineSerialPipelineStageDeployParameter[];
    /**
     * Skaffold profiles to use when rendering the manifest for this stage's `Target`.
     */
    profiles?: string[];
    /**
     * Optional. The strategy to use for a `Rollout` to this stage.
     */
    strategy?: outputs.ClouddeployDeliveryPipelineSerialPipelineStageStrategy;
    /**
     * The target_id to which this stage points. This field refers exclusively to the last segment of a target name. For example, this field would just be `my-target` (rather than `projects/project/locations/location/targets/my-target`). The location of the `Target` is inferred to be the same as the location of the `DeliveryPipeline` that contains this `Stage`.
     */
    targetId?: string;
}

export interface ClouddeployDeliveryPipelineSerialPipelineStageDeployParameter {
    /**
     * Optional. Deploy parameters are applied to targets with match labels. If unspecified, deploy parameters are applied to all targets (including child targets of a multi-target).
     */
    matchTargetLabels?: {[key: string]: string};
    /**
     * Required. Values are deploy parameters in key-value pairs.
     */
    values: {[key: string]: string};
}

export interface ClouddeployDeliveryPipelineSerialPipelineStageStrategy {
    /**
     * Canary deployment strategy provides progressive percentage based deployments to a Target.
     */
    canary?: outputs.ClouddeployDeliveryPipelineSerialPipelineStageStrategyCanary;
    /**
     * Standard deployment strategy executes a single deploy and allows verifying the deployment.
     */
    standard?: outputs.ClouddeployDeliveryPipelineSerialPipelineStageStrategyStandard;
}

export interface ClouddeployDeliveryPipelineSerialPipelineStageStrategyCanary {
    /**
     * Configures the progressive based deployment for a Target.
     */
    canaryDeployment?: outputs.ClouddeployDeliveryPipelineSerialPipelineStageStrategyCanaryCanaryDeployment;
    /**
     * Configures the progressive based deployment for a Target, but allows customizing at the phase level where a phase represents each of the percentage deployments.
     */
    customCanaryDeployment?: outputs.ClouddeployDeliveryPipelineSerialPipelineStageStrategyCanaryCustomCanaryDeployment;
    /**
     * Optional. Runtime specific configurations for the deployment strategy. The runtime configuration is used to determine how Cloud Deploy will split traffic to enable a progressive deployment.
     */
    runtimeConfig?: outputs.ClouddeployDeliveryPipelineSerialPipelineStageStrategyCanaryRuntimeConfig;
}

export interface ClouddeployDeliveryPipelineSerialPipelineStageStrategyCanaryCanaryDeployment {
    /**
     * Required. The percentage based deployments that will occur as a part of a `Rollout`. List is expected in ascending order and each integer n is 0 <= n < 100.
     */
    percentages: number[];
    /**
     * Optional. Configuration for the postdeploy job of the last phase. If this is not configured, postdeploy job will not be present.
     */
    postdeploy?: outputs.ClouddeployDeliveryPipelineSerialPipelineStageStrategyCanaryCanaryDeploymentPostdeploy;
    /**
     * Optional. Configuration for the predeploy job of the first phase. If this is not configured, predeploy job will not be present.
     */
    predeploy?: outputs.ClouddeployDeliveryPipelineSerialPipelineStageStrategyCanaryCanaryDeploymentPredeploy;
    /**
     * Whether to run verify tests after each percentage deployment.
     */
    verify?: boolean;
}

export interface ClouddeployDeliveryPipelineSerialPipelineStageStrategyCanaryCanaryDeploymentPostdeploy {
    /**
     * Optional. A sequence of skaffold custom actions to invoke during execution of the postdeploy job.
     */
    actions?: string[];
}

export interface ClouddeployDeliveryPipelineSerialPipelineStageStrategyCanaryCanaryDeploymentPredeploy {
    /**
     * Optional. A sequence of skaffold custom actions to invoke during execution of the predeploy job.
     */
    actions?: string[];
}

export interface ClouddeployDeliveryPipelineSerialPipelineStageStrategyCanaryCustomCanaryDeployment {
    /**
     * Required. Configuration for each phase in the canary deployment in the order executed.
     */
    phaseConfigs: outputs.ClouddeployDeliveryPipelineSerialPipelineStageStrategyCanaryCustomCanaryDeploymentPhaseConfig[];
}

export interface ClouddeployDeliveryPipelineSerialPipelineStageStrategyCanaryCustomCanaryDeploymentPhaseConfig {
    /**
     * Required. Percentage deployment for the phase.
     */
    percentage: number;
    /**
     * Required. The ID to assign to the `Rollout` phase. This value must consist of lower-case letters, numbers, and hyphens, start with a letter and end with a letter or a number, and have a max length of 63 characters. In other words, it must match the following regex: `^a-z?$`.
     */
    phaseId: string;
    /**
     * Optional. Configuration for the postdeploy job of this phase. If this is not configured, postdeploy job will not be present for this phase.
     */
    postdeploy?: outputs.ClouddeployDeliveryPipelineSerialPipelineStageStrategyCanaryCustomCanaryDeploymentPhaseConfigPostdeploy;
    /**
     * Optional. Configuration for the predeploy job of this phase. If this is not configured, predeploy job will not be present for this phase.
     */
    predeploy?: outputs.ClouddeployDeliveryPipelineSerialPipelineStageStrategyCanaryCustomCanaryDeploymentPhaseConfigPredeploy;
    /**
     * Skaffold profiles to use when rendering the manifest for this phase. These are in addition to the profiles list specified in the `DeliveryPipeline` stage.
     */
    profiles?: string[];
    /**
     * Whether to run verify tests after the deployment.
     */
    verify?: boolean;
}

export interface ClouddeployDeliveryPipelineSerialPipelineStageStrategyCanaryCustomCanaryDeploymentPhaseConfigPostdeploy {
    /**
     * Optional. A sequence of skaffold custom actions to invoke during execution of the postdeploy job.
     */
    actions?: string[];
}

export interface ClouddeployDeliveryPipelineSerialPipelineStageStrategyCanaryCustomCanaryDeploymentPhaseConfigPredeploy {
    /**
     * Optional. A sequence of skaffold custom actions to invoke during execution of the predeploy job.
     */
    actions?: string[];
}

export interface ClouddeployDeliveryPipelineSerialPipelineStageStrategyCanaryRuntimeConfig {
    /**
     * Cloud Run runtime configuration.
     */
    cloudRun?: outputs.ClouddeployDeliveryPipelineSerialPipelineStageStrategyCanaryRuntimeConfigCloudRun;
    /**
     * Kubernetes runtime configuration.
     */
    kubernetes?: outputs.ClouddeployDeliveryPipelineSerialPipelineStageStrategyCanaryRuntimeConfigKubernetes;
}

export interface ClouddeployDeliveryPipelineSerialPipelineStageStrategyCanaryRuntimeConfigCloudRun {
    /**
     * Whether Cloud Deploy should update the traffic stanza in a Cloud Run Service on the user's behalf to facilitate traffic splitting. This is required to be true for CanaryDeployments, but optional for CustomCanaryDeployments.
     */
    automaticTrafficControl?: boolean;
    /**
     * Optional. A list of tags that are added to the canary revision while the canary phase is in progress.
     */
    canaryRevisionTags?: string[];
    /**
     * Optional. A list of tags that are added to the prior revision while the canary phase is in progress.
     */
    priorRevisionTags?: string[];
    /**
     * Optional. A list of tags that are added to the final stable revision when the stable phase is applied.
     */
    stableRevisionTags?: string[];
}

export interface ClouddeployDeliveryPipelineSerialPipelineStageStrategyCanaryRuntimeConfigKubernetes {
    /**
     * Kubernetes Gateway API service mesh configuration.
     */
    gatewayServiceMesh?: outputs.ClouddeployDeliveryPipelineSerialPipelineStageStrategyCanaryRuntimeConfigKubernetesGatewayServiceMesh;
    /**
     * Kubernetes Service networking configuration.
     */
    serviceNetworking?: outputs.ClouddeployDeliveryPipelineSerialPipelineStageStrategyCanaryRuntimeConfigKubernetesServiceNetworking;
}

export interface ClouddeployDeliveryPipelineSerialPipelineStageStrategyCanaryRuntimeConfigKubernetesGatewayServiceMesh {
    /**
     * Required. Name of the Kubernetes Deployment whose traffic is managed by the specified HTTPRoute and Service.
     */
    deployment: string;
    /**
     * Required. Name of the Gateway API HTTPRoute.
     */
    httpRoute: string;
    /**
     * Optional. The label to use when selecting Pods for the Deployment and Service resources. This label must already be present in both resources.
     */
    podSelectorLabel?: string;
    /**
     * Optional. The time to wait for route updates to propagate. The maximum configurable time is 3 hours, in seconds format. If unspecified, there is no wait time.
     */
    routeUpdateWaitTime?: string;
    /**
     * Required. Name of the Kubernetes Service.
     */
    service: string;
    /**
     * Optional. The amount of time to migrate traffic back from the canary Service to the original Service during the stable phase deployment. If specified, must be between 15s and 3600s. If unspecified, there is no cutback time.
     */
    stableCutbackDuration?: string;
}

export interface ClouddeployDeliveryPipelineSerialPipelineStageStrategyCanaryRuntimeConfigKubernetesServiceNetworking {
    /**
     * Required. Name of the Kubernetes Deployment whose traffic is managed by the specified Service.
     */
    deployment: string;
    /**
     * Optional. Whether to disable Pod overprovisioning. If Pod overprovisioning is disabled then Cloud Deploy will limit the number of total Pods used for the deployment strategy to the number of Pods the Deployment has on the cluster.
     */
    disablePodOverprovisioning?: boolean;
    /**
     * Optional. The label to use when selecting Pods for the Deployment resource. This label must already be present in the Deployment.
     */
    podSelectorLabel?: string;
    /**
     * Required. Name of the Kubernetes Service.
     */
    service: string;
}

export interface ClouddeployDeliveryPipelineSerialPipelineStageStrategyStandard {
    /**
     * Optional. Configuration for the postdeploy job. If this is not configured, postdeploy job will not be present.
     */
    postdeploy?: outputs.ClouddeployDeliveryPipelineSerialPipelineStageStrategyStandardPostdeploy;
    /**
     * Optional. Configuration for the predeploy job. If this is not configured, predeploy job will not be present.
     */
    predeploy?: outputs.ClouddeployDeliveryPipelineSerialPipelineStageStrategyStandardPredeploy;
    /**
     * Whether to verify a deployment.
     */
    verify?: boolean;
}

export interface ClouddeployDeliveryPipelineSerialPipelineStageStrategyStandardPostdeploy {
    /**
     * Optional. A sequence of skaffold custom actions to invoke during execution of the postdeploy job.
     */
    actions?: string[];
}

export interface ClouddeployDeliveryPipelineSerialPipelineStageStrategyStandardPredeploy {
    /**
     * Optional. A sequence of skaffold custom actions to invoke during execution of the predeploy job.
     */
    actions?: string[];
}

export interface ClouddeployDeliveryPipelineTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ClouddeployTargetAnthosCluster {
    /**
     * Membership of the GKE Hub-registered cluster to which to apply the Skaffold configuration. Format is `projects/{project}/locations/{location}/memberships/{membership_name}`.
     */
    membership?: string;
}

export interface ClouddeployTargetCustomTarget {
    /**
     * Required. The name of the CustomTargetType. Format must be `projects/{project}/locations/{location}/customTargetTypes/{custom_target_type}`.
     */
    customTargetType: string;
}

export interface ClouddeployTargetExecutionConfig {
    /**
     * Optional. Cloud Storage location in which to store execution outputs. This can either be a bucket ("gs://my-bucket") or a path within a bucket ("gs://my-bucket/my-dir"). If unspecified, a default bucket located in the same region will be used.
     */
    artifactStorage: string;
    /**
     * Optional. Execution timeout for a Cloud Build Execution. This must be between 10m and 24h in seconds format. If unspecified, a default timeout of 1h is used.
     */
    executionTimeout: string;
    /**
     * Optional. Google service account to use for execution. If unspecified, the project execution service account (-compute@developer.gserviceaccount.com) is used.
     */
    serviceAccount: string;
    /**
     * Required. Usages when this configuration should be applied.
     */
    usages: string[];
    /**
     * Optional. If true, additional logging will be enabled when running builds in this execution environment.
     */
    verbose?: boolean;
    /**
     * Optional. The resource name of the `WorkerPool`, with the format `projects/{project}/locations/{location}/workerPools/{worker_pool}`. If this optional field is unspecified, the default Cloud Build pool will be used.
     */
    workerPool?: string;
}

export interface ClouddeployTargetGke {
    /**
     * Information specifying a GKE Cluster. Format is `projects/{project_id}/locations/{location_id}/clusters/{cluster_id}.
     */
    cluster?: string;
    /**
     * Optional. If true, `cluster` is accessed using the private IP address of the control plane endpoint. Otherwise, the default IP address of the control plane endpoint is used. The default IP address is the private IP address for clusters with private control-plane endpoints and the public IP address otherwise. Only specify this option when `cluster` is a [private GKE cluster](https://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept).
     */
    internalIp?: boolean;
    /**
     * Optional. If set, used to configure a [proxy](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/#proxy) to the Kubernetes server.
     */
    proxyUrl?: string;
}

export interface ClouddeployTargetIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ClouddeployTargetIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ClouddeployTargetMultiTarget {
    /**
     * Required. The target_ids of this multiTarget.
     */
    targetIds: string[];
}

export interface ClouddeployTargetRun {
    /**
     * Required. The location where the Cloud Run Service should be located. Format is `projects/{project}/locations/{location}`.
     */
    location: string;
}

export interface ClouddeployTargetTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ClouddomainsRegistrationContactSettings {
    /**
     * Caution: Anyone with access to this email address, phone number, and/or postal address can take control of the domain.
     *
     * Warning: For new Registrations, the registrant receives an email confirmation that they must complete within 15 days to
     * avoid domain suspension.
     */
    adminContact: outputs.ClouddomainsRegistrationContactSettingsAdminContact;
    /**
     * Required. Privacy setting for the contacts associated with the Registration.
     * Values are PUBLIC_CONTACT_DATA, PRIVATE_CONTACT_DATA, and REDACTED_CONTACT_DATA
     */
    privacy: string;
    /**
     * Caution: Anyone with access to this email address, phone number, and/or postal address can take control of the domain.
     *
     * Warning: For new Registrations, the registrant receives an email confirmation that they must complete within 15 days to
     * avoid domain suspension.
     */
    registrantContact: outputs.ClouddomainsRegistrationContactSettingsRegistrantContact;
    /**
     * Caution: Anyone with access to this email address, phone number, and/or postal address can take control of the domain.
     *
     * Warning: For new Registrations, the registrant receives an email confirmation that they must complete within 15 days to
     * avoid domain suspension.
     */
    technicalContact: outputs.ClouddomainsRegistrationContactSettingsTechnicalContact;
}

export interface ClouddomainsRegistrationContactSettingsAdminContact {
    /**
     * Required. Email address of the contact.
     */
    email: string;
    /**
     * Fax number of the contact in international format. For example, "+1-800-555-0123".
     */
    faxNumber?: string;
    /**
     * Required. Phone number of the contact in international format. For example, "+1-800-555-0123".
     */
    phoneNumber: string;
    /**
     * Required. Postal address of the contact.
     */
    postalAddress: outputs.ClouddomainsRegistrationContactSettingsAdminContactPostalAddress;
}

export interface ClouddomainsRegistrationContactSettingsAdminContactPostalAddress {
    /**
     * Unstructured address lines describing the lower levels of an address.
     * Because values in addressLines do not have type information and may sometimes contain multiple values in a single
     * field (e.g. "Austin, TX"), it is important that the line order is clear. The order of address lines should be
     * "envelope order" for the country/region of the address. In places where this can vary (e.g. Japan), address_language
     * is used to make it explicit (e.g. "ja" for large-to-small ordering and "ja-Latn" or "en" for small-to-large). This way,
     * the most specific line of an address can be selected based on the language.
     */
    addressLines?: string[];
    /**
     * Highest administrative subdivision which is used for postal addresses of a country or region. For example, this can be a state,
     * a province, an oblast, or a prefecture. Specifically, for Spain this is the province and not the autonomous community
     * (e.g. "Barcelona" and not "Catalonia"). Many countries don't use an administrative area in postal addresses. E.g. in Switzerland
     * this should be left unpopulated.
     */
    administrativeArea?: string;
    /**
     * Generally refers to the city/town portion of the address. Examples: US city, IT comune, UK post town. In regions of the world
     * where localities are not well defined or do not fit into this structure well, leave locality empty and use addressLines.
     */
    locality?: string;
    /**
     * The name of the organization at the address.
     */
    organization?: string;
    /**
     * Postal code of the address. Not all countries use or require postal codes to be present, but where they are used,
     * they may trigger additional validation with other parts of the address (e.g. state/zip validation in the U.S.A.).
     */
    postalCode?: string;
    /**
     * The recipient at the address. This field may, under certain circumstances, contain multiline information. For example,
     * it might contain "care of" information.
     */
    recipients?: string[];
    /**
     * Required. CLDR region code of the country/region of the address. This is never inferred and it is up to the user to
     * ensure the value is correct. See https://cldr.unicode.org/ and
     * https://www.unicode.org/cldr/charts/30/supplemental/territory_information.html for details. Example: "CH" for Switzerland.
     */
    regionCode: string;
}

export interface ClouddomainsRegistrationContactSettingsRegistrantContact {
    /**
     * Required. Email address of the contact.
     */
    email: string;
    /**
     * Fax number of the contact in international format. For example, "+1-800-555-0123".
     */
    faxNumber?: string;
    /**
     * Required. Phone number of the contact in international format. For example, "+1-800-555-0123".
     */
    phoneNumber: string;
    /**
     * Required. Postal address of the contact.
     */
    postalAddress: outputs.ClouddomainsRegistrationContactSettingsRegistrantContactPostalAddress;
}

export interface ClouddomainsRegistrationContactSettingsRegistrantContactPostalAddress {
    /**
     * Unstructured address lines describing the lower levels of an address.
     * Because values in addressLines do not have type information and may sometimes contain multiple values in a single
     * field (e.g. "Austin, TX"), it is important that the line order is clear. The order of address lines should be
     * "envelope order" for the country/region of the address. In places where this can vary (e.g. Japan), address_language
     * is used to make it explicit (e.g. "ja" for large-to-small ordering and "ja-Latn" or "en" for small-to-large). This way,
     * the most specific line of an address can be selected based on the language.
     */
    addressLines?: string[];
    /**
     * Highest administrative subdivision which is used for postal addresses of a country or region. For example, this can be a state,
     * a province, an oblast, or a prefecture. Specifically, for Spain this is the province and not the autonomous community
     * (e.g. "Barcelona" and not "Catalonia"). Many countries don't use an administrative area in postal addresses. E.g. in Switzerland
     * this should be left unpopulated.
     */
    administrativeArea?: string;
    /**
     * Generally refers to the city/town portion of the address. Examples: US city, IT comune, UK post town. In regions of the world
     * where localities are not well defined or do not fit into this structure well, leave locality empty and use addressLines.
     */
    locality?: string;
    /**
     * The name of the organization at the address.
     */
    organization?: string;
    /**
     * Postal code of the address. Not all countries use or require postal codes to be present, but where they are used,
     * they may trigger additional validation with other parts of the address (e.g. state/zip validation in the U.S.A.).
     */
    postalCode?: string;
    /**
     * The recipient at the address. This field may, under certain circumstances, contain multiline information. For example,
     * it might contain "care of" information.
     */
    recipients?: string[];
    /**
     * Required. CLDR region code of the country/region of the address. This is never inferred and it is up to the user to
     * ensure the value is correct. See https://cldr.unicode.org/ and
     * https://www.unicode.org/cldr/charts/30/supplemental/territory_information.html for details. Example: "CH" for Switzerland.
     */
    regionCode: string;
}

export interface ClouddomainsRegistrationContactSettingsTechnicalContact {
    /**
     * Required. Email address of the contact.
     */
    email: string;
    /**
     * Fax number of the contact in international format. For example, "+1-800-555-0123".
     */
    faxNumber?: string;
    /**
     * Required. Phone number of the contact in international format. For example, "+1-800-555-0123".
     */
    phoneNumber: string;
    /**
     * Required. Postal address of the contact.
     */
    postalAddress: outputs.ClouddomainsRegistrationContactSettingsTechnicalContactPostalAddress;
}

export interface ClouddomainsRegistrationContactSettingsTechnicalContactPostalAddress {
    /**
     * Unstructured address lines describing the lower levels of an address.
     * Because values in addressLines do not have type information and may sometimes contain multiple values in a single
     * field (e.g. "Austin, TX"), it is important that the line order is clear. The order of address lines should be
     * "envelope order" for the country/region of the address. In places where this can vary (e.g. Japan), address_language
     * is used to make it explicit (e.g. "ja" for large-to-small ordering and "ja-Latn" or "en" for small-to-large). This way,
     * the most specific line of an address can be selected based on the language.
     */
    addressLines?: string[];
    /**
     * Highest administrative subdivision which is used for postal addresses of a country or region. For example, this can be a state,
     * a province, an oblast, or a prefecture. Specifically, for Spain this is the province and not the autonomous community
     * (e.g. "Barcelona" and not "Catalonia"). Many countries don't use an administrative area in postal addresses. E.g. in Switzerland
     * this should be left unpopulated.
     */
    administrativeArea?: string;
    /**
     * Generally refers to the city/town portion of the address. Examples: US city, IT comune, UK post town. In regions of the world
     * where localities are not well defined or do not fit into this structure well, leave locality empty and use addressLines.
     */
    locality?: string;
    /**
     * The name of the organization at the address.
     */
    organization?: string;
    /**
     * Postal code of the address. Not all countries use or require postal codes to be present, but where they are used,
     * they may trigger additional validation with other parts of the address (e.g. state/zip validation in the U.S.A.).
     */
    postalCode?: string;
    /**
     * The recipient at the address. This field may, under certain circumstances, contain multiline information. For example,
     * it might contain "care of" information.
     */
    recipients?: string[];
    /**
     * Required. CLDR region code of the country/region of the address. This is never inferred and it is up to the user to
     * ensure the value is correct. See https://cldr.unicode.org/ and
     * https://www.unicode.org/cldr/charts/30/supplemental/territory_information.html for details. Example: "CH" for Switzerland.
     */
    regionCode: string;
}

export interface ClouddomainsRegistrationDnsSettings {
    /**
     * Configuration for an arbitrary DNS provider.
     */
    customDns?: outputs.ClouddomainsRegistrationDnsSettingsCustomDns;
    /**
     * The list of glue records for this Registration. Commonly empty.
     */
    glueRecords?: outputs.ClouddomainsRegistrationDnsSettingsGlueRecord[];
}

export interface ClouddomainsRegistrationDnsSettingsCustomDns {
    /**
     * The list of DS records for this domain, which are used to enable DNSSEC. The domain's DNS provider can provide
     * the values to set here. If this field is empty, DNSSEC is disabled.
     */
    dsRecords?: outputs.ClouddomainsRegistrationDnsSettingsCustomDnsDsRecord[];
    /**
     * Required. A list of name servers that store the DNS zone for this domain. Each name server is a domain
     * name, with Unicode domain names expressed in Punycode format.
     */
    nameServers: string[];
}

export interface ClouddomainsRegistrationDnsSettingsCustomDnsDsRecord {
    /**
     * The algorithm used to generate the referenced DNSKEY.
     */
    algorithm?: string;
    /**
     * The digest generated from the referenced DNSKEY.
     */
    digest?: string;
    /**
     * The hash function used to generate the digest of the referenced DNSKEY.
     */
    digestType?: string;
    /**
     * The key tag of the record. Must be set in range 0 -- 65535.
     */
    keyTag?: number;
}

export interface ClouddomainsRegistrationDnsSettingsGlueRecord {
    /**
     * Required. Domain name of the host in Punycode format.
     */
    hostName: string;
    /**
     * List of IPv4 addresses corresponding to this host in the standard decimal format (e.g. 198.51.100.1).
     * At least one of ipv4_address and ipv6_address must be set.
     */
    ipv4Addresses?: string[];
    /**
     * List of IPv4 addresses corresponding to this host in the standard decimal format (e.g. 198.51.100.1).
     * At least one of ipv4_address and ipv6_address must be set.
     */
    ipv6Addresses?: string[];
}

export interface ClouddomainsRegistrationManagementSettings {
    /**
     * The desired renewal method for this Registration. The actual renewalMethod is automatically updated to reflect this choice.
     * If unset or equal to RENEWAL_METHOD_UNSPECIFIED, the actual renewalMethod is treated as if it were set to AUTOMATIC_RENEWAL.
     * You cannot use RENEWAL_DISABLED during resource creation, and you can update the renewal status only when the Registration
     * resource has state ACTIVE or SUSPENDED.
     *
     * When preferredRenewalMethod is set to AUTOMATIC_RENEWAL, the actual renewalMethod can be set to RENEWAL_DISABLED in case of
     * problems with the billing account or reported domain abuse. In such cases, check the issues field on the Registration. After
     * the problem is resolved, the renewalMethod is automatically updated to preferredRenewalMethod in a few hours.
     */
    preferredRenewalMethod: string;
    /**
     * Output only. The actual renewal method for this Registration. When preferredRenewalMethod is set to AUTOMATIC_RENEWAL,
     * the actual renewalMethod can be equal to RENEWAL_DISABLED—for example, when there are problems with the billing account
     * or reported domain abuse. In such cases, check the issues field on the Registration. After the problem is resolved, the
     * renewalMethod is automatically updated to preferredRenewalMethod in a few hours.
     */
    renewalMethod: string;
    /**
     * Controls whether the domain can be transferred to another registrar. Values are UNLOCKED or LOCKED.
     */
    transferLockState: string;
}

export interface ClouddomainsRegistrationTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ClouddomainsRegistrationYearlyPrice {
    /**
     * The three-letter currency code defined in ISO 4217.
     */
    currencyCode?: string;
    /**
     * The whole units of the amount. For example if currencyCode is "USD", then 1 unit is one US dollar.
     */
    units?: string;
}

export interface Cloudfunctions2FunctionBuildConfig {
    /**
     * Security patches are applied automatically to the runtime without requiring
     * the function to be redeployed.
     */
    automaticUpdatePolicy?: outputs.Cloudfunctions2FunctionBuildConfigAutomaticUpdatePolicy;
    /**
     * The Cloud Build name of the latest successful
     * deployment of the function.
     */
    build: string;
    /**
     * User managed repository created in Artifact Registry optionally with a customer managed encryption key.
     */
    dockerRepository: string;
    /**
     * The name of the function (as defined in source code) that will be executed.
     * Defaults to the resource name suffix, if not specified. For backward
     * compatibility, if function with given name is not found, then the system
     * will try to use function named "function". For Node.js this is name of a
     * function exported by the module specified in source_location.
     */
    entryPoint?: string;
    /**
     * User-provided build-time environment variables for the function.
     */
    environmentVariables: {[key: string]: string};
    /**
     * Security patches are only applied when a function is redeployed.
     */
    onDeployUpdatePolicy?: outputs.Cloudfunctions2FunctionBuildConfigOnDeployUpdatePolicy;
    /**
     * The runtime in which to run the function. Required when deploying a new
     * function, optional when updating an existing function.
     */
    runtime?: string;
    /**
     * The fully-qualified name of the service account to be used for building the container.
     */
    serviceAccount: string;
    /**
     * The location of the function source code.
     */
    source?: outputs.Cloudfunctions2FunctionBuildConfigSource;
    /**
     * Name of the Cloud Build Custom Worker Pool that should be used to build the function.
     */
    workerPool?: string;
}

export interface Cloudfunctions2FunctionBuildConfigAutomaticUpdatePolicy {
}

export interface Cloudfunctions2FunctionBuildConfigOnDeployUpdatePolicy {
    /**
     * The runtime version which was used during latest function deployment.
     */
    runtimeVersion: string;
}

export interface Cloudfunctions2FunctionBuildConfigSource {
    /**
     * If provided, get the source from this location in a Cloud Source Repository.
     */
    repoSource?: outputs.Cloudfunctions2FunctionBuildConfigSourceRepoSource;
    /**
     * If provided, get the source from this location in Google Cloud Storage.
     */
    storageSource?: outputs.Cloudfunctions2FunctionBuildConfigSourceStorageSource;
}

export interface Cloudfunctions2FunctionBuildConfigSourceRepoSource {
    /**
     * Regex matching branches to build.
     */
    branchName?: string;
    /**
     * Regex matching tags to build.
     */
    commitSha?: string;
    /**
     * Directory, relative to the source root, in which to run the build.
     */
    dir?: string;
    /**
     * Only trigger a build if the revision regex does
     * NOT match the revision regex.
     */
    invertRegex?: boolean;
    /**
     * ID of the project that owns the Cloud Source Repository. If omitted, the
     * project ID requesting the build is assumed.
     */
    projectId?: string;
    /**
     * Name of the Cloud Source Repository.
     */
    repoName?: string;
    /**
     * Regex matching tags to build.
     */
    tagName?: string;
}

export interface Cloudfunctions2FunctionBuildConfigSourceStorageSource {
    /**
     * Google Cloud Storage bucket containing the source
     */
    bucket?: string;
    /**
     * Google Cloud Storage generation for the object. If the generation
     * is omitted, the latest generation will be used.
     */
    generation: number;
    /**
     * Google Cloud Storage object containing the source.
     */
    object?: string;
}

export interface Cloudfunctions2FunctionEventTrigger {
    /**
     * Criteria used to filter events.
     */
    eventFilters?: outputs.Cloudfunctions2FunctionEventTriggerEventFilter[];
    /**
     * Required. The type of event to observe.
     */
    eventType?: string;
    /**
     * The name of a Pub/Sub topic in the same project that will be used
     * as the transport topic for the event delivery.
     */
    pubsubTopic: string;
    /**
     * Describes the retry policy in case of function's execution failure.
     * Retried execution is charged as any other execution. Possible values: ["RETRY_POLICY_UNSPECIFIED", "RETRY_POLICY_DO_NOT_RETRY", "RETRY_POLICY_RETRY"]
     */
    retryPolicy?: string;
    /**
     * Optional. The email of the trigger's service account. The service account
     * must have permission to invoke Cloud Run services. If empty, defaults to the
     * Compute Engine default service account: {project_number}-compute@developer.gserviceaccount.com.
     */
    serviceAccountEmail: string;
    /**
     * Output only. The resource name of the Eventarc trigger.
     */
    trigger: string;
    /**
     * The region that the trigger will be in. The trigger will only receive
     * events originating in this region. It can be the same
     * region as the function, a different region or multi-region, or the global
     * region. If not provided, defaults to the same region as the function.
     */
    triggerRegion: string;
}

export interface Cloudfunctions2FunctionEventTriggerEventFilter {
    /**
     * 'Required. The name of a CloudEvents attribute.
     * Currently, only a subset of attributes are supported for filtering. Use the 'gcloud eventarc providers describe' command to learn more about events and their attributes.
     * Do not filter for the 'type' attribute here, as this is already achieved by the resource's 'event_type' attribute.
     */
    attribute: string;
    /**
     * Optional. The operator used for matching the events with the value of
     * the filter. If not specified, only events that have an exact key-value
     * pair specified in the filter are matched.
     * The only allowed value is 'match-path-pattern'.
     * [See documentation on path patterns here](https://cloud.google.com/eventarc/docs/path-patterns)'
     */
    operator?: string;
    /**
     * Required. The value for the attribute.
     * If the operator field is set as 'match-path-pattern', this value can be a path pattern instead of an exact value.
     */
    value: string;
}

export interface Cloudfunctions2FunctionIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface Cloudfunctions2FunctionIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface Cloudfunctions2FunctionServiceConfig {
    /**
     * Whether 100% of traffic is routed to the latest revision. Defaults to true.
     */
    allTrafficOnLatestRevision?: boolean;
    /**
     * The number of CPUs used in a single container instance. Default value is calculated from available memory.
     */
    availableCpu: string;
    /**
     * The amount of memory available for a function.
     * Defaults to 256M. Supported units are k, M, G, Mi, Gi. If no unit is
     * supplied the value is interpreted as bytes.
     */
    availableMemory: string;
    /**
     * Environment variables that shall be available during function execution.
     */
    environmentVariables: {[key: string]: string};
    /**
     * URIs of the Service deployed
     */
    gcfUri: string;
    /**
     * Available ingress settings. Defaults to "ALLOW_ALL" if unspecified. Default value: "ALLOW_ALL" Possible values: ["ALLOW_ALL", "ALLOW_INTERNAL_ONLY", "ALLOW_INTERNAL_AND_GCLB"]
     */
    ingressSettings?: string;
    /**
     * The limit on the maximum number of function instances that may coexist at a
     * given time.
     */
    maxInstanceCount: number;
    /**
     * Sets the maximum number of concurrent requests that each instance can receive. Defaults to 1.
     */
    maxInstanceRequestConcurrency: number;
    /**
     * The limit on the minimum number of function instances that may coexist at a
     * given time.
     */
    minInstanceCount?: number;
    /**
     * Secret environment variables configuration.
     */
    secretEnvironmentVariables?: outputs.Cloudfunctions2FunctionServiceConfigSecretEnvironmentVariable[];
    /**
     * Secret volumes configuration.
     */
    secretVolumes?: outputs.Cloudfunctions2FunctionServiceConfigSecretVolume[];
    /**
     * Name of the service associated with a Function.
     */
    service: string;
    /**
     * The email of the service account for this function.
     */
    serviceAccountEmail: string;
    /**
     * The function execution timeout. Execution is considered failed and
     * can be terminated if the function is not completed at the end of the
     * timeout period. Defaults to 60 seconds.
     */
    timeoutSeconds: number;
    /**
     * URI of the Service deployed.
     */
    uri: string;
    /**
     * The Serverless VPC Access connector that this cloud function can connect to.
     */
    vpcConnector?: string;
    /**
     * Available egress settings. Possible values: ["VPC_CONNECTOR_EGRESS_SETTINGS_UNSPECIFIED", "PRIVATE_RANGES_ONLY", "ALL_TRAFFIC"]
     */
    vpcConnectorEgressSettings?: string;
}

export interface Cloudfunctions2FunctionServiceConfigSecretEnvironmentVariable {
    /**
     * Name of the environment variable.
     */
    key: string;
    /**
     * Project identifier (preferrably project number but can also be the project ID) of the project that contains the secret. If not set, it will be populated with the function's project assuming that the secret exists in the same project as of the function.
     */
    projectId: string;
    /**
     * Name of the secret in secret manager (not the full resource name).
     */
    secret: string;
    /**
     * Version of the secret (version number or the string 'latest'). It is recommended to use a numeric version for secret environment variables as any updates to the secret value is not reflected until new instances start.
     */
    version: string;
}

export interface Cloudfunctions2FunctionServiceConfigSecretVolume {
    /**
     * The path within the container to mount the secret volume. For example, setting the mountPath as /etc/secrets would mount the secret value files under the /etc/secrets directory. This directory will also be completely shadowed and unavailable to mount any other secrets. Recommended mount path: /etc/secrets
     */
    mountPath: string;
    /**
     * Project identifier (preferrably project number but can also be the project ID) of the project that contains the secret. If not set, it will be populated with the function's project assuming that the secret exists in the same project as of the function.
     */
    projectId: string;
    /**
     * Name of the secret in secret manager (not the full resource name).
     */
    secret: string;
    /**
     * List of secret versions to mount for this secret. If empty, the latest version of the secret will be made available in a file named after the secret under the mount point.'
     */
    versions?: outputs.Cloudfunctions2FunctionServiceConfigSecretVolumeVersion[];
}

export interface Cloudfunctions2FunctionServiceConfigSecretVolumeVersion {
    /**
     * Relative path of the file under the mount path where the secret value for this version will be fetched and made available. For example, setting the mountPath as '/etc/secrets' and path as secret_foo would mount the secret value file at /etc/secrets/secret_foo.
     */
    path: string;
    /**
     * Version of the secret (version number or the string 'latest'). It is preferable to use latest version with secret volumes as secret value changes are reflected immediately.
     */
    version: string;
}

export interface Cloudfunctions2FunctionTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface CloudfunctionsFunctionEventTrigger {
    /**
     * The type of event to observe. For example: "google.storage.object.finalize". See the documentation on calling Cloud Functions for a full reference of accepted triggers.
     */
    eventType: string;
    /**
     * Specifies policy for failed executions
     */
    failurePolicy?: outputs.CloudfunctionsFunctionEventTriggerFailurePolicy;
    /**
     * The name or partial URI of the resource from which to observe events. For example, "myBucket" or "projects/my-project/topics/my-topic"
     */
    resource: string;
}

export interface CloudfunctionsFunctionEventTriggerFailurePolicy {
    /**
     * Whether the function should be retried on failure. Defaults to false.
     */
    retry: boolean;
}

export interface CloudfunctionsFunctionIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface CloudfunctionsFunctionIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface CloudfunctionsFunctionSecretEnvironmentVariable {
    /**
     * Name of the environment variable.
     */
    key: string;
    /**
     * Project identifier (due to a known limitation, only project number is supported by this field) of the project that contains the secret. If not set, it will be populated with the function's project, assuming that the secret exists in the same project as of the function.
     */
    projectId: string;
    /**
     * ID of the secret in secret manager (not the full resource name).
     */
    secret: string;
    /**
     * Version of the secret (version number or the string "latest"). It is recommended to use a numeric version for secret environment variables as any updates to the secret value is not reflected until new clones start.
     */
    version: string;
}

export interface CloudfunctionsFunctionSecretVolume {
    /**
     * The path within the container to mount the secret volume. For example, setting the mount_path as "/etc/secrets" would mount the secret value files under the "/etc/secrets" directory. This directory will also be completely shadowed and unavailable to mount any other secrets. Recommended mount paths: "/etc/secrets" Restricted mount paths: "/cloudsql", "/dev/log", "/pod", "/proc", "/var/log".
     */
    mountPath: string;
    /**
     * Project identifier (due to a known limitation, only project number is supported by this field) of the project that contains the secret. If not set, it will be populated with the function's project, assuming that the secret exists in the same project as of the function.
     */
    projectId: string;
    /**
     * ID of the secret in secret manager (not the full resource name).
     */
    secret: string;
    /**
     * List of secret versions to mount for this secret. If empty, the "latest" version of the secret will be made available in a file named after the secret under the mount point.
     */
    versions?: outputs.CloudfunctionsFunctionSecretVolumeVersion[];
}

export interface CloudfunctionsFunctionSecretVolumeVersion {
    /**
     * Relative path of the file under the mount path where the secret value for this version will be fetched and made available. For example, setting the mount_path as "/etc/secrets" and path as "/secret_foo" would mount the secret value file at "/etc/secrets/secret_foo".
     */
    path: string;
    /**
     * Version of the secret (version number or the string "latest"). It is preferable to use "latest" version with secret volumes as secret value changes are reflected immediately.
     */
    version: string;
}

export interface CloudfunctionsFunctionSourceRepository {
    /**
     * The URL pointing to the hosted repository where the function was defined at the time of deployment.
     */
    deployedUrl: string;
    /**
     * The URL pointing to the hosted repository where the function is defined.
     */
    url: string;
}

export interface CloudfunctionsFunctionTimeouts {
    create?: string;
    delete?: string;
    read?: string;
    update?: string;
}

export interface ComposerEnvironmentConfig {
    /**
     * The URI of the Apache Airflow Web UI hosted within this environment.
     */
    airflowUri: string;
    /**
     * The Cloud Storage prefix of the DAGs for this environment. Although Cloud Storage objects reside in a flat namespace, a hierarchical file tree can be simulated using '/'-delimited object name prefixes. DAG objects for this environment reside in a simulated directory with this prefix.
     */
    dagGcsPrefix: string;
    /**
     * The configuration setting for Airflow data retention mechanism. This field is supported for Cloud Composer environments in versions composer-2.0.32-airflow-2.1.4. or newer
     */
    dataRetentionConfig?: outputs.ComposerEnvironmentConfigDataRetentionConfig;
    /**
     * The configuration of Cloud SQL instance that is used by the Apache Airflow software. This field is supported for Cloud Composer environments in versions composer-1.*.*-airflow-*.*.*.
     */
    databaseConfig?: outputs.ComposerEnvironmentConfigDatabaseConfig;
    /**
     * The encryption options for the Composer environment and its dependencies.
     */
    encryptionConfig?: outputs.ComposerEnvironmentConfigEncryptionConfig;
    /**
     * The size of the Cloud Composer environment. This field is supported for Cloud Composer environments in versions composer-2.*.*-airflow-*.*.* and newer.
     */
    environmentSize: string;
    /**
     * The Kubernetes Engine cluster used to run this environment.
     */
    gkeCluster: string;
    /**
     * The configuration for Cloud Composer maintenance window.
     */
    maintenanceWindow?: outputs.ComposerEnvironmentConfigMaintenanceWindow;
    /**
     * Configuration options for the master authorized networks feature. Enabled master authorized networks will disallow all external traffic to access Kubernetes master through HTTPS except traffic from the given CIDR blocks, Google Compute Engine Public IPs and Google Prod IPs.
     */
    masterAuthorizedNetworksConfig?: outputs.ComposerEnvironmentConfigMasterAuthorizedNetworksConfig;
    /**
     * The configuration used for the Kubernetes Engine cluster.
     */
    nodeConfig?: outputs.ComposerEnvironmentConfigNodeConfig;
    /**
     * The number of nodes in the Kubernetes Engine cluster that will be used to run this environment. This field is supported for Cloud Composer environments in versions composer-1.*.*-airflow-*.*.*.
     */
    nodeCount: number;
    /**
     * The configuration used for the Private IP Cloud Composer environment.
     */
    privateEnvironmentConfig?: outputs.ComposerEnvironmentConfigPrivateEnvironmentConfig;
    /**
     * The recovery configuration settings for the Cloud Composer environment
     */
    recoveryConfig?: outputs.ComposerEnvironmentConfigRecoveryConfig;
    /**
     * Whether high resilience is enabled or not. This field is supported for Cloud Composer environments in versions composer-2.1.15-airflow-*.*.* and newer.
     */
    resilienceMode: string;
    /**
     * The configuration settings for software inside the environment.
     */
    softwareConfig?: outputs.ComposerEnvironmentConfigSoftwareConfig;
    /**
     * The configuration settings for the Airflow web server App Engine instance. This field is supported for Cloud Composer environments in versions composer-1.*.*-airflow-*.*.*.
     */
    webServerConfig?: outputs.ComposerEnvironmentConfigWebServerConfig;
    /**
     * Network-level access control policy for the Airflow web server.
     */
    webServerNetworkAccessControl?: outputs.ComposerEnvironmentConfigWebServerNetworkAccessControl;
    /**
     * The workloads configuration settings for the GKE cluster associated with the Cloud Composer environment. Supported for Cloud Composer environments in versions composer-2.*.*-airflow-*.*.* and newer.
     */
    workloadsConfig?: outputs.ComposerEnvironmentConfigWorkloadsConfig;
}

export interface ComposerEnvironmentConfigDataRetentionConfig {
    /**
     * Optional. The configuration setting for Task Logs.
     */
    taskLogsRetentionConfigs: outputs.ComposerEnvironmentConfigDataRetentionConfigTaskLogsRetentionConfig[];
}

export interface ComposerEnvironmentConfigDataRetentionConfigTaskLogsRetentionConfig {
    /**
     * Whether logs in cloud logging only is enabled or not. This field is supported for Cloud Composer environments in versions composer-2.0.32-airflow-2.1.4 and newer.
     */
    storageMode?: string;
}

export interface ComposerEnvironmentConfigDatabaseConfig {
    /**
     * Optional. Cloud SQL machine type used by Airflow database. It has to be one of: db-n1-standard-2, db-n1-standard-4, db-n1-standard-8 or db-n1-standard-16. If not specified, db-n1-standard-2 will be used.
     */
    machineType?: string;
    /**
     * Optional. Cloud SQL database preferred zone.
     */
    zone?: string;
}

export interface ComposerEnvironmentConfigEncryptionConfig {
    /**
     * Optional. Customer-managed Encryption Key available through Google's Key Management Service. Cannot be updated.
     */
    kmsKeyName: string;
}

export interface ComposerEnvironmentConfigMaintenanceWindow {
    /**
     * Maintenance window end time. It is used only to calculate the duration of the maintenance window. The value for end-time must be in the future, relative to 'start_time'.
     */
    endTime: string;
    /**
     * Maintenance window recurrence. Format is a subset of RFC-5545 (https://tools.ietf.org/html/rfc5545) 'RRULE'. The only allowed values for 'FREQ' field are 'FREQ=DAILY' and 'FREQ=WEEKLY;BYDAY=...'. Example values: 'FREQ=WEEKLY;BYDAY=TU,WE', 'FREQ=DAILY'.
     */
    recurrence: string;
    /**
     * Start time of the first recurrence of the maintenance window.
     */
    startTime: string;
}

export interface ComposerEnvironmentConfigMasterAuthorizedNetworksConfig {
    /**
     * cidr_blocks define up to 50 external networks that could access Kubernetes master through HTTPS.
     */
    cidrBlocks?: outputs.ComposerEnvironmentConfigMasterAuthorizedNetworksConfigCidrBlock[];
    /**
     * Whether or not master authorized networks is enabled.
     */
    enabled: boolean;
}

export interface ComposerEnvironmentConfigMasterAuthorizedNetworksConfigCidrBlock {
    /**
     * cidr_block must be specified in CIDR notation.
     */
    cidrBlock: string;
    /**
     * display_name is a field for users to identify CIDR blocks.
     */
    displayName?: string;
}

export interface ComposerEnvironmentConfigNodeConfig {
    /**
     * The disk size in GB used for node VMs. Minimum size is 20GB. If unspecified, defaults to 100GB. Cannot be updated. This field is supported for Cloud Composer environments in versions composer-1.*.*-airflow-*.*.*.
     */
    diskSizeGb: number;
    /**
     * Deploys 'ip-masq-agent' daemon set in the GKE cluster and defines nonMasqueradeCIDRs equals to pod IP range so IP masquerading is used for all destination addresses, except between pods traffic. See: https://cloud.google.com/kubernetes-engine/docs/how-to/ip-masquerade-agent
     */
    enableIpMasqAgent: boolean;
    /**
     * Configuration for controlling how IPs are allocated in the GKE cluster. Cannot be updated.
     */
    ipAllocationPolicy?: outputs.ComposerEnvironmentConfigNodeConfigIpAllocationPolicy;
    /**
     * The Compute Engine machine type used for cluster instances, specified as a name or relative resource name. For example: "projects/{project}/zones/{zone}/machineTypes/{machineType}". Must belong to the enclosing environment's project and region/zone. This field is supported for Cloud Composer environments in versions composer-1.*.*-airflow-*.*.*.
     */
    machineType: string;
    /**
     * The Compute Engine machine type used for cluster instances, specified as a name or relative resource name. For example: "projects/{project}/zones/{zone}/machineTypes/{machineType}". Must belong to the enclosing environment's project and region/zone. The network must belong to the environment's project. If unspecified, the "default" network ID in the environment's project is used. If a Custom Subnet Network is provided, subnetwork must also be provided.
     */
    network: string;
    /**
     * The set of Google API scopes to be made available on all node VMs. Cannot be updated. If empty, defaults to ["https://www.googleapis.com/auth/cloud-platform"]. This field is supported for Cloud Composer environments in versions composer-1.*.*-airflow-*.*.*.
     */
    oauthScopes: string[];
    /**
     * The Google Cloud Platform Service Account to be used by the node VMs. If a service account is not specified, the "default" Compute Engine service account is used. Cannot be updated. If given, note that the service account must have roles/composer.worker for any GCP resources created under the Cloud Composer Environment.
     */
    serviceAccount: string;
    /**
     * The Compute Engine subnetwork to be used for machine communications, specified as a self-link, relative resource name (e.g. "projects/{project}/regions/{region}/subnetworks/{subnetwork}"), or by name. If subnetwork is provided, network must also be provided and the subnetwork must belong to the enclosing environment's project and region.
     */
    subnetwork?: string;
    /**
     * The list of instance tags applied to all node VMs. Tags are used to identify valid sources or targets for network firewalls. Each tag within the list must comply with RFC1035. Cannot be updated.
     */
    tags?: string[];
    /**
     * The Compute Engine zone in which to deploy the VMs running the Apache Airflow software, specified as the zone name or relative resource name (e.g. "projects/{project}/zones/{zone}"). Must belong to the enclosing environment's project and region. This field is supported for Cloud Composer environments in versions composer-1.*.*-airflow-*.*.*.
     */
    zone: string;
}

export interface ComposerEnvironmentConfigNodeConfigIpAllocationPolicy {
    /**
     * The IP address range used to allocate IP addresses to pods in the cluster. For Cloud Composer environments in versions composer-1.*.*-airflow-*.*.*, this field is applicable only when use_ip_aliases is true. Set to blank to have GKE choose a range with the default size. Set to /netmask (e.g. /14) to have GKE choose a range with a specific netmask. Set to a CIDR notation (e.g. 10.96.0.0/14) from the RFC-1918 private networks (e.g. 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16) to pick a specific range to use. Specify either cluster_secondary_range_name or cluster_ipv4_cidr_block but not both.
     */
    clusterIpv4CidrBlock?: string;
    /**
     * The name of the cluster's secondary range used to allocate IP addresses to pods. Specify either cluster_secondary_range_name or cluster_ipv4_cidr_block but not both. For Cloud Composer environments in versions composer-1.*.*-airflow-*.*.*, this field is applicable only when use_ip_aliases is true.
     */
    clusterSecondaryRangeName?: string;
    /**
     * The IP address range used to allocate IP addresses in this cluster. For Cloud Composer environments in versions composer-1.*.*-airflow-*.*.*, this field is applicable only when use_ip_aliases is true. Set to blank to have GKE choose a range with the default size. Set to /netmask (e.g. /14) to have GKE choose a range with a specific netmask. Set to a CIDR notation (e.g. 10.96.0.0/14) from the RFC-1918 private networks (e.g. 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16) to pick a specific range to use. Specify either services_secondary_range_name or services_ipv4_cidr_block but not both.
     */
    servicesIpv4CidrBlock?: string;
    /**
     * The name of the services' secondary range used to allocate IP addresses to the cluster. Specify either services_secondary_range_name or services_ipv4_cidr_block but not both. For Cloud Composer environments in versions composer-1.*.*-airflow-*.*.*, this field is applicable only when use_ip_aliases is true.
     */
    servicesSecondaryRangeName?: string;
    /**
     * Whether or not to enable Alias IPs in the GKE cluster. If true, a VPC-native cluster is created. Defaults to true if the ip_allocation_policy block is present in config. This field is only supported for Cloud Composer environments in versions composer-1.*.*-airflow-*.*.*. Environments in newer versions always use VPC-native GKE clusters.
     */
    useIpAliases?: boolean;
}

export interface ComposerEnvironmentConfigPrivateEnvironmentConfig {
    /**
     * When specified, the environment will use Private Service Connect instead of VPC peerings to connect to Cloud SQL in the Tenant Project, and the PSC endpoint in the Customer Project will use an IP address from this subnetwork. This field is supported for Cloud Composer environments in versions composer-2.*.*-airflow-*.*.* and newer.
     */
    cloudComposerConnectionSubnetwork: string;
    /**
     * The CIDR block from which IP range for Cloud Composer Network in tenant project will be reserved. Needs to be disjoint from private_cluster_config.master_ipv4_cidr_block and cloud_sql_ipv4_cidr_block. This field is supported for Cloud Composer environments in versions composer-2.*.*-airflow-*.*.* and newer.
     */
    cloudComposerNetworkIpv4CidrBlock: string;
    /**
     * The CIDR block from which IP range in tenant project will be reserved for Cloud SQL. Needs to be disjoint from web_server_ipv4_cidr_block.
     */
    cloudSqlIpv4CidrBlock: string;
    /**
     * Mode of internal communication within the Composer environment. Must be one of "VPC_PEERING" or "PRIVATE_SERVICE_CONNECT".
     */
    connectionType: string;
    /**
     * If true, access to the public endpoint of the GKE cluster is denied. If this field is set to true, ip_allocation_policy.use_ip_aliases must be set to true for Cloud Composer environments in versions composer-1.*.*-airflow-*.*.*.
     */
    enablePrivateEndpoint?: boolean;
    /**
     * When enabled, IPs from public (non-RFC1918) ranges can be used for ip_allocation_policy.cluster_ipv4_cidr_block and ip_allocation_policy.service_ipv4_cidr_block.
     */
    enablePrivatelyUsedPublicIps: boolean;
    /**
     * The IP range in CIDR notation to use for the hosted master network. This range is used for assigning internal IP addresses to the cluster master or set of masters and to the internal load balancer virtual IP. This range must not overlap with any other ranges in use within the cluster's network. If left blank, the default value of '172.16.0.0/28' is used.
     */
    masterIpv4CidrBlock: string;
    /**
     * The CIDR block from which IP range for web server will be reserved. Needs to be disjoint from master_ipv4_cidr_block and cloud_sql_ipv4_cidr_block. This field is supported for Cloud Composer environments in versions composer-1.*.*-airflow-*.*.*.
     */
    webServerIpv4CidrBlock: string;
}

export interface ComposerEnvironmentConfigRecoveryConfig {
    /**
     * The configuration settings for scheduled snapshots.
     */
    scheduledSnapshotsConfig?: outputs.ComposerEnvironmentConfigRecoveryConfigScheduledSnapshotsConfig;
}

export interface ComposerEnvironmentConfigRecoveryConfigScheduledSnapshotsConfig {
    /**
     * When enabled, Cloud Composer periodically saves snapshots of your environment to a Cloud Storage bucket.
     */
    enabled: boolean;
    /**
     * Snapshot schedule, in the unix-cron format.
     */
    snapshotCreationSchedule?: string;
    /**
     * the URI of a bucket folder where to save the snapshot.
     */
    snapshotLocation?: string;
    /**
     * A time zone for the schedule. This value is a time offset and does not take into account daylight saving time changes. Valid values are from UTC-12 to UTC+12. Examples: UTC, UTC-01, UTC+03.
     */
    timeZone?: string;
}

export interface ComposerEnvironmentConfigSoftwareConfig {
    /**
     * Apache Airflow configuration properties to override. Property keys contain the section and property names, separated by a hyphen, for example "core-dags_are_paused_at_creation". Section names must not contain hyphens ("-"), opening square brackets ("["), or closing square brackets ("]"). The property name must not be empty and cannot contain "=" or ";". Section and property names cannot contain characters: "." Apache Airflow configuration property names must be written in snake_case. Property values can contain any character, and can be written in any lower/upper case format. Certain Apache Airflow configuration property values are blacklisted, and cannot be overridden.
     */
    airflowConfigOverrides?: {[key: string]: string};
    /**
     * Additional environment variables to provide to the Apache Airflow scheduler, worker, and webserver processes. Environment variable names must match the regular expression [a-zA-Z_][a-zA-Z0-9_]*. They cannot specify Apache Airflow software configuration overrides (they cannot match the regular expression AIRFLOW__[A-Z0-9_]+__[A-Z0-9_]+), and they cannot match any of the following reserved names: AIRFLOW_HOME C_FORCE_ROOT CONTAINER_NAME DAGS_FOLDER GCP_PROJECT GCS_BUCKET GKE_CLUSTER_NAME SQL_DATABASE SQL_INSTANCE SQL_PASSWORD SQL_PROJECT SQL_REGION SQL_USER.
     */
    envVariables?: {[key: string]: string};
    /**
     * The version of the software running in the environment. This encapsulates both the version of Cloud Composer functionality and the version of Apache Airflow. It must match the regular expression composer-([0-9]+(\.[0-9]+\.[0-9]+(-preview\.[0-9]+)?)?|latest)-airflow-([0-9]+(\.[0-9]+(\.[0-9]+)?)?). The Cloud Composer portion of the image version is a full semantic version, or an alias in the form of major version number or 'latest'. The Apache Airflow portion of the image version is a full semantic version that points to one of the supported Apache Airflow versions, or an alias in the form of only major or major.minor versions specified. See documentation for more details and version list.
     */
    imageVersion: string;
    /**
     * Custom Python Package Index (PyPI) packages to be installed in the environment. Keys refer to the lowercase package name (e.g. "numpy"). Values are the lowercase extras and version specifier (e.g. "==1.12.0", "[devel,gcp_api]", "[devel]>=1.8.2, <1.9.2"). To specify a package without pinning it to a version specifier, use the empty string as the value.
     */
    pypiPackages?: {[key: string]: string};
    /**
     * The major version of Python used to run the Apache Airflow scheduler, worker, and webserver processes. Can be set to '2' or '3'. If not specified, the default is '2'. Cannot be updated. This field is supported for Cloud Composer environments in versions composer-1.*.*-airflow-*.*.*. Environments in newer versions always use Python major version 3.
     */
    pythonVersion: string;
    /**
     * The number of schedulers for Airflow. This field is supported for Cloud Composer environments in versions composer-1.*.*-airflow-2.*.*.
     */
    schedulerCount: number;
}

export interface ComposerEnvironmentConfigWebServerConfig {
    /**
     * Optional. Machine type on which Airflow web server is running. It has to be one of: composer-n1-webserver-2, composer-n1-webserver-4 or composer-n1-webserver-8. If not specified, composer-n1-webserver-2 will be used. Value custom is returned only in response, if Airflow web server parameters were manually changed to a non-standard values.
     */
    machineType: string;
}

export interface ComposerEnvironmentConfigWebServerNetworkAccessControl {
    /**
     * A collection of allowed IP ranges with descriptions.
     */
    allowedIpRanges?: outputs.ComposerEnvironmentConfigWebServerNetworkAccessControlAllowedIpRange[];
}

export interface ComposerEnvironmentConfigWebServerNetworkAccessControlAllowedIpRange {
    /**
     * A description of this ip range.
     */
    description?: string;
    /**
     * IP address or range, defined using CIDR notation, of requests that this rule applies to. Examples: 192.168.1.1 or 192.168.0.0/16 or 2001:db8::/32 or 2001:0db8:0000:0042:0000:8a2e:0370:7334. IP range prefixes should be properly truncated. For example, 1.2.3.4/24 should be truncated to 1.2.3.0/24. Similarly, for IPv6, 2001:db8::1/32 should be truncated to 2001:db8::/32.
     */
    value: string;
}

export interface ComposerEnvironmentConfigWorkloadsConfig {
    /**
     * Configuration for resources used by Airflow schedulers.
     */
    scheduler?: outputs.ComposerEnvironmentConfigWorkloadsConfigScheduler;
    /**
     * Configuration for resources used by Airflow triggerers.
     */
    triggerer?: outputs.ComposerEnvironmentConfigWorkloadsConfigTriggerer;
    /**
     * Configuration for resources used by Airflow web server.
     */
    webServer?: outputs.ComposerEnvironmentConfigWorkloadsConfigWebServer;
    /**
     * Configuration for resources used by Airflow workers.
     */
    worker?: outputs.ComposerEnvironmentConfigWorkloadsConfigWorker;
}

export interface ComposerEnvironmentConfigWorkloadsConfigScheduler {
    /**
     * The number of schedulers.
     */
    count: number;
    /**
     * CPU request and limit for a single Airflow scheduler replica
     */
    cpu: number;
    /**
     * Memory (GB) request and limit for a single Airflow scheduler replica.
     */
    memoryGb: number;
    /**
     * Storage (GB) request and limit for a single Airflow scheduler replica.
     */
    storageGb: number;
}

export interface ComposerEnvironmentConfigWorkloadsConfigTriggerer {
    /**
     * The number of triggerers.
     */
    count: number;
    /**
     * CPU request and limit for a single Airflow triggerer replica.
     */
    cpu: number;
    /**
     * Memory (GB) request and limit for a single Airflow triggerer replica.
     */
    memoryGb: number;
}

export interface ComposerEnvironmentConfigWorkloadsConfigWebServer {
    /**
     * CPU request and limit for Airflow web server.
     */
    cpu: number;
    /**
     * Memory (GB) request and limit for Airflow web server.
     */
    memoryGb: number;
    /**
     * Storage (GB) request and limit for Airflow web server.
     */
    storageGb: number;
}

export interface ComposerEnvironmentConfigWorkloadsConfigWorker {
    /**
     * CPU request and limit for a single Airflow worker replica.
     */
    cpu: number;
    /**
     * Maximum number of workers for autoscaling.
     */
    maxCount: number;
    /**
     * Memory (GB) request and limit for a single Airflow worker replica.
     */
    memoryGb: number;
    /**
     * Minimum number of workers for autoscaling.
     */
    minCount: number;
    /**
     * Storage (GB) request and limit for a single Airflow worker replica.
     */
    storageGb: number;
}

export interface ComposerEnvironmentStorageConfig {
    /**
     * Optional. Name of an existing Cloud Storage bucket to be used by the environment.
     */
    bucket: string;
}

export interface ComposerEnvironmentTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeAddressTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeAttachedDiskTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeAutoscalerAutoscalingPolicy {
    /**
     * The number of seconds that the autoscaler should wait before it
     * starts collecting information from a new instance. This prevents
     * the autoscaler from collecting information when the instance is
     * initializing, during which the collected usage would not be
     * reliable. The default time autoscaler waits is 60 seconds.
     *
     * Virtual machine initialization times might vary because of
     * numerous factors. We recommend that you test how long an
     * instance may take to initialize. To do this, create an instance
     * and time the startup process.
     */
    cooldownPeriod?: number;
    /**
     * Defines the CPU utilization policy that allows the autoscaler to
     * scale based on the average CPU utilization of a managed instance
     * group.
     */
    cpuUtilization?: outputs.ComputeAutoscalerAutoscalingPolicyCpuUtilization;
    /**
     * Configuration parameters of autoscaling based on a load balancer.
     */
    loadBalancingUtilization?: outputs.ComputeAutoscalerAutoscalingPolicyLoadBalancingUtilization;
    /**
     * The maximum number of instances that the autoscaler can scale up
     * to. This is required when creating or updating an autoscaler. The
     * maximum number of replicas should not be lower than minimal number
     * of replicas.
     */
    maxReplicas: number;
    /**
     * Configuration parameters of autoscaling based on a custom metric.
     */
    metrics?: outputs.ComputeAutoscalerAutoscalingPolicyMetric[];
    /**
     * The minimum number of replicas that the autoscaler can scale down
     * to. This cannot be less than 0. If not provided, autoscaler will
     * choose a default value depending on maximum number of instances
     * allowed.
     */
    minReplicas: number;
    /**
     * Defines operating mode for this policy.
     */
    mode?: string;
    /**
     * Defines scale in controls to reduce the risk of response latency
     * and outages due to abrupt scale-in events
     */
    scaleInControl?: outputs.ComputeAutoscalerAutoscalingPolicyScaleInControl;
    /**
     * Scaling schedules defined for an autoscaler. Multiple schedules can be set on an autoscaler and they can overlap.
     */
    scalingSchedules?: outputs.ComputeAutoscalerAutoscalingPolicyScalingSchedule[];
}

export interface ComputeAutoscalerAutoscalingPolicyCpuUtilization {
    /**
     * Indicates whether predictive autoscaling based on CPU metric is enabled. Valid values are:
     *
     * - NONE (default). No predictive method is used. The autoscaler scales the group to meet current demand based on real-time metrics.
     *
     * - OPTIMIZE_AVAILABILITY. Predictive autoscaling improves availability by monitoring daily and weekly load patterns and scaling out ahead of anticipated demand.
     */
    predictiveMethod?: string;
    /**
     * The target CPU utilization that the autoscaler should maintain.
     * Must be a float value in the range (0, 1]. If not specified, the
     * default is 0.6.
     *
     * If the CPU level is below the target utilization, the autoscaler
     * scales down the number of instances until it reaches the minimum
     * number of instances you specified or until the average CPU of
     * your instances reaches the target utilization.
     *
     * If the average CPU is above the target utilization, the autoscaler
     * scales up until it reaches the maximum number of instances you
     * specified or until the average utilization reaches the target
     * utilization.
     */
    target: number;
}

export interface ComputeAutoscalerAutoscalingPolicyLoadBalancingUtilization {
    /**
     * Fraction of backend capacity utilization (set in HTTP(s) load
     * balancing configuration) that autoscaler should maintain. Must
     * be a positive float value. If not defined, the default is 0.8.
     */
    target: number;
}

export interface ComputeAutoscalerAutoscalingPolicyMetric {
    /**
     * The identifier (type) of the Stackdriver Monitoring metric.
     * The metric cannot have negative values.
     *
     * The metric must have a value type of INT64 or DOUBLE.
     */
    name: string;
    /**
     * The target value of the metric that autoscaler should
     * maintain. This must be a positive value. A utilization
     * metric scales number of virtual machines handling requests
     * to increase or decrease proportionally to the metric.
     *
     * For example, a good metric to use as a utilizationTarget is
     * www.googleapis.com/compute/instance/network/received_bytes_count.
     * The autoscaler will work to keep this value constant for each
     * of the instances.
     */
    target?: number;
    /**
     * Defines how target utilization value is expressed for a
     * Stackdriver Monitoring metric. Possible values: ["GAUGE", "DELTA_PER_SECOND", "DELTA_PER_MINUTE"]
     */
    type?: string;
}

export interface ComputeAutoscalerAutoscalingPolicyScaleInControl {
    /**
     * A nested object resource
     */
    maxScaledInReplicas?: outputs.ComputeAutoscalerAutoscalingPolicyScaleInControlMaxScaledInReplicas;
    /**
     * How long back autoscaling should look when computing recommendations
     * to include directives regarding slower scale down, as described above.
     */
    timeWindowSec?: number;
}

export interface ComputeAutoscalerAutoscalingPolicyScaleInControlMaxScaledInReplicas {
    /**
     * Specifies a fixed number of VM instances. This must be a positive
     * integer.
     */
    fixed?: number;
    /**
     * Specifies a percentage of instances between 0 to 100%, inclusive.
     * For example, specify 80 for 80%.
     */
    percent?: number;
}

export interface ComputeAutoscalerAutoscalingPolicyScalingSchedule {
    /**
     * A description of a scaling schedule.
     */
    description?: string;
    /**
     * A boolean value that specifies if a scaling schedule can influence autoscaler recommendations. If set to true, then a scaling schedule has no effect.
     */
    disabled?: boolean;
    /**
     * The duration of time intervals (in seconds) for which this scaling schedule will be running. The minimum allowed value is 300.
     */
    durationSec: number;
    /**
     * Minimum number of VM instances that autoscaler will recommend in time intervals starting according to schedule.
     */
    minRequiredReplicas: number;
    name: string;
    /**
     * The start timestamps of time intervals when this scaling schedule should provide a scaling signal. This field uses the extended cron format (with an optional year field).
     */
    schedule: string;
    /**
     * The time zone to be used when interpreting the schedule. The value of this field must be a time zone name from the tz database: http://en.wikipedia.org/wiki/Tz_database.
     */
    timeZone?: string;
}

export interface ComputeAutoscalerTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeBackendBucketCdnPolicy {
    /**
     * Bypass the cache when the specified request headers are matched - e.g. Pragma or Authorization headers. Up to 5 headers can be specified. The cache is bypassed for all cdnPolicy.cacheMode settings.
     */
    bypassCacheOnRequestHeaders?: outputs.ComputeBackendBucketCdnPolicyBypassCacheOnRequestHeader[];
    /**
     * The CacheKeyPolicy for this CdnPolicy.
     */
    cacheKeyPolicy?: outputs.ComputeBackendBucketCdnPolicyCacheKeyPolicy;
    /**
     * Specifies the cache setting for all responses from this backend.
     * The possible values are: USE_ORIGIN_HEADERS, FORCE_CACHE_ALL and CACHE_ALL_STATIC Possible values: ["USE_ORIGIN_HEADERS", "FORCE_CACHE_ALL", "CACHE_ALL_STATIC"]
     */
    cacheMode: string;
    /**
     * Specifies the maximum allowed TTL for cached content served by this origin.
     */
    clientTtl: number;
    /**
     * Specifies the default TTL for cached content served by this origin for responses
     * that do not have an existing valid TTL (max-age or s-max-age).
     */
    defaultTtl: number;
    /**
     * Specifies the maximum allowed TTL for cached content served by this origin.
     */
    maxTtl: number;
    /**
     * Negative caching allows per-status code TTLs to be set, in order to apply fine-grained caching for common errors or redirects.
     */
    negativeCaching: boolean;
    /**
     * Sets a cache TTL for the specified HTTP status code. negativeCaching must be enabled to configure negativeCachingPolicy.
     * Omitting the policy and leaving negativeCaching enabled will use Cloud CDN's default cache TTLs.
     */
    negativeCachingPolicies?: outputs.ComputeBackendBucketCdnPolicyNegativeCachingPolicy[];
    /**
     * If true then Cloud CDN will combine multiple concurrent cache fill requests into a small number of requests to the origin.
     */
    requestCoalescing?: boolean;
    /**
     * Serve existing content from the cache (if available) when revalidating content with the origin, or when an error is encountered when refreshing the cache.
     */
    serveWhileStale: number;
    /**
     * Maximum number of seconds the response to a signed URL request will
     * be considered fresh. After this time period,
     * the response will be revalidated before being served.
     * When serving responses to signed URL requests,
     * Cloud CDN will internally behave as though
     * all responses from this backend had a "Cache-Control: public,
     * max-age=[TTL]" header, regardless of any existing Cache-Control
     * header. The actual headers served in responses will not be altered.
     */
    signedUrlCacheMaxAgeSec?: number;
}

export interface ComputeBackendBucketCdnPolicyBypassCacheOnRequestHeader {
    /**
     * The header field name to match on when bypassing cache. Values are case-insensitive.
     */
    headerName?: string;
}

export interface ComputeBackendBucketCdnPolicyCacheKeyPolicy {
    /**
     * Allows HTTP request headers (by name) to be used in the
     * cache key.
     */
    includeHttpHeaders?: string[];
    /**
     * Names of query string parameters to include in cache keys.
     * Default parameters are always included. '&' and '=' will
     * be percent encoded and not treated as delimiters.
     */
    queryStringWhitelists?: string[];
}

export interface ComputeBackendBucketCdnPolicyNegativeCachingPolicy {
    /**
     * The HTTP status code to define a TTL against. Only HTTP status codes 300, 301, 308, 404, 405, 410, 421, 451 and 501
     * can be specified as values, and you cannot specify a status code more than once.
     */
    code?: number;
    /**
     * The TTL (in seconds) for which to cache responses with the corresponding status code. The maximum allowed value is 1800s
     * (30 minutes), noting that infrequently accessed objects may be evicted from the cache before the defined TTL.
     */
    ttl?: number;
}

export interface ComputeBackendBucketSignedUrlKeyTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeBackendBucketTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeBackendServiceBackend {
    /**
     * Specifies the balancing mode for this backend.
     *
     * For global HTTP(S) or TCP/SSL load balancing, the default is
     * UTILIZATION. Valid values are UTILIZATION, RATE (for HTTP(S))
     * and CONNECTION (for TCP/SSL).
     *
     * See the [Backend Services Overview](https://cloud.google.com/load-balancing/docs/backend-service#balancing-mode)
     * for an explanation of load balancing modes. Default value: "UTILIZATION" Possible values: ["UTILIZATION", "RATE", "CONNECTION"]
     */
    balancingMode?: string;
    /**
     * A multiplier applied to the group's maximum servicing capacity
     * (based on UTILIZATION, RATE or CONNECTION).
     *
     * Default value is 1, which means the group will serve up to 100%
     * of its configured capacity (depending on balancingMode). A
     * setting of 0 means the group is completely drained, offering
     * 0% of its available Capacity. Valid range is [0.0,1.0].
     */
    capacityScaler?: number;
    /**
     * An optional description of this resource.
     * Provide this property when you create the resource.
     */
    description?: string;
    /**
     * The fully-qualified URL of an Instance Group or Network Endpoint
     * Group resource. In case of instance group this defines the list
     * of instances that serve traffic. Member virtual machine
     * instances from each instance group must live in the same zone as
     * the instance group itself. No two backends in a backend service
     * are allowed to use same Instance Group resource.
     *
     * For Network Endpoint Groups this defines list of endpoints. All
     * endpoints of Network Endpoint Group must be hosted on instances
     * located in the same zone as the Network Endpoint Group.
     *
     * Backend services cannot mix Instance Group and
     * Network Endpoint Group backends.
     *
     * Note that you must specify an Instance Group or Network Endpoint
     * Group resource using the fully-qualified URL, rather than a
     * partial URL.
     */
    group: string;
    /**
     * The max number of simultaneous connections for the group. Can
     * be used with either CONNECTION or UTILIZATION balancing modes.
     *
     * For CONNECTION mode, either maxConnections or one
     * of maxConnectionsPerInstance or maxConnectionsPerEndpoint,
     * as appropriate for group type, must be set.
     */
    maxConnections: number;
    /**
     * The max number of simultaneous connections that a single backend
     * network endpoint can handle. This is used to calculate the
     * capacity of the group. Can be used in either CONNECTION or
     * UTILIZATION balancing modes.
     *
     * For CONNECTION mode, either
     * maxConnections or maxConnectionsPerEndpoint must be set.
     */
    maxConnectionsPerEndpoint: number;
    /**
     * The max number of simultaneous connections that a single
     * backend instance can handle. This is used to calculate the
     * capacity of the group. Can be used in either CONNECTION or
     * UTILIZATION balancing modes.
     *
     * For CONNECTION mode, either maxConnections or
     * maxConnectionsPerInstance must be set.
     */
    maxConnectionsPerInstance: number;
    /**
     * The max requests per second (RPS) of the group.
     *
     * Can be used with either RATE or UTILIZATION balancing modes,
     * but required if RATE mode. For RATE mode, either maxRate or one
     * of maxRatePerInstance or maxRatePerEndpoint, as appropriate for
     * group type, must be set.
     */
    maxRate: number;
    /**
     * The max requests per second (RPS) that a single backend network
     * endpoint can handle. This is used to calculate the capacity of
     * the group. Can be used in either balancing mode. For RATE mode,
     * either maxRate or maxRatePerEndpoint must be set.
     */
    maxRatePerEndpoint: number;
    /**
     * The max requests per second (RPS) that a single backend
     * instance can handle. This is used to calculate the capacity of
     * the group. Can be used in either balancing mode. For RATE mode,
     * either maxRate or maxRatePerInstance must be set.
     */
    maxRatePerInstance: number;
    /**
     * Used when balancingMode is UTILIZATION. This ratio defines the
     * CPU utilization target for the group. Valid range is [0.0, 1.0].
     */
    maxUtilization: number;
}

export interface ComputeBackendServiceCdnPolicy {
    /**
     * Bypass the cache when the specified request headers are matched - e.g. Pragma or Authorization headers. Up to 5 headers can be specified.
     * The cache is bypassed for all cdnPolicy.cacheMode settings.
     */
    bypassCacheOnRequestHeaders?: outputs.ComputeBackendServiceCdnPolicyBypassCacheOnRequestHeader[];
    /**
     * The CacheKeyPolicy for this CdnPolicy.
     */
    cacheKeyPolicy?: outputs.ComputeBackendServiceCdnPolicyCacheKeyPolicy;
    /**
     * Specifies the cache setting for all responses from this backend.
     * The possible values are: USE_ORIGIN_HEADERS, FORCE_CACHE_ALL and CACHE_ALL_STATIC Possible values: ["USE_ORIGIN_HEADERS", "FORCE_CACHE_ALL", "CACHE_ALL_STATIC"]
     */
    cacheMode: string;
    /**
     * Specifies the maximum allowed TTL for cached content served by this origin.
     */
    clientTtl: number;
    /**
     * Specifies the default TTL for cached content served by this origin for responses
     * that do not have an existing valid TTL (max-age or s-max-age).
     */
    defaultTtl: number;
    /**
     * Specifies the maximum allowed TTL for cached content served by this origin.
     */
    maxTtl: number;
    /**
     * Negative caching allows per-status code TTLs to be set, in order to apply fine-grained caching for common errors or redirects.
     */
    negativeCaching: boolean;
    /**
     * Sets a cache TTL for the specified HTTP status code. negativeCaching must be enabled to configure negativeCachingPolicy.
     * Omitting the policy and leaving negativeCaching enabled will use Cloud CDN's default cache TTLs.
     */
    negativeCachingPolicies?: outputs.ComputeBackendServiceCdnPolicyNegativeCachingPolicy[];
    /**
     * Serve existing content from the cache (if available) when revalidating content with the origin, or when an error is encountered when refreshing the cache.
     */
    serveWhileStale: number;
    /**
     * Maximum number of seconds the response to a signed URL request
     * will be considered fresh, defaults to 1hr (3600s). After this
     * time period, the response will be revalidated before
     * being served.
     *
     * When serving responses to signed URL requests, Cloud CDN will
     * internally behave as though all responses from this backend had a
     * "Cache-Control: public, max-age=[TTL]" header, regardless of any
     * existing Cache-Control header. The actual headers served in
     * responses will not be altered.
     */
    signedUrlCacheMaxAgeSec?: number;
}

export interface ComputeBackendServiceCdnPolicyBypassCacheOnRequestHeader {
    /**
     * The header field name to match on when bypassing cache. Values are case-insensitive.
     */
    headerName: string;
}

export interface ComputeBackendServiceCdnPolicyCacheKeyPolicy {
    /**
     * If true requests to different hosts will be cached separately.
     */
    includeHost?: boolean;
    /**
     * Allows HTTP request headers (by name) to be used in the
     * cache key.
     */
    includeHttpHeaders?: string[];
    /**
     * Names of cookies to include in cache keys.
     */
    includeNamedCookies?: string[];
    /**
     * If true, http and https requests will be cached separately.
     */
    includeProtocol?: boolean;
    /**
     * If true, include query string parameters in the cache key
     * according to query_string_whitelist and
     * query_string_blacklist. If neither is set, the entire query
     * string will be included.
     *
     * If false, the query string will be excluded from the cache
     * key entirely.
     */
    includeQueryString?: boolean;
    /**
     * Names of query string parameters to exclude in cache keys.
     *
     * All other parameters will be included. Either specify
     * query_string_whitelist or query_string_blacklist, not both.
     * '&' and '=' will be percent encoded and not treated as
     * delimiters.
     */
    queryStringBlacklists?: string[];
    /**
     * Names of query string parameters to include in cache keys.
     *
     * All other parameters will be excluded. Either specify
     * query_string_whitelist or query_string_blacklist, not both.
     * '&' and '=' will be percent encoded and not treated as
     * delimiters.
     */
    queryStringWhitelists?: string[];
}

export interface ComputeBackendServiceCdnPolicyNegativeCachingPolicy {
    /**
     * The HTTP status code to define a TTL against. Only HTTP status codes 300, 301, 308, 404, 405, 410, 421, 451 and 501
     * can be specified as values, and you cannot specify a status code more than once.
     */
    code?: number;
    /**
     * The TTL (in seconds) for which to cache responses with the corresponding status code. The maximum allowed value is 1800s
     * (30 minutes), noting that infrequently accessed objects may be evicted from the cache before the defined TTL.
     */
    ttl?: number;
}

export interface ComputeBackendServiceCircuitBreakers {
    /**
     * The maximum number of connections to the backend cluster.
     * Defaults to 1024.
     */
    maxConnections?: number;
    /**
     * The maximum number of pending requests to the backend cluster.
     * Defaults to 1024.
     */
    maxPendingRequests?: number;
    /**
     * The maximum number of parallel requests to the backend cluster.
     * Defaults to 1024.
     */
    maxRequests?: number;
    /**
     * Maximum requests for a single backend connection. This parameter
     * is respected by both the HTTP/1.1 and HTTP/2 implementations. If
     * not specified, there is no limit. Setting this parameter to 1
     * will effectively disable keep alive.
     */
    maxRequestsPerConnection?: number;
    /**
     * The maximum number of parallel retries to the backend cluster.
     * Defaults to 3.
     */
    maxRetries?: number;
}

export interface ComputeBackendServiceConsistentHash {
    /**
     * Hash is based on HTTP Cookie. This field describes a HTTP cookie
     * that will be used as the hash key for the consistent hash load
     * balancer. If the cookie is not present, it will be generated.
     * This field is applicable if the sessionAffinity is set to HTTP_COOKIE.
     */
    httpCookie?: outputs.ComputeBackendServiceConsistentHashHttpCookie;
    /**
     * The hash based on the value of the specified header field.
     * This field is applicable if the sessionAffinity is set to HEADER_FIELD.
     */
    httpHeaderName?: string;
    /**
     * The minimum number of virtual nodes to use for the hash ring.
     * Larger ring sizes result in more granular load
     * distributions. If the number of hosts in the load balancing pool
     * is larger than the ring size, each host will be assigned a single
     * virtual node.
     * Defaults to 1024.
     */
    minimumRingSize?: number;
}

export interface ComputeBackendServiceConsistentHashHttpCookie {
    /**
     * Name of the cookie.
     */
    name?: string;
    /**
     * Path to set for the cookie.
     */
    path?: string;
    /**
     * Lifetime of the cookie.
     */
    ttl?: outputs.ComputeBackendServiceConsistentHashHttpCookieTtl;
}

export interface ComputeBackendServiceConsistentHashHttpCookieTtl {
    /**
     * Span of time that's a fraction of a second at nanosecond
     * resolution. Durations less than one second are represented
     * with a 0 seconds field and a positive nanos field. Must
     * be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second.
     * Must be from 0 to 315,576,000,000 inclusive.
     */
    seconds: number;
}

export interface ComputeBackendServiceIap {
    /**
     * Whether the serving infrastructure will authenticate and authorize all incoming requests.
     */
    enabled: boolean;
    /**
     * OAuth2 Client ID for IAP
     */
    oauth2ClientId?: string;
    /**
     * OAuth2 Client Secret for IAP
     */
    oauth2ClientSecret?: string;
    /**
     * OAuth2 Client Secret SHA-256 for IAP
     */
    oauth2ClientSecretSha256: string;
}

export interface ComputeBackendServiceLocalityLbPolicy {
    /**
     * The configuration for a custom policy implemented by the user and
     * deployed with the client.
     */
    customPolicy?: outputs.ComputeBackendServiceLocalityLbPolicyCustomPolicy;
    /**
     * The configuration for a built-in load balancing policy.
     */
    policy?: outputs.ComputeBackendServiceLocalityLbPolicyPolicy;
}

export interface ComputeBackendServiceLocalityLbPolicyCustomPolicy {
    /**
     * An optional, arbitrary JSON object with configuration data, understood
     * by a locally installed custom policy implementation.
     */
    data?: string;
    /**
     * Identifies the custom policy.
     *
     * The value should match the type the custom implementation is registered
     * with on the gRPC clients. It should follow protocol buffer
     * message naming conventions and include the full path (e.g.
     * myorg.CustomLbPolicy). The maximum length is 256 characters.
     *
     * Note that specifying the same custom policy more than once for a
     * backend is not a valid configuration and will be rejected.
     */
    name: string;
}

export interface ComputeBackendServiceLocalityLbPolicyPolicy {
    /**
     * The name of a locality load balancer policy to be used. The value
     * should be one of the predefined ones as supported by localityLbPolicy,
     * although at the moment only ROUND_ROBIN is supported.
     *
     * This field should only be populated when the customPolicy field is not
     * used.
     *
     * Note that specifying the same policy more than once for a backend is
     * not a valid configuration and will be rejected.
     *
     * The possible values are:
     *
     * * 'ROUND_ROBIN': This is a simple policy in which each healthy backend
     *                 is selected in round robin order.
     *
     * * 'LEAST_REQUEST': An O(1) algorithm which selects two random healthy
     *                   hosts and picks the host which has fewer active requests.
     *
     * * 'RING_HASH': The ring/modulo hash load balancer implements consistent
     *               hashing to backends. The algorithm has the property that the
     *               addition/removal of a host from a set of N hosts only affects
     *               1/N of the requests.
     *
     * * 'RANDOM': The load balancer selects a random healthy host.
     *
     * * 'ORIGINAL_DESTINATION': Backend host is selected based on the client
     *                           connection metadata, i.e., connections are opened
     *                           to the same address as the destination address of
     *                           the incoming connection before the connection
     *                           was redirected to the load balancer.
     *
     * * 'MAGLEV': used as a drop in replacement for the ring hash load balancer.
     *             Maglev is not as stable as ring hash but has faster table lookup
     *             build times and host selection times. For more information about
     *             Maglev, refer to https://ai.google/research/pubs/pub44824 Possible values: ["ROUND_ROBIN", "LEAST_REQUEST", "RING_HASH", "RANDOM", "ORIGINAL_DESTINATION", "MAGLEV"]
     */
    name: string;
}

export interface ComputeBackendServiceLogConfig {
    /**
     * Whether to enable logging for the load balancer traffic served by this backend service.
     */
    enable?: boolean;
    /**
     * This field can only be specified if logging is enabled for this backend service. The value of
     * the field must be in [0, 1]. This configures the sampling rate of requests to the load balancer
     * where 1.0 means all logged requests are reported and 0.0 means no logged requests are reported.
     * The default value is 1.0.
     */
    sampleRate?: number;
}

export interface ComputeBackendServiceOutlierDetection {
    /**
     * The base time that a host is ejected for. The real time is equal to the base
     * time multiplied by the number of times the host has been ejected. Defaults to
     * 30000ms or 30s.
     */
    baseEjectionTime?: outputs.ComputeBackendServiceOutlierDetectionBaseEjectionTime;
    /**
     * Number of errors before a host is ejected from the connection pool. When the
     * backend host is accessed over HTTP, a 5xx return code qualifies as an error.
     * Defaults to 5.
     */
    consecutiveErrors?: number;
    /**
     * The number of consecutive gateway failures (502, 503, 504 status or connection
     * errors that are mapped to one of those status codes) before a consecutive
     * gateway failure ejection occurs. Defaults to 5.
     */
    consecutiveGatewayFailure?: number;
    /**
     * The percentage chance that a host will be actually ejected when an outlier
     * status is detected through consecutive 5xx. This setting can be used to disable
     * ejection or to ramp it up slowly. Defaults to 100.
     */
    enforcingConsecutiveErrors?: number;
    /**
     * The percentage chance that a host will be actually ejected when an outlier
     * status is detected through consecutive gateway failures. This setting can be
     * used to disable ejection or to ramp it up slowly. Defaults to 0.
     */
    enforcingConsecutiveGatewayFailure?: number;
    /**
     * The percentage chance that a host will be actually ejected when an outlier
     * status is detected through success rate statistics. This setting can be used to
     * disable ejection or to ramp it up slowly. Defaults to 100.
     */
    enforcingSuccessRate?: number;
    /**
     * Time interval between ejection sweep analysis. This can result in both new
     * ejections as well as hosts being returned to service. Defaults to 10 seconds.
     */
    interval?: outputs.ComputeBackendServiceOutlierDetectionInterval;
    /**
     * Maximum percentage of hosts in the load balancing pool for the backend service
     * that can be ejected. Defaults to 10%.
     */
    maxEjectionPercent?: number;
    /**
     * The number of hosts in a cluster that must have enough request volume to detect
     * success rate outliers. If the number of hosts is less than this setting, outlier
     * detection via success rate statistics is not performed for any host in the
     * cluster. Defaults to 5.
     */
    successRateMinimumHosts?: number;
    /**
     * The minimum number of total requests that must be collected in one interval (as
     * defined by the interval duration above) to include this host in success rate
     * based outlier detection. If the volume is lower than this setting, outlier
     * detection via success rate statistics is not performed for that host. Defaults
     * to 100.
     */
    successRateRequestVolume?: number;
    /**
     * This factor is used to determine the ejection threshold for success rate outlier
     * ejection. The ejection threshold is the difference between the mean success
     * rate, and the product of this factor and the standard deviation of the mean
     * success rate: mean - (stdev * success_rate_stdev_factor). This factor is divided
     * by a thousand to get a double. That is, if the desired factor is 1.9, the
     * runtime value should be 1900. Defaults to 1900.
     */
    successRateStdevFactor?: number;
}

export interface ComputeBackendServiceOutlierDetectionBaseEjectionTime {
    /**
     * Span of time that's a fraction of a second at nanosecond resolution. Durations
     * less than one second are represented with a 0 'seconds' field and a positive
     * 'nanos' field. Must be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000
     * inclusive.
     */
    seconds: number;
}

export interface ComputeBackendServiceOutlierDetectionInterval {
    /**
     * Span of time that's a fraction of a second at nanosecond resolution. Durations
     * less than one second are represented with a 0 'seconds' field and a positive
     * 'nanos' field. Must be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000
     * inclusive.
     */
    seconds: number;
}

export interface ComputeBackendServiceSecuritySettings {
    /**
     * The configuration needed to generate a signature for access to private storage buckets that support AWS's Signature Version 4 for authentication.
     * Allowed only for INTERNET_IP_PORT and INTERNET_FQDN_PORT NEG backends.
     */
    awsV4Authentication?: outputs.ComputeBackendServiceSecuritySettingsAwsV4Authentication;
    /**
     * ClientTlsPolicy is a resource that specifies how a client should authenticate
     * connections to backends of a service. This resource itself does not affect
     * configuration unless it is attached to a backend service resource.
     */
    clientTlsPolicy?: string;
    /**
     * A list of alternate names to verify the subject identity in the certificate.
     * If specified, the client will verify that the server certificate's subject
     * alt name matches one of the specified values.
     */
    subjectAltNames?: string[];
}

export interface ComputeBackendServiceSecuritySettingsAwsV4Authentication {
    /**
     * The access key used for s3 bucket authentication.
     * Required for updating or creating a backend that uses AWS v4 signature authentication, but will not be returned as part of the configuration when queried with a REST API GET request.
     */
    accessKey?: string;
    /**
     * The identifier of an access key used for s3 bucket authentication.
     */
    accessKeyId?: string;
    /**
     * The optional version identifier for the access key. You can use this to keep track of different iterations of your access key.
     */
    accessKeyVersion?: string;
    /**
     * The name of the cloud region of your origin. This is a free-form field with the name of the region your cloud uses to host your origin.
     * For example, "us-east-1" for AWS or "us-ashburn-1" for OCI.
     */
    originRegion?: string;
}

export interface ComputeBackendServiceSignedUrlKeyTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeBackendServiceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeDiskAsyncPrimaryDisk {
    /**
     * Primary disk for asynchronous disk replication.
     */
    disk: string;
}

export interface ComputeDiskAsyncReplicationSecondaryDisk {
    /**
     * Secondary disk for asynchronous replication.
     */
    disk: string;
    /**
     * Output-only. Status of replication on the secondary disk.
     */
    state: string;
}

export interface ComputeDiskAsyncReplicationTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeDiskDiskEncryptionKey {
    /**
     * The self link of the encryption key used to encrypt the disk. Also called KmsKeyName
     * in the cloud console. Your project's Compute Engine System service account
     * ('service-{{PROJECT_NUMBER}}@compute-system.iam.gserviceaccount.com') must have
     * 'roles/cloudkms.cryptoKeyEncrypterDecrypter' to use this feature.
     * See https://cloud.google.com/compute/docs/disks/customer-managed-encryption#encrypt_a_new_persistent_disk_with_your_own_keys
     */
    kmsKeySelfLink?: string;
    /**
     * The service account used for the encryption request for the given KMS key.
     * If absent, the Compute Engine Service Agent service account is used.
     */
    kmsKeyServiceAccount?: string;
    /**
     * Specifies a 256-bit customer-supplied encryption key, encoded in
     * RFC 4648 base64 to either encrypt or decrypt this resource.
     */
    rawKey?: string;
    /**
     * Specifies an RFC 4648 base64 encoded, RSA-wrapped 2048-bit
     * customer-supplied encryption key to either encrypt or decrypt
     * this resource. You can provide either the rawKey or the rsaEncryptedKey.
     */
    rsaEncryptedKey?: string;
    /**
     * The RFC 4648 base64 encoded SHA-256 hash of the customer-supplied
     * encryption key that protects this resource.
     */
    sha256: string;
}

export interface ComputeDiskGuestOsFeature {
    /**
     * The type of supported feature. Read [Enabling guest operating system features](https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images#guest-os-features) to see a list of available options.
     */
    type: string;
}

export interface ComputeDiskIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ComputeDiskIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ComputeDiskResourcePolicyAttachmentTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeDiskSourceImageEncryptionKey {
    /**
     * The self link of the encryption key used to encrypt the disk. Also called KmsKeyName
     * in the cloud console. Your project's Compute Engine System service account
     * ('service-{{PROJECT_NUMBER}}@compute-system.iam.gserviceaccount.com') must have
     * 'roles/cloudkms.cryptoKeyEncrypterDecrypter' to use this feature.
     * See https://cloud.google.com/compute/docs/disks/customer-managed-encryption#encrypt_a_new_persistent_disk_with_your_own_keys
     */
    kmsKeySelfLink?: string;
    /**
     * The service account used for the encryption request for the given KMS key.
     * If absent, the Compute Engine Service Agent service account is used.
     */
    kmsKeyServiceAccount?: string;
    /**
     * Specifies a 256-bit customer-supplied encryption key, encoded in
     * RFC 4648 base64 to either encrypt or decrypt this resource.
     */
    rawKey?: string;
    /**
     * The RFC 4648 base64 encoded SHA-256 hash of the customer-supplied
     * encryption key that protects this resource.
     */
    sha256: string;
}

export interface ComputeDiskSourceSnapshotEncryptionKey {
    /**
     * The self link of the encryption key used to encrypt the disk. Also called KmsKeyName
     * in the cloud console. Your project's Compute Engine System service account
     * ('service-{{PROJECT_NUMBER}}@compute-system.iam.gserviceaccount.com') must have
     * 'roles/cloudkms.cryptoKeyEncrypterDecrypter' to use this feature.
     * See https://cloud.google.com/compute/docs/disks/customer-managed-encryption#encrypt_a_new_persistent_disk_with_your_own_keys
     */
    kmsKeySelfLink?: string;
    /**
     * The service account used for the encryption request for the given KMS key.
     * If absent, the Compute Engine Service Agent service account is used.
     */
    kmsKeyServiceAccount?: string;
    /**
     * Specifies a 256-bit customer-supplied encryption key, encoded in
     * RFC 4648 base64 to either encrypt or decrypt this resource.
     */
    rawKey?: string;
    /**
     * The RFC 4648 base64 encoded SHA-256 hash of the customer-supplied
     * encryption key that protects this resource.
     */
    sha256: string;
}

export interface ComputeDiskTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeExternalVpnGatewayInterface {
    /**
     * The numeric ID for this interface. Allowed values are based on the redundancy type
     * of this external VPN gateway
     * * '0 - SINGLE_IP_INTERNALLY_REDUNDANT'
     * * '0, 1 - TWO_IPS_REDUNDANCY'
     * * '0, 1, 2, 3 - FOUR_IPS_REDUNDANCY'
     */
    id?: number;
    /**
     * IP address of the interface in the external VPN gateway.
     * Only IPv4 is supported. This IP address can be either from
     * your on-premise gateway or another Cloud provider's VPN gateway,
     * it cannot be an IP address from Google Compute Engine.
     */
    ipAddress?: string;
}

export interface ComputeExternalVpnGatewayTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeFirewallAllow {
    /**
     * An optional list of ports to which this rule applies. This field
     * is only applicable for UDP or TCP protocol. Each entry must be
     * either an integer or a range. If not specified, this rule
     * applies to connections through any port.
     *
     * Example inputs include: [22], [80, 443], and
     * ["12345-12349"].
     */
    ports?: string[];
    /**
     * The IP protocol to which this rule applies. The protocol type is
     * required when creating a firewall rule. This value can either be
     * one of the following well known protocol strings (tcp, udp,
     * icmp, esp, ah, sctp, ipip, all), or the IP protocol number.
     */
    protocol: string;
}

export interface ComputeFirewallDeny {
    /**
     * An optional list of ports to which this rule applies. This field
     * is only applicable for UDP or TCP protocol. Each entry must be
     * either an integer or a range. If not specified, this rule
     * applies to connections through any port.
     *
     * Example inputs include: [22], [80, 443], and
     * ["12345-12349"].
     */
    ports?: string[];
    /**
     * The IP protocol to which this rule applies. The protocol type is
     * required when creating a firewall rule. This value can either be
     * one of the following well known protocol strings (tcp, udp,
     * icmp, esp, ah, sctp, ipip, all), or the IP protocol number.
     */
    protocol: string;
}

export interface ComputeFirewallLogConfig {
    /**
     * This field denotes whether to include or exclude metadata for firewall logs. Possible values: ["EXCLUDE_ALL_METADATA", "INCLUDE_ALL_METADATA"]
     */
    metadata: string;
}

export interface ComputeFirewallPolicyAssociationTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeFirewallPolicyRuleMatch {
    /**
     * Address groups which should be matched against the traffic destination. Maximum number of destination address groups is 10. Destination address groups is only supported in Egress rules.
     */
    destAddressGroups?: string[];
    /**
     * Domain names that will be used to match against the resolved domain name of destination of traffic. Can only be specified if DIRECTION is egress.
     */
    destFqdns?: string[];
    /**
     * CIDR IP address range. Maximum number of destination CIDR IP ranges allowed is 256.
     */
    destIpRanges?: string[];
    /**
     * The Unicode country codes whose IP addresses will be used to match against the source of traffic. Can only be specified if DIRECTION is egress.
     */
    destRegionCodes?: string[];
    /**
     * Name of the Google Cloud Threat Intelligence list.
     */
    destThreatIntelligences?: string[];
    /**
     * Pairs of IP protocols and ports that the rule should match.
     */
    layer4Configs: outputs.ComputeFirewallPolicyRuleMatchLayer4Config[];
    /**
     * Address groups which should be matched against the traffic source. Maximum number of source address groups is 10. Source address groups is only supported in Ingress rules.
     */
    srcAddressGroups?: string[];
    /**
     * Domain names that will be used to match against the resolved domain name of source of traffic. Can only be specified if DIRECTION is ingress.
     */
    srcFqdns?: string[];
    /**
     * CIDR IP address range. Maximum number of source CIDR IP ranges allowed is 256.
     */
    srcIpRanges?: string[];
    /**
     * The Unicode country codes whose IP addresses will be used to match against the source of traffic. Can only be specified if DIRECTION is ingress.
     */
    srcRegionCodes?: string[];
    /**
     * Name of the Google Cloud Threat Intelligence list.
     */
    srcThreatIntelligences?: string[];
}

export interface ComputeFirewallPolicyRuleMatchLayer4Config {
    /**
     * The IP protocol to which this rule applies. The protocol type is required when creating a firewall rule. This value can either be one of the following well known protocol strings (`tcp`, `udp`, `icmp`, `esp`, `ah`, `ipip`, `sctp`), or the IP protocol number.
     */
    ipProtocol: string;
    /**
     * An optional list of ports to which this rule applies. This field is only applicable for UDP or TCP protocol. Each entry must be either an integer or a range. If not specified, this rule applies to connections through any port. Example inputs include: ``.
     */
    ports?: string[];
}

export interface ComputeFirewallPolicyRuleTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeFirewallPolicyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeFirewallTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeForwardingRuleServiceDirectoryRegistrations {
    /**
     * Service Directory namespace to register the forwarding rule under.
     */
    namespace: string;
    /**
     * Service Directory service to register the forwarding rule under.
     */
    service?: string;
}

export interface ComputeForwardingRuleTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeGlobalAddressTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeGlobalForwardingRuleMetadataFilter {
    /**
     * The list of label value pairs that must match labels in the
     * provided metadata based on filterMatchCriteria
     *
     * This list must not be empty and can have at the most 64 entries.
     */
    filterLabels: outputs.ComputeGlobalForwardingRuleMetadataFilterFilterLabel[];
    /**
     * Specifies how individual filterLabel matches within the list of
     * filterLabels contribute towards the overall metadataFilter match.
     *
     * MATCH_ANY - At least one of the filterLabels must have a matching
     * label in the provided metadata.
     * MATCH_ALL - All filterLabels must have matching labels in the
     * provided metadata. Possible values: ["MATCH_ANY", "MATCH_ALL"]
     */
    filterMatchCriteria: string;
}

export interface ComputeGlobalForwardingRuleMetadataFilterFilterLabel {
    /**
     * Name of the metadata label. The length must be between
     * 1 and 1024 characters, inclusive.
     */
    name: string;
    /**
     * The value that the label must match. The value has a maximum
     * length of 1024 characters.
     */
    value: string;
}

export interface ComputeGlobalForwardingRuleServiceDirectoryRegistrations {
    /**
     * Service Directory namespace to register the forwarding rule under.
     */
    namespace: string;
    /**
     * [Optional] Service Directory region to register this global forwarding rule under.
     * Default to "us-central1". Only used for PSC for Google APIs. All PSC for
     * Google APIs Forwarding Rules on the same network should use the same Service
     * Directory region.
     */
    serviceDirectoryRegion?: string;
}

export interface ComputeGlobalForwardingRuleTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeGlobalNetworkEndpointGroupTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeGlobalNetworkEndpointTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeHaVpnGatewayTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeHaVpnGatewayVpnInterface {
    /**
     * The numeric ID of this VPN gateway interface.
     */
    id?: number;
    /**
     * URL of the interconnect attachment resource. When the value
     * of this field is present, the VPN Gateway will be used for
     * IPsec-encrypted Cloud Interconnect; all Egress or Ingress
     * traffic for this VPN Gateway interface will go through the
     * specified interconnect attachment resource.
     *
     * Not currently available publicly.
     */
    interconnectAttachment?: string;
    /**
     * The external IP address for this VPN gateway interface.
     */
    ipAddress: string;
}

export interface ComputeHealthCheckGrpcHealthCheck {
    /**
     * The gRPC service name for the health check.
     * The value of grpcServiceName has the following meanings by convention:
     *   - Empty serviceName means the overall status of all services at the backend.
     *   - Non-empty serviceName means the health of that gRPC service, as defined by the owner of the service.
     * The grpcServiceName can only be ASCII.
     */
    grpcServiceName?: string;
    /**
     * The port number for the health check request.
     * Must be specified if portName and portSpecification are not set
     * or if port_specification is USE_FIXED_PORT. Valid values are 1 through 65535.
     */
    port?: number;
    /**
     * Port name as defined in InstanceGroup#NamedPort#name. If both port and
     * port_name are defined, port takes precedence.
     */
    portName?: string;
    /**
     * Specifies how port is selected for health checking, can be one of the
     * following values:
     *
     *   * 'USE_FIXED_PORT': The port number in 'port' is used for health checking.
     *
     *   * 'USE_NAMED_PORT': The 'portName' is used for health checking.
     *
     *   * 'USE_SERVING_PORT': For NetworkEndpointGroup, the port specified for each
     *   network endpoint is used for health checking. For other backends, the
     *   port or named port specified in the Backend Service is used for health
     *   checking.
     *
     * If not specified, gRPC health check follows behavior specified in 'port' and
     * 'portName' fields. Possible values: ["USE_FIXED_PORT", "USE_NAMED_PORT", "USE_SERVING_PORT"]
     */
    portSpecification?: string;
}

export interface ComputeHealthCheckHttp2HealthCheck {
    /**
     * The value of the host header in the HTTP2 health check request.
     * If left empty (default value), the public IP on behalf of which this health
     * check is performed will be used.
     */
    host?: string;
    /**
     * The TCP port number for the HTTP2 health check request.
     * The default value is 443.
     */
    port?: number;
    /**
     * Port name as defined in InstanceGroup#NamedPort#name. If both port and
     * port_name are defined, port takes precedence.
     */
    portName?: string;
    /**
     * Specifies how port is selected for health checking, can be one of the
     * following values:
     *
     *   * 'USE_FIXED_PORT': The port number in 'port' is used for health checking.
     *
     *   * 'USE_NAMED_PORT': The 'portName' is used for health checking.
     *
     *   * 'USE_SERVING_PORT': For NetworkEndpointGroup, the port specified for each
     *   network endpoint is used for health checking. For other backends, the
     *   port or named port specified in the Backend Service is used for health
     *   checking.
     *
     * If not specified, HTTP2 health check follows behavior specified in 'port' and
     * 'portName' fields. Possible values: ["USE_FIXED_PORT", "USE_NAMED_PORT", "USE_SERVING_PORT"]
     */
    portSpecification?: string;
    /**
     * Specifies the type of proxy header to append before sending data to the
     * backend. Default value: "NONE" Possible values: ["NONE", "PROXY_V1"]
     */
    proxyHeader?: string;
    /**
     * The request path of the HTTP2 health check request.
     * The default value is /.
     */
    requestPath?: string;
    /**
     * The bytes to match against the beginning of the response data. If left empty
     * (the default value), any response will indicate health. The response data
     * can only be ASCII.
     */
    response?: string;
}

export interface ComputeHealthCheckHttpHealthCheck {
    /**
     * The value of the host header in the HTTP health check request.
     * If left empty (default value), the public IP on behalf of which this health
     * check is performed will be used.
     */
    host?: string;
    /**
     * The TCP port number for the HTTP health check request.
     * The default value is 80.
     */
    port?: number;
    /**
     * Port name as defined in InstanceGroup#NamedPort#name. If both port and
     * port_name are defined, port takes precedence.
     */
    portName?: string;
    /**
     * Specifies how port is selected for health checking, can be one of the
     * following values:
     *
     *   * 'USE_FIXED_PORT': The port number in 'port' is used for health checking.
     *
     *   * 'USE_NAMED_PORT': The 'portName' is used for health checking.
     *
     *   * 'USE_SERVING_PORT': For NetworkEndpointGroup, the port specified for each
     *   network endpoint is used for health checking. For other backends, the
     *   port or named port specified in the Backend Service is used for health
     *   checking.
     *
     * If not specified, HTTP health check follows behavior specified in 'port' and
     * 'portName' fields. Possible values: ["USE_FIXED_PORT", "USE_NAMED_PORT", "USE_SERVING_PORT"]
     */
    portSpecification?: string;
    /**
     * Specifies the type of proxy header to append before sending data to the
     * backend. Default value: "NONE" Possible values: ["NONE", "PROXY_V1"]
     */
    proxyHeader?: string;
    /**
     * The request path of the HTTP health check request.
     * The default value is /.
     */
    requestPath?: string;
    /**
     * The bytes to match against the beginning of the response data. If left empty
     * (the default value), any response will indicate health. The response data
     * can only be ASCII.
     */
    response?: string;
}

export interface ComputeHealthCheckHttpsHealthCheck {
    /**
     * The value of the host header in the HTTPS health check request.
     * If left empty (default value), the public IP on behalf of which this health
     * check is performed will be used.
     */
    host?: string;
    /**
     * The TCP port number for the HTTPS health check request.
     * The default value is 443.
     */
    port?: number;
    /**
     * Port name as defined in InstanceGroup#NamedPort#name. If both port and
     * port_name are defined, port takes precedence.
     */
    portName?: string;
    /**
     * Specifies how port is selected for health checking, can be one of the
     * following values:
     *
     *   * 'USE_FIXED_PORT': The port number in 'port' is used for health checking.
     *
     *   * 'USE_NAMED_PORT': The 'portName' is used for health checking.
     *
     *   * 'USE_SERVING_PORT': For NetworkEndpointGroup, the port specified for each
     *   network endpoint is used for health checking. For other backends, the
     *   port or named port specified in the Backend Service is used for health
     *   checking.
     *
     * If not specified, HTTPS health check follows behavior specified in 'port' and
     * 'portName' fields. Possible values: ["USE_FIXED_PORT", "USE_NAMED_PORT", "USE_SERVING_PORT"]
     */
    portSpecification?: string;
    /**
     * Specifies the type of proxy header to append before sending data to the
     * backend. Default value: "NONE" Possible values: ["NONE", "PROXY_V1"]
     */
    proxyHeader?: string;
    /**
     * The request path of the HTTPS health check request.
     * The default value is /.
     */
    requestPath?: string;
    /**
     * The bytes to match against the beginning of the response data. If left empty
     * (the default value), any response will indicate health. The response data
     * can only be ASCII.
     */
    response?: string;
}

export interface ComputeHealthCheckLogConfig {
    /**
     * Indicates whether or not to export logs. This is false by default,
     * which means no health check logging will be done.
     */
    enable?: boolean;
}

export interface ComputeHealthCheckSslHealthCheck {
    /**
     * The TCP port number for the SSL health check request.
     * The default value is 443.
     */
    port?: number;
    /**
     * Port name as defined in InstanceGroup#NamedPort#name. If both port and
     * port_name are defined, port takes precedence.
     */
    portName?: string;
    /**
     * Specifies how port is selected for health checking, can be one of the
     * following values:
     *
     *   * 'USE_FIXED_PORT': The port number in 'port' is used for health checking.
     *
     *   * 'USE_NAMED_PORT': The 'portName' is used for health checking.
     *
     *   * 'USE_SERVING_PORT': For NetworkEndpointGroup, the port specified for each
     *   network endpoint is used for health checking. For other backends, the
     *   port or named port specified in the Backend Service is used for health
     *   checking.
     *
     * If not specified, SSL health check follows behavior specified in 'port' and
     * 'portName' fields. Possible values: ["USE_FIXED_PORT", "USE_NAMED_PORT", "USE_SERVING_PORT"]
     */
    portSpecification?: string;
    /**
     * Specifies the type of proxy header to append before sending data to the
     * backend. Default value: "NONE" Possible values: ["NONE", "PROXY_V1"]
     */
    proxyHeader?: string;
    /**
     * The application data to send once the SSL connection has been
     * established (default value is empty). If both request and response are
     * empty, the connection establishment alone will indicate health. The request
     * data can only be ASCII.
     */
    request?: string;
    /**
     * The bytes to match against the beginning of the response data. If left empty
     * (the default value), any response will indicate health. The response data
     * can only be ASCII.
     */
    response?: string;
}

export interface ComputeHealthCheckTcpHealthCheck {
    /**
     * The TCP port number for the TCP health check request.
     * The default value is 443.
     */
    port?: number;
    /**
     * Port name as defined in InstanceGroup#NamedPort#name. If both port and
     * port_name are defined, port takes precedence.
     */
    portName?: string;
    /**
     * Specifies how port is selected for health checking, can be one of the
     * following values:
     *
     *   * 'USE_FIXED_PORT': The port number in 'port' is used for health checking.
     *
     *   * 'USE_NAMED_PORT': The 'portName' is used for health checking.
     *
     *   * 'USE_SERVING_PORT': For NetworkEndpointGroup, the port specified for each
     *   network endpoint is used for health checking. For other backends, the
     *   port or named port specified in the Backend Service is used for health
     *   checking.
     *
     * If not specified, TCP health check follows behavior specified in 'port' and
     * 'portName' fields. Possible values: ["USE_FIXED_PORT", "USE_NAMED_PORT", "USE_SERVING_PORT"]
     */
    portSpecification?: string;
    /**
     * Specifies the type of proxy header to append before sending data to the
     * backend. Default value: "NONE" Possible values: ["NONE", "PROXY_V1"]
     */
    proxyHeader?: string;
    /**
     * The application data to send once the TCP connection has been
     * established (default value is empty). If both request and response are
     * empty, the connection establishment alone will indicate health. The request
     * data can only be ASCII.
     */
    request?: string;
    /**
     * The bytes to match against the beginning of the response data. If left empty
     * (the default value), any response will indicate health. The response data
     * can only be ASCII.
     */
    response?: string;
}

export interface ComputeHealthCheckTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeHttpHealthCheckTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeHttpsHealthCheckTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeImageGuestOsFeature {
    /**
     * The type of supported feature. Read [Enabling guest operating system features](https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images#guest-os-features) to see a list of available options. Possible values: ["MULTI_IP_SUBNET", "SECURE_BOOT", "SEV_CAPABLE", "UEFI_COMPATIBLE", "VIRTIO_SCSI_MULTIQUEUE", "WINDOWS", "GVNIC", "SEV_LIVE_MIGRATABLE", "SEV_SNP_CAPABLE", "SUSPEND_RESUME_COMPATIBLE", "TDX_CAPABLE", "SEV_LIVE_MIGRATABLE_V2"]
     */
    type: string;
}

export interface ComputeImageIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ComputeImageIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ComputeImageImageEncryptionKey {
    /**
     * The self link of the encryption key that is stored in Google Cloud
     * KMS.
     */
    kmsKeySelfLink?: string;
    /**
     * The service account being used for the encryption request for the
     * given KMS key. If absent, the Compute Engine default service
     * account is used.
     */
    kmsKeyServiceAccount?: string;
}

export interface ComputeImageRawDisk {
    /**
     * The format used to encode and transmit the block device, which
     * should be TAR. This is just a container and transmission format
     * and not a runtime format. Provided by the client when the disk
     * image is created. Default value: "TAR" Possible values: ["TAR"]
     */
    containerType?: string;
    /**
     * An optional SHA1 checksum of the disk image before unpackaging.
     * This is provided by the client when the disk image is created.
     */
    sha1?: string;
    /**
     * The full Google Cloud Storage URL where disk storage is stored
     * You must provide either this property or the sourceDisk property
     * but not both.
     */
    source: string;
}

export interface ComputeImageTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeInstanceAdvancedMachineFeatures {
    /**
     * Whether to enable nested virtualization or not.
     */
    enableNestedVirtualization?: boolean;
    /**
     * The number of threads per physical core. To disable simultaneous multithreading (SMT) set this to 1. If unset, the maximum number of threads supported per core by the underlying processor is assumed.
     */
    threadsPerCore?: number;
    /**
     * The number of physical cores to expose to an instance. Multiply by the number of threads per core to compute the total number of virtual CPUs to expose to the instance. If unset, the number of cores is inferred from the instance\'s nominal CPU count and the underlying platform\'s SMT width.
     */
    visibleCoreCount?: number;
}

export interface ComputeInstanceAttachedDisk {
    /**
     * Name with which the attached disk is accessible under /dev/disk/by-id/
     */
    deviceName: string;
    /**
     * A 256-bit customer-supplied encryption key, encoded in RFC 4648 base64 to encrypt this disk. Only one of kms_key_self_link and disk_encryption_key_raw may be set.
     */
    diskEncryptionKeyRaw?: string;
    /**
     * The RFC 4648 base64 encoded SHA-256 hash of the customer-supplied encryption key that protects this resource.
     */
    diskEncryptionKeySha256: string;
    /**
     * The self_link of the encryption key that is stored in Google Cloud KMS to encrypt this disk. Only one of kms_key_self_link and disk_encryption_key_raw may be set.
     */
    kmsKeySelfLink: string;
    /**
     * Read/write mode for the disk. One of "READ_ONLY" or "READ_WRITE".
     */
    mode?: string;
    /**
     * The name or self_link of the disk attached to this instance.
     */
    source: string;
}

export interface ComputeInstanceBootDisk {
    /**
     * Whether the disk will be auto-deleted when the instance is deleted.
     */
    autoDelete?: boolean;
    /**
     * Name with which attached disk will be accessible under /dev/disk/by-id/
     */
    deviceName: string;
    /**
     * A 256-bit customer-supplied encryption key, encoded in RFC 4648 base64 to encrypt this disk. Only one of kms_key_self_link and disk_encryption_key_raw may be set.
     */
    diskEncryptionKeyRaw?: string;
    /**
     * The RFC 4648 base64 encoded SHA-256 hash of the customer-supplied encryption key that protects this resource.
     */
    diskEncryptionKeySha256: string;
    /**
     * Parameters with which a disk was created alongside the instance.
     */
    initializeParams?: outputs.ComputeInstanceBootDiskInitializeParams;
    /**
     * The self_link of the encryption key that is stored in Google Cloud KMS to encrypt this disk. Only one of kms_key_self_link and disk_encryption_key_raw may be set.
     */
    kmsKeySelfLink: string;
    /**
     * Read/write mode for the disk. One of "READ_ONLY" or "READ_WRITE".
     */
    mode?: string;
    /**
     * The name or self_link of the disk attached to this instance.
     */
    source: string;
}

export interface ComputeInstanceBootDiskInitializeParams {
    /**
     * A flag to enable confidential compute mode on boot disk
     */
    enableConfidentialCompute?: boolean;
    /**
     * The image from which this disk was initialised.
     */
    image: string;
    /**
     * A set of key/value label pairs assigned to the disk.
     */
    labels: {[key: string]: string};
    /**
     * Indicates how many IOPS to provision for the disk. This sets the number of I/O operations per second that the disk can handle.
     */
    provisionedIops: number;
    /**
     * Indicates how much throughput to provision for the disk. This sets the number of throughput mb per second that the disk can handle.
     */
    provisionedThroughput: number;
    /**
     * A map of resource manager tags. Resource manager tag keys and values have the same definition as resource manager tags. Keys must be in the format tagKeys/{tag_key_id}, and values are in the format tagValues/456. The field is ignored (both PUT & PATCH) when empty.
     */
    resourceManagerTags?: {[key: string]: string};
    /**
     * The size of the image in gigabytes.
     */
    size: number;
    /**
     * The URL of the storage pool in which the new disk is created
     */
    storagePool?: string;
    /**
     * The Google Compute Engine disk type. Such as pd-standard, pd-ssd or pd-balanced.
     */
    type: string;
}

export interface ComputeInstanceConfidentialInstanceConfig {
    /**
     * The confidential computing technology the instance uses.
     * 								SEV is an AMD feature. TDX is an Intel feature. One of the following
     * 								values is required: SEV, SEV_SNP, TDX. If SEV_SNP, min_cpu_platform =
     * 								"AMD Milan" is currently required. TDX is only available in beta.
     */
    confidentialInstanceType?: string;
    /**
     * Defines whether the instance should have confidential compute enabled. Field will be deprecated in a future release
     */
    enableConfidentialCompute?: boolean;
}

export interface ComputeInstanceFromTemplateAdvancedMachineFeatures {
    /**
     * Whether to enable nested virtualization or not.
     */
    enableNestedVirtualization: boolean;
    /**
     * The number of threads per physical core. To disable simultaneous multithreading (SMT) set this to 1. If unset, the maximum number of threads supported per core by the underlying processor is assumed.
     */
    threadsPerCore: number;
    /**
     * The number of physical cores to expose to an instance. Multiply by the number of threads per core to compute the total number of virtual CPUs to expose to the instance. If unset, the number of cores is inferred from the instance\'s nominal CPU count and the underlying platform\'s SMT width.
     */
    visibleCoreCount: number;
}

export interface ComputeInstanceFromTemplateAttachedDisk {
    /**
     * Name with which the attached disk is accessible under /dev/disk/by-id/
     */
    deviceName: string;
    /**
     * A 256-bit customer-supplied encryption key, encoded in RFC 4648 base64 to encrypt this disk. Only one of kms_key_self_link and disk_encryption_key_raw may be set.
     */
    diskEncryptionKeyRaw: string;
    /**
     * The RFC 4648 base64 encoded SHA-256 hash of the customer-supplied encryption key that protects this resource.
     */
    diskEncryptionKeySha256: string;
    /**
     * The self_link of the encryption key that is stored in Google Cloud KMS to encrypt this disk. Only one of kms_key_self_link and disk_encryption_key_raw may be set.
     */
    kmsKeySelfLink: string;
    /**
     * Read/write mode for the disk. One of "READ_ONLY" or "READ_WRITE".
     */
    mode: string;
    /**
     * The name or self_link of the disk attached to this instance.
     */
    source: string;
}

export interface ComputeInstanceFromTemplateBootDisk {
    /**
     * Whether the disk will be auto-deleted when the instance is deleted.
     */
    autoDelete: boolean;
    /**
     * Name with which attached disk will be accessible under /dev/disk/by-id/
     */
    deviceName: string;
    /**
     * A 256-bit customer-supplied encryption key, encoded in RFC 4648 base64 to encrypt this disk. Only one of kms_key_self_link and disk_encryption_key_raw may be set.
     */
    diskEncryptionKeyRaw: string;
    /**
     * The RFC 4648 base64 encoded SHA-256 hash of the customer-supplied encryption key that protects this resource.
     */
    diskEncryptionKeySha256: string;
    /**
     * Parameters with which a disk was created alongside the instance.
     */
    initializeParams?: outputs.ComputeInstanceFromTemplateBootDiskInitializeParams;
    /**
     * The self_link of the encryption key that is stored in Google Cloud KMS to encrypt this disk. Only one of kms_key_self_link and disk_encryption_key_raw may be set.
     */
    kmsKeySelfLink: string;
    /**
     * Read/write mode for the disk. One of "READ_ONLY" or "READ_WRITE".
     */
    mode: string;
    /**
     * The name or self_link of the disk attached to this instance.
     */
    source: string;
}

export interface ComputeInstanceFromTemplateBootDiskInitializeParams {
    /**
     * A flag to enable confidential compute mode on boot disk
     */
    enableConfidentialCompute: boolean;
    /**
     * The image from which this disk was initialised.
     */
    image: string;
    /**
     * A set of key/value label pairs assigned to the disk.
     */
    labels: {[key: string]: string};
    /**
     * Indicates how many IOPS to provision for the disk. This sets the number of I/O operations per second that the disk can handle.
     */
    provisionedIops: number;
    /**
     * Indicates how much throughput to provision for the disk. This sets the number of throughput mb per second that the disk can handle.
     */
    provisionedThroughput: number;
    /**
     * A map of resource manager tags. Resource manager tag keys and values have the same definition as resource manager tags. Keys must be in the format tagKeys/{tag_key_id}, and values are in the format tagValues/456. The field is ignored (both PUT & PATCH) when empty.
     */
    resourceManagerTags: {[key: string]: string};
    /**
     * The size of the image in gigabytes.
     */
    size: number;
    /**
     * The URL of the storage pool in which the new disk is created
     */
    storagePool: string;
    /**
     * The Google Compute Engine disk type. Such as pd-standard, pd-ssd or pd-balanced.
     */
    type: string;
}

export interface ComputeInstanceFromTemplateConfidentialInstanceConfig {
    /**
     * The confidential computing technology the instance uses.
     * 								SEV is an AMD feature. TDX is an Intel feature. One of the following
     * 								values is required: SEV, SEV_SNP, TDX. If SEV_SNP, min_cpu_platform =
     * 								"AMD Milan" is currently required. TDX is only available in beta.
     */
    confidentialInstanceType: string;
    /**
     * Defines whether the instance should have confidential compute enabled. Field will be deprecated in a future release
     */
    enableConfidentialCompute: boolean;
}

export interface ComputeInstanceFromTemplateGuestAccelerator {
    /**
     * The number of the guest accelerator cards exposed to this instance.
     */
    count: number;
    /**
     * The accelerator type resource exposed to this instance. E.g. nvidia-tesla-k80.
     */
    type: string;
}

export interface ComputeInstanceFromTemplateNetworkInterface {
    /**
     * Access configurations, i.e. IPs via which this instance can be accessed via the Internet.
     */
    accessConfigs?: outputs.ComputeInstanceFromTemplateNetworkInterfaceAccessConfig[];
    /**
     * An array of alias IP ranges for this network interface.
     */
    aliasIpRanges?: outputs.ComputeInstanceFromTemplateNetworkInterfaceAliasIpRange[];
    /**
     * The prefix length of the primary internal IPv6 range.
     */
    internalIpv6PrefixLength: number;
    /**
     * An array of IPv6 access configurations for this interface. Currently, only one IPv6 access config, DIRECT_IPV6, is supported. If there is no ipv6AccessConfig specified, then this instance will have no external IPv6 Internet access.
     */
    ipv6AccessConfigs?: outputs.ComputeInstanceFromTemplateNetworkInterfaceIpv6AccessConfig[];
    /**
     * One of EXTERNAL, INTERNAL to indicate whether the IP can be accessed from the Internet. This field is always inherited from its subnetwork.
     */
    ipv6AccessType: string;
    /**
     * An IPv6 internal network address for this network interface. If not specified, Google Cloud will automatically assign an internal IPv6 address from the instance's subnetwork.
     */
    ipv6Address: string;
    /**
     * The name of the interface
     */
    name: string;
    /**
     * The name or self_link of the network attached to this interface.
     */
    network: string;
    /**
     * The private IP address assigned to the instance.
     */
    networkIp: string;
    /**
     * The type of vNIC to be used on this interface. Possible values:GVNIC, VIRTIO_NET
     */
    nicType: string;
    /**
     * The networking queue count that's specified by users for the network interface. Both Rx and Tx queues will be set to this number. It will be empty if not specified.
     */
    queueCount: number;
    /**
     * The stack type for this network interface to identify whether the IPv6 feature is enabled or not. If not specified, IPV4_ONLY will be used.
     */
    stackType: string;
    /**
     * The name or self_link of the subnetwork attached to this interface.
     */
    subnetwork: string;
    /**
     * The project in which the subnetwork belongs.
     */
    subnetworkProject: string;
}

export interface ComputeInstanceFromTemplateNetworkInterfaceAccessConfig {
    /**
     * The IP address that is be 1:1 mapped to the instance's network ip.
     */
    natIp: string;
    /**
     * The networking tier used for configuring this instance. One of PREMIUM or STANDARD.
     */
    networkTier: string;
    /**
     * The DNS domain name for the public PTR record.
     */
    publicPtrDomainName: string;
}

export interface ComputeInstanceFromTemplateNetworkInterfaceAliasIpRange {
    /**
     * The IP CIDR range represented by this alias IP range.
     */
    ipCidrRange: string;
    /**
     * The subnetwork secondary range name specifying the secondary range from which to allocate the IP CIDR range for this alias IP range.
     */
    subnetworkRangeName: string;
}

export interface ComputeInstanceFromTemplateNetworkInterfaceIpv6AccessConfig {
    /**
     * The first IPv6 address of the external IPv6 range associated with this instance, prefix length is stored in externalIpv6PrefixLength in ipv6AccessConfig. To use a static external IP address, it must be unused and in the same region as the instance's zone. If not specified, Google Cloud will automatically assign an external IPv6 address from the instance's subnetwork.
     */
    externalIpv6: string;
    /**
     * The prefix length of the external IPv6 range.
     */
    externalIpv6PrefixLength: string;
    /**
     * The name of this access configuration. In ipv6AccessConfigs, the recommended name is External IPv6.
     */
    name: string;
    /**
     * The service-level to be provided for IPv6 traffic when the subnet has an external subnet. Only PREMIUM tier is valid for IPv6
     */
    networkTier: string;
    /**
     * The domain name to be used when creating DNSv6 records for the external IPv6 ranges.
     */
    publicPtrDomainName: string;
}

export interface ComputeInstanceFromTemplateNetworkPerformanceConfig {
    /**
     * The egress bandwidth tier to enable. Possible values:TIER_1, DEFAULT
     */
    totalEgressBandwidthTier: string;
}

export interface ComputeInstanceFromTemplateParams {
    /**
     * A map of resource manager tags. Resource manager tag keys and values have the same definition as resource manager tags. Keys must be in the format tagKeys/{tag_key_id}, and values are in the format tagValues/456. The field is ignored (both PUT & PATCH) when empty.
     */
    resourceManagerTags: {[key: string]: string};
}

export interface ComputeInstanceFromTemplateReservationAffinity {
    /**
     * Specifies the label selector for the reservation to use.
     */
    specificReservation?: outputs.ComputeInstanceFromTemplateReservationAffinitySpecificReservation;
    /**
     * The type of reservation from which this instance can consume resources.
     */
    type: string;
}

export interface ComputeInstanceFromTemplateReservationAffinitySpecificReservation {
    /**
     * Corresponds to the label key of a reservation resource. To target a SPECIFIC_RESERVATION by name, specify compute.googleapis.com/reservation-name as the key and specify the name of your reservation as the only value.
     */
    key: string;
    /**
     * Corresponds to the label values of a reservation resource.
     */
    values: string[];
}

export interface ComputeInstanceFromTemplateScheduling {
    /**
     * Specifies if the instance should be restarted if it was terminated by Compute Engine (not a user).
     */
    automaticRestart: boolean;
    /**
     * Specifies the action GCE should take when SPOT VM is preempted.
     */
    instanceTerminationAction: string;
    /**
     * Specifies the maximum amount of time a Local Ssd Vm should wait while
     *   recovery of the Local Ssd state is attempted. Its value should be in
     *   between 0 and 168 hours with hour granularity and the default value being 1
     *   hour.
     */
    localSsdRecoveryTimeout?: outputs.ComputeInstanceFromTemplateSchedulingLocalSsdRecoveryTimeout;
    /**
     * The timeout for new network connections to hosts.
     */
    maxRunDuration?: outputs.ComputeInstanceFromTemplateSchedulingMaxRunDuration;
    minNodeCpus: number;
    /**
     * Specifies node affinities or anti-affinities to determine which sole-tenant nodes your instances and managed instance groups will use as host systems.
     */
    nodeAffinities?: outputs.ComputeInstanceFromTemplateSchedulingNodeAffinity[];
    /**
     * Describes maintenance behavior for the instance. One of MIGRATE or TERMINATE,
     */
    onHostMaintenance: string;
    /**
     * Defines the behaviour for instances with the instance_termination_action.
     */
    onInstanceStopAction?: outputs.ComputeInstanceFromTemplateSchedulingOnInstanceStopAction;
    /**
     * Whether the instance is preemptible.
     */
    preemptible: boolean;
    /**
     * Whether the instance is spot. If this is set as SPOT.
     */
    provisioningModel: string;
}

export interface ComputeInstanceFromTemplateSchedulingLocalSsdRecoveryTimeout {
    /**
     * Span of time that's a fraction of a second at nanosecond
     * resolution. Durations less than one second are represented
     * with a 0 seconds field and a positive nanos field. Must
     * be from 0 to 999,999,999 inclusive.
     */
    nanos: number;
    /**
     * Span of time at a resolution of a second.
     * Must be from 0 to 315,576,000,000 inclusive.
     */
    seconds: number;
}

export interface ComputeInstanceFromTemplateSchedulingMaxRunDuration {
    /**
     * Span of time that's a fraction of a second at nanosecond
     * resolution. Durations less than one second are represented
     * with a 0 seconds field and a positive nanos field. Must
     * be from 0 to 999,999,999 inclusive.
     */
    nanos: number;
    /**
     * Span of time at a resolution of a second.
     * Must be from 0 to 315,576,000,000 inclusive.
     */
    seconds: number;
}

export interface ComputeInstanceFromTemplateSchedulingNodeAffinity {
    key: string;
    operator: string;
    values: string[];
}

export interface ComputeInstanceFromTemplateSchedulingOnInstanceStopAction {
    /**
     * If true, the contents of any attached Local SSD disks will be discarded.
     */
    discardLocalSsd: boolean;
}

export interface ComputeInstanceFromTemplateScratchDisk {
    /**
     * Name with which the attached disk is accessible under /dev/disk/by-id/
     */
    deviceName: string;
    /**
     * The disk interface used for attaching this disk. One of SCSI or NVME.
     */
    interface: string;
    /**
     * The size of the disk in gigabytes. One of 375 or 3000.
     */
    size: number;
}

export interface ComputeInstanceFromTemplateServiceAccount {
    /**
     * The service account e-mail address.
     */
    email: string;
    /**
     * A list of service scopes.
     */
    scopes: string[];
}

export interface ComputeInstanceFromTemplateShieldedInstanceConfig {
    /**
     * Whether integrity monitoring is enabled for the instance.
     */
    enableIntegrityMonitoring: boolean;
    /**
     * Whether secure boot is enabled for the instance.
     */
    enableSecureBoot: boolean;
    /**
     * Whether the instance uses vTPM.
     */
    enableVtpm: boolean;
}

export interface ComputeInstanceFromTemplateTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeInstanceGroupManagerAllInstancesConfig {
    /**
     * The label key-value pairs that you want to patch onto the instance,
     */
    labels?: {[key: string]: string};
    /**
     * The metadata key-value pairs that you want to patch onto the instance. For more information, see Project and instance metadata,
     */
    metadata?: {[key: string]: string};
}

export interface ComputeInstanceGroupManagerAutoHealingPolicies {
    /**
     * The health check resource that signals autohealing.
     */
    healthCheck: string;
    /**
     * The number of seconds that the managed instance group waits before it applies autohealing policies to new instances or recently recreated instances. Between 0 and 3600.
     */
    initialDelaySec: number;
}

export interface ComputeInstanceGroupManagerInstanceLifecyclePolicy {
    /**
     * Default behavior for all instance or health check failures.
     */
    defaultActionOnFailure?: string;
    /**
     * Specifies whether to apply the group's latest configuration when repairing a VM. Valid options are: YES, NO. If YES and you updated the group's instance template or per-instance configurations after the VM was created, then these changes are applied when VM is repaired. If NO (default), then updates are applied in accordance with the group's update policy type.
     */
    forceUpdateOnRepair?: string;
}

export interface ComputeInstanceGroupManagerNamedPort {
    /**
     * The name of the port.
     */
    name: string;
    /**
     * The port number.
     */
    port: number;
}

export interface ComputeInstanceGroupManagerStatefulDisk {
    /**
     * A value that prescribes what should happen to the stateful disk when the VM instance is deleted. The available options are NEVER and ON_PERMANENT_INSTANCE_DELETION. NEVER - detach the disk when the VM is deleted, but do not delete the disk. ON_PERMANENT_INSTANCE_DELETION will delete the stateful disk when the VM is permanently deleted from the instance group. The default is NEVER.
     */
    deleteRule?: string;
    /**
     * The device name of the disk to be attached.
     */
    deviceName: string;
}

export interface ComputeInstanceGroupManagerStatefulExternalIp {
    /**
     * A value that prescribes what should happen to an associated static Address resource when a VM instance is permanently deleted. The available options are NEVER and ON_PERMANENT_INSTANCE_DELETION. NEVER - detach the IP when the VM is deleted, but do not delete the address resource. ON_PERMANENT_INSTANCE_DELETION will delete the stateful address when the VM is permanently deleted from the instance group. The default is NEVER.
     */
    deleteRule?: string;
    /**
     * The network interface name
     */
    interfaceName?: string;
}

export interface ComputeInstanceGroupManagerStatefulInternalIp {
    /**
     * A value that prescribes what should happen to an associated static Address resource when a VM instance is permanently deleted. The available options are NEVER and ON_PERMANENT_INSTANCE_DELETION. NEVER - detach the IP when the VM is deleted, but do not delete the address resource. ON_PERMANENT_INSTANCE_DELETION will delete the stateful address when the VM is permanently deleted from the instance group. The default is NEVER.
     */
    deleteRule?: string;
    /**
     * The network interface name
     */
    interfaceName?: string;
}

export interface ComputeInstanceGroupManagerStatus {
    allInstancesConfigs: outputs.ComputeInstanceGroupManagerStatusAllInstancesConfig[];
    isStable: boolean;
    statefuls: outputs.ComputeInstanceGroupManagerStatusStateful[];
    versionTargets: outputs.ComputeInstanceGroupManagerStatusVersionTarget[];
}

export interface ComputeInstanceGroupManagerStatusAllInstancesConfig {
    currentRevision: string;
    effective: boolean;
}

export interface ComputeInstanceGroupManagerStatusStateful {
    hasStatefulConfig: boolean;
    perInstanceConfigs: outputs.ComputeInstanceGroupManagerStatusStatefulPerInstanceConfig[];
}

export interface ComputeInstanceGroupManagerStatusStatefulPerInstanceConfig {
    allEffective: boolean;
}

export interface ComputeInstanceGroupManagerStatusVersionTarget {
    isReached: boolean;
}

export interface ComputeInstanceGroupManagerTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeInstanceGroupManagerUpdatePolicy {
    /**
     * Specifies a fixed number of VM instances. This must be a positive integer. Conflicts with max_surge_percent. Both cannot be 0
     */
    maxSurgeFixed: number;
    /**
     * Specifies a percentage of instances between 0 to 100%, inclusive. For example, specify 80 for 80%. Conflicts with max_surge_fixed.
     */
    maxSurgePercent?: number;
    /**
     * Specifies a fixed number of VM instances. This must be a positive integer.
     */
    maxUnavailableFixed: number;
    /**
     * Specifies a percentage of instances between 0 to 100%, inclusive. For example, specify 80 for 80%.
     */
    maxUnavailablePercent?: number;
    /**
     * Minimal action to be taken on an instance. You can specify either NONE to forbid any actions, REFRESH to update without stopping instances, RESTART to restart existing instances or REPLACE to delete and create new instances from the target template. If you specify a REFRESH, the Updater will attempt to perform that action only. However, if the Updater determines that the minimal action you specify is not enough to perform the update, it might perform a more disruptive action.
     */
    minimalAction: string;
    /**
     * Most disruptive action that is allowed to be taken on an instance. You can specify either NONE to forbid any actions, REFRESH to allow actions that do not need instance restart, RESTART to allow actions that can be applied without instance replacing or REPLACE to allow all possible actions. If the Updater determines that the minimal update action needed is more disruptive than most disruptive allowed action you specify it will not perform the update at all.
     */
    mostDisruptiveAllowedAction?: string;
    /**
     * The instance replacement method for managed instance groups. Valid values are: "RECREATE", "SUBSTITUTE". If SUBSTITUTE (default), the group replaces VM instances with new instances that have randomly generated names. If RECREATE, instance names are preserved.  You must also set max_unavailable_fixed or max_unavailable_percent to be greater than 0.
     */
    replacementMethod?: string;
    /**
     * The type of update process. You can specify either PROACTIVE so that the instance group manager proactively executes actions in order to bring instances to their target versions or OPPORTUNISTIC so that no action is proactively executed but the update will be performed as part of other actions (for example, resizes or recreateInstances calls).
     */
    type: string;
}

export interface ComputeInstanceGroupManagerVersion {
    /**
     * The full URL to an instance template from which all new instances of this version will be created.
     */
    instanceTemplate: string;
    /**
     * Version name.
     */
    name?: string;
    /**
     * The number of instances calculated as a fixed number or a percentage depending on the settings.
     */
    targetSize?: outputs.ComputeInstanceGroupManagerVersionTargetSize;
}

export interface ComputeInstanceGroupManagerVersionTargetSize {
    /**
     * The number of instances which are managed for this version. Conflicts with percent.
     */
    fixed?: number;
    /**
     * The number of instances (calculated as percentage) which are managed for this version. Conflicts with fixed. Note that when using percent, rounding will be in favor of explicitly set target_size values; a managed instance group with 2 instances and 2 versions, one of which has a target_size.percent of 60 will create 2 instances of that version.
     */
    percent?: number;
}

export interface ComputeInstanceGroupMembershipTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeInstanceGroupNamedPort {
    /**
     * The name which the port will be mapped to.
     */
    name: string;
    /**
     * The port number to map the name to.
     */
    port: number;
}

export interface ComputeInstanceGroupNamedPortTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeInstanceGroupTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeInstanceGuestAccelerator {
    /**
     * The number of the guest accelerator cards exposed to this instance.
     */
    count: number;
    /**
     * The accelerator type resource exposed to this instance. E.g. nvidia-tesla-k80.
     */
    type: string;
}

export interface ComputeInstanceIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ComputeInstanceIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ComputeInstanceNetworkInterface {
    /**
     * Access configurations, i.e. IPs via which this instance can be accessed via the Internet.
     */
    accessConfigs?: outputs.ComputeInstanceNetworkInterfaceAccessConfig[];
    /**
     * An array of alias IP ranges for this network interface.
     */
    aliasIpRanges?: outputs.ComputeInstanceNetworkInterfaceAliasIpRange[];
    /**
     * The prefix length of the primary internal IPv6 range.
     */
    internalIpv6PrefixLength: number;
    /**
     * An array of IPv6 access configurations for this interface. Currently, only one IPv6 access config, DIRECT_IPV6, is supported. If there is no ipv6AccessConfig specified, then this instance will have no external IPv6 Internet access.
     */
    ipv6AccessConfigs?: outputs.ComputeInstanceNetworkInterfaceIpv6AccessConfig[];
    /**
     * One of EXTERNAL, INTERNAL to indicate whether the IP can be accessed from the Internet. This field is always inherited from its subnetwork.
     */
    ipv6AccessType: string;
    /**
     * An IPv6 internal network address for this network interface. If not specified, Google Cloud will automatically assign an internal IPv6 address from the instance's subnetwork.
     */
    ipv6Address: string;
    /**
     * The name of the interface
     */
    name: string;
    /**
     * The name or self_link of the network attached to this interface.
     */
    network: string;
    /**
     * The private IP address assigned to the instance.
     */
    networkIp: string;
    /**
     * The type of vNIC to be used on this interface. Possible values:GVNIC, VIRTIO_NET
     */
    nicType?: string;
    /**
     * The networking queue count that's specified by users for the network interface. Both Rx and Tx queues will be set to this number. It will be empty if not specified.
     */
    queueCount?: number;
    /**
     * The stack type for this network interface to identify whether the IPv6 feature is enabled or not. If not specified, IPV4_ONLY will be used.
     */
    stackType: string;
    /**
     * The name or self_link of the subnetwork attached to this interface.
     */
    subnetwork: string;
    /**
     * The project in which the subnetwork belongs.
     */
    subnetworkProject: string;
}

export interface ComputeInstanceNetworkInterfaceAccessConfig {
    /**
     * The IP address that is be 1:1 mapped to the instance's network ip.
     */
    natIp: string;
    /**
     * The networking tier used for configuring this instance. One of PREMIUM or STANDARD.
     */
    networkTier: string;
    /**
     * The DNS domain name for the public PTR record.
     */
    publicPtrDomainName?: string;
}

export interface ComputeInstanceNetworkInterfaceAliasIpRange {
    /**
     * The IP CIDR range represented by this alias IP range.
     */
    ipCidrRange: string;
    /**
     * The subnetwork secondary range name specifying the secondary range from which to allocate the IP CIDR range for this alias IP range.
     */
    subnetworkRangeName?: string;
}

export interface ComputeInstanceNetworkInterfaceIpv6AccessConfig {
    /**
     * The first IPv6 address of the external IPv6 range associated with this instance, prefix length is stored in externalIpv6PrefixLength in ipv6AccessConfig. To use a static external IP address, it must be unused and in the same region as the instance's zone. If not specified, Google Cloud will automatically assign an external IPv6 address from the instance's subnetwork.
     */
    externalIpv6: string;
    /**
     * The prefix length of the external IPv6 range.
     */
    externalIpv6PrefixLength: string;
    /**
     * The name of this access configuration. In ipv6AccessConfigs, the recommended name is External IPv6.
     */
    name: string;
    /**
     * The service-level to be provided for IPv6 traffic when the subnet has an external subnet. Only PREMIUM tier is valid for IPv6
     */
    networkTier: string;
    /**
     * The domain name to be used when creating DNSv6 records for the external IPv6 ranges.
     */
    publicPtrDomainName?: string;
}

export interface ComputeInstanceNetworkPerformanceConfig {
    /**
     * The egress bandwidth tier to enable. Possible values:TIER_1, DEFAULT
     */
    totalEgressBandwidthTier: string;
}

export interface ComputeInstanceParams {
    /**
     * A map of resource manager tags. Resource manager tag keys and values have the same definition as resource manager tags. Keys must be in the format tagKeys/{tag_key_id}, and values are in the format tagValues/456. The field is ignored (both PUT & PATCH) when empty.
     */
    resourceManagerTags?: {[key: string]: string};
}

export interface ComputeInstanceReservationAffinity {
    /**
     * Specifies the label selector for the reservation to use.
     */
    specificReservation?: outputs.ComputeInstanceReservationAffinitySpecificReservation;
    /**
     * The type of reservation from which this instance can consume resources.
     */
    type: string;
}

export interface ComputeInstanceReservationAffinitySpecificReservation {
    /**
     * Corresponds to the label key of a reservation resource. To target a SPECIFIC_RESERVATION by name, specify compute.googleapis.com/reservation-name as the key and specify the name of your reservation as the only value.
     */
    key: string;
    /**
     * Corresponds to the label values of a reservation resource.
     */
    values: string[];
}

export interface ComputeInstanceScheduling {
    /**
     * Specifies if the instance should be restarted if it was terminated by Compute Engine (not a user).
     */
    automaticRestart?: boolean;
    /**
     * Specifies the action GCE should take when SPOT VM is preempted.
     */
    instanceTerminationAction?: string;
    /**
     * Specifies the maximum amount of time a Local Ssd Vm should wait while
     *   recovery of the Local Ssd state is attempted. Its value should be in
     *   between 0 and 168 hours with hour granularity and the default value being 1
     *   hour.
     */
    localSsdRecoveryTimeout?: outputs.ComputeInstanceSchedulingLocalSsdRecoveryTimeout;
    /**
     * The timeout for new network connections to hosts.
     */
    maxRunDuration?: outputs.ComputeInstanceSchedulingMaxRunDuration;
    minNodeCpus?: number;
    /**
     * Specifies node affinities or anti-affinities to determine which sole-tenant nodes your instances and managed instance groups will use as host systems.
     */
    nodeAffinities?: outputs.ComputeInstanceSchedulingNodeAffinity[];
    /**
     * Describes maintenance behavior for the instance. One of MIGRATE or TERMINATE,
     */
    onHostMaintenance: string;
    /**
     * Defines the behaviour for instances with the instance_termination_action.
     */
    onInstanceStopAction?: outputs.ComputeInstanceSchedulingOnInstanceStopAction;
    /**
     * Whether the instance is preemptible.
     */
    preemptible?: boolean;
    /**
     * Whether the instance is spot. If this is set as SPOT.
     */
    provisioningModel: string;
}

export interface ComputeInstanceSchedulingLocalSsdRecoveryTimeout {
    /**
     * Span of time that's a fraction of a second at nanosecond
     * resolution. Durations less than one second are represented
     * with a 0 seconds field and a positive nanos field. Must
     * be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second.
     * Must be from 0 to 315,576,000,000 inclusive.
     */
    seconds: number;
}

export interface ComputeInstanceSchedulingMaxRunDuration {
    /**
     * Span of time that's a fraction of a second at nanosecond
     * resolution. Durations less than one second are represented
     * with a 0 seconds field and a positive nanos field. Must
     * be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second.
     * Must be from 0 to 315,576,000,000 inclusive.
     */
    seconds: number;
}

export interface ComputeInstanceSchedulingNodeAffinity {
    key: string;
    operator: string;
    values: string[];
}

export interface ComputeInstanceSchedulingOnInstanceStopAction {
    /**
     * If true, the contents of any attached Local SSD disks will be discarded.
     */
    discardLocalSsd?: boolean;
}

export interface ComputeInstanceScratchDisk {
    /**
     * Name with which the attached disk is accessible under /dev/disk/by-id/
     */
    deviceName: string;
    /**
     * The disk interface used for attaching this disk. One of SCSI or NVME.
     */
    interface: string;
    /**
     * The size of the disk in gigabytes. One of 375 or 3000.
     */
    size?: number;
}

export interface ComputeInstanceServiceAccount {
    /**
     * The service account e-mail address.
     */
    email: string;
    /**
     * A list of service scopes.
     */
    scopes: string[];
}

export interface ComputeInstanceSettingsMetadata {
    /**
     * A metadata key/value items map. The total size of all keys and values must be less than 512KB
     */
    items?: {[key: string]: string};
}

export interface ComputeInstanceSettingsTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeInstanceShieldedInstanceConfig {
    /**
     * Whether integrity monitoring is enabled for the instance.
     */
    enableIntegrityMonitoring?: boolean;
    /**
     * Whether secure boot is enabled for the instance.
     */
    enableSecureBoot?: boolean;
    /**
     * Whether the instance uses vTPM.
     */
    enableVtpm?: boolean;
}

export interface ComputeInstanceTemplateAdvancedMachineFeatures {
    /**
     * Whether to enable nested virtualization or not.
     */
    enableNestedVirtualization?: boolean;
    /**
     * The number of threads per physical core. To disable simultaneous multithreading (SMT) set this to 1. If unset, the maximum number of threads supported per core by the underlying processor is assumed.
     */
    threadsPerCore?: number;
    /**
     * The number of physical cores to expose to an instance. Multiply by the number of threads per core to compute the total number of virtual CPUs to expose to the instance. If unset, the number of cores is inferred from the instance\'s nominal CPU count and the underlying platform\'s SMT width.
     */
    visibleCoreCount?: number;
}

export interface ComputeInstanceTemplateConfidentialInstanceConfig {
    /**
     * The confidential computing technology the instance uses.
     * 								SEV is an AMD feature. TDX is an Intel feature. One of the following
     * 								values is required: SEV, SEV_SNP, TDX. If SEV_SNP, min_cpu_platform =
     * 								"AMD Milan" is currently required. TDX is only available in beta.
     */
    confidentialInstanceType?: string;
    /**
     * Defines whether the instance should have confidential compute enabled. Field will be deprecated in a future release.
     */
    enableConfidentialCompute?: boolean;
}

export interface ComputeInstanceTemplateDisk {
    /**
     * Whether or not the disk should be auto-deleted. This defaults to true.
     */
    autoDelete?: boolean;
    /**
     * Indicates that this is a boot disk.
     */
    boot: boolean;
    /**
     * A unique device name that is reflected into the /dev/ tree of a Linux operating system running within the instance. If not specified, the server chooses a default device name to apply to this disk.
     */
    deviceName: string;
    /**
     * Encrypts or decrypts a disk using a customer-supplied encryption key.
     */
    diskEncryptionKey?: outputs.ComputeInstanceTemplateDiskDiskEncryptionKey;
    /**
     * Name of the disk. When not provided, this defaults to the name of the instance.
     */
    diskName?: string;
    /**
     * The size of the image in gigabytes. If not specified, it will inherit the size of its base image. For SCRATCH disks, the size must be one of 375 or 3000 GB, with a default of 375 GB.
     */
    diskSizeGb: number;
    /**
     * The Google Compute Engine disk type. Such as "pd-ssd", "local-ssd", "pd-balanced" or "pd-standard".
     */
    diskType: string;
    /**
     * Specifies the disk interface to use for attaching this disk.
     */
    interface: string;
    /**
     * A set of key/value label pairs to assign to disks,
     */
    labels?: {[key: string]: string};
    /**
     * The mode in which to attach this disk, either READ_WRITE or READ_ONLY. If you are attaching or creating a boot disk, this must read-write mode.
     */
    mode: string;
    /**
     * Indicates how many IOPS to provision for the disk. This sets the number of I/O operations per second that the disk can handle. Values must be between 10,000 and 120,000. For more details, see the [Extreme persistent disk documentation](https://cloud.google.com/compute/docs/disks/extreme-persistent-disk).
     */
    provisionedIops: number;
    /**
     * A map of resource manager tags. Resource manager tag keys and values have the same definition as resource manager tags. Keys must be in the format tagKeys/{tag_key_id}, and values are in the format tagValues/456. The field is ignored (both PUT & PATCH) when empty.
     */
    resourceManagerTags?: {[key: string]: string};
    /**
     * A list (short name or id) of resource policies to attach to this disk. Currently a max of 1 resource policy is supported.
     */
    resourcePolicies?: string[];
    /**
     * The name (not self_link) of the disk (such as those managed by google_compute_disk) to attach. > Note: Either source or source_image is required when creating a new instance except for when creating a local SSD.
     */
    source?: string;
    /**
     * The image from which to initialize this disk. This can be one of: the image's self_link, projects/{project}/global/images/{image}, projects/{project}/global/images/family/{family}, global/images/{image}, global/images/family/{family}, family/{family}, {project}/{family}, {project}/{image}, {family}, or {image}. > Note: Either source or source_image is required when creating a new instance except for when creating a local SSD.
     */
    sourceImage: string;
    /**
     * The customer-supplied encryption key of the source
     * image. Required if the source image is protected by a
     * customer-supplied encryption key.
     *
     * Instance templates do not store customer-supplied
     * encryption keys, so you cannot create disks for
     * instances in a managed instance group if the source
     * images are encrypted with your own keys.
     */
    sourceImageEncryptionKey?: outputs.ComputeInstanceTemplateDiskSourceImageEncryptionKey;
    /**
     * The source snapshot to create this disk. When creating
     * a new instance, one of initializeParams.sourceSnapshot,
     * initializeParams.sourceImage, or disks.source is
     * required except for local SSD.
     */
    sourceSnapshot?: string;
    /**
     * The customer-supplied encryption key of the source snapshot.
     */
    sourceSnapshotEncryptionKey?: outputs.ComputeInstanceTemplateDiskSourceSnapshotEncryptionKey;
    /**
     * The type of Google Compute Engine disk, can be either "SCRATCH" or "PERSISTENT".
     */
    type: string;
}

export interface ComputeInstanceTemplateDiskDiskEncryptionKey {
    /**
     * The self link of the encryption key that is stored in Google Cloud KMS.
     */
    kmsKeySelfLink: string;
}

export interface ComputeInstanceTemplateDiskSourceImageEncryptionKey {
    /**
     * The self link of the encryption key that is stored in
     * Google Cloud KMS.
     */
    kmsKeySelfLink: string;
    /**
     * The service account being used for the encryption
     * request for the given KMS key. If absent, the Compute
     * Engine default service account is used.
     */
    kmsKeyServiceAccount?: string;
}

export interface ComputeInstanceTemplateDiskSourceSnapshotEncryptionKey {
    /**
     * The self link of the encryption key that is stored in
     * Google Cloud KMS.
     */
    kmsKeySelfLink: string;
    /**
     * The service account being used for the encryption
     * request for the given KMS key. If absent, the Compute
     * Engine default service account is used.
     */
    kmsKeyServiceAccount?: string;
}

export interface ComputeInstanceTemplateGuestAccelerator {
    /**
     * The number of the guest accelerator cards exposed to this instance.
     */
    count: number;
    /**
     * The accelerator type resource to expose to this instance. E.g. nvidia-tesla-k80.
     */
    type: string;
}

export interface ComputeInstanceTemplateNetworkInterface {
    accessConfigs?: outputs.ComputeInstanceTemplateNetworkInterfaceAccessConfig[];
    /**
     * An array of alias IP ranges for this network interface. Can only be specified for network interfaces on subnet-mode networks.
     */
    aliasIpRanges?: outputs.ComputeInstanceTemplateNetworkInterfaceAliasIpRange[];
    /**
     * The prefix length of the primary internal IPv6 range.
     */
    internalIpv6PrefixLength: number;
    /**
     * An array of IPv6 access configurations for this interface. Currently, only one IPv6 access config, DIRECT_IPV6, is supported. If there is no ipv6AccessConfig specified, then this instance will have no external IPv6 Internet access.
     */
    ipv6AccessConfigs?: outputs.ComputeInstanceTemplateNetworkInterfaceIpv6AccessConfig[];
    /**
     * One of EXTERNAL, INTERNAL to indicate whether the IP can be accessed from the Internet. This field is always inherited from its subnetwork.
     */
    ipv6AccessType: string;
    /**
     * An IPv6 internal network address for this network interface. If not specified, Google Cloud will automatically assign an internal IPv6 address from the instance's subnetwork.
     */
    ipv6Address: string;
    /**
     * The name of the network_interface.
     */
    name: string;
    /**
     * The name or self_link of the network to attach this interface to. Use network attribute for Legacy or Auto subnetted networks and subnetwork for custom subnetted networks.
     */
    network: string;
    /**
     * The private IP address to assign to the instance. If empty, the address will be automatically assigned.
     */
    networkIp?: string;
    /**
     * The type of vNIC to be used on this interface. Possible values:GVNIC, VIRTIO_NET
     */
    nicType?: string;
    /**
     * The networking queue count that's specified by users for the network interface. Both Rx and Tx queues will be set to this number. It will be empty if not specified.
     */
    queueCount?: number;
    /**
     * The stack type for this network interface to identify whether the IPv6 feature is enabled or not. If not specified, IPV4_ONLY will be used.
     */
    stackType: string;
    /**
     * The name of the subnetwork to attach this interface to. The subnetwork must exist in the same region this instance will be created in. Either network or subnetwork must be provided.
     */
    subnetwork: string;
    /**
     * The ID of the project in which the subnetwork belongs. If it is not provided, the provider project is used.
     */
    subnetworkProject: string;
}

export interface ComputeInstanceTemplateNetworkInterfaceAccessConfig {
    /**
     * The IP address that will be 1:1 mapped to the instance's network ip. If not given, one will be generated.
     */
    natIp: string;
    /**
     * The networking tier used for configuring this instance template. This field can take the following values: PREMIUM, STANDARD, FIXED_STANDARD. If this field is not specified, it is assumed to be PREMIUM.
     */
    networkTier: string;
    /**
     * The DNS domain name for the public PTR record.The DNS domain name for the public PTR record.
     */
    publicPtrDomainName: string;
}

export interface ComputeInstanceTemplateNetworkInterfaceAliasIpRange {
    /**
     * The IP CIDR range represented by this alias IP range. This IP CIDR range must belong to the specified subnetwork and cannot contain IP addresses reserved by system or used by other network interfaces. At the time of writing only a netmask (e.g. /24) may be supplied, with a CIDR format resulting in an API error.
     */
    ipCidrRange: string;
    /**
     * The subnetwork secondary range name specifying the secondary range from which to allocate the IP CIDR range for this alias IP range. If left unspecified, the primary range of the subnetwork will be used.
     */
    subnetworkRangeName?: string;
}

export interface ComputeInstanceTemplateNetworkInterfaceIpv6AccessConfig {
    /**
     * The first IPv6 address of the external IPv6 range associated with this instance, prefix length is stored in externalIpv6PrefixLength in ipv6AccessConfig. The field is output only, an IPv6 address from a subnetwork associated with the instance will be allocated dynamically.
     */
    externalIpv6: string;
    /**
     * The prefix length of the external IPv6 range.
     */
    externalIpv6PrefixLength: string;
    /**
     * The name of this access configuration.
     */
    name: string;
    /**
     * The service-level to be provided for IPv6 traffic when the subnet has an external subnet. Only PREMIUM tier is valid for IPv6
     */
    networkTier: string;
    /**
     * The domain name to be used when creating DNSv6 records for the external IPv6 ranges.
     */
    publicPtrDomainName: string;
}

export interface ComputeInstanceTemplateNetworkPerformanceConfig {
    /**
     * The egress bandwidth tier to enable. Possible values:TIER_1, DEFAULT
     */
    totalEgressBandwidthTier: string;
}

export interface ComputeInstanceTemplateReservationAffinity {
    /**
     * Specifies the label selector for the reservation to use.
     */
    specificReservation?: outputs.ComputeInstanceTemplateReservationAffinitySpecificReservation;
    /**
     * The type of reservation from which this instance can consume resources.
     */
    type: string;
}

export interface ComputeInstanceTemplateReservationAffinitySpecificReservation {
    /**
     * Corresponds to the label key of a reservation resource. To target a SPECIFIC_RESERVATION by name, specify compute.googleapis.com/reservation-name as the key and specify the name of your reservation as the only value.
     */
    key: string;
    /**
     * Corresponds to the label values of a reservation resource.
     */
    values: string[];
}

export interface ComputeInstanceTemplateScheduling {
    /**
     * Specifies whether the instance should be automatically restarted if it is terminated by Compute Engine (not terminated by a user). This defaults to true.
     */
    automaticRestart?: boolean;
    /**
     * Specifies the action GCE should take when SPOT VM is preempted.
     */
    instanceTerminationAction?: string;
    /**
     * Specifies the maximum amount of time a Local Ssd Vm should wait while
     *   recovery of the Local Ssd state is attempted. Its value should be in
     *   between 0 and 168 hours with hour granularity and the default value being 1
     *   hour.
     */
    localSsdRecoveryTimeouts?: outputs.ComputeInstanceTemplateSchedulingLocalSsdRecoveryTimeout[];
    /**
     * The timeout for new network connections to hosts.
     */
    maxRunDuration?: outputs.ComputeInstanceTemplateSchedulingMaxRunDuration;
    /**
     * Minimum number of cpus for the instance.
     */
    minNodeCpus?: number;
    /**
     * Specifies node affinities or anti-affinities to determine which sole-tenant nodes your instances and managed instance groups will use as host systems.
     */
    nodeAffinities?: outputs.ComputeInstanceTemplateSchedulingNodeAffinity[];
    /**
     * Defines the maintenance behavior for this instance.
     */
    onHostMaintenance: string;
    /**
     * Defines the behaviour for instances with the instance_termination_action.
     */
    onInstanceStopAction?: outputs.ComputeInstanceTemplateSchedulingOnInstanceStopAction;
    /**
     * Allows instance to be preempted. This defaults to false.
     */
    preemptible?: boolean;
    /**
     * Whether the instance is spot. If this is set as SPOT.
     */
    provisioningModel: string;
}

export interface ComputeInstanceTemplateSchedulingLocalSsdRecoveryTimeout {
    /**
     * Span of time that's a fraction of a second at nanosecond
     * resolution. Durations less than one second are represented
     * with a 0 seconds field and a positive nanos field. Must
     * be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second.
     * Must be from 0 to 315,576,000,000 inclusive.
     */
    seconds: number;
}

export interface ComputeInstanceTemplateSchedulingMaxRunDuration {
    /**
     * Span of time that's a fraction of a second at nanosecond
     * resolution. Durations less than one second are represented
     * with a 0 seconds field and a positive nanos field. Must
     * be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second.
     * Must be from 0 to 315,576,000,000 inclusive.
     */
    seconds: number;
}

export interface ComputeInstanceTemplateSchedulingNodeAffinity {
    key: string;
    operator: string;
    values: string[];
}

export interface ComputeInstanceTemplateSchedulingOnInstanceStopAction {
    /**
     * If true, the contents of any attached Local SSD disks will be discarded.
     */
    discardLocalSsd?: boolean;
}

export interface ComputeInstanceTemplateServiceAccount {
    /**
     * The service account e-mail address. If not given, the default Google Compute Engine service account is used.
     */
    email: string;
    /**
     * A list of service scopes. Both OAuth2 URLs and gcloud short names are supported. To allow full access to all Cloud APIs, use the cloud-platform scope.
     */
    scopes: string[];
}

export interface ComputeInstanceTemplateShieldedInstanceConfig {
    /**
     * Compare the most recent boot measurements to the integrity policy baseline and return a pair of pass/fail results depending on whether they match or not. Defaults to true.
     */
    enableIntegrityMonitoring?: boolean;
    /**
     * Verify the digital signature of all boot components, and halt the boot process if signature verification fails. Defaults to false.
     */
    enableSecureBoot?: boolean;
    /**
     * Use a virtualized trusted platform module, which is a specialized computer chip you can use to encrypt objects like keys and certificates. Defaults to true.
     */
    enableVtpm?: boolean;
}

export interface ComputeInstanceTemplateTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeInstanceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeInterconnectAttachmentPrivateInterconnectInfo {
    tag8021q: number;
}

export interface ComputeInterconnectAttachmentTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeInterconnectCircuitInfo {
    customerDemarcId: string;
    googleCircuitId: string;
    googleDemarcId: string;
}

export interface ComputeInterconnectExpectedOutage {
    affectedCircuits: string[];
    description: string;
    endTime: string;
    issueType: string;
    name: string;
    source: string;
    startTime: string;
    state: string;
}

export interface ComputeInterconnectMacsec {
    /**
     * A keychain placeholder describing a set of named key objects along with their
     * start times. A MACsec CKN/CAK is generated for each key in the key chain.
     * Google router automatically picks the key with the most recent startTime when establishing
     * or re-establishing a MACsec secure link.
     */
    preSharedKeys: outputs.ComputeInterconnectMacsecPreSharedKey[];
}

export interface ComputeInterconnectMacsecPreSharedKey {
    /**
     * If set to true, the Interconnect connection is configured with a should-secure
     * MACsec security policy, that allows the Google router to fallback to cleartext
     * traffic if the MKA session cannot be established. By default, the Interconnect
     * connection is configured with a must-secure security policy that drops all traffic
     * if the MKA session cannot be established with your router.
     */
    failOpen?: boolean;
    /**
     * A name for this pre-shared key. The name must be 1-63 characters long, and
     *  comply with RFC1035. Specifically, the name must be 1-63 characters long and match
     *  the regular expression 'a-z?' which means the first character
     *  must be a lowercase letter, and all following characters must be a dash, lowercase
     *  letter, or digit, except the last character, which cannot be a dash.
     */
    name: string;
    /**
     * A RFC3339 timestamp on or after which the key is valid. startTime can be in the
     * future. If the keychain has a single key, startTime can be omitted. If the keychain
     * has multiple keys, startTime is mandatory for each key. The start times of keys must
     * be in increasing order. The start times of two consecutive keys must be at least 6
     * hours apart.
     */
    startTime?: string;
}

export interface ComputeInterconnectTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeManagedSslCertificateManaged {
    /**
     * Domains for which a managed SSL certificate will be valid.  Currently,
     * there can be up to 100 domains in this list.
     */
    domains: string[];
}

export interface ComputeManagedSslCertificateTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeNetworkAttachmentConnectionEndpoint {
    ipAddress: string;
    projectIdOrNum: string;
    secondaryIpCidrRanges: string;
    status: string;
    subnetwork: string;
}

export interface ComputeNetworkAttachmentTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeNetworkEndpointGroupTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeNetworkEndpointTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeNetworkEndpointsNetworkEndpoint {
    /**
     * The name for a specific VM instance that the IP address belongs to.
     * This is required for network endpoints of type GCE_VM_IP_PORT.
     * The instance must be in the same zone as the network endpoint group.
     */
    instance?: string;
    /**
     * IPv4 address of network endpoint. The IP address must belong
     * to a VM in GCE (either the primary IP or as part of an aliased IP
     * range).
     */
    ipAddress: string;
    /**
     * Port number of network endpoint.
     * **Note** 'port' is required unless the Network Endpoint Group is created
     * with the type of 'GCE_VM_IP'
     */
    port?: number;
}

export interface ComputeNetworkEndpointsTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeNetworkFirewallPolicyAssociationTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeNetworkFirewallPolicyRuleMatch {
    /**
     * Address groups which should be matched against the traffic destination. Maximum number of destination address groups is 10. Destination address groups is only supported in Egress rules.
     */
    destAddressGroups?: string[];
    /**
     * Domain names that will be used to match against the resolved domain name of destination of traffic. Can only be specified if DIRECTION is egress.
     */
    destFqdns?: string[];
    /**
     * CIDR IP address range. Maximum number of destination CIDR IP ranges allowed is 5000.
     */
    destIpRanges?: string[];
    /**
     * The Unicode country codes whose IP addresses will be used to match against the source of traffic. Can only be specified if DIRECTION is egress.
     */
    destRegionCodes?: string[];
    /**
     * Name of the Google Cloud Threat Intelligence list.
     */
    destThreatIntelligences?: string[];
    /**
     * Pairs of IP protocols and ports that the rule should match.
     */
    layer4Configs: outputs.ComputeNetworkFirewallPolicyRuleMatchLayer4Config[];
    /**
     * Address groups which should be matched against the traffic source. Maximum number of source address groups is 10. Source address groups is only supported in Ingress rules.
     */
    srcAddressGroups?: string[];
    /**
     * Domain names that will be used to match against the resolved domain name of source of traffic. Can only be specified if DIRECTION is ingress.
     */
    srcFqdns?: string[];
    /**
     * CIDR IP address range. Maximum number of source CIDR IP ranges allowed is 5000.
     */
    srcIpRanges?: string[];
    /**
     * The Unicode country codes whose IP addresses will be used to match against the source of traffic. Can only be specified if DIRECTION is ingress.
     */
    srcRegionCodes?: string[];
    /**
     * List of secure tag values, which should be matched at the source of the traffic. For INGRESS rule, if all the <code>srcSecureTag</code> are INEFFECTIVE, and there is no <code>srcIpRange</code>, this rule will be ignored. Maximum number of source tag values allowed is 256.
     */
    srcSecureTags?: outputs.ComputeNetworkFirewallPolicyRuleMatchSrcSecureTag[];
    /**
     * Name of the Google Cloud Threat Intelligence list.
     */
    srcThreatIntelligences?: string[];
}

export interface ComputeNetworkFirewallPolicyRuleMatchLayer4Config {
    /**
     * The IP protocol to which this rule applies. The protocol type is required when creating a firewall rule. This value can either be one of the following well known protocol strings (`tcp`, `udp`, `icmp`, `esp`, `ah`, `ipip`, `sctp`), or the IP protocol number.
     */
    ipProtocol: string;
    /**
     * An optional list of ports to which this rule applies. This field is only applicable for UDP or TCP protocol. Each entry must be either an integer or a range. If not specified, this rule applies to connections through any port. Example inputs include: ``.
     */
    ports?: string[];
}

export interface ComputeNetworkFirewallPolicyRuleMatchSrcSecureTag {
    /**
     * Name of the secure tag, created with TagManager's TagValue API. @pattern tagValues/[0-9]+
     */
    name: string;
    /**
     * [Output Only] State of the secure tag, either `EFFECTIVE` or `INEFFECTIVE`. A secure tag is `INEFFECTIVE` when it is deleted or its network is deleted.
     */
    state: string;
}

export interface ComputeNetworkFirewallPolicyRuleTargetSecureTag {
    /**
     * Name of the secure tag, created with TagManager's TagValue API. @pattern tagValues/[0-9]+
     */
    name: string;
    /**
     * [Output Only] State of the secure tag, either `EFFECTIVE` or `INEFFECTIVE`. A secure tag is `INEFFECTIVE` when it is deleted or its network is deleted.
     */
    state: string;
}

export interface ComputeNetworkFirewallPolicyRuleTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeNetworkFirewallPolicyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeNetworkPeeringRoutesConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeNetworkPeeringTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeNetworkTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeNodeGroupAutoscalingPolicy {
    /**
     * Maximum size of the node group. Set to a value less than or equal
     * to 100 and greater than or equal to min-nodes.
     */
    maxNodes: number;
    /**
     * Minimum size of the node group. Must be less
     * than or equal to max-nodes. The default value is 0.
     */
    minNodes: number;
    /**
     * The autoscaling mode. Set to one of the following:
     *   - OFF: Disables the autoscaler.
     *   - ON: Enables scaling in and scaling out.
     *   - ONLY_SCALE_OUT: Enables only scaling out.
     *   You must use this mode if your node groups are configured to
     *   restart their hosted VMs on minimal servers. Possible values: ["OFF", "ON", "ONLY_SCALE_OUT"]
     */
    mode: string;
}

export interface ComputeNodeGroupMaintenanceWindow {
    /**
     * instances.start time of the window. This must be in UTC format that resolves to one of 00:00, 04:00, 08:00, 12:00, 16:00, or 20:00. For example, both 13:00-5 and 08:00 are valid.
     */
    startTime: string;
}

export interface ComputeNodeGroupShareSettings {
    /**
     * A map of project id and project config. This is only valid when shareType's value is SPECIFIC_PROJECTS.
     */
    projectMaps?: outputs.ComputeNodeGroupShareSettingsProjectMap[];
    /**
     * Node group sharing type. Possible values: ["ORGANIZATION", "SPECIFIC_PROJECTS", "LOCAL"]
     */
    shareType: string;
}

export interface ComputeNodeGroupShareSettingsProjectMap {
    id: string;
    /**
     * The project id/number should be the same as the key of this project config in the project map.
     */
    projectId: string;
}

export interface ComputeNodeGroupTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeNodeTemplateAccelerator {
    /**
     * The number of the guest accelerator cards exposed to this
     * node template.
     */
    acceleratorCount?: number;
    /**
     * Full or partial URL of the accelerator type resource to expose
     * to this node template.
     */
    acceleratorType?: string;
}

export interface ComputeNodeTemplateNodeTypeFlexibility {
    /**
     * Number of virtual CPUs to use.
     */
    cpus?: string;
    /**
     * Use local SSD
     */
    localSsd: string;
    /**
     * Physical memory available to the node, defined in MB.
     */
    memory?: string;
}

export interface ComputeNodeTemplateServerBinding {
    /**
     * Type of server binding policy. If 'RESTART_NODE_ON_ANY_SERVER',
     * nodes using this template will restart on any physical server
     * following a maintenance event.
     *
     * If 'RESTART_NODE_ON_MINIMAL_SERVER', nodes using this template
     * will restart on the same physical server following a maintenance
     * event, instead of being live migrated to or restarted on a new
     * physical server. This option may be useful if you are using
     * software licenses tied to the underlying server characteristics
     * such as physical sockets or cores, to avoid the need for
     * additional licenses when maintenance occurs. However, VMs on such
     * nodes will experience outages while maintenance is applied. Possible values: ["RESTART_NODE_ON_ANY_SERVER", "RESTART_NODE_ON_MINIMAL_SERVERS"]
     */
    type: string;
}

export interface ComputeNodeTemplateTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputePacketMirroringCollectorIlb {
    /**
     * The URL of the forwarding rule.
     */
    url: string;
}

export interface ComputePacketMirroringFilter {
    /**
     * IP CIDR ranges that apply as a filter on the source (ingress) or
     * destination (egress) IP in the IP header. Only IPv4 is supported.
     */
    cidrRanges?: string[];
    /**
     * Direction of traffic to mirror. Default value: "BOTH" Possible values: ["INGRESS", "EGRESS", "BOTH"]
     */
    direction?: string;
    /**
     * Possible IP protocols including tcp, udp, icmp and esp
     */
    ipProtocols?: string[];
}

export interface ComputePacketMirroringMirroredResources {
    /**
     * All the listed instances will be mirrored.  Specify at most 50.
     */
    instances?: outputs.ComputePacketMirroringMirroredResourcesInstance[];
    /**
     * All instances in one of these subnetworks will be mirrored.
     */
    subnetworks?: outputs.ComputePacketMirroringMirroredResourcesSubnetwork[];
    /**
     * All instances with these tags will be mirrored.
     */
    tags?: string[];
}

export interface ComputePacketMirroringMirroredResourcesInstance {
    /**
     * The URL of the instances where this rule should be active.
     */
    url: string;
}

export interface ComputePacketMirroringMirroredResourcesSubnetwork {
    /**
     * The URL of the subnetwork where this rule should be active.
     */
    url: string;
}

export interface ComputePacketMirroringNetwork {
    /**
     * The full self_link URL of the network where this rule is active.
     */
    url: string;
}

export interface ComputePacketMirroringTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputePerInstanceConfigPreservedState {
    /**
     * Stateful disks for the instance.
     */
    disks?: outputs.ComputePerInstanceConfigPreservedStateDisk[];
    /**
     * Preserved external IPs defined for this instance. This map is keyed with the name of the network interface.
     */
    externalIps?: outputs.ComputePerInstanceConfigPreservedStateExternalIp[];
    /**
     * Preserved internal IPs defined for this instance. This map is keyed with the name of the network interface.
     */
    internalIps?: outputs.ComputePerInstanceConfigPreservedStateInternalIp[];
    /**
     * Preserved metadata defined for this instance. This is a list of key->value pairs.
     */
    metadata?: {[key: string]: string};
}

export interface ComputePerInstanceConfigPreservedStateDisk {
    /**
     * A value that prescribes what should happen to the stateful disk when the VM instance is deleted.
     * The available options are 'NEVER' and 'ON_PERMANENT_INSTANCE_DELETION'.
     * 'NEVER' - detach the disk when the VM is deleted, but do not delete the disk.
     * 'ON_PERMANENT_INSTANCE_DELETION' will delete the stateful disk when the VM is permanently
     * deleted from the instance group. Default value: "NEVER" Possible values: ["NEVER", "ON_PERMANENT_INSTANCE_DELETION"]
     */
    deleteRule?: string;
    /**
     * A unique device name that is reflected into the /dev/ tree of a Linux operating system running within the instance.
     */
    deviceName: string;
    /**
     * The mode of the disk. Default value: "READ_WRITE" Possible values: ["READ_ONLY", "READ_WRITE"]
     */
    mode?: string;
    /**
     * The URI of an existing persistent disk to attach under the specified device-name in the format
     * 'projects/project-id/zones/zone/disks/disk-name'.
     */
    source: string;
}

export interface ComputePerInstanceConfigPreservedStateExternalIp {
    /**
     * These stateful IPs will never be released during autohealing, update or VM instance recreate operations. This flag is used to configure if the IP reservation should be deleted after it is no longer used by the group, e.g. when the given instance or the whole group is deleted. Default value: "NEVER" Possible values: ["NEVER", "ON_PERMANENT_INSTANCE_DELETION"]
     */
    autoDelete?: string;
    interfaceName: string;
    /**
     * Ip address representation
     */
    ipAddress?: outputs.ComputePerInstanceConfigPreservedStateExternalIpIpAddress;
}

export interface ComputePerInstanceConfigPreservedStateExternalIpIpAddress {
    /**
     * The URL of the reservation for this IP address.
     */
    address?: string;
}

export interface ComputePerInstanceConfigPreservedStateInternalIp {
    /**
     * These stateful IPs will never be released during autohealing, update or VM instance recreate operations. This flag is used to configure if the IP reservation should be deleted after it is no longer used by the group, e.g. when the given instance or the whole group is deleted. Default value: "NEVER" Possible values: ["NEVER", "ON_PERMANENT_INSTANCE_DELETION"]
     */
    autoDelete?: string;
    interfaceName: string;
    /**
     * Ip address representation
     */
    ipAddress?: outputs.ComputePerInstanceConfigPreservedStateInternalIpIpAddress;
}

export interface ComputePerInstanceConfigPreservedStateInternalIpIpAddress {
    /**
     * The URL of the reservation for this IP address.
     */
    address?: string;
}

export interface ComputePerInstanceConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeProjectCloudArmorTierTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeProjectDefaultNetworkTierTimeouts {
    create?: string;
}

export interface ComputeProjectMetadataItemTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeProjectMetadataTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputePublicAdvertisedPrefixTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputePublicDelegatedPrefixTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeRegionAutoscalerAutoscalingPolicy {
    /**
     * The number of seconds that the autoscaler should wait before it
     * starts collecting information from a new instance. This prevents
     * the autoscaler from collecting information when the instance is
     * initializing, during which the collected usage would not be
     * reliable. The default time autoscaler waits is 60 seconds.
     *
     * Virtual machine initialization times might vary because of
     * numerous factors. We recommend that you test how long an
     * instance may take to initialize. To do this, create an instance
     * and time the startup process.
     */
    cooldownPeriod?: number;
    /**
     * Defines the CPU utilization policy that allows the autoscaler to
     * scale based on the average CPU utilization of a managed instance
     * group.
     */
    cpuUtilization?: outputs.ComputeRegionAutoscalerAutoscalingPolicyCpuUtilization;
    /**
     * Configuration parameters of autoscaling based on a load balancer.
     */
    loadBalancingUtilization?: outputs.ComputeRegionAutoscalerAutoscalingPolicyLoadBalancingUtilization;
    /**
     * The maximum number of instances that the autoscaler can scale up
     * to. This is required when creating or updating an autoscaler. The
     * maximum number of replicas should not be lower than minimal number
     * of replicas.
     */
    maxReplicas: number;
    /**
     * Configuration parameters of autoscaling based on a custom metric.
     */
    metrics?: outputs.ComputeRegionAutoscalerAutoscalingPolicyMetric[];
    /**
     * The minimum number of replicas that the autoscaler can scale down
     * to. This cannot be less than 0. If not provided, autoscaler will
     * choose a default value depending on maximum number of instances
     * allowed.
     */
    minReplicas: number;
    /**
     * Defines operating mode for this policy.
     */
    mode?: string;
    /**
     * Defines scale in controls to reduce the risk of response latency
     * and outages due to abrupt scale-in events
     */
    scaleInControl?: outputs.ComputeRegionAutoscalerAutoscalingPolicyScaleInControl;
    /**
     * Scaling schedules defined for an autoscaler. Multiple schedules can be set on an autoscaler and they can overlap.
     */
    scalingSchedules?: outputs.ComputeRegionAutoscalerAutoscalingPolicyScalingSchedule[];
}

export interface ComputeRegionAutoscalerAutoscalingPolicyCpuUtilization {
    /**
     * Indicates whether predictive autoscaling based on CPU metric is enabled. Valid values are:
     *
     * - NONE (default). No predictive method is used. The autoscaler scales the group to meet current demand based on real-time metrics.
     *
     * - OPTIMIZE_AVAILABILITY. Predictive autoscaling improves availability by monitoring daily and weekly load patterns and scaling out ahead of anticipated demand.
     */
    predictiveMethod?: string;
    /**
     * The target CPU utilization that the autoscaler should maintain.
     * Must be a float value in the range (0, 1]. If not specified, the
     * default is 0.6.
     *
     * If the CPU level is below the target utilization, the autoscaler
     * scales down the number of instances until it reaches the minimum
     * number of instances you specified or until the average CPU of
     * your instances reaches the target utilization.
     *
     * If the average CPU is above the target utilization, the autoscaler
     * scales up until it reaches the maximum number of instances you
     * specified or until the average utilization reaches the target
     * utilization.
     */
    target: number;
}

export interface ComputeRegionAutoscalerAutoscalingPolicyLoadBalancingUtilization {
    /**
     * Fraction of backend capacity utilization (set in HTTP(s) load
     * balancing configuration) that autoscaler should maintain. Must
     * be a positive float value. If not defined, the default is 0.8.
     */
    target: number;
}

export interface ComputeRegionAutoscalerAutoscalingPolicyMetric {
    /**
     * A filter string to be used as the filter string for
     * a Stackdriver Monitoring TimeSeries.list API call.
     * This filter is used to select a specific TimeSeries for
     * the purpose of autoscaling and to determine whether the metric
     * is exporting per-instance or per-group data.
     *
     * You can only use the AND operator for joining selectors.
     * You can only use direct equality comparison operator (=) without
     * any functions for each selector.
     * You can specify the metric in both the filter string and in the
     * metric field. However, if specified in both places, the metric must
     * be identical.
     *
     * The monitored resource type determines what kind of values are
     * expected for the metric. If it is a gce_instance, the autoscaler
     * expects the metric to include a separate TimeSeries for each
     * instance in a group. In such a case, you cannot filter on resource
     * labels.
     *
     * If the resource type is any other value, the autoscaler expects
     * this metric to contain values that apply to the entire autoscaled
     * instance group and resource label filtering can be performed to
     * point autoscaler at the correct TimeSeries to scale upon.
     * This is called a per-group metric for the purpose of autoscaling.
     *
     * If not specified, the type defaults to gce_instance.
     *
     * You should provide a filter that is selective enough to pick just
     * one TimeSeries for the autoscaled group or for each of the instances
     * (if you are using gce_instance resource type). If multiple
     * TimeSeries are returned upon the query execution, the autoscaler
     * will sum their respective values to obtain its scaling value.
     */
    filter?: string;
    /**
     * The identifier (type) of the Stackdriver Monitoring metric.
     * The metric cannot have negative values.
     *
     * The metric must have a value type of INT64 or DOUBLE.
     */
    name: string;
    /**
     * If scaling is based on a per-group metric value that represents the
     * total amount of work to be done or resource usage, set this value to
     * an amount assigned for a single instance of the scaled group.
     * The autoscaler will keep the number of instances proportional to the
     * value of this metric, the metric itself should not change value due
     * to group resizing.
     *
     * For example, a good metric to use with the target is
     * 'pubsub.googleapis.com/subscription/num_undelivered_messages'
     * or a custom metric exporting the total number of requests coming to
     * your instances.
     *
     * A bad example would be a metric exporting an average or median
     * latency, since this value can't include a chunk assignable to a
     * single instance, it could be better used with utilization_target
     * instead.
     */
    singleInstanceAssignment?: number;
    /**
     * The target value of the metric that autoscaler should
     * maintain. This must be a positive value. A utilization
     * metric scales number of virtual machines handling requests
     * to increase or decrease proportionally to the metric.
     *
     * For example, a good metric to use as a utilizationTarget is
     * www.googleapis.com/compute/instance/network/received_bytes_count.
     * The autoscaler will work to keep this value constant for each
     * of the instances.
     */
    target?: number;
    /**
     * Defines how target utilization value is expressed for a
     * Stackdriver Monitoring metric. Possible values: ["GAUGE", "DELTA_PER_SECOND", "DELTA_PER_MINUTE"]
     */
    type?: string;
}

export interface ComputeRegionAutoscalerAutoscalingPolicyScaleInControl {
    /**
     * A nested object resource
     */
    maxScaledInReplicas?: outputs.ComputeRegionAutoscalerAutoscalingPolicyScaleInControlMaxScaledInReplicas;
    /**
     * How long back autoscaling should look when computing recommendations
     * to include directives regarding slower scale down, as described above.
     */
    timeWindowSec?: number;
}

export interface ComputeRegionAutoscalerAutoscalingPolicyScaleInControlMaxScaledInReplicas {
    /**
     * Specifies a fixed number of VM instances. This must be a positive
     * integer.
     */
    fixed?: number;
    /**
     * Specifies a percentage of instances between 0 to 100%, inclusive.
     * For example, specify 80 for 80%.
     */
    percent?: number;
}

export interface ComputeRegionAutoscalerAutoscalingPolicyScalingSchedule {
    /**
     * A description of a scaling schedule.
     */
    description?: string;
    /**
     * A boolean value that specifies if a scaling schedule can influence autoscaler recommendations. If set to true, then a scaling schedule has no effect.
     */
    disabled?: boolean;
    /**
     * The duration of time intervals (in seconds) for which this scaling schedule will be running. The minimum allowed value is 300.
     */
    durationSec: number;
    /**
     * Minimum number of VM instances that autoscaler will recommend in time intervals starting according to schedule.
     */
    minRequiredReplicas: number;
    name: string;
    /**
     * The start timestamps of time intervals when this scaling schedule should provide a scaling signal. This field uses the extended cron format (with an optional year field).
     */
    schedule: string;
    /**
     * The time zone to be used when interpreting the schedule. The value of this field must be a time zone name from the tz database: http://en.wikipedia.org/wiki/Tz_database.
     */
    timeZone?: string;
}

export interface ComputeRegionAutoscalerTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeRegionBackendServiceBackend {
    /**
     * Specifies the balancing mode for this backend.
     *
     * See the [Backend Services Overview](https://cloud.google.com/load-balancing/docs/backend-service#balancing-mode)
     * for an explanation of load balancing modes. Default value: "UTILIZATION" Possible values: ["UTILIZATION", "RATE", "CONNECTION"]
     */
    balancingMode?: string;
    /**
     * A multiplier applied to the group's maximum servicing capacity
     * (based on UTILIZATION, RATE or CONNECTION).
     *
     * ~>**NOTE**: This field cannot be set for
     * INTERNAL region backend services (default loadBalancingScheme),
     * but is required for non-INTERNAL backend service. The total
     * capacity_scaler for all backends must be non-zero.
     *
     * A setting of 0 means the group is completely drained, offering
     * 0% of its available Capacity. Valid range is [0.0,1.0].
     */
    capacityScaler?: number;
    /**
     * An optional description of this resource.
     * Provide this property when you create the resource.
     */
    description?: string;
    /**
     * This field designates whether this is a failover backend. More
     * than one failover backend can be configured for a given RegionBackendService.
     */
    failover: boolean;
    /**
     * The fully-qualified URL of an Instance Group or Network Endpoint
     * Group resource. In case of instance group this defines the list
     * of instances that serve traffic. Member virtual machine
     * instances from each instance group must live in the same zone as
     * the instance group itself. No two backends in a backend service
     * are allowed to use same Instance Group resource.
     *
     * For Network Endpoint Groups this defines list of endpoints. All
     * endpoints of Network Endpoint Group must be hosted on instances
     * located in the same zone as the Network Endpoint Group.
     *
     * Backend services cannot mix Instance Group and
     * Network Endpoint Group backends.
     *
     * When the 'load_balancing_scheme' is INTERNAL, only instance groups
     * are supported.
     *
     * Note that you must specify an Instance Group or Network Endpoint
     * Group resource using the fully-qualified URL, rather than a
     * partial URL.
     */
    group: string;
    /**
     * The max number of simultaneous connections for the group. Can
     * be used with either CONNECTION or UTILIZATION balancing modes.
     * Cannot be set for INTERNAL backend services.
     *
     * For CONNECTION mode, either maxConnections or one
     * of maxConnectionsPerInstance or maxConnectionsPerEndpoint,
     * as appropriate for group type, must be set.
     */
    maxConnections?: number;
    /**
     * The max number of simultaneous connections that a single backend
     * network endpoint can handle. Cannot be set
     * for INTERNAL backend services.
     *
     * This is used to calculate the capacity of the group. Can be
     * used in either CONNECTION or UTILIZATION balancing modes. For
     * CONNECTION mode, either maxConnections or
     * maxConnectionsPerEndpoint must be set.
     */
    maxConnectionsPerEndpoint?: number;
    /**
     * The max number of simultaneous connections that a single
     * backend instance can handle. Cannot be set for INTERNAL backend
     * services.
     *
     * This is used to calculate the capacity of the group.
     * Can be used in either CONNECTION or UTILIZATION balancing modes.
     * For CONNECTION mode, either maxConnections or
     * maxConnectionsPerInstance must be set.
     */
    maxConnectionsPerInstance?: number;
    /**
     * The max requests per second (RPS) of the group. Cannot be set
     * for INTERNAL backend services.
     *
     * Can be used with either RATE or UTILIZATION balancing modes,
     * but required if RATE mode. Either maxRate or one
     * of maxRatePerInstance or maxRatePerEndpoint, as appropriate for
     * group type, must be set.
     */
    maxRate?: number;
    /**
     * The max requests per second (RPS) that a single backend network
     * endpoint can handle. This is used to calculate the capacity of
     * the group. Can be used in either balancing mode. For RATE mode,
     * either maxRate or maxRatePerEndpoint must be set. Cannot be set
     * for INTERNAL backend services.
     */
    maxRatePerEndpoint?: number;
    /**
     * The max requests per second (RPS) that a single backend
     * instance can handle. This is used to calculate the capacity of
     * the group. Can be used in either balancing mode. For RATE mode,
     * either maxRate or maxRatePerInstance must be set. Cannot be set
     * for INTERNAL backend services.
     */
    maxRatePerInstance?: number;
    /**
     * Used when balancingMode is UTILIZATION. This ratio defines the
     * CPU utilization target for the group. Valid range is [0.0, 1.0].
     * Cannot be set for INTERNAL backend services.
     */
    maxUtilization?: number;
}

export interface ComputeRegionBackendServiceCdnPolicy {
    /**
     * The CacheKeyPolicy for this CdnPolicy.
     */
    cacheKeyPolicy?: outputs.ComputeRegionBackendServiceCdnPolicyCacheKeyPolicy;
    /**
     * Specifies the cache setting for all responses from this backend.
     * The possible values are: USE_ORIGIN_HEADERS, FORCE_CACHE_ALL and CACHE_ALL_STATIC Possible values: ["USE_ORIGIN_HEADERS", "FORCE_CACHE_ALL", "CACHE_ALL_STATIC"]
     */
    cacheMode: string;
    /**
     * Specifies the maximum allowed TTL for cached content served by this origin.
     */
    clientTtl: number;
    /**
     * Specifies the default TTL for cached content served by this origin for responses
     * that do not have an existing valid TTL (max-age or s-max-age).
     */
    defaultTtl: number;
    /**
     * Specifies the maximum allowed TTL for cached content served by this origin.
     */
    maxTtl: number;
    /**
     * Negative caching allows per-status code TTLs to be set, in order to apply fine-grained caching for common errors or redirects.
     */
    negativeCaching: boolean;
    /**
     * Sets a cache TTL for the specified HTTP status code. negativeCaching must be enabled to configure negativeCachingPolicy.
     * Omitting the policy and leaving negativeCaching enabled will use Cloud CDN's default cache TTLs.
     */
    negativeCachingPolicies?: outputs.ComputeRegionBackendServiceCdnPolicyNegativeCachingPolicy[];
    /**
     * Serve existing content from the cache (if available) when revalidating content with the origin, or when an error is encountered when refreshing the cache.
     */
    serveWhileStale: number;
    /**
     * Maximum number of seconds the response to a signed URL request
     * will be considered fresh, defaults to 1hr (3600s). After this
     * time period, the response will be revalidated before
     * being served.
     *
     * When serving responses to signed URL requests, Cloud CDN will
     * internally behave as though all responses from this backend had a
     * "Cache-Control: public, max-age=[TTL]" header, regardless of any
     * existing Cache-Control header. The actual headers served in
     * responses will not be altered.
     */
    signedUrlCacheMaxAgeSec?: number;
}

export interface ComputeRegionBackendServiceCdnPolicyCacheKeyPolicy {
    /**
     * If true requests to different hosts will be cached separately.
     */
    includeHost?: boolean;
    /**
     * Names of cookies to include in cache keys.
     */
    includeNamedCookies?: string[];
    /**
     * If true, http and https requests will be cached separately.
     */
    includeProtocol?: boolean;
    /**
     * If true, include query string parameters in the cache key
     * according to query_string_whitelist and
     * query_string_blacklist. If neither is set, the entire query
     * string will be included.
     *
     * If false, the query string will be excluded from the cache
     * key entirely.
     */
    includeQueryString?: boolean;
    /**
     * Names of query string parameters to exclude in cache keys.
     *
     * All other parameters will be included. Either specify
     * query_string_whitelist or query_string_blacklist, not both.
     * '&' and '=' will be percent encoded and not treated as
     * delimiters.
     */
    queryStringBlacklists?: string[];
    /**
     * Names of query string parameters to include in cache keys.
     *
     * All other parameters will be excluded. Either specify
     * query_string_whitelist or query_string_blacklist, not both.
     * '&' and '=' will be percent encoded and not treated as
     * delimiters.
     */
    queryStringWhitelists?: string[];
}

export interface ComputeRegionBackendServiceCdnPolicyNegativeCachingPolicy {
    /**
     * The HTTP status code to define a TTL against. Only HTTP status codes 300, 301, 308, 404, 405, 410, 421, 451 and 501
     * can be specified as values, and you cannot specify a status code more than once.
     */
    code?: number;
}

export interface ComputeRegionBackendServiceCircuitBreakers {
    /**
     * The maximum number of connections to the backend cluster.
     * Defaults to 1024.
     */
    maxConnections?: number;
    /**
     * The maximum number of pending requests to the backend cluster.
     * Defaults to 1024.
     */
    maxPendingRequests?: number;
    /**
     * The maximum number of parallel requests to the backend cluster.
     * Defaults to 1024.
     */
    maxRequests?: number;
    /**
     * Maximum requests for a single backend connection. This parameter
     * is respected by both the HTTP/1.1 and HTTP/2 implementations. If
     * not specified, there is no limit. Setting this parameter to 1
     * will effectively disable keep alive.
     */
    maxRequestsPerConnection?: number;
    /**
     * The maximum number of parallel retries to the backend cluster.
     * Defaults to 3.
     */
    maxRetries?: number;
}

export interface ComputeRegionBackendServiceConsistentHash {
    /**
     * Hash is based on HTTP Cookie. This field describes a HTTP cookie
     * that will be used as the hash key for the consistent hash load
     * balancer. If the cookie is not present, it will be generated.
     * This field is applicable if the sessionAffinity is set to HTTP_COOKIE.
     */
    httpCookie?: outputs.ComputeRegionBackendServiceConsistentHashHttpCookie;
    /**
     * The hash based on the value of the specified header field.
     * This field is applicable if the sessionAffinity is set to HEADER_FIELD.
     */
    httpHeaderName?: string;
    /**
     * The minimum number of virtual nodes to use for the hash ring.
     * Larger ring sizes result in more granular load
     * distributions. If the number of hosts in the load balancing pool
     * is larger than the ring size, each host will be assigned a single
     * virtual node.
     * Defaults to 1024.
     */
    minimumRingSize?: number;
}

export interface ComputeRegionBackendServiceConsistentHashHttpCookie {
    /**
     * Name of the cookie.
     */
    name?: string;
    /**
     * Path to set for the cookie.
     */
    path?: string;
    /**
     * Lifetime of the cookie.
     */
    ttl?: outputs.ComputeRegionBackendServiceConsistentHashHttpCookieTtl;
}

export interface ComputeRegionBackendServiceConsistentHashHttpCookieTtl {
    /**
     * Span of time that's a fraction of a second at nanosecond
     * resolution. Durations less than one second are represented
     * with a 0 seconds field and a positive nanos field. Must
     * be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second.
     * Must be from 0 to 315,576,000,000 inclusive.
     */
    seconds: number;
}

export interface ComputeRegionBackendServiceFailoverPolicy {
    /**
     * On failover or failback, this field indicates whether connection drain
     * will be honored. Setting this to true has the following effect: connections
     * to the old active pool are not drained. Connections to the new active pool
     * use the timeout of 10 min (currently fixed). Setting to false has the
     * following effect: both old and new connections will have a drain timeout
     * of 10 min.
     * This can be set to true only if the protocol is TCP.
     * The default is false.
     */
    disableConnectionDrainOnFailover: boolean;
    /**
     * This option is used only when no healthy VMs are detected in the primary
     * and backup instance groups. When set to true, traffic is dropped. When
     * set to false, new connections are sent across all VMs in the primary group.
     * The default is false.
     */
    dropTrafficIfUnhealthy: boolean;
    /**
     * The value of the field must be in [0, 1]. If the ratio of the healthy
     * VMs in the primary backend is at or below this number, traffic arriving
     * at the load-balanced IP will be directed to the failover backend.
     * In case where 'failoverRatio' is not set or all the VMs in the backup
     * backend are unhealthy, the traffic will be directed back to the primary
     * backend in the "force" mode, where traffic will be spread to the healthy
     * VMs with the best effort, or to all VMs when no VM is healthy.
     * This field is only used with l4 load balancing.
     */
    failoverRatio?: number;
}

export interface ComputeRegionBackendServiceIap {
    /**
     * Whether the serving infrastructure will authenticate and authorize all incoming requests.
     */
    enabled: boolean;
    /**
     * OAuth2 Client ID for IAP
     */
    oauth2ClientId?: string;
    /**
     * OAuth2 Client Secret for IAP
     */
    oauth2ClientSecret?: string;
    /**
     * OAuth2 Client Secret SHA-256 for IAP
     */
    oauth2ClientSecretSha256: string;
}

export interface ComputeRegionBackendServiceLogConfig {
    /**
     * Whether to enable logging for the load balancer traffic served by this backend service.
     */
    enable?: boolean;
    /**
     * This field can only be specified if logging is enabled for this backend service. The value of
     * the field must be in [0, 1]. This configures the sampling rate of requests to the load balancer
     * where 1.0 means all logged requests are reported and 0.0 means no logged requests are reported.
     * The default value is 1.0.
     */
    sampleRate?: number;
}

export interface ComputeRegionBackendServiceOutlierDetection {
    /**
     * The base time that a host is ejected for. The real time is equal to the base
     * time multiplied by the number of times the host has been ejected. Defaults to
     * 30000ms or 30s.
     */
    baseEjectionTime?: outputs.ComputeRegionBackendServiceOutlierDetectionBaseEjectionTime;
    /**
     * Number of errors before a host is ejected from the connection pool. When the
     * backend host is accessed over HTTP, a 5xx return code qualifies as an error.
     * Defaults to 5.
     */
    consecutiveErrors?: number;
    /**
     * The number of consecutive gateway failures (502, 503, 504 status or connection
     * errors that are mapped to one of those status codes) before a consecutive
     * gateway failure ejection occurs. Defaults to 5.
     */
    consecutiveGatewayFailure?: number;
    /**
     * The percentage chance that a host will be actually ejected when an outlier
     * status is detected through consecutive 5xx. This setting can be used to disable
     * ejection or to ramp it up slowly. Defaults to 100.
     */
    enforcingConsecutiveErrors?: number;
    /**
     * The percentage chance that a host will be actually ejected when an outlier
     * status is detected through consecutive gateway failures. This setting can be
     * used to disable ejection or to ramp it up slowly. Defaults to 0.
     */
    enforcingConsecutiveGatewayFailure?: number;
    /**
     * The percentage chance that a host will be actually ejected when an outlier
     * status is detected through success rate statistics. This setting can be used to
     * disable ejection or to ramp it up slowly. Defaults to 100.
     */
    enforcingSuccessRate?: number;
    /**
     * Time interval between ejection sweep analysis. This can result in both new
     * ejections as well as hosts being returned to service. Defaults to 10 seconds.
     */
    interval?: outputs.ComputeRegionBackendServiceOutlierDetectionInterval;
    /**
     * Maximum percentage of hosts in the load balancing pool for the backend service
     * that can be ejected. Defaults to 10%.
     */
    maxEjectionPercent?: number;
    /**
     * The number of hosts in a cluster that must have enough request volume to detect
     * success rate outliers. If the number of hosts is less than this setting, outlier
     * detection via success rate statistics is not performed for any host in the
     * cluster. Defaults to 5.
     */
    successRateMinimumHosts?: number;
    /**
     * The minimum number of total requests that must be collected in one interval (as
     * defined by the interval duration above) to include this host in success rate
     * based outlier detection. If the volume is lower than this setting, outlier
     * detection via success rate statistics is not performed for that host. Defaults
     * to 100.
     */
    successRateRequestVolume?: number;
    /**
     * This factor is used to determine the ejection threshold for success rate outlier
     * ejection. The ejection threshold is the difference between the mean success
     * rate, and the product of this factor and the standard deviation of the mean
     * success rate: mean - (stdev * success_rate_stdev_factor). This factor is divided
     * by a thousand to get a double. That is, if the desired factor is 1.9, the
     * runtime value should be 1900. Defaults to 1900.
     */
    successRateStdevFactor?: number;
}

export interface ComputeRegionBackendServiceOutlierDetectionBaseEjectionTime {
    /**
     * Span of time that's a fraction of a second at nanosecond resolution. Durations
     * less than one second are represented with a 0 'seconds' field and a positive
     * 'nanos' field. Must be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000
     * inclusive.
     */
    seconds: number;
}

export interface ComputeRegionBackendServiceOutlierDetectionInterval {
    /**
     * Span of time that's a fraction of a second at nanosecond resolution. Durations
     * less than one second are represented with a 0 'seconds' field and a positive
     * 'nanos' field. Must be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000
     * inclusive.
     */
    seconds: number;
}

export interface ComputeRegionBackendServiceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeRegionCommitmentLicenseResource {
    /**
     * The number of licenses purchased.
     */
    amount?: string;
    /**
     * Specifies the core range of the instance for which this license applies.
     */
    coresPerLicense?: string;
    /**
     * Any applicable license URI.
     */
    license: string;
}

export interface ComputeRegionCommitmentResource {
    /**
     * Name of the accelerator type resource. Applicable only when the type is ACCELERATOR.
     */
    acceleratorType?: string;
    /**
     * The amount of the resource purchased (in a type-dependent unit,
     * such as bytes). For vCPUs, this can just be an integer. For memory,
     * this must be provided in MB. Memory must be a multiple of 256 MB,
     * with up to 6.5GB of memory per every vCPU.
     */
    amount?: string;
    /**
     * Type of resource for which this commitment applies.
     * Possible values are VCPU, MEMORY, LOCAL_SSD, and ACCELERATOR.
     */
    type?: string;
}

export interface ComputeRegionCommitmentTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeRegionDiskAsyncPrimaryDisk {
    /**
     * Primary disk for asynchronous disk replication.
     */
    disk: string;
}

export interface ComputeRegionDiskDiskEncryptionKey {
    /**
     * The name of the encryption key that is stored in Google Cloud KMS.
     */
    kmsKeyName?: string;
    /**
     * Specifies a 256-bit customer-supplied encryption key, encoded in
     * RFC 4648 base64 to either encrypt or decrypt this resource.
     */
    rawKey?: string;
    /**
     * The RFC 4648 base64 encoded SHA-256 hash of the customer-supplied
     * encryption key that protects this resource.
     */
    sha256: string;
}

export interface ComputeRegionDiskGuestOsFeature {
    /**
     * The type of supported feature. Read [Enabling guest operating system features](https://cloud.google.com/compute/docs/images/create-delete-deprecate-private-images#guest-os-features) to see a list of available options. Possible values: ["MULTI_IP_SUBNET", "SECURE_BOOT", "SEV_CAPABLE", "UEFI_COMPATIBLE", "VIRTIO_SCSI_MULTIQUEUE", "WINDOWS", "GVNIC", "SEV_LIVE_MIGRATABLE", "SEV_SNP_CAPABLE", "SUSPEND_RESUME_COMPATIBLE", "TDX_CAPABLE"]
     */
    type: string;
}

export interface ComputeRegionDiskIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ComputeRegionDiskIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ComputeRegionDiskResourcePolicyAttachmentTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeRegionDiskSourceSnapshotEncryptionKey {
    /**
     * Specifies a 256-bit customer-supplied encryption key, encoded in
     * RFC 4648 base64 to either encrypt or decrypt this resource.
     */
    rawKey?: string;
    /**
     * The RFC 4648 base64 encoded SHA-256 hash of the customer-supplied
     * encryption key that protects this resource.
     */
    sha256: string;
}

export interface ComputeRegionDiskTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeRegionHealthCheckGrpcHealthCheck {
    /**
     * The gRPC service name for the health check.
     * The value of grpcServiceName has the following meanings by convention:
     *
     * * Empty serviceName means the overall status of all services at the backend.
     * * Non-empty serviceName means the health of that gRPC service, as defined by the owner of the service.
     *
     * The grpcServiceName can only be ASCII.
     */
    grpcServiceName?: string;
    /**
     * The port number for the health check request.
     * Must be specified if portName and portSpecification are not set
     * or if port_specification is USE_FIXED_PORT. Valid values are 1 through 65535.
     */
    port?: number;
    /**
     * Port name as defined in InstanceGroup#NamedPort#name. If both port and
     * port_name are defined, port takes precedence.
     */
    portName?: string;
    /**
     * Specifies how port is selected for health checking, can be one of the
     * following values:
     *
     *   * 'USE_FIXED_PORT': The port number in 'port' is used for health checking.
     *
     *   * 'USE_NAMED_PORT': The 'portName' is used for health checking.
     *
     *   * 'USE_SERVING_PORT': For NetworkEndpointGroup, the port specified for each
     *   network endpoint is used for health checking. For other backends, the
     *   port or named port specified in the Backend Service is used for health
     *   checking.
     *
     * If not specified, gRPC health check follows behavior specified in 'port' and
     * 'portName' fields. Possible values: ["USE_FIXED_PORT", "USE_NAMED_PORT", "USE_SERVING_PORT"]
     */
    portSpecification?: string;
}

export interface ComputeRegionHealthCheckHttp2HealthCheck {
    /**
     * The value of the host header in the HTTP2 health check request.
     * If left empty (default value), the public IP on behalf of which this health
     * check is performed will be used.
     */
    host?: string;
    /**
     * The TCP port number for the HTTP2 health check request.
     * The default value is 443.
     */
    port?: number;
    /**
     * Port name as defined in InstanceGroup#NamedPort#name. If both port and
     * port_name are defined, port takes precedence.
     */
    portName?: string;
    /**
     * Specifies how port is selected for health checking, can be one of the
     * following values:
     *
     *   * 'USE_FIXED_PORT': The port number in 'port' is used for health checking.
     *
     *   * 'USE_NAMED_PORT': The 'portName' is used for health checking.
     *
     *   * 'USE_SERVING_PORT': For NetworkEndpointGroup, the port specified for each
     *   network endpoint is used for health checking. For other backends, the
     *   port or named port specified in the Backend Service is used for health
     *   checking.
     *
     * If not specified, HTTP2 health check follows behavior specified in 'port' and
     * 'portName' fields. Possible values: ["USE_FIXED_PORT", "USE_NAMED_PORT", "USE_SERVING_PORT"]
     */
    portSpecification?: string;
    /**
     * Specifies the type of proxy header to append before sending data to the
     * backend. Default value: "NONE" Possible values: ["NONE", "PROXY_V1"]
     */
    proxyHeader?: string;
    /**
     * The request path of the HTTP2 health check request.
     * The default value is /.
     */
    requestPath?: string;
    /**
     * The bytes to match against the beginning of the response data. If left empty
     * (the default value), any response will indicate health. The response data
     * can only be ASCII.
     */
    response?: string;
}

export interface ComputeRegionHealthCheckHttpHealthCheck {
    /**
     * The value of the host header in the HTTP health check request.
     * If left empty (default value), the public IP on behalf of which this health
     * check is performed will be used.
     */
    host?: string;
    /**
     * The TCP port number for the HTTP health check request.
     * The default value is 80.
     */
    port?: number;
    /**
     * Port name as defined in InstanceGroup#NamedPort#name. If both port and
     * port_name are defined, port takes precedence.
     */
    portName?: string;
    /**
     * Specifies how port is selected for health checking, can be one of the
     * following values:
     *
     *   * 'USE_FIXED_PORT': The port number in 'port' is used for health checking.
     *
     *   * 'USE_NAMED_PORT': The 'portName' is used for health checking.
     *
     *   * 'USE_SERVING_PORT': For NetworkEndpointGroup, the port specified for each
     *   network endpoint is used for health checking. For other backends, the
     *   port or named port specified in the Backend Service is used for health
     *   checking.
     *
     * If not specified, HTTP health check follows behavior specified in 'port' and
     * 'portName' fields. Possible values: ["USE_FIXED_PORT", "USE_NAMED_PORT", "USE_SERVING_PORT"]
     */
    portSpecification?: string;
    /**
     * Specifies the type of proxy header to append before sending data to the
     * backend. Default value: "NONE" Possible values: ["NONE", "PROXY_V1"]
     */
    proxyHeader?: string;
    /**
     * The request path of the HTTP health check request.
     * The default value is /.
     */
    requestPath?: string;
    /**
     * The bytes to match against the beginning of the response data. If left empty
     * (the default value), any response will indicate health. The response data
     * can only be ASCII.
     */
    response?: string;
}

export interface ComputeRegionHealthCheckHttpsHealthCheck {
    /**
     * The value of the host header in the HTTPS health check request.
     * If left empty (default value), the public IP on behalf of which this health
     * check is performed will be used.
     */
    host?: string;
    /**
     * The TCP port number for the HTTPS health check request.
     * The default value is 443.
     */
    port?: number;
    /**
     * Port name as defined in InstanceGroup#NamedPort#name. If both port and
     * port_name are defined, port takes precedence.
     */
    portName?: string;
    /**
     * Specifies how port is selected for health checking, can be one of the
     * following values:
     *
     *   * 'USE_FIXED_PORT': The port number in 'port' is used for health checking.
     *
     *   * 'USE_NAMED_PORT': The 'portName' is used for health checking.
     *
     *   * 'USE_SERVING_PORT': For NetworkEndpointGroup, the port specified for each
     *   network endpoint is used for health checking. For other backends, the
     *   port or named port specified in the Backend Service is used for health
     *   checking.
     *
     * If not specified, HTTPS health check follows behavior specified in 'port' and
     * 'portName' fields. Possible values: ["USE_FIXED_PORT", "USE_NAMED_PORT", "USE_SERVING_PORT"]
     */
    portSpecification?: string;
    /**
     * Specifies the type of proxy header to append before sending data to the
     * backend. Default value: "NONE" Possible values: ["NONE", "PROXY_V1"]
     */
    proxyHeader?: string;
    /**
     * The request path of the HTTPS health check request.
     * The default value is /.
     */
    requestPath?: string;
    /**
     * The bytes to match against the beginning of the response data. If left empty
     * (the default value), any response will indicate health. The response data
     * can only be ASCII.
     */
    response?: string;
}

export interface ComputeRegionHealthCheckLogConfig {
    /**
     * Indicates whether or not to export logs. This is false by default,
     * which means no health check logging will be done.
     */
    enable?: boolean;
}

export interface ComputeRegionHealthCheckSslHealthCheck {
    /**
     * The TCP port number for the SSL health check request.
     * The default value is 443.
     */
    port?: number;
    /**
     * Port name as defined in InstanceGroup#NamedPort#name. If both port and
     * port_name are defined, port takes precedence.
     */
    portName?: string;
    /**
     * Specifies how port is selected for health checking, can be one of the
     * following values:
     *
     *   * 'USE_FIXED_PORT': The port number in 'port' is used for health checking.
     *
     *   * 'USE_NAMED_PORT': The 'portName' is used for health checking.
     *
     *   * 'USE_SERVING_PORT': For NetworkEndpointGroup, the port specified for each
     *   network endpoint is used for health checking. For other backends, the
     *   port or named port specified in the Backend Service is used for health
     *   checking.
     *
     * If not specified, SSL health check follows behavior specified in 'port' and
     * 'portName' fields. Possible values: ["USE_FIXED_PORT", "USE_NAMED_PORT", "USE_SERVING_PORT"]
     */
    portSpecification?: string;
    /**
     * Specifies the type of proxy header to append before sending data to the
     * backend. Default value: "NONE" Possible values: ["NONE", "PROXY_V1"]
     */
    proxyHeader?: string;
    /**
     * The application data to send once the SSL connection has been
     * established (default value is empty). If both request and response are
     * empty, the connection establishment alone will indicate health. The request
     * data can only be ASCII.
     */
    request?: string;
    /**
     * The bytes to match against the beginning of the response data. If left empty
     * (the default value), any response will indicate health. The response data
     * can only be ASCII.
     */
    response?: string;
}

export interface ComputeRegionHealthCheckTcpHealthCheck {
    /**
     * The TCP port number for the TCP health check request.
     * The default value is 80.
     */
    port?: number;
    /**
     * Port name as defined in InstanceGroup#NamedPort#name. If both port and
     * port_name are defined, port takes precedence.
     */
    portName?: string;
    /**
     * Specifies how port is selected for health checking, can be one of the
     * following values:
     *
     *   * 'USE_FIXED_PORT': The port number in 'port' is used for health checking.
     *
     *   * 'USE_NAMED_PORT': The 'portName' is used for health checking.
     *
     *   * 'USE_SERVING_PORT': For NetworkEndpointGroup, the port specified for each
     *   network endpoint is used for health checking. For other backends, the
     *   port or named port specified in the Backend Service is used for health
     *   checking.
     *
     * If not specified, TCP health check follows behavior specified in 'port' and
     * 'portName' fields. Possible values: ["USE_FIXED_PORT", "USE_NAMED_PORT", "USE_SERVING_PORT"]
     */
    portSpecification?: string;
    /**
     * Specifies the type of proxy header to append before sending data to the
     * backend. Default value: "NONE" Possible values: ["NONE", "PROXY_V1"]
     */
    proxyHeader?: string;
    /**
     * The application data to send once the TCP connection has been
     * established (default value is empty). If both request and response are
     * empty, the connection establishment alone will indicate health. The request
     * data can only be ASCII.
     */
    request?: string;
    /**
     * The bytes to match against the beginning of the response data. If left empty
     * (the default value), any response will indicate health. The response data
     * can only be ASCII.
     */
    response?: string;
}

export interface ComputeRegionHealthCheckTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeRegionInstanceGroupManagerAllInstancesConfig {
    /**
     * The label key-value pairs that you want to patch onto the instance,
     */
    labels?: {[key: string]: string};
    /**
     * The metadata key-value pairs that you want to patch onto the instance. For more information, see Project and instance metadata,
     */
    metadata?: {[key: string]: string};
}

export interface ComputeRegionInstanceGroupManagerAutoHealingPolicies {
    /**
     * The health check resource that signals autohealing.
     */
    healthCheck: string;
    /**
     * The number of seconds that the managed instance group waits before it applies autohealing policies to new instances or recently recreated instances. Between 0 and 3600.
     */
    initialDelaySec: number;
}

export interface ComputeRegionInstanceGroupManagerInstanceLifecyclePolicy {
    /**
     * Default behavior for all instance or health check failures.
     */
    defaultActionOnFailure?: string;
    /**
     * Specifies whether to apply the group's latest configuration when repairing a VM. Valid options are: YES, NO. If YES and you updated the group's instance template or per-instance configurations after the VM was created, then these changes are applied when VM is repaired. If NO (default), then updates are applied in accordance with the group's update policy type.
     */
    forceUpdateOnRepair?: string;
}

export interface ComputeRegionInstanceGroupManagerNamedPort {
    /**
     * The name of the port.
     */
    name: string;
    /**
     * The port number.
     */
    port: number;
}

export interface ComputeRegionInstanceGroupManagerStatefulDisk {
    /**
     * A value that prescribes what should happen to the stateful disk when the VM instance is deleted. The available options are NEVER and ON_PERMANENT_INSTANCE_DELETION. NEVER - detach the disk when the VM is deleted, but do not delete the disk. ON_PERMANENT_INSTANCE_DELETION will delete the stateful disk when the VM is permanently deleted from the instance group. The default is NEVER.
     */
    deleteRule?: string;
    /**
     * The device name of the disk to be attached.
     */
    deviceName: string;
}

export interface ComputeRegionInstanceGroupManagerStatefulExternalIp {
    /**
     * A value that prescribes what should happen to an associated static Address resource when a VM instance is permanently deleted. The available options are NEVER and ON_PERMANENT_INSTANCE_DELETION. NEVER - detach the IP when the VM is deleted, but do not delete the address resource. ON_PERMANENT_INSTANCE_DELETION will delete the stateful address when the VM is permanently deleted from the instance group. The default is NEVER.
     */
    deleteRule?: string;
    /**
     * The network interface name
     */
    interfaceName?: string;
}

export interface ComputeRegionInstanceGroupManagerStatefulInternalIp {
    /**
     * A value that prescribes what should happen to an associated static Address resource when a VM instance is permanently deleted. The available options are NEVER and ON_PERMANENT_INSTANCE_DELETION. NEVER - detach the IP when the VM is deleted, but do not delete the address resource. ON_PERMANENT_INSTANCE_DELETION will delete the stateful address when the VM is permanently deleted from the instance group. The default is NEVER.
     */
    deleteRule?: string;
    /**
     * The network interface name
     */
    interfaceName?: string;
}

export interface ComputeRegionInstanceGroupManagerStatus {
    allInstancesConfigs: outputs.ComputeRegionInstanceGroupManagerStatusAllInstancesConfig[];
    isStable: boolean;
    statefuls: outputs.ComputeRegionInstanceGroupManagerStatusStateful[];
    versionTargets: outputs.ComputeRegionInstanceGroupManagerStatusVersionTarget[];
}

export interface ComputeRegionInstanceGroupManagerStatusAllInstancesConfig {
    currentRevision: string;
    effective: boolean;
}

export interface ComputeRegionInstanceGroupManagerStatusStateful {
    hasStatefulConfig: boolean;
    perInstanceConfigs: outputs.ComputeRegionInstanceGroupManagerStatusStatefulPerInstanceConfig[];
}

export interface ComputeRegionInstanceGroupManagerStatusStatefulPerInstanceConfig {
    allEffective: boolean;
}

export interface ComputeRegionInstanceGroupManagerStatusVersionTarget {
    isReached: boolean;
}

export interface ComputeRegionInstanceGroupManagerTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeRegionInstanceGroupManagerUpdatePolicy {
    /**
     * The instance redistribution policy for regional managed instance groups. Valid values are: "PROACTIVE", "NONE". If PROACTIVE (default), the group attempts to maintain an even distribution of VM instances across zones in the region. If NONE, proactive redistribution is disabled.
     */
    instanceRedistributionType?: string;
    /**
     * Specifies a fixed number of VM instances. This must be a positive integer. Conflicts with max_surge_percent. Both cannot be 0
     */
    maxSurgeFixed: number;
    /**
     * Specifies a percentage of instances between 0 to 100%, inclusive. For example, specify 80 for 80%. Conflicts with max_surge_fixed.
     */
    maxSurgePercent?: number;
    /**
     * Specifies a fixed number of VM instances. This must be a positive integer.
     */
    maxUnavailableFixed: number;
    /**
     * Specifies a percentage of instances between 0 to 100%, inclusive. For example, specify 80 for 80%.
     */
    maxUnavailablePercent?: number;
    /**
     * Minimal action to be taken on an instance. You can specify either NONE to forbid any actions, REFRESH to update without stopping instances, RESTART to restart existing instances or REPLACE to delete and create new instances from the target template. If you specify a REFRESH, the Updater will attempt to perform that action only. However, if the Updater determines that the minimal action you specify is not enough to perform the update, it might perform a more disruptive action.
     */
    minimalAction: string;
    /**
     * Most disruptive action that is allowed to be taken on an instance. You can specify either NONE to forbid any actions, REFRESH to allow actions that do not need instance restart, RESTART to allow actions that can be applied without instance replacing or REPLACE to allow all possible actions. If the Updater determines that the minimal update action needed is more disruptive than most disruptive allowed action you specify it will not perform the update at all.
     */
    mostDisruptiveAllowedAction?: string;
    /**
     * The instance replacement method for regional managed instance groups. Valid values are: "RECREATE", "SUBSTITUTE". If SUBSTITUTE (default), the group replaces VM instances with new instances that have randomly generated names. If RECREATE, instance names are preserved.  You must also set max_unavailable_fixed or max_unavailable_percent to be greater than 0.
     */
    replacementMethod?: string;
    /**
     * The type of update process. You can specify either PROACTIVE so that the instance group manager proactively executes actions in order to bring instances to their target versions or OPPORTUNISTIC so that no action is proactively executed but the update will be performed as part of other actions (for example, resizes or recreateInstances calls).
     */
    type: string;
}

export interface ComputeRegionInstanceGroupManagerVersion {
    /**
     * The full URL to an instance template from which all new instances of this version will be created.
     */
    instanceTemplate: string;
    /**
     * Version name.
     */
    name?: string;
    /**
     * The number of instances calculated as a fixed number or a percentage depending on the settings.
     */
    targetSize?: outputs.ComputeRegionInstanceGroupManagerVersionTargetSize;
}

export interface ComputeRegionInstanceGroupManagerVersionTargetSize {
    /**
     * The number of instances which are managed for this version. Conflicts with percent.
     */
    fixed?: number;
    /**
     * The number of instances (calculated as percentage) which are managed for this version. Conflicts with fixed. Note that when using percent, rounding will be in favor of explicitly set target_size values; a managed instance group with 2 instances and 2 versions, one of which has a target_size.percent of 60 will create 2 instances of that version.
     */
    percent?: number;
}

export interface ComputeRegionInstanceTemplateAdvancedMachineFeatures {
    /**
     * Whether to enable nested virtualization or not.
     */
    enableNestedVirtualization?: boolean;
    /**
     * The number of threads per physical core. To disable simultaneous multithreading (SMT) set this to 1. If unset, the maximum number of threads supported per core by the underlying processor is assumed.
     */
    threadsPerCore?: number;
    /**
     * The number of physical cores to expose to an instance. Multiply by the number of threads per core to compute the total number of virtual CPUs to expose to the instance. If unset, the number of cores is inferred from the instance\'s nominal CPU count and the underlying platform\'s SMT width.
     */
    visibleCoreCount?: number;
}

export interface ComputeRegionInstanceTemplateConfidentialInstanceConfig {
    /**
     * Specifies which confidential computing technology to use.
     * 								This could be one of the following values: SEV, SEV_SNP.
     * 								If SEV_SNP, min_cpu_platform = "AMD Milan" is currently required.
     */
    confidentialInstanceType?: string;
    /**
     * Defines whether the instance should have confidential compute enabled. Field will be deprecated in a future release.
     */
    enableConfidentialCompute?: boolean;
}

export interface ComputeRegionInstanceTemplateDisk {
    /**
     * Whether or not the disk should be auto-deleted. This defaults to true.
     */
    autoDelete?: boolean;
    /**
     * Indicates that this is a boot disk.
     */
    boot: boolean;
    /**
     * A unique device name that is reflected into the /dev/ tree of a Linux operating system running within the instance. If not specified, the server chooses a default device name to apply to this disk.
     */
    deviceName: string;
    /**
     * Encrypts or decrypts a disk using a customer-supplied encryption key.
     */
    diskEncryptionKey?: outputs.ComputeRegionInstanceTemplateDiskDiskEncryptionKey;
    /**
     * Name of the disk. When not provided, this defaults to the name of the instance.
     */
    diskName?: string;
    /**
     * The size of the image in gigabytes. If not specified, it will inherit the size of its base image. For SCRATCH disks, the size must be one of 375 or 3000 GB, with a default of 375 GB.
     */
    diskSizeGb: number;
    /**
     * The Google Compute Engine disk type. Such as "pd-ssd", "local-ssd", "pd-balanced" or "pd-standard".
     */
    diskType: string;
    /**
     * Specifies the disk interface to use for attaching this disk.
     */
    interface: string;
    /**
     * A set of key/value label pairs to assign to disks,
     */
    labels?: {[key: string]: string};
    /**
     * The mode in which to attach this disk, either READ_WRITE or READ_ONLY. If you are attaching or creating a boot disk, this must read-write mode.
     */
    mode: string;
    /**
     * Indicates how many IOPS to provision for the disk. This sets the number of I/O operations per second that the disk can handle. Values must be between 10,000 and 120,000. For more details, see the [Extreme persistent disk documentation](https://cloud.google.com/compute/docs/disks/extreme-persistent-disk).
     */
    provisionedIops: number;
    /**
     * A map of resource manager tags. Resource manager tag keys and values have the same definition as resource manager tags. Keys must be in the format tagKeys/{tag_key_id}, and values are in the format tagValues/456. The field is ignored (both PUT & PATCH) when empty.
     */
    resourceManagerTags?: {[key: string]: string};
    /**
     * A list (short name or id) of resource policies to attach to this disk. Currently a max of 1 resource policy is supported.
     */
    resourcePolicies?: string[];
    /**
     * The name (not self_link) of the disk (such as those managed by google_compute_disk) to attach. > Note: Either source or source_image is required when creating a new instance except for when creating a local SSD.
     */
    source?: string;
    /**
     * The image from which to initialize this disk. This can be one of: the image's self_link, projects/{project}/global/images/{image}, projects/{project}/global/images/family/{family}, global/images/{image}, global/images/family/{family}, family/{family}, {project}/{family}, {project}/{image}, {family}, or {image}. > Note: Either source or source_image is required when creating a new instance except for when creating a local SSD.
     */
    sourceImage: string;
    /**
     * The customer-supplied encryption key of the source
     * image. Required if the source image is protected by a
     * customer-supplied encryption key.
     *
     * Instance templates do not store customer-supplied
     * encryption keys, so you cannot create disks for
     * instances in a managed instance group if the source
     * images are encrypted with your own keys.
     */
    sourceImageEncryptionKey?: outputs.ComputeRegionInstanceTemplateDiskSourceImageEncryptionKey;
    /**
     * The source snapshot to create this disk. When creating
     * a new instance, one of initializeParams.sourceSnapshot,
     * initializeParams.sourceImage, or disks.source is
     * required except for local SSD.
     */
    sourceSnapshot?: string;
    /**
     * The customer-supplied encryption key of the source snapshot.
     */
    sourceSnapshotEncryptionKey?: outputs.ComputeRegionInstanceTemplateDiskSourceSnapshotEncryptionKey;
    /**
     * The type of Google Compute Engine disk, can be either "SCRATCH" or "PERSISTENT".
     */
    type: string;
}

export interface ComputeRegionInstanceTemplateDiskDiskEncryptionKey {
    /**
     * The self link of the encryption key that is stored in Google Cloud KMS.
     */
    kmsKeySelfLink: string;
}

export interface ComputeRegionInstanceTemplateDiskSourceImageEncryptionKey {
    /**
     * The self link of the encryption key that is stored in
     * Google Cloud KMS.
     */
    kmsKeySelfLink: string;
    /**
     * The service account being used for the encryption
     * request for the given KMS key. If absent, the Compute
     * Engine default service account is used.
     */
    kmsKeyServiceAccount?: string;
}

export interface ComputeRegionInstanceTemplateDiskSourceSnapshotEncryptionKey {
    /**
     * The self link of the encryption key that is stored in
     * Google Cloud KMS.
     */
    kmsKeySelfLink: string;
    /**
     * The service account being used for the encryption
     * request for the given KMS key. If absent, the Compute
     * Engine default service account is used.
     */
    kmsKeyServiceAccount?: string;
}

export interface ComputeRegionInstanceTemplateGuestAccelerator {
    /**
     * The number of the guest accelerator cards exposed to this instance.
     */
    count: number;
    /**
     * The accelerator type resource to expose to this instance. E.g. nvidia-tesla-k80.
     */
    type: string;
}

export interface ComputeRegionInstanceTemplateNetworkInterface {
    accessConfigs?: outputs.ComputeRegionInstanceTemplateNetworkInterfaceAccessConfig[];
    /**
     * An array of alias IP ranges for this network interface. Can only be specified for network interfaces on subnet-mode networks.
     */
    aliasIpRanges?: outputs.ComputeRegionInstanceTemplateNetworkInterfaceAliasIpRange[];
    /**
     * The prefix length of the primary internal IPv6 range.
     */
    internalIpv6PrefixLength: number;
    /**
     * An array of IPv6 access configurations for this interface. Currently, only one IPv6 access config, DIRECT_IPV6, is supported. If there is no ipv6AccessConfig specified, then this instance will have no external IPv6 Internet access.
     */
    ipv6AccessConfigs?: outputs.ComputeRegionInstanceTemplateNetworkInterfaceIpv6AccessConfig[];
    /**
     * One of EXTERNAL, INTERNAL to indicate whether the IP can be accessed from the Internet. This field is always inherited from its subnetwork.
     */
    ipv6AccessType: string;
    /**
     * An IPv6 internal network address for this network interface. If not specified, Google Cloud will automatically assign an internal IPv6 address from the instance's subnetwork.
     */
    ipv6Address: string;
    /**
     * The name of the network_interface.
     */
    name: string;
    /**
     * The name or self_link of the network to attach this interface to. Use network attribute for Legacy or Auto subnetted networks and subnetwork for custom subnetted networks.
     */
    network: string;
    /**
     * The private IP address to assign to the instance. If empty, the address will be automatically assigned.
     */
    networkIp?: string;
    /**
     * The type of vNIC to be used on this interface. Possible values:GVNIC, VIRTIO_NET
     */
    nicType?: string;
    /**
     * The networking queue count that's specified by users for the network interface. Both Rx and Tx queues will be set to this number. It will be empty if not specified.
     */
    queueCount?: number;
    /**
     * The stack type for this network interface to identify whether the IPv6 feature is enabled or not. If not specified, IPV4_ONLY will be used.
     */
    stackType: string;
    /**
     * The name of the subnetwork to attach this interface to. The subnetwork must exist in the same region this instance will be created in. Either network or subnetwork must be provided.
     */
    subnetwork: string;
    /**
     * The ID of the project in which the subnetwork belongs. If it is not provided, the provider project is used.
     */
    subnetworkProject: string;
}

export interface ComputeRegionInstanceTemplateNetworkInterfaceAccessConfig {
    /**
     * The IP address that will be 1:1 mapped to the instance's network ip. If not given, one will be generated.
     */
    natIp: string;
    /**
     * The networking tier used for configuring this instance template. This field can take the following values: PREMIUM, STANDARD, FIXED_STANDARD. If this field is not specified, it is assumed to be PREMIUM.
     */
    networkTier: string;
    /**
     * The DNS domain name for the public PTR record.The DNS domain name for the public PTR record.
     */
    publicPtrDomainName: string;
}

export interface ComputeRegionInstanceTemplateNetworkInterfaceAliasIpRange {
    /**
     * The IP CIDR range represented by this alias IP range. This IP CIDR range must belong to the specified subnetwork and cannot contain IP addresses reserved by system or used by other network interfaces. At the time of writing only a netmask (e.g. /24) may be supplied, with a CIDR format resulting in an API error.
     */
    ipCidrRange: string;
    /**
     * The subnetwork secondary range name specifying the secondary range from which to allocate the IP CIDR range for this alias IP range. If left unspecified, the primary range of the subnetwork will be used.
     */
    subnetworkRangeName?: string;
}

export interface ComputeRegionInstanceTemplateNetworkInterfaceIpv6AccessConfig {
    /**
     * The first IPv6 address of the external IPv6 range associated with this instance, prefix length is stored in externalIpv6PrefixLength in ipv6AccessConfig. The field is output only, an IPv6 address from a subnetwork associated with the instance will be allocated dynamically.
     */
    externalIpv6: string;
    /**
     * The prefix length of the external IPv6 range.
     */
    externalIpv6PrefixLength: string;
    /**
     * The name of this access configuration.
     */
    name: string;
    /**
     * The service-level to be provided for IPv6 traffic when the subnet has an external subnet. Only PREMIUM tier is valid for IPv6
     */
    networkTier: string;
    /**
     * The domain name to be used when creating DNSv6 records for the external IPv6 ranges.
     */
    publicPtrDomainName: string;
}

export interface ComputeRegionInstanceTemplateNetworkPerformanceConfig {
    /**
     * The egress bandwidth tier to enable. Possible values:TIER_1, DEFAULT
     */
    totalEgressBandwidthTier: string;
}

export interface ComputeRegionInstanceTemplateReservationAffinity {
    /**
     * Specifies the label selector for the reservation to use.
     */
    specificReservation?: outputs.ComputeRegionInstanceTemplateReservationAffinitySpecificReservation;
    /**
     * The type of reservation from which this instance can consume resources.
     */
    type: string;
}

export interface ComputeRegionInstanceTemplateReservationAffinitySpecificReservation {
    /**
     * Corresponds to the label key of a reservation resource. To target a SPECIFIC_RESERVATION by name, specify compute.googleapis.com/reservation-name as the key and specify the name of your reservation as the only value.
     */
    key: string;
    /**
     * Corresponds to the label values of a reservation resource.
     */
    values: string[];
}

export interface ComputeRegionInstanceTemplateScheduling {
    /**
     * Specifies whether the instance should be automatically restarted if it is terminated by Compute Engine (not terminated by a user). This defaults to true.
     */
    automaticRestart?: boolean;
    /**
     * Specifies the action GCE should take when SPOT VM is preempted.
     */
    instanceTerminationAction?: string;
    /**
     * Specifies the maximum amount of time a Local Ssd Vm should wait while
     *   recovery of the Local Ssd state is attempted. Its value should be in
     *   between 0 and 168 hours with hour granularity and the default value being 1
     *   hour.
     */
    localSsdRecoveryTimeouts?: outputs.ComputeRegionInstanceTemplateSchedulingLocalSsdRecoveryTimeout[];
    /**
     * The timeout for new network connections to hosts.
     */
    maxRunDuration?: outputs.ComputeRegionInstanceTemplateSchedulingMaxRunDuration;
    /**
     * Minimum number of cpus for the instance.
     */
    minNodeCpus?: number;
    /**
     * Specifies node affinities or anti-affinities to determine which sole-tenant nodes your instances and managed instance groups will use as host systems.
     */
    nodeAffinities?: outputs.ComputeRegionInstanceTemplateSchedulingNodeAffinity[];
    /**
     * Defines the maintenance behavior for this instance.
     */
    onHostMaintenance: string;
    /**
     * Defines the behaviour for instances with the instance_termination_action.
     */
    onInstanceStopAction?: outputs.ComputeRegionInstanceTemplateSchedulingOnInstanceStopAction;
    /**
     * Allows instance to be preempted. This defaults to false.
     */
    preemptible?: boolean;
    /**
     * Whether the instance is spot. If this is set as SPOT.
     */
    provisioningModel: string;
}

export interface ComputeRegionInstanceTemplateSchedulingLocalSsdRecoveryTimeout {
    /**
     * Span of time that's a fraction of a second at nanosecond
     * resolution. Durations less than one second are represented
     * with a 0 seconds field and a positive nanos field. Must
     * be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second.
     * Must be from 0 to 315,576,000,000 inclusive.
     */
    seconds: number;
}

export interface ComputeRegionInstanceTemplateSchedulingMaxRunDuration {
    /**
     * Span of time that's a fraction of a second at nanosecond
     * resolution. Durations less than one second are represented
     * with a 0 seconds field and a positive nanos field. Must
     * be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second.
     * Must be from 0 to 315,576,000,000 inclusive.
     */
    seconds: number;
}

export interface ComputeRegionInstanceTemplateSchedulingNodeAffinity {
    key: string;
    operator: string;
    values: string[];
}

export interface ComputeRegionInstanceTemplateSchedulingOnInstanceStopAction {
    /**
     * If true, the contents of any attached Local SSD disks will be discarded.
     */
    discardLocalSsd?: boolean;
}

export interface ComputeRegionInstanceTemplateServiceAccount {
    /**
     * The service account e-mail address. If not given, the default Google Compute Engine service account is used.
     */
    email: string;
    /**
     * A list of service scopes. Both OAuth2 URLs and gcloud short names are supported. To allow full access to all Cloud APIs, use the cloud-platform scope.
     */
    scopes: string[];
}

export interface ComputeRegionInstanceTemplateShieldedInstanceConfig {
    /**
     * Compare the most recent boot measurements to the integrity policy baseline and return a pair of pass/fail results depending on whether they match or not. Defaults to true.
     */
    enableIntegrityMonitoring?: boolean;
    /**
     * Verify the digital signature of all boot components, and halt the boot process if signature verification fails. Defaults to false.
     */
    enableSecureBoot?: boolean;
    /**
     * Use a virtualized trusted platform module, which is a specialized computer chip you can use to encrypt objects like keys and certificates. Defaults to true.
     */
    enableVtpm?: boolean;
}

export interface ComputeRegionInstanceTemplateTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeRegionNetworkEndpointGroupAppEngine {
    /**
     * Optional serving service.
     * The service name must be 1-63 characters long, and comply with RFC1035.
     * Example value: "default", "my-service".
     */
    service?: string;
    /**
     * A template to parse service and version fields from a request URL.
     * URL mask allows for routing to multiple App Engine services without
     * having to create multiple Network Endpoint Groups and backend services.
     *
     * For example, the request URLs "foo1-dot-appname.appspot.com/v1" and
     * "foo1-dot-appname.appspot.com/v2" can be backed by the same Serverless NEG with
     * URL mask "-dot-appname.appspot.com/". The URL mask will parse
     * them to { service = "foo1", version = "v1" } and { service = "foo1", version = "v2" } respectively.
     */
    urlMask?: string;
    /**
     * Optional serving version.
     * The version must be 1-63 characters long, and comply with RFC1035.
     * Example value: "v1", "v2".
     */
    version?: string;
}

export interface ComputeRegionNetworkEndpointGroupCloudFunction {
    /**
     * A user-defined name of the Cloud Function.
     * The function name is case-sensitive and must be 1-63 characters long.
     * Example value: "func1".
     */
    function?: string;
    /**
     * A template to parse function field from a request URL. URL mask allows
     * for routing to multiple Cloud Functions without having to create
     * multiple Network Endpoint Groups and backend services.
     *
     * For example, request URLs "mydomain.com/function1" and "mydomain.com/function2"
     * can be backed by the same Serverless NEG with URL mask "/". The URL mask
     * will parse them to { function = "function1" } and { function = "function2" } respectively.
     */
    urlMask?: string;
}

export interface ComputeRegionNetworkEndpointGroupCloudRun {
    /**
     * Cloud Run service is the main resource of Cloud Run.
     * The service must be 1-63 characters long, and comply with RFC1035.
     * Example value: "run-service".
     */
    service?: string;
    /**
     * Cloud Run tag represents the "named-revision" to provide
     * additional fine-grained traffic routing information.
     * The tag must be 1-63 characters long, and comply with RFC1035.
     * Example value: "revision-0010".
     */
    tag?: string;
    /**
     * A template to parse service and tag fields from a request URL.
     * URL mask allows for routing to multiple Run services without having
     * to create multiple network endpoint groups and backend services.
     *
     * For example, request URLs "foo1.domain.com/bar1" and "foo1.domain.com/bar2"
     * an be backed by the same Serverless Network Endpoint Group (NEG) with
     * URL mask ".domain.com/". The URL mask will parse them to { service="bar1", tag="foo1" }
     * and { service="bar2", tag="foo2" } respectively.
     */
    urlMask?: string;
}

export interface ComputeRegionNetworkEndpointGroupTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeRegionNetworkEndpointTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeRegionNetworkFirewallPolicyAssociationTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeRegionNetworkFirewallPolicyRuleMatch {
    /**
     * Address groups which should be matched against the traffic destination. Maximum number of destination address groups is 10. Destination address groups is only supported in Egress rules.
     */
    destAddressGroups?: string[];
    /**
     * Domain names that will be used to match against the resolved domain name of destination of traffic. Can only be specified if DIRECTION is egress.
     */
    destFqdns?: string[];
    /**
     * CIDR IP address range. Maximum number of destination CIDR IP ranges allowed is 5000.
     */
    destIpRanges?: string[];
    /**
     * The Unicode country codes whose IP addresses will be used to match against the source of traffic. Can only be specified if DIRECTION is egress.
     */
    destRegionCodes?: string[];
    /**
     * Name of the Google Cloud Threat Intelligence list.
     */
    destThreatIntelligences?: string[];
    /**
     * Pairs of IP protocols and ports that the rule should match.
     */
    layer4Configs: outputs.ComputeRegionNetworkFirewallPolicyRuleMatchLayer4Config[];
    /**
     * Address groups which should be matched against the traffic source. Maximum number of source address groups is 10. Source address groups is only supported in Ingress rules.
     */
    srcAddressGroups?: string[];
    /**
     * Domain names that will be used to match against the resolved domain name of source of traffic. Can only be specified if DIRECTION is ingress.
     */
    srcFqdns?: string[];
    /**
     * CIDR IP address range. Maximum number of source CIDR IP ranges allowed is 5000.
     */
    srcIpRanges?: string[];
    /**
     * The Unicode country codes whose IP addresses will be used to match against the source of traffic. Can only be specified if DIRECTION is ingress.
     */
    srcRegionCodes?: string[];
    /**
     * List of secure tag values, which should be matched at the source of the traffic. For INGRESS rule, if all the <code>srcSecureTag</code> are INEFFECTIVE, and there is no <code>srcIpRange</code>, this rule will be ignored. Maximum number of source tag values allowed is 256.
     */
    srcSecureTags?: outputs.ComputeRegionNetworkFirewallPolicyRuleMatchSrcSecureTag[];
    /**
     * Name of the Google Cloud Threat Intelligence list.
     */
    srcThreatIntelligences?: string[];
}

export interface ComputeRegionNetworkFirewallPolicyRuleMatchLayer4Config {
    /**
     * The IP protocol to which this rule applies. The protocol type is required when creating a firewall rule. This value can either be one of the following well known protocol strings (`tcp`, `udp`, `icmp`, `esp`, `ah`, `ipip`, `sctp`), or the IP protocol number.
     */
    ipProtocol: string;
    /**
     * An optional list of ports to which this rule applies. This field is only applicable for UDP or TCP protocol. Each entry must be either an integer or a range. If not specified, this rule applies to connections through any port. Example inputs include: ``.
     */
    ports?: string[];
}

export interface ComputeRegionNetworkFirewallPolicyRuleMatchSrcSecureTag {
    /**
     * Name of the secure tag, created with TagManager's TagValue API. @pattern tagValues/[0-9]+
     */
    name: string;
    /**
     * [Output Only] State of the secure tag, either `EFFECTIVE` or `INEFFECTIVE`. A secure tag is `INEFFECTIVE` when it is deleted or its network is deleted.
     */
    state: string;
}

export interface ComputeRegionNetworkFirewallPolicyRuleTargetSecureTag {
    /**
     * Name of the secure tag, created with TagManager's TagValue API. @pattern tagValues/[0-9]+
     */
    name: string;
    /**
     * [Output Only] State of the secure tag, either `EFFECTIVE` or `INEFFECTIVE`. A secure tag is `INEFFECTIVE` when it is deleted or its network is deleted.
     */
    state: string;
}

export interface ComputeRegionNetworkFirewallPolicyRuleTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeRegionNetworkFirewallPolicyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeRegionPerInstanceConfigPreservedState {
    /**
     * Stateful disks for the instance.
     */
    disks?: outputs.ComputeRegionPerInstanceConfigPreservedStateDisk[];
    /**
     * Preserved external IPs defined for this instance. This map is keyed with the name of the network interface.
     */
    externalIps?: outputs.ComputeRegionPerInstanceConfigPreservedStateExternalIp[];
    /**
     * Preserved internal IPs defined for this instance. This map is keyed with the name of the network interface.
     */
    internalIps?: outputs.ComputeRegionPerInstanceConfigPreservedStateInternalIp[];
    /**
     * Preserved metadata defined for this instance. This is a list of key->value pairs.
     */
    metadata?: {[key: string]: string};
}

export interface ComputeRegionPerInstanceConfigPreservedStateDisk {
    /**
     * A value that prescribes what should happen to the stateful disk when the VM instance is deleted.
     * The available options are 'NEVER' and 'ON_PERMANENT_INSTANCE_DELETION'.
     * 'NEVER' - detach the disk when the VM is deleted, but do not delete the disk.
     * 'ON_PERMANENT_INSTANCE_DELETION' will delete the stateful disk when the VM is permanently
     * deleted from the instance group. Default value: "NEVER" Possible values: ["NEVER", "ON_PERMANENT_INSTANCE_DELETION"]
     */
    deleteRule?: string;
    /**
     * A unique device name that is reflected into the /dev/ tree of a Linux operating system running within the instance.
     */
    deviceName: string;
    /**
     * The mode of the disk. Default value: "READ_WRITE" Possible values: ["READ_ONLY", "READ_WRITE"]
     */
    mode?: string;
    /**
     * The URI of an existing persistent disk to attach under the specified device-name in the format
     * 'projects/project-id/zones/zone/disks/disk-name'.
     */
    source: string;
}

export interface ComputeRegionPerInstanceConfigPreservedStateExternalIp {
    /**
     * These stateful IPs will never be released during autohealing, update or VM instance recreate operations. This flag is used to configure if the IP reservation should be deleted after it is no longer used by the group, e.g. when the given instance or the whole group is deleted. Default value: "NEVER" Possible values: ["NEVER", "ON_PERMANENT_INSTANCE_DELETION"]
     */
    autoDelete?: string;
    interfaceName: string;
    /**
     * Ip address representation
     */
    ipAddress?: outputs.ComputeRegionPerInstanceConfigPreservedStateExternalIpIpAddress;
}

export interface ComputeRegionPerInstanceConfigPreservedStateExternalIpIpAddress {
    /**
     * The URL of the reservation for this IP address.
     */
    address?: string;
}

export interface ComputeRegionPerInstanceConfigPreservedStateInternalIp {
    /**
     * These stateful IPs will never be released during autohealing, update or VM instance recreate operations. This flag is used to configure if the IP reservation should be deleted after it is no longer used by the group, e.g. when the given instance or the whole group is deleted. Default value: "NEVER" Possible values: ["NEVER", "ON_PERMANENT_INSTANCE_DELETION"]
     */
    autoDelete?: string;
    interfaceName: string;
    /**
     * Ip address representation
     */
    ipAddress?: outputs.ComputeRegionPerInstanceConfigPreservedStateInternalIpIpAddress;
}

export interface ComputeRegionPerInstanceConfigPreservedStateInternalIpIpAddress {
    /**
     * The URL of the reservation for this IP address.
     */
    address?: string;
}

export interface ComputeRegionPerInstanceConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeRegionSslCertificateTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeRegionSslPolicyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeRegionTargetHttpProxyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeRegionTargetHttpsProxyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeRegionTargetTcpProxyTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeRegionUrlMapDefaultRouteAction {
    /**
     * The specification for allowing client side cross-origin requests. Please see
     * [W3C Recommendation for Cross Origin Resource Sharing](https://www.w3.org/TR/cors/)
     */
    corsPolicy?: outputs.ComputeRegionUrlMapDefaultRouteActionCorsPolicy;
    /**
     * The specification for fault injection introduced into traffic to test the resiliency of clients to backend service failure.
     * As part of fault injection, when clients send requests to a backend service, delays can be introduced by a load balancer on a percentage of requests before sending those requests to the backend service.
     * Similarly requests from clients can be aborted by the load balancer for a percentage of requests.
     * timeout and retryPolicy is ignored by clients that are configured with a faultInjectionPolicy if: 1. The traffic is generated by fault injection AND 2. The fault injection is not a delay fault injection.
     * Fault injection is not supported with the global external HTTP(S) load balancer (classic). To see which load balancers support fault injection, see Load balancing: [Routing and traffic management features](https://cloud.google.com/load-balancing/docs/features#routing-traffic-management).
     */
    faultInjectionPolicy?: outputs.ComputeRegionUrlMapDefaultRouteActionFaultInjectionPolicy;
    /**
     * Specifies the policy on how requests intended for the route's backends are shadowed to a separate mirrored backend service.
     * The load balancer does not wait for responses from the shadow service. Before sending traffic to the shadow service, the host / authority header is suffixed with -shadow.
     * Not supported when the URL map is bound to a target gRPC proxy that has the validateForProxyless field set to true.
     */
    requestMirrorPolicy?: outputs.ComputeRegionUrlMapDefaultRouteActionRequestMirrorPolicy;
    /**
     * Specifies the retry policy associated with this route.
     */
    retryPolicy?: outputs.ComputeRegionUrlMapDefaultRouteActionRetryPolicy;
    /**
     * Specifies the timeout for the selected route. Timeout is computed from the time the request has been fully processed (known as end-of-stream) up until the response has been processed. Timeout includes all retries.
     * If not specified, this field uses the largest timeout among all backend services associated with the route.
     * Not supported when the URL map is bound to a target gRPC proxy that has validateForProxyless field set to true.
     */
    timeout?: outputs.ComputeRegionUrlMapDefaultRouteActionTimeout;
    /**
     * The spec to modify the URL of the request, before forwarding the request to the matched service.
     * urlRewrite is the only action supported in UrlMaps for external HTTP(S) load balancers.
     * Not supported when the URL map is bound to a target gRPC proxy that has the validateForProxyless field set to true.
     */
    urlRewrite?: outputs.ComputeRegionUrlMapDefaultRouteActionUrlRewrite;
    /**
     * A list of weighted backend services to send traffic to when a route match occurs. The weights determine the fraction of traffic that flows to their corresponding backend service. If all traffic needs to go to a single backend service, there must be one weightedBackendService with weight set to a non-zero number.
     * After a backend service is identified and before forwarding the request to the backend service, advanced routing actions such as URL rewrites and header transformations are applied depending on additional settings specified in this HttpRouteAction.
     */
    weightedBackendServices?: outputs.ComputeRegionUrlMapDefaultRouteActionWeightedBackendService[];
}

export interface ComputeRegionUrlMapDefaultRouteActionCorsPolicy {
    /**
     * In response to a preflight request, setting this to true indicates that the actual request can include user credentials. This field translates to the Access-Control-Allow-Credentials header.
     * Default is false.
     */
    allowCredentials?: boolean;
    /**
     * Specifies the content for the Access-Control-Allow-Headers header.
     */
    allowHeaders?: string[];
    /**
     * Specifies the content for the Access-Control-Allow-Methods header.
     */
    allowMethods?: string[];
    /**
     * Specifies the regualar expression patterns that match allowed origins. For regular expression grammar
     * please see en.cppreference.com/w/cpp/regex/ecmascript
     * An origin is allowed if it matches either an item in allowOrigins or an item in allowOriginRegexes.
     */
    allowOriginRegexes?: string[];
    /**
     * Specifies the list of origins that will be allowed to do CORS requests.
     * An origin is allowed if it matches either an item in allowOrigins or an item in allowOriginRegexes.
     */
    allowOrigins?: string[];
    /**
     * If true, the setting specifies the CORS policy is disabled. The default value of false, which indicates that the CORS policy is in effect.
     */
    disabled?: boolean;
    /**
     * Specifies the content for the Access-Control-Expose-Headers header.
     */
    exposeHeaders?: string[];
    /**
     * Specifies how long results of a preflight request can be cached in seconds.
     * This translates to the Access-Control-Max-Age header.
     */
    maxAge?: number;
}

export interface ComputeRegionUrlMapDefaultRouteActionFaultInjectionPolicy {
    /**
     * The specification for how client requests are aborted as part of fault injection.
     */
    abort?: outputs.ComputeRegionUrlMapDefaultRouteActionFaultInjectionPolicyAbort;
    /**
     * The specification for how client requests are delayed as part of fault injection, before being sent to a backend service.
     */
    delay?: outputs.ComputeRegionUrlMapDefaultRouteActionFaultInjectionPolicyDelay;
}

export interface ComputeRegionUrlMapDefaultRouteActionFaultInjectionPolicyAbort {
    /**
     * The HTTP status code used to abort the request.
     * The value must be between 200 and 599 inclusive.
     */
    httpStatus?: number;
    /**
     * The percentage of traffic (connections/operations/requests) which will be aborted as part of fault injection.
     * The value must be between 0.0 and 100.0 inclusive.
     */
    percentage?: number;
}

export interface ComputeRegionUrlMapDefaultRouteActionFaultInjectionPolicyDelay {
    /**
     * Specifies the value of the fixed delay interval.
     */
    fixedDelay?: outputs.ComputeRegionUrlMapDefaultRouteActionFaultInjectionPolicyDelayFixedDelay;
    /**
     * The percentage of traffic (connections/operations/requests) on which delay will be introduced as part of fault injection.
     * The value must be between 0.0 and 100.0 inclusive.
     */
    percentage?: number;
}

export interface ComputeRegionUrlMapDefaultRouteActionFaultInjectionPolicyDelayFixedDelay {
    /**
     * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are
     * represented with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive.
     * Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
     */
    seconds?: string;
}

export interface ComputeRegionUrlMapDefaultRouteActionRequestMirrorPolicy {
    /**
     * The full or partial URL to the RegionBackendService resource being mirrored to.
     * The backend service configured for a mirroring policy must reference backends that are of the same type as the original backend service matched in the URL map.
     * Serverless NEG backends are not currently supported as a mirrored backend service.
     */
    backendService?: string;
}

export interface ComputeRegionUrlMapDefaultRouteActionRetryPolicy {
    /**
     * Specifies the allowed number retries. This number must be > 0. If not specified, defaults to 1.
     */
    numRetries?: number;
    /**
     * Specifies a non-zero timeout per retry attempt.
     *
     * If not specified, will use the timeout set in HttpRouteAction. If timeout in HttpRouteAction is not set,
     * will use the largest timeout among all backend services associated with the route.
     */
    perTryTimeout?: outputs.ComputeRegionUrlMapDefaultRouteActionRetryPolicyPerTryTimeout;
    /**
     * Specifies one or more conditions when this retry policy applies.
     * Valid values are listed below. Only the following codes are supported when the URL map is bound to target gRPC proxy that has validateForProxyless field set to true: cancelled, deadline-exceeded, internal, resource-exhausted, unavailable.
     *   - 5xx : retry is attempted if the instance or endpoint responds with any 5xx response code, or if the instance or endpoint does not respond at all. For example, disconnects, reset, read timeout, connection failure, and refused streams.
     *   - gateway-error : Similar to 5xx, but only applies to response codes 502, 503 or 504.
     *   - connect-failure : a retry is attempted on failures connecting to the instance or endpoint. For example, connection timeouts.
     *   - retriable-4xx : a retry is attempted if the instance or endpoint responds with a 4xx response code. The only error that you can retry is error code 409.
     *   - refused-stream : a retry is attempted if the instance or endpoint resets the stream with a REFUSED_STREAM error code. This reset type indicates that it is safe to retry.
     *   - cancelled : a retry is attempted if the gRPC status code in the response header is set to cancelled.
     *   - deadline-exceeded : a retry is attempted if the gRPC status code in the response header is set to deadline-exceeded.
     *   - internal :  a retry is attempted if the gRPC status code in the response header is set to internal.
     *   - resource-exhausted : a retry is attempted if the gRPC status code in the response header is set to resource-exhausted.
     *   - unavailable : a retry is attempted if the gRPC status code in the response header is set to unavailable.
     */
    retryConditions?: string[];
}

export interface ComputeRegionUrlMapDefaultRouteActionRetryPolicyPerTryTimeout {
    /**
     * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are
     * represented with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive.
     * Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
     */
    seconds?: string;
}

export interface ComputeRegionUrlMapDefaultRouteActionTimeout {
    /**
     * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are represented with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive. Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
     */
    seconds?: string;
}

export interface ComputeRegionUrlMapDefaultRouteActionUrlRewrite {
    /**
     * Before forwarding the request to the selected service, the request's host header is replaced with contents of hostRewrite.
     * The value must be from 1 to 255 characters.
     */
    hostRewrite?: string;
    /**
     * Before forwarding the request to the selected backend service, the matching portion of the request's path is replaced by pathPrefixRewrite.
     * The value must be from 1 to 1024 characters.
     */
    pathPrefixRewrite?: string;
}

export interface ComputeRegionUrlMapDefaultRouteActionWeightedBackendService {
    /**
     * The full or partial URL to the default BackendService resource. Before forwarding the request to backendService, the load balancer applies any relevant headerActions specified as part of this backendServiceWeight.
     */
    backendService?: string;
    /**
     * Specifies changes to request and response headers that need to take effect for the selected backendService.
     * headerAction specified here take effect before headerAction in the enclosing HttpRouteRule, PathMatcher and UrlMap.
     * headerAction is not supported for load balancers that have their loadBalancingScheme set to EXTERNAL.
     * Not supported when the URL map is bound to a target gRPC proxy that has validateForProxyless field set to true.
     */
    headerAction?: outputs.ComputeRegionUrlMapDefaultRouteActionWeightedBackendServiceHeaderAction;
    /**
     * Specifies the fraction of traffic sent to a backend service, computed as weight / (sum of all weightedBackendService weights in routeAction) .
     * The selection of a backend service is determined only for new traffic. Once a user's request has been directed to a backend service, subsequent requests are sent to the same backend service as determined by the backend service's session affinity policy.
     * The value must be from 0 to 1000.
     */
    weight?: number;
}

export interface ComputeRegionUrlMapDefaultRouteActionWeightedBackendServiceHeaderAction {
    /**
     * Headers to add to a matching request before forwarding the request to the backendService.
     */
    requestHeadersToAdds?: outputs.ComputeRegionUrlMapDefaultRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd[];
    /**
     * A list of header names for headers that need to be removed from the request before forwarding the request to the backendService.
     */
    requestHeadersToRemoves?: string[];
    /**
     * Headers to add the response before sending the response back to the client.
     */
    responseHeadersToAdds?: outputs.ComputeRegionUrlMapDefaultRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd[];
    /**
     * A list of header names for headers that need to be removed from the response before sending the response back to the client.
     */
    responseHeadersToRemoves?: string[];
}

export interface ComputeRegionUrlMapDefaultRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd {
    /**
     * The name of the header.
     */
    headerName?: string;
    /**
     * The value of the header to add.
     */
    headerValue?: string;
    /**
     * If false, headerValue is appended to any values that already exist for the header. If true, headerValue is set for the header, discarding any values that were set for that header.
     * The default value is false.
     */
    replace?: boolean;
}

export interface ComputeRegionUrlMapDefaultRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd {
    /**
     * The name of the header.
     */
    headerName?: string;
    /**
     * The value of the header to add.
     */
    headerValue?: string;
    /**
     * If false, headerValue is appended to any values that already exist for the header. If true, headerValue is set for the header, discarding any values that were set for that header.
     * The default value is false.
     */
    replace?: boolean;
}

export interface ComputeRegionUrlMapDefaultUrlRedirect {
    /**
     * The host that will be used in the redirect response instead of the one that was
     * supplied in the request. The value must be between 1 and 255 characters.
     */
    hostRedirect?: string;
    /**
     * If set to true, the URL scheme in the redirected request is set to https. If set to
     * false, the URL scheme of the redirected request will remain the same as that of the
     * request. This must only be set for UrlMaps used in TargetHttpProxys. Setting this
     * true for TargetHttpsProxy is not permitted. The default is set to false.
     */
    httpsRedirect?: boolean;
    /**
     * The path that will be used in the redirect response instead of the one that was
     * supplied in the request. pathRedirect cannot be supplied together with
     * prefixRedirect. Supply one alone or neither. If neither is supplied, the path of the
     * original request will be used for the redirect. The value must be between 1 and 1024
     * characters.
     */
    pathRedirect?: string;
    /**
     * The prefix that replaces the prefixMatch specified in the HttpRouteRuleMatch,
     * retaining the remaining portion of the URL before redirecting the request.
     * prefixRedirect cannot be supplied together with pathRedirect. Supply one alone or
     * neither. If neither is supplied, the path of the original request will be used for
     * the redirect. The value must be between 1 and 1024 characters.
     */
    prefixRedirect?: string;
    /**
     * The HTTP Status code to use for this RedirectAction. Supported values are:
     *
     * * MOVED_PERMANENTLY_DEFAULT, which is the default value and corresponds to 301.
     *
     * * FOUND, which corresponds to 302.
     *
     * * SEE_OTHER which corresponds to 303.
     *
     * * TEMPORARY_REDIRECT, which corresponds to 307. In this case, the request method
     * will be retained.
     *
     * * PERMANENT_REDIRECT, which corresponds to 308. In this case,
     * the request method will be retained. Possible values: ["FOUND", "MOVED_PERMANENTLY_DEFAULT", "PERMANENT_REDIRECT", "SEE_OTHER", "TEMPORARY_REDIRECT"]
     */
    redirectResponseCode?: string;
    /**
     * If set to true, any accompanying query portion of the original URL is removed prior
     * to redirecting the request. If set to false, the query portion of the original URL is
     * retained.
     *  This field is required to ensure an empty block is not set. The normal default value is false.
     */
    stripQuery: boolean;
}

export interface ComputeRegionUrlMapHostRule {
    /**
     * An optional description of this HostRule. Provide this property
     * when you create the resource.
     */
    description?: string;
    /**
     * The list of host patterns to match. They must be valid
     * hostnames, except * will match any string of ([a-z0-9-.]*). In
     * that case, * must be the first character and must be followed in
     * the pattern by either - or ..
     */
    hosts: string[];
    /**
     * The name of the PathMatcher to use to match the path portion of
     * the URL if the hostRule matches the URL's host portion.
     */
    pathMatcher: string;
}

export interface ComputeRegionUrlMapPathMatcher {
    /**
     * A reference to a RegionBackendService resource. This will be used if
     * none of the pathRules defined by this PathMatcher is matched by
     * the URL's path portion.
     */
    defaultService?: string;
    /**
     * When none of the specified hostRules match, the request is redirected to a URL specified
     * by defaultUrlRedirect. If defaultUrlRedirect is specified, defaultService or
     * defaultRouteAction must not be set.
     */
    defaultUrlRedirect?: outputs.ComputeRegionUrlMapPathMatcherDefaultUrlRedirect;
    /**
     * An optional description of this resource.
     */
    description?: string;
    /**
     * The name to which this PathMatcher is referred by the HostRule.
     */
    name: string;
    /**
     * The list of path rules. Use this list instead of routeRules when routing based
     * on simple path matching is all that's required. The order by which path rules
     * are specified does not matter. Matches are always done on the longest-path-first
     * basis. For example: a pathRule with a path /a/b/c/* will match before /a/b/*
     * irrespective of the order in which those paths appear in this list. Within a
     * given pathMatcher, only one of pathRules or routeRules must be set.
     */
    pathRules?: outputs.ComputeRegionUrlMapPathMatcherPathRule[];
    /**
     * The list of ordered HTTP route rules. Use this list instead of pathRules when
     * advanced route matching and routing actions are desired. The order of specifying
     * routeRules matters: the first rule that matches will cause its specified routing
     * action to take effect. Within a given pathMatcher, only one of pathRules or
     * routeRules must be set. routeRules are not supported in UrlMaps intended for
     * External load balancers.
     */
    routeRules?: outputs.ComputeRegionUrlMapPathMatcherRouteRule[];
}

export interface ComputeRegionUrlMapPathMatcherDefaultUrlRedirect {
    /**
     * The host that will be used in the redirect response instead of the one that was
     * supplied in the request. The value must be between 1 and 255 characters.
     */
    hostRedirect?: string;
    /**
     * If set to true, the URL scheme in the redirected request is set to https. If set to
     * false, the URL scheme of the redirected request will remain the same as that of the
     * request. This must only be set for UrlMaps used in TargetHttpProxys. Setting this
     * true for TargetHttpsProxy is not permitted. The default is set to false.
     */
    httpsRedirect?: boolean;
    /**
     * The path that will be used in the redirect response instead of the one that was
     * supplied in the request. pathRedirect cannot be supplied together with
     * prefixRedirect. Supply one alone or neither. If neither is supplied, the path of the
     * original request will be used for the redirect. The value must be between 1 and 1024
     * characters.
     */
    pathRedirect?: string;
    /**
     * The prefix that replaces the prefixMatch specified in the HttpRouteRuleMatch,
     * retaining the remaining portion of the URL before redirecting the request.
     * prefixRedirect cannot be supplied together with pathRedirect. Supply one alone or
     * neither. If neither is supplied, the path of the original request will be used for
     * the redirect. The value must be between 1 and 1024 characters.
     */
    prefixRedirect?: string;
    /**
     * The HTTP Status code to use for this RedirectAction. Supported values are:
     *
     * * MOVED_PERMANENTLY_DEFAULT, which is the default value and corresponds to 301.
     *
     * * FOUND, which corresponds to 302.
     *
     * * SEE_OTHER which corresponds to 303.
     *
     * * TEMPORARY_REDIRECT, which corresponds to 307. In this case, the request method
     * will be retained.
     *
     * * PERMANENT_REDIRECT, which corresponds to 308. In this case,
     * the request method will be retained. Possible values: ["FOUND", "MOVED_PERMANENTLY_DEFAULT", "PERMANENT_REDIRECT", "SEE_OTHER", "TEMPORARY_REDIRECT"]
     */
    redirectResponseCode?: string;
    /**
     * If set to true, any accompanying query portion of the original URL is removed prior
     * to redirecting the request. If set to false, the query portion of the original URL is
     * retained.
     *  This field is required to ensure an empty block is not set. The normal default value is false.
     */
    stripQuery: boolean;
}

export interface ComputeRegionUrlMapPathMatcherPathRule {
    /**
     * The list of path patterns to match. Each must start with / and the only place a
     * \* is allowed is at the end following a /. The string fed to the path matcher
     * does not include any text after the first ? or #, and those chars are not
     * allowed here.
     */
    paths: string[];
    /**
     * In response to a matching path, the load balancer performs advanced routing
     * actions like URL rewrites, header transformations, etc. prior to forwarding the
     * request to the selected backend. If routeAction specifies any
     * weightedBackendServices, service must not be set. Conversely if service is set,
     * routeAction cannot contain any  weightedBackendServices. Only one of routeAction
     * or urlRedirect must be set.
     */
    routeAction?: outputs.ComputeRegionUrlMapPathMatcherPathRuleRouteAction;
    /**
     * The region backend service resource to which traffic is
     * directed if this rule is matched. If routeAction is additionally specified,
     * advanced routing actions like URL Rewrites, etc. take effect prior to sending
     * the request to the backend. However, if service is specified, routeAction cannot
     * contain any weightedBackendService s. Conversely, if routeAction specifies any
     * weightedBackendServices, service must not be specified. Only one of urlRedirect,
     * service or routeAction.weightedBackendService must be set.
     */
    service?: string;
    /**
     * When a path pattern is matched, the request is redirected to a URL specified
     * by urlRedirect. If urlRedirect is specified, service or routeAction must not
     * be set.
     */
    urlRedirect?: outputs.ComputeRegionUrlMapPathMatcherPathRuleUrlRedirect;
}

export interface ComputeRegionUrlMapPathMatcherPathRuleRouteAction {
    /**
     * The specification for allowing client side cross-origin requests. Please see W3C
     * Recommendation for Cross Origin Resource Sharing
     */
    corsPolicy?: outputs.ComputeRegionUrlMapPathMatcherPathRuleRouteActionCorsPolicy;
    /**
     * The specification for fault injection introduced into traffic to test the
     * resiliency of clients to backend service failure. As part of fault injection,
     * when clients send requests to a backend service, delays can be introduced by
     * Loadbalancer on a percentage of requests before sending those request to the
     * backend service. Similarly requests from clients can be aborted by the
     * Loadbalancer for a percentage of requests. timeout and retry_policy will be
     * ignored by clients that are configured with a fault_injection_policy.
     */
    faultInjectionPolicy?: outputs.ComputeRegionUrlMapPathMatcherPathRuleRouteActionFaultInjectionPolicy;
    /**
     * Specifies the policy on how requests intended for the route's backends are
     * shadowed to a separate mirrored backend service. Loadbalancer does not wait for
     * responses from the shadow service. Prior to sending traffic to the shadow
     * service, the host / authority header is suffixed with -shadow.
     */
    requestMirrorPolicy?: outputs.ComputeRegionUrlMapPathMatcherPathRuleRouteActionRequestMirrorPolicy;
    /**
     * Specifies the retry policy associated with this route.
     */
    retryPolicy?: outputs.ComputeRegionUrlMapPathMatcherPathRuleRouteActionRetryPolicy;
    /**
     * Specifies the timeout for the selected route. Timeout is computed from the time
     * the request is has been fully processed (i.e. end-of-stream) up until the
     * response has been completely processed. Timeout includes all retries. If not
     * specified, the default value is 15 seconds.
     */
    timeout?: outputs.ComputeRegionUrlMapPathMatcherPathRuleRouteActionTimeout;
    /**
     * The spec to modify the URL of the request, prior to forwarding the request to
     * the matched service
     */
    urlRewrite?: outputs.ComputeRegionUrlMapPathMatcherPathRuleRouteActionUrlRewrite;
    /**
     * A list of weighted backend services to send traffic to when a route match
     * occurs. The weights determine the fraction of traffic that flows to their
     * corresponding backend service. If all traffic needs to go to a single backend
     * service, there must be one  weightedBackendService with weight set to a non 0
     * number. Once a backendService is identified and before forwarding the request to
     * the backend service, advanced routing actions like Url rewrites and header
     * transformations are applied depending on additional settings specified in this
     * HttpRouteAction.
     */
    weightedBackendServices?: outputs.ComputeRegionUrlMapPathMatcherPathRuleRouteActionWeightedBackendService[];
}

export interface ComputeRegionUrlMapPathMatcherPathRuleRouteActionCorsPolicy {
    /**
     * In response to a preflight request, setting this to true indicates that the
     * actual request can include user credentials. This translates to the Access-
     * Control-Allow-Credentials header. Defaults to false.
     */
    allowCredentials?: boolean;
    /**
     * Specifies the content for the Access-Control-Allow-Headers header.
     */
    allowHeaders?: string[];
    /**
     * Specifies the content for the Access-Control-Allow-Methods header.
     */
    allowMethods?: string[];
    /**
     * Specifies the regular expression patterns that match allowed origins. For
     * regular expression grammar please see en.cppreference.com/w/cpp/regex/ecmascript
     * An origin is allowed if it matches either allow_origins or allow_origin_regex.
     */
    allowOriginRegexes?: string[];
    /**
     * Specifies the list of origins that will be allowed to do CORS requests. An
     * origin is allowed if it matches either allow_origins or allow_origin_regex.
     */
    allowOrigins?: string[];
    /**
     * If true, specifies the CORS policy is disabled.
     */
    disabled: boolean;
    /**
     * Specifies the content for the Access-Control-Expose-Headers header.
     */
    exposeHeaders?: string[];
    /**
     * Specifies how long the results of a preflight request can be cached. This
     * translates to the content for the Access-Control-Max-Age header.
     */
    maxAge?: number;
}

export interface ComputeRegionUrlMapPathMatcherPathRuleRouteActionFaultInjectionPolicy {
    /**
     * The specification for how client requests are aborted as part of fault
     * injection.
     */
    abort?: outputs.ComputeRegionUrlMapPathMatcherPathRuleRouteActionFaultInjectionPolicyAbort;
    /**
     * The specification for how client requests are delayed as part of fault
     * injection, before being sent to a backend service.
     */
    delay?: outputs.ComputeRegionUrlMapPathMatcherPathRuleRouteActionFaultInjectionPolicyDelay;
}

export interface ComputeRegionUrlMapPathMatcherPathRuleRouteActionFaultInjectionPolicyAbort {
    /**
     * The HTTP status code used to abort the request. The value must be between 200
     * and 599 inclusive.
     */
    httpStatus: number;
    /**
     * The percentage of traffic (connections/operations/requests) which will be
     * aborted as part of fault injection. The value must be between 0.0 and 100.0
     * inclusive.
     */
    percentage: number;
}

export interface ComputeRegionUrlMapPathMatcherPathRuleRouteActionFaultInjectionPolicyDelay {
    /**
     * Specifies the value of the fixed delay interval.
     */
    fixedDelay: outputs.ComputeRegionUrlMapPathMatcherPathRuleRouteActionFaultInjectionPolicyDelayFixedDelay;
    /**
     * The percentage of traffic (connections/operations/requests) on which delay will
     * be introduced as part of fault injection. The value must be between 0.0 and
     * 100.0 inclusive.
     */
    percentage: number;
}

export interface ComputeRegionUrlMapPathMatcherPathRuleRouteActionFaultInjectionPolicyDelayFixedDelay {
    /**
     * Span of time that's a fraction of a second at nanosecond resolution. Durations
     * less than one second are represented with a 0 'seconds' field and a positive
     * 'nanos' field. Must be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000
     * inclusive.
     */
    seconds: string;
}

export interface ComputeRegionUrlMapPathMatcherPathRuleRouteActionRequestMirrorPolicy {
    /**
     * The RegionBackendService resource being mirrored to.
     */
    backendService: string;
}

export interface ComputeRegionUrlMapPathMatcherPathRuleRouteActionRetryPolicy {
    /**
     * Specifies the allowed number retries. This number must be > 0.
     */
    numRetries?: number;
    /**
     * Specifies a non-zero timeout per retry attempt.
     */
    perTryTimeout?: outputs.ComputeRegionUrlMapPathMatcherPathRuleRouteActionRetryPolicyPerTryTimeout;
    /**
     * Specifies one or more conditions when this retry rule applies. Valid values are:
     *
     * - 5xx: Loadbalancer will attempt a retry if the backend service responds with
     * any 5xx response code, or if the backend service does not respond at all,
     * for example: disconnects, reset, read timeout, connection failure, and refused
     * streams.
     * - gateway-error: Similar to 5xx, but only applies to response codes
     * 502, 503 or 504.
     * - connect-failure: Loadbalancer will retry on failures
     * connecting to backend services, for example due to connection timeouts.
     * - retriable-4xx: Loadbalancer will retry for retriable 4xx response codes.
     * Currently the only retriable error supported is 409.
     * - refused-stream: Loadbalancer will retry if the backend service resets the stream with a
     * REFUSED_STREAM error code. This reset type indicates that it is safe to retry.
     * - cancelled: Loadbalancer will retry if the gRPC status code in the response
     * header is set to cancelled
     * - deadline-exceeded: Loadbalancer will retry if the
     * gRPC status code in the response header is set to deadline-exceeded
     * - resource-exhausted: Loadbalancer will retry if the gRPC status code in the response
     * header is set to resource-exhausted
     * - unavailable: Loadbalancer will retry if
     * the gRPC status code in the response header is set to unavailable
     */
    retryConditions?: string[];
}

export interface ComputeRegionUrlMapPathMatcherPathRuleRouteActionRetryPolicyPerTryTimeout {
    /**
     * Span of time that's a fraction of a second at nanosecond resolution. Durations
     * less than one second are represented with a 0 'seconds' field and a positive
     * 'nanos' field. Must be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000
     * inclusive.
     */
    seconds: string;
}

export interface ComputeRegionUrlMapPathMatcherPathRuleRouteActionTimeout {
    /**
     * Span of time that's a fraction of a second at nanosecond resolution. Durations
     * less than one second are represented with a 0 'seconds' field and a positive
     * 'nanos' field. Must be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000
     * inclusive.
     */
    seconds: string;
}

export interface ComputeRegionUrlMapPathMatcherPathRuleRouteActionUrlRewrite {
    /**
     * Prior to forwarding the request to the selected service, the request's host
     * header is replaced with contents of hostRewrite. The value must be between 1 and
     * 255 characters.
     */
    hostRewrite?: string;
    /**
     * Prior to forwarding the request to the selected backend service, the matching
     * portion of the request's path is replaced by pathPrefixRewrite. The value must
     * be between 1 and 1024 characters.
     */
    pathPrefixRewrite?: string;
}

export interface ComputeRegionUrlMapPathMatcherPathRuleRouteActionWeightedBackendService {
    /**
     * The default RegionBackendService resource. Before
     * forwarding the request to backendService, the loadbalancer applies any relevant
     * headerActions specified as part of this backendServiceWeight.
     */
    backendService: string;
    /**
     * Specifies changes to request and response headers that need to take effect for
     * the selected backendService. headerAction specified here take effect before
     * headerAction in the enclosing HttpRouteRule, PathMatcher and UrlMap.
     */
    headerAction?: outputs.ComputeRegionUrlMapPathMatcherPathRuleRouteActionWeightedBackendServiceHeaderAction;
    /**
     * Specifies the fraction of traffic sent to backendService, computed as weight /
     * (sum of all weightedBackendService weights in routeAction) . The selection of a
     * backend service is determined only for new traffic. Once a user's request has
     * been directed to a backendService, subsequent requests will be sent to the same
     * backendService as determined by the BackendService's session affinity policy.
     * The value must be between 0 and 1000
     */
    weight: number;
}

export interface ComputeRegionUrlMapPathMatcherPathRuleRouteActionWeightedBackendServiceHeaderAction {
    /**
     * Headers to add to a matching request prior to forwarding the request to the
     * backendService.
     */
    requestHeadersToAdds?: outputs.ComputeRegionUrlMapPathMatcherPathRuleRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd[];
    /**
     * A list of header names for headers that need to be removed from the request
     * prior to forwarding the request to the backendService.
     */
    requestHeadersToRemoves?: string[];
    /**
     * Headers to add the response prior to sending the response back to the client.
     */
    responseHeadersToAdds?: outputs.ComputeRegionUrlMapPathMatcherPathRuleRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd[];
    /**
     * A list of header names for headers that need to be removed from the response
     * prior to sending the response back to the client.
     */
    responseHeadersToRemoves?: string[];
}

export interface ComputeRegionUrlMapPathMatcherPathRuleRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd {
    /**
     * The name of the header.
     */
    headerName: string;
    /**
     * The value of the header to add.
     */
    headerValue: string;
    /**
     * If false, headerValue is appended to any values that already exist for the
     * header. If true, headerValue is set for the header, discarding any values that
     * were set for that header.
     */
    replace: boolean;
}

export interface ComputeRegionUrlMapPathMatcherPathRuleRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd {
    /**
     * The name of the header.
     */
    headerName: string;
    /**
     * The value of the header to add.
     */
    headerValue: string;
    /**
     * If false, headerValue is appended to any values that already exist for the
     * header. If true, headerValue is set for the header, discarding any values that
     * were set for that header.
     */
    replace: boolean;
}

export interface ComputeRegionUrlMapPathMatcherPathRuleUrlRedirect {
    /**
     * The host that will be used in the redirect response instead of the one
     * that was supplied in the request. The value must be between 1 and 255
     * characters.
     */
    hostRedirect?: string;
    /**
     * If set to true, the URL scheme in the redirected request is set to https.
     * If set to false, the URL scheme of the redirected request will remain the
     * same as that of the request. This must only be set for UrlMaps used in
     * TargetHttpProxys. Setting this true for TargetHttpsProxy is not
     * permitted. The default is set to false.
     */
    httpsRedirect?: boolean;
    /**
     * The path that will be used in the redirect response instead of the one
     * that was supplied in the request. pathRedirect cannot be supplied
     * together with prefixRedirect. Supply one alone or neither. If neither is
     * supplied, the path of the original request will be used for the redirect.
     * The value must be between 1 and 1024 characters.
     */
    pathRedirect?: string;
    /**
     * The prefix that replaces the prefixMatch specified in the
     * HttpRouteRuleMatch, retaining the remaining portion of the URL before
     * redirecting the request. prefixRedirect cannot be supplied together with
     * pathRedirect. Supply one alone or neither. If neither is supplied, the
     * path of the original request will be used for the redirect. The value
     * must be between 1 and 1024 characters.
     */
    prefixRedirect?: string;
    /**
     * The HTTP Status code to use for this RedirectAction. Supported values are:
     *
     * * MOVED_PERMANENTLY_DEFAULT, which is the default value and corresponds to 301.
     *
     * * FOUND, which corresponds to 302.
     *
     * * SEE_OTHER which corresponds to 303.
     *
     * * TEMPORARY_REDIRECT, which corresponds to 307. In this case, the request method
     * will be retained.
     *
     * * PERMANENT_REDIRECT, which corresponds to 308. In this case,
     * the request method will be retained. Possible values: ["FOUND", "MOVED_PERMANENTLY_DEFAULT", "PERMANENT_REDIRECT", "SEE_OTHER", "TEMPORARY_REDIRECT"]
     */
    redirectResponseCode?: string;
    /**
     * If set to true, any accompanying query portion of the original URL is removed
     * prior to redirecting the request. If set to false, the query portion of the
     * original URL is retained.
     *  This field is required to ensure an empty block is not set. The normal default value is false.
     */
    stripQuery: boolean;
}

export interface ComputeRegionUrlMapPathMatcherRouteRule {
    /**
     * Specifies changes to request and response headers that need to take effect for
     * the selected backendService. The headerAction specified here are applied before
     * the matching pathMatchers[].headerAction and after pathMatchers[].routeRules[].r
     * outeAction.weightedBackendService.backendServiceWeightAction[].headerAction
     */
    headerAction?: outputs.ComputeRegionUrlMapPathMatcherRouteRuleHeaderAction;
    /**
     * The rules for determining a match.
     */
    matchRules?: outputs.ComputeRegionUrlMapPathMatcherRouteRuleMatchRule[];
    /**
     * For routeRules within a given pathMatcher, priority determines the order
     * in which load balancer will interpret routeRules. RouteRules are evaluated
     * in order of priority, from the lowest to highest number. The priority of
     * a rule decreases as its number increases (1, 2, 3, N+1). The first rule
     * that matches the request is applied.
     *
     * You cannot configure two or more routeRules with the same priority.
     * Priority for each rule must be set to a number between 0 and
     * 2147483647 inclusive.
     *
     * Priority numbers can have gaps, which enable you to add or remove rules
     * in the future without affecting the rest of the rules. For example,
     * 1, 2, 3, 4, 5, 9, 12, 16 is a valid series of priority numbers to which
     * you could add rules numbered from 6 to 8, 10 to 11, and 13 to 15 in the
     * future without any impact on existing rules.
     */
    priority: number;
    /**
     * In response to a matching matchRule, the load balancer performs advanced routing
     * actions like URL rewrites, header transformations, etc. prior to forwarding the
     * request to the selected backend. If  routeAction specifies any
     * weightedBackendServices, service must not be set. Conversely if service is set,
     * routeAction cannot contain any  weightedBackendServices. Only one of routeAction
     * or urlRedirect must be set.
     */
    routeAction?: outputs.ComputeRegionUrlMapPathMatcherRouteRuleRouteAction;
    /**
     * The region backend service resource to which traffic is
     * directed if this rule is matched. If routeAction is additionally specified,
     * advanced routing actions like URL Rewrites, etc. take effect prior to sending
     * the request to the backend. However, if service is specified, routeAction cannot
     * contain any weightedBackendService s. Conversely, if routeAction specifies any
     * weightedBackendServices, service must not be specified. Only one of urlRedirect,
     * service or routeAction.weightedBackendService must be set.
     */
    service?: string;
    /**
     * When this rule is matched, the request is redirected to a URL specified by
     * urlRedirect. If urlRedirect is specified, service or routeAction must not be
     * set.
     */
    urlRedirect?: outputs.ComputeRegionUrlMapPathMatcherRouteRuleUrlRedirect;
}

export interface ComputeRegionUrlMapPathMatcherRouteRuleHeaderAction {
    /**
     * Headers to add to a matching request prior to forwarding the request to the
     * backendService.
     */
    requestHeadersToAdds?: outputs.ComputeRegionUrlMapPathMatcherRouteRuleHeaderActionRequestHeadersToAdd[];
    /**
     * A list of header names for headers that need to be removed from the request
     * prior to forwarding the request to the backendService.
     */
    requestHeadersToRemoves?: string[];
    /**
     * Headers to add the response prior to sending the response back to the client.
     */
    responseHeadersToAdds?: outputs.ComputeRegionUrlMapPathMatcherRouteRuleHeaderActionResponseHeadersToAdd[];
    /**
     * A list of header names for headers that need to be removed from the response
     * prior to sending the response back to the client.
     */
    responseHeadersToRemoves?: string[];
}

export interface ComputeRegionUrlMapPathMatcherRouteRuleHeaderActionRequestHeadersToAdd {
    /**
     * The name of the header.
     */
    headerName: string;
    /**
     * The value of the header to add.
     */
    headerValue: string;
    /**
     * If false, headerValue is appended to any values that already exist for the
     * header. If true, headerValue is set for the header, discarding any values that
     * were set for that header.
     */
    replace: boolean;
}

export interface ComputeRegionUrlMapPathMatcherRouteRuleHeaderActionResponseHeadersToAdd {
    /**
     * The name of the header.
     */
    headerName: string;
    /**
     * The value of the header to add.
     */
    headerValue: string;
    /**
     * If false, headerValue is appended to any values that already exist for the
     * header. If true, headerValue is set for the header, discarding any values that
     * were set for that header.
     */
    replace: boolean;
}

export interface ComputeRegionUrlMapPathMatcherRouteRuleMatchRule {
    /**
     * For satisfying the matchRule condition, the path of the request must exactly
     * match the value specified in fullPathMatch after removing any query parameters
     * and anchor that may be part of the original URL. FullPathMatch must be between 1
     * and 1024 characters. Only one of prefixMatch, fullPathMatch or regexMatch must
     * be specified.
     */
    fullPathMatch?: string;
    /**
     * Specifies a list of header match criteria, all of which must match corresponding
     * headers in the request.
     */
    headerMatches?: outputs.ComputeRegionUrlMapPathMatcherRouteRuleMatchRuleHeaderMatch[];
    /**
     * Specifies that prefixMatch and fullPathMatch matches are case sensitive.
     * Defaults to false.
     */
    ignoreCase?: boolean;
    /**
     * Opaque filter criteria used by Loadbalancer to restrict routing configuration to
     * a limited set xDS compliant clients. In their xDS requests to Loadbalancer, xDS
     * clients present node metadata. If a match takes place, the relevant routing
     * configuration is made available to those proxies. For each metadataFilter in
     * this list, if its filterMatchCriteria is set to MATCH_ANY, at least one of the
     * filterLabels must match the corresponding label provided in the metadata. If its
     * filterMatchCriteria is set to MATCH_ALL, then all of its filterLabels must match
     * with corresponding labels in the provided metadata. metadataFilters specified
     * here can be overrides those specified in ForwardingRule that refers to this
     * UrlMap. metadataFilters only applies to Loadbalancers that have their
     * loadBalancingScheme set to INTERNAL_SELF_MANAGED.
     */
    metadataFilters?: outputs.ComputeRegionUrlMapPathMatcherRouteRuleMatchRuleMetadataFilter[];
    /**
     * For satisfying the matchRule condition, the path of the request
     * must match the wildcard pattern specified in pathTemplateMatch
     * after removing any query parameters and anchor that may be part
     * of the original URL.
     *
     * pathTemplateMatch must be between 1 and 255 characters
     * (inclusive).  The pattern specified by pathTemplateMatch may
     * have at most 5 wildcard operators and at most 5 variable
     * captures in total.
     */
    pathTemplateMatch?: string;
    /**
     * For satisfying the matchRule condition, the request's path must begin with the
     * specified prefixMatch. prefixMatch must begin with a /. The value must be
     * between 1 and 1024 characters. Only one of prefixMatch, fullPathMatch or
     * regexMatch must be specified.
     */
    prefixMatch?: string;
    /**
     * Specifies a list of query parameter match criteria, all of which must match
     * corresponding query parameters in the request.
     */
    queryParameterMatches?: outputs.ComputeRegionUrlMapPathMatcherRouteRuleMatchRuleQueryParameterMatch[];
    /**
     * For satisfying the matchRule condition, the path of the request must satisfy the
     * regular expression specified in regexMatch after removing any query parameters
     * and anchor supplied with the original URL. For regular expression grammar please
     * see en.cppreference.com/w/cpp/regex/ecmascript  Only one of prefixMatch,
     * fullPathMatch or regexMatch must be specified.
     */
    regexMatch?: string;
}

export interface ComputeRegionUrlMapPathMatcherRouteRuleMatchRuleHeaderMatch {
    /**
     * The value should exactly match contents of exactMatch. Only one of exactMatch,
     * prefixMatch, suffixMatch, regexMatch, presentMatch or rangeMatch must be set.
     */
    exactMatch?: string;
    /**
     * The name of the HTTP header to match. For matching against the HTTP request's
     * authority, use a headerMatch with the header name ":authority". For matching a
     * request's method, use the headerName ":method".
     */
    headerName: string;
    /**
     * If set to false, the headerMatch is considered a match if the match criteria
     * above are met. If set to true, the headerMatch is considered a match if the
     * match criteria above are NOT met. Defaults to false.
     */
    invertMatch?: boolean;
    /**
     * The value of the header must start with the contents of prefixMatch. Only one of
     * exactMatch, prefixMatch, suffixMatch, regexMatch, presentMatch or rangeMatch
     * must be set.
     */
    prefixMatch?: string;
    /**
     * A header with the contents of headerName must exist. The match takes place
     * whether or not the request's header has a value or not. Only one of exactMatch,
     * prefixMatch, suffixMatch, regexMatch, presentMatch or rangeMatch must be set.
     */
    presentMatch?: boolean;
    /**
     * The header value must be an integer and its value must be in the range specified
     * in rangeMatch. If the header does not contain an integer, number or is empty,
     * the match fails. For example for a range [-5, 0]
     *
     * * -3 will match
     * * 0 will not match
     * * 0.25 will not match
     * * -3someString will not match.
     *
     * Only one of exactMatch, prefixMatch, suffixMatch, regexMatch, presentMatch or
     * rangeMatch must be set.
     */
    rangeMatch?: outputs.ComputeRegionUrlMapPathMatcherRouteRuleMatchRuleHeaderMatchRangeMatch;
    /**
     * The value of the header must match the regular expression specified in
     * regexMatch. For regular expression grammar, please see:
     * en.cppreference.com/w/cpp/regex/ecmascript  For matching against a port
     * specified in the HTTP request, use a headerMatch with headerName set to PORT and
     * a regular expression that satisfies the RFC2616 Host header's port specifier.
     * Only one of exactMatch, prefixMatch, suffixMatch, regexMatch, presentMatch or
     * rangeMatch must be set.
     */
    regexMatch?: string;
    /**
     * The value of the header must end with the contents of suffixMatch. Only one of
     * exactMatch, prefixMatch, suffixMatch, regexMatch, presentMatch or rangeMatch
     * must be set.
     */
    suffixMatch?: string;
}

export interface ComputeRegionUrlMapPathMatcherRouteRuleMatchRuleHeaderMatchRangeMatch {
    /**
     * The end of the range (exclusive).
     */
    rangeEnd: number;
    /**
     * The start of the range (inclusive).
     */
    rangeStart: number;
}

export interface ComputeRegionUrlMapPathMatcherRouteRuleMatchRuleMetadataFilter {
    /**
     * The list of label value pairs that must match labels in the provided metadata
     * based on filterMatchCriteria  This list must not be empty and can have at the
     * most 64 entries.
     */
    filterLabels: outputs.ComputeRegionUrlMapPathMatcherRouteRuleMatchRuleMetadataFilterFilterLabel[];
    /**
     * Specifies how individual filterLabel matches within the list of filterLabels
     * contribute towards the overall metadataFilter match. Supported values are:
     *
     * * MATCH_ANY: At least one of the filterLabels must have a matching label in the
     * provided metadata.
     * * MATCH_ALL: All filterLabels must have matching labels in
     * the provided metadata. Possible values: ["MATCH_ALL", "MATCH_ANY"]
     */
    filterMatchCriteria: string;
}

export interface ComputeRegionUrlMapPathMatcherRouteRuleMatchRuleMetadataFilterFilterLabel {
    /**
     * Name of metadata label. The name can have a maximum length of 1024 characters
     * and must be at least 1 character long.
     */
    name: string;
    /**
     * The value of the label must match the specified value. value can have a maximum
     * length of 1024 characters.
     */
    value: string;
}

export interface ComputeRegionUrlMapPathMatcherRouteRuleMatchRuleQueryParameterMatch {
    /**
     * The queryParameterMatch matches if the value of the parameter exactly matches
     * the contents of exactMatch. Only one of presentMatch, exactMatch and regexMatch
     * must be set.
     */
    exactMatch?: string;
    /**
     * The name of the query parameter to match. The query parameter must exist in the
     * request, in the absence of which the request match fails.
     */
    name: string;
    /**
     * Specifies that the queryParameterMatch matches if the request contains the query
     * parameter, irrespective of whether the parameter has a value or not. Only one of
     * presentMatch, exactMatch and regexMatch must be set.
     */
    presentMatch?: boolean;
    /**
     * The queryParameterMatch matches if the value of the parameter matches the
     * regular expression specified by regexMatch. For the regular expression grammar,
     * please see en.cppreference.com/w/cpp/regex/ecmascript  Only one of presentMatch,
     * exactMatch and regexMatch must be set.
     */
    regexMatch?: string;
}

export interface ComputeRegionUrlMapPathMatcherRouteRuleRouteAction {
    /**
     * The specification for allowing client side cross-origin requests. Please see W3C
     * Recommendation for Cross Origin Resource Sharing
     */
    corsPolicy?: outputs.ComputeRegionUrlMapPathMatcherRouteRuleRouteActionCorsPolicy;
    /**
     * The specification for fault injection introduced into traffic to test the
     * resiliency of clients to backend service failure. As part of fault injection,
     * when clients send requests to a backend service, delays can be introduced by
     * Loadbalancer on a percentage of requests before sending those request to the
     * backend service. Similarly requests from clients can be aborted by the
     * Loadbalancer for a percentage of requests. timeout and retry_policy will be
     * ignored by clients that are configured with a fault_injection_policy.
     */
    faultInjectionPolicy?: outputs.ComputeRegionUrlMapPathMatcherRouteRuleRouteActionFaultInjectionPolicy;
    /**
     * Specifies the policy on how requests intended for the route's backends are
     * shadowed to a separate mirrored backend service. Loadbalancer does not wait for
     * responses from the shadow service. Prior to sending traffic to the shadow
     * service, the host / authority header is suffixed with -shadow.
     */
    requestMirrorPolicy?: outputs.ComputeRegionUrlMapPathMatcherRouteRuleRouteActionRequestMirrorPolicy;
    /**
     * Specifies the retry policy associated with this route.
     */
    retryPolicy?: outputs.ComputeRegionUrlMapPathMatcherRouteRuleRouteActionRetryPolicy;
    /**
     * Specifies the timeout for the selected route. Timeout is computed from the time
     * the request is has been fully processed (i.e. end-of-stream) up until the
     * response has been completely processed. Timeout includes all retries. If not
     * specified, the default value is 15 seconds.
     */
    timeout?: outputs.ComputeRegionUrlMapPathMatcherRouteRuleRouteActionTimeout;
    /**
     * The spec to modify the URL of the request, prior to forwarding the request to
     * the matched service
     */
    urlRewrite?: outputs.ComputeRegionUrlMapPathMatcherRouteRuleRouteActionUrlRewrite;
    /**
     * A list of weighted backend services to send traffic to when a route match
     * occurs. The weights determine the fraction of traffic that flows to their
     * corresponding backend service. If all traffic needs to go to a single backend
     * service, there must be one  weightedBackendService with weight set to a non 0
     * number. Once a backendService is identified and before forwarding the request to
     * the backend service, advanced routing actions like Url rewrites and header
     * transformations are applied depending on additional settings specified in this
     * HttpRouteAction.
     */
    weightedBackendServices?: outputs.ComputeRegionUrlMapPathMatcherRouteRuleRouteActionWeightedBackendService[];
}

export interface ComputeRegionUrlMapPathMatcherRouteRuleRouteActionCorsPolicy {
    /**
     * In response to a preflight request, setting this to true indicates that the
     * actual request can include user credentials. This translates to the Access-
     * Control-Allow-Credentials header. Defaults to false.
     */
    allowCredentials?: boolean;
    /**
     * Specifies the content for the Access-Control-Allow-Headers header.
     */
    allowHeaders?: string[];
    /**
     * Specifies the content for the Access-Control-Allow-Methods header.
     */
    allowMethods?: string[];
    /**
     * Specifies the regular expression patterns that match allowed origins. For
     * regular expression grammar please see en.cppreference.com/w/cpp/regex/ecmascript
     * An origin is allowed if it matches either allow_origins or allow_origin_regex.
     */
    allowOriginRegexes?: string[];
    /**
     * Specifies the list of origins that will be allowed to do CORS requests. An
     * origin is allowed if it matches either allow_origins or allow_origin_regex.
     */
    allowOrigins?: string[];
    /**
     * If true, specifies the CORS policy is disabled.
     * which indicates that the CORS policy is in effect. Defaults to false.
     */
    disabled?: boolean;
    /**
     * Specifies the content for the Access-Control-Expose-Headers header.
     */
    exposeHeaders?: string[];
    /**
     * Specifies how long the results of a preflight request can be cached. This
     * translates to the content for the Access-Control-Max-Age header.
     */
    maxAge?: number;
}

export interface ComputeRegionUrlMapPathMatcherRouteRuleRouteActionFaultInjectionPolicy {
    /**
     * The specification for how client requests are aborted as part of fault
     * injection.
     */
    abort?: outputs.ComputeRegionUrlMapPathMatcherRouteRuleRouteActionFaultInjectionPolicyAbort;
    /**
     * The specification for how client requests are delayed as part of fault
     * injection, before being sent to a backend service.
     */
    delay?: outputs.ComputeRegionUrlMapPathMatcherRouteRuleRouteActionFaultInjectionPolicyDelay;
}

export interface ComputeRegionUrlMapPathMatcherRouteRuleRouteActionFaultInjectionPolicyAbort {
    /**
     * The HTTP status code used to abort the request. The value must be between 200
     * and 599 inclusive.
     */
    httpStatus?: number;
    /**
     * The percentage of traffic (connections/operations/requests) which will be
     * aborted as part of fault injection. The value must be between 0.0 and 100.0
     * inclusive.
     */
    percentage?: number;
}

export interface ComputeRegionUrlMapPathMatcherRouteRuleRouteActionFaultInjectionPolicyDelay {
    /**
     * Specifies the value of the fixed delay interval.
     */
    fixedDelay?: outputs.ComputeRegionUrlMapPathMatcherRouteRuleRouteActionFaultInjectionPolicyDelayFixedDelay;
    /**
     * The percentage of traffic (connections/operations/requests) on which delay will
     * be introduced as part of fault injection. The value must be between 0.0 and
     * 100.0 inclusive.
     */
    percentage?: number;
}

export interface ComputeRegionUrlMapPathMatcherRouteRuleRouteActionFaultInjectionPolicyDelayFixedDelay {
    /**
     * Span of time that's a fraction of a second at nanosecond resolution. Durations
     * less than one second are represented with a 0 'seconds' field and a positive
     * 'nanos' field. Must be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000
     * inclusive.
     */
    seconds: string;
}

export interface ComputeRegionUrlMapPathMatcherRouteRuleRouteActionRequestMirrorPolicy {
    /**
     * The RegionBackendService resource being mirrored to.
     */
    backendService: string;
}

export interface ComputeRegionUrlMapPathMatcherRouteRuleRouteActionRetryPolicy {
    /**
     * Specifies the allowed number retries. This number must be > 0.
     */
    numRetries: number;
    /**
     * Specifies a non-zero timeout per retry attempt.
     */
    perTryTimeout?: outputs.ComputeRegionUrlMapPathMatcherRouteRuleRouteActionRetryPolicyPerTryTimeout;
    /**
     * Specifies one or more conditions when this retry rule applies. Valid values are:
     *
     * * 5xx: Loadbalancer will attempt a retry if the backend service responds with
     *   any 5xx response code, or if the backend service does not respond at all,
     *   for example: disconnects, reset, read timeout, connection failure, and refused
     *   streams.
     * * gateway-error: Similar to 5xx, but only applies to response codes
     *   502, 503 or 504.
     * * connect-failure: Loadbalancer will retry on failures
     *   connecting to backend services, for example due to connection timeouts.
     * * retriable-4xx: Loadbalancer will retry for retriable 4xx response codes.
     *   Currently the only retriable error supported is 409.
     * * refused-stream: Loadbalancer will retry if the backend service resets the stream with a
     *   REFUSED_STREAM error code. This reset type indicates that it is safe to retry.
     * * cancelled: Loadbalancer will retry if the gRPC status code in the response
     *   header is set to cancelled
     * * deadline-exceeded: Loadbalancer will retry if the
     *   gRPC status code in the response header is set to deadline-exceeded
     * * resource-exhausted: Loadbalancer will retry if the gRPC status code in the response
     *   header is set to resource-exhausted
     * * unavailable: Loadbalancer will retry if the gRPC status code in
     *   the response header is set to unavailable
     */
    retryConditions?: string[];
}

export interface ComputeRegionUrlMapPathMatcherRouteRuleRouteActionRetryPolicyPerTryTimeout {
    /**
     * Span of time that's a fraction of a second at nanosecond resolution. Durations
     * less than one second are represented with a 0 'seconds' field and a positive
     * 'nanos' field. Must be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000
     * inclusive.
     */
    seconds: string;
}

export interface ComputeRegionUrlMapPathMatcherRouteRuleRouteActionTimeout {
    /**
     * Span of time that's a fraction of a second at nanosecond resolution. Durations
     * less than one second are represented with a 0 'seconds' field and a positive
     * 'nanos' field. Must be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000
     * inclusive.
     */
    seconds: string;
}

export interface ComputeRegionUrlMapPathMatcherRouteRuleRouteActionUrlRewrite {
    /**
     * Prior to forwarding the request to the selected service, the request's host
     * header is replaced with contents of hostRewrite. The value must be between 1 and
     * 255 characters.
     */
    hostRewrite?: string;
    /**
     * Prior to forwarding the request to the selected backend service, the matching
     * portion of the request's path is replaced by pathPrefixRewrite. The value must
     * be between 1 and 1024 characters.
     */
    pathPrefixRewrite?: string;
    /**
     * Prior to forwarding the request to the selected origin, if the
     * request matched a pathTemplateMatch, the matching portion of the
     * request's path is replaced re-written using the pattern specified
     * by pathTemplateRewrite.
     *
     * pathTemplateRewrite must be between 1 and 255 characters
     * (inclusive), must start with a '/', and must only use variables
     * captured by the route's pathTemplate matchers.
     *
     * pathTemplateRewrite may only be used when all of a route's
     * MatchRules specify pathTemplate.
     *
     * Only one of pathPrefixRewrite and pathTemplateRewrite may be
     * specified.
     */
    pathTemplateRewrite?: string;
}

export interface ComputeRegionUrlMapPathMatcherRouteRuleRouteActionWeightedBackendService {
    /**
     * The default RegionBackendService resource. Before
     * forwarding the request to backendService, the loadbalancer applies any relevant
     * headerActions specified as part of this backendServiceWeight.
     */
    backendService: string;
    /**
     * Specifies changes to request and response headers that need to take effect for
     * the selected backendService. headerAction specified here take effect before
     * headerAction in the enclosing HttpRouteRule, PathMatcher and UrlMap.
     */
    headerAction?: outputs.ComputeRegionUrlMapPathMatcherRouteRuleRouteActionWeightedBackendServiceHeaderAction;
    /**
     * Specifies the fraction of traffic sent to backendService, computed as weight /
     * (sum of all weightedBackendService weights in routeAction) . The selection of a
     * backend service is determined only for new traffic. Once a user's request has
     * been directed to a backendService, subsequent requests will be sent to the same
     * backendService as determined by the BackendService's session affinity policy.
     * The value must be between 0 and 1000
     */
    weight: number;
}

export interface ComputeRegionUrlMapPathMatcherRouteRuleRouteActionWeightedBackendServiceHeaderAction {
    /**
     * Headers to add to a matching request prior to forwarding the request to the
     * backendService.
     */
    requestHeadersToAdds?: outputs.ComputeRegionUrlMapPathMatcherRouteRuleRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd[];
    /**
     * A list of header names for headers that need to be removed from the request
     * prior to forwarding the request to the backendService.
     */
    requestHeadersToRemoves?: string[];
    /**
     * Headers to add the response prior to sending the response back to the client.
     */
    responseHeadersToAdds?: outputs.ComputeRegionUrlMapPathMatcherRouteRuleRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd[];
    /**
     * A list of header names for headers that need to be removed from the response
     * prior to sending the response back to the client.
     */
    responseHeadersToRemoves?: string[];
}

export interface ComputeRegionUrlMapPathMatcherRouteRuleRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd {
    /**
     * The name of the header.
     */
    headerName: string;
    /**
     * The value of the header to add.
     */
    headerValue: string;
    /**
     * If false, headerValue is appended to any values that already exist for the
     * header. If true, headerValue is set for the header, discarding any values that
     * were set for that header.
     */
    replace: boolean;
}

export interface ComputeRegionUrlMapPathMatcherRouteRuleRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd {
    /**
     * The name of the header.
     */
    headerName: string;
    /**
     * The value of the header to add.
     */
    headerValue: string;
    /**
     * If false, headerValue is appended to any values that already exist for the
     * header. If true, headerValue is set for the header, discarding any values that
     * were set for that header.
     */
    replace: boolean;
}

export interface ComputeRegionUrlMapPathMatcherRouteRuleUrlRedirect {
    /**
     * The host that will be used in the redirect response instead of the one
     * that was supplied in the request. The value must be between 1 and 255
     * characters.
     */
    hostRedirect?: string;
    /**
     * If set to true, the URL scheme in the redirected request is set to https.
     * If set to false, the URL scheme of the redirected request will remain the
     * same as that of the request. This must only be set for UrlMaps used in
     * TargetHttpProxys. Setting this true for TargetHttpsProxy is not
     * permitted. The default is set to false.
     */
    httpsRedirect?: boolean;
    /**
     * The path that will be used in the redirect response instead of the one
     * that was supplied in the request. pathRedirect cannot be supplied
     * together with prefixRedirect. Supply one alone or neither. If neither is
     * supplied, the path of the original request will be used for the redirect.
     * The value must be between 1 and 1024 characters.
     */
    pathRedirect?: string;
    /**
     * The prefix that replaces the prefixMatch specified in the
     * HttpRouteRuleMatch, retaining the remaining portion of the URL before
     * redirecting the request. prefixRedirect cannot be supplied together with
     * pathRedirect. Supply one alone or neither. If neither is supplied, the
     * path of the original request will be used for the redirect. The value
     * must be between 1 and 1024 characters.
     */
    prefixRedirect?: string;
    /**
     * The HTTP Status code to use for this RedirectAction. Supported values are:
     *
     * * MOVED_PERMANENTLY_DEFAULT, which is the default value and corresponds to 301.
     *
     * * FOUND, which corresponds to 302.
     *
     * * SEE_OTHER which corresponds to 303.
     *
     * * TEMPORARY_REDIRECT, which corresponds to 307. In this case, the request method
     * will be retained.
     *
     * * PERMANENT_REDIRECT, which corresponds to 308. In this case,
     * the request method will be retained. Possible values: ["FOUND", "MOVED_PERMANENTLY_DEFAULT", "PERMANENT_REDIRECT", "SEE_OTHER", "TEMPORARY_REDIRECT"]
     */
    redirectResponseCode?: string;
    /**
     * If set to true, any accompanying query portion of the original URL is
     * removed prior to redirecting the request. If set to false, the query
     * portion of the original URL is retained. The default value is false.
     */
    stripQuery?: boolean;
}

export interface ComputeRegionUrlMapTest {
    /**
     * Description of this test case.
     */
    description?: string;
    /**
     * Host portion of the URL.
     */
    host: string;
    /**
     * Path portion of the URL.
     */
    path: string;
    /**
     * A reference to expected RegionBackendService resource the given URL should be mapped to.
     */
    service: string;
}

export interface ComputeRegionUrlMapTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeReservationShareSettings {
    /**
     * A map of project number and project config. This is only valid when shareType's value is SPECIFIC_PROJECTS.
     */
    projectMaps?: outputs.ComputeReservationShareSettingsProjectMap[];
    /**
     * Type of sharing for this shared-reservation Possible values: ["LOCAL", "SPECIFIC_PROJECTS"]
     */
    shareType: string;
}

export interface ComputeReservationShareSettingsProjectMap {
    id: string;
    /**
     * The project id/number, should be same as the key of this project config in the project map.
     */
    projectId?: string;
}

export interface ComputeReservationSpecificReservation {
    /**
     * The number of resources that are allocated.
     */
    count: number;
    /**
     * How many instances are in use.
     */
    inUseCount: number;
    /**
     * The instance properties for the reservation.
     */
    instanceProperties: outputs.ComputeReservationSpecificReservationInstanceProperties;
}

export interface ComputeReservationSpecificReservationInstanceProperties {
    /**
     * Guest accelerator type and count.
     */
    guestAccelerators?: outputs.ComputeReservationSpecificReservationInstancePropertiesGuestAccelerator[];
    /**
     * The amount of local ssd to reserve with each instance. This
     * reserves disks of type 'local-ssd'.
     */
    localSsds?: outputs.ComputeReservationSpecificReservationInstancePropertiesLocalSsd[];
    /**
     * The name of the machine type to reserve.
     */
    machineType: string;
    /**
     * The minimum CPU platform for the reservation. For example,
     * '"Intel Skylake"'. See
     * the CPU platform availability reference](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform#availablezones)
     * for information on available CPU platforms.
     */
    minCpuPlatform: string;
}

export interface ComputeReservationSpecificReservationInstancePropertiesGuestAccelerator {
    /**
     * The number of the guest accelerator cards exposed to
     * this instance.
     */
    acceleratorCount: number;
    /**
     * The full or partial URL of the accelerator type to
     * attach to this instance. For example:
     * 'projects/my-project/zones/us-central1-c/acceleratorTypes/nvidia-tesla-p100'
     *
     * If you are creating an instance template, specify only the accelerator name.
     */
    acceleratorType: string;
}

export interface ComputeReservationSpecificReservationInstancePropertiesLocalSsd {
    /**
     * The size of the disk in base-2 GB.
     */
    diskSizeGb: number;
    /**
     * The disk interface to use for attaching this disk. Default value: "SCSI" Possible values: ["SCSI", "NVME"]
     */
    interface?: string;
}

export interface ComputeReservationTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeResizeRequestRequestedRunDuration {
    /**
     * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are represented with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive. Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
     */
    seconds: string;
}

export interface ComputeResizeRequestStatus {
    errors: outputs.ComputeResizeRequestStatusError[];
    lastAttempts: outputs.ComputeResizeRequestStatusLastAttempt[];
}

export interface ComputeResizeRequestStatusError {
    errors: outputs.ComputeResizeRequestStatusErrorError[];
}

export interface ComputeResizeRequestStatusErrorError {
    code: string;
    errorDetails: outputs.ComputeResizeRequestStatusErrorErrorErrorDetail[];
    location: string;
    message: string;
}

export interface ComputeResizeRequestStatusErrorErrorErrorDetail {
    errorInfos: outputs.ComputeResizeRequestStatusErrorErrorErrorDetailErrorInfo[];
    helps: outputs.ComputeResizeRequestStatusErrorErrorErrorDetailHelp[];
    localizedMessages: outputs.ComputeResizeRequestStatusErrorErrorErrorDetailLocalizedMessage[];
    quotaInfos: outputs.ComputeResizeRequestStatusErrorErrorErrorDetailQuotaInfo[];
}

export interface ComputeResizeRequestStatusErrorErrorErrorDetailErrorInfo {
    domain: string;
    metadatas: {[key: string]: string};
    reason: string;
}

export interface ComputeResizeRequestStatusErrorErrorErrorDetailHelp {
    links: outputs.ComputeResizeRequestStatusErrorErrorErrorDetailHelpLink[];
}

export interface ComputeResizeRequestStatusErrorErrorErrorDetailHelpLink {
    description: string;
    url: string;
}

export interface ComputeResizeRequestStatusErrorErrorErrorDetailLocalizedMessage {
    locale: string;
    message: string;
}

export interface ComputeResizeRequestStatusErrorErrorErrorDetailQuotaInfo {
    dimensions: {[key: string]: string};
    futureLimit: number;
    limit: number;
    limitName: string;
    metricName: string;
    rolloutStatus: string;
}

export interface ComputeResizeRequestStatusLastAttempt {
    errors: outputs.ComputeResizeRequestStatusLastAttemptError[];
}

export interface ComputeResizeRequestStatusLastAttemptError {
    errors: outputs.ComputeResizeRequestStatusLastAttemptErrorError[];
}

export interface ComputeResizeRequestStatusLastAttemptErrorError {
    code: string;
    errorDetails: outputs.ComputeResizeRequestStatusLastAttemptErrorErrorErrorDetail[];
    location: string;
    message: string;
}

export interface ComputeResizeRequestStatusLastAttemptErrorErrorErrorDetail {
    errorInfos: outputs.ComputeResizeRequestStatusLastAttemptErrorErrorErrorDetailErrorInfo[];
    helps: outputs.ComputeResizeRequestStatusLastAttemptErrorErrorErrorDetailHelp[];
    localizedMessages: outputs.ComputeResizeRequestStatusLastAttemptErrorErrorErrorDetailLocalizedMessage[];
    quotaInfos: outputs.ComputeResizeRequestStatusLastAttemptErrorErrorErrorDetailQuotaInfo[];
}

export interface ComputeResizeRequestStatusLastAttemptErrorErrorErrorDetailErrorInfo {
    domain: string;
    metadatas: {[key: string]: string};
    reason: string;
}

export interface ComputeResizeRequestStatusLastAttemptErrorErrorErrorDetailHelp {
    links: outputs.ComputeResizeRequestStatusLastAttemptErrorErrorErrorDetailHelpLink[];
}

export interface ComputeResizeRequestStatusLastAttemptErrorErrorErrorDetailHelpLink {
    description: string;
    url: string;
}

export interface ComputeResizeRequestStatusLastAttemptErrorErrorErrorDetailLocalizedMessage {
    locale: string;
    message: string;
}

export interface ComputeResizeRequestStatusLastAttemptErrorErrorErrorDetailQuotaInfo {
    dimensions: {[key: string]: string};
    futureLimit: number;
    limit: number;
    limitName: string;
    metricName: string;
    rolloutStatus: string;
}

export interface ComputeResizeRequestTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeResourcePolicyDiskConsistencyGroupPolicy {
    /**
     * Enable disk consistency on the resource policy.
     */
    enabled: boolean;
}

export interface ComputeResourcePolicyGroupPlacementPolicy {
    /**
     * The number of availability domains instances will be spread across. If two instances are in different
     * availability domain, they will not be put in the same low latency network
     */
    availabilityDomainCount?: number;
    /**
     * Collocation specifies whether to place VMs inside the same availability domain on the same low-latency network.
     * Specify 'COLLOCATED' to enable collocation. Can only be specified with 'vm_count'. If compute instances are created
     * with a COLLOCATED policy, then exactly 'vm_count' instances must be created at the same time with the resource policy
     * attached. Possible values: ["COLLOCATED"]
     */
    collocation?: string;
    /**
     * Number of VMs in this placement group. Google does not recommend that you use this field
     * unless you use a compact policy and you want your policy to work only if it contains this
     * exact number of VMs.
     */
    vmCount?: number;
}

export interface ComputeResourcePolicyInstanceSchedulePolicy {
    /**
     * The expiration time of the schedule. The timestamp is an RFC3339 string.
     */
    expirationTime?: string;
    /**
     * The start time of the schedule. The timestamp is an RFC3339 string.
     */
    startTime?: string;
    /**
     * Specifies the time zone to be used in interpreting the schedule. The value of this field must be a time zone name
     * from the tz database: http://en.wikipedia.org/wiki/Tz_database.
     */
    timeZone: string;
    /**
     * Specifies the schedule for starting instances.
     */
    vmStartSchedule?: outputs.ComputeResourcePolicyInstanceSchedulePolicyVmStartSchedule;
    /**
     * Specifies the schedule for stopping instances.
     */
    vmStopSchedule?: outputs.ComputeResourcePolicyInstanceSchedulePolicyVmStopSchedule;
}

export interface ComputeResourcePolicyInstanceSchedulePolicyVmStartSchedule {
    /**
     * Specifies the frequency for the operation, using the unix-cron format.
     */
    schedule: string;
}

export interface ComputeResourcePolicyInstanceSchedulePolicyVmStopSchedule {
    /**
     * Specifies the frequency for the operation, using the unix-cron format.
     */
    schedule: string;
}

export interface ComputeResourcePolicySnapshotSchedulePolicy {
    /**
     * Retention policy applied to snapshots created by this resource policy.
     */
    retentionPolicy?: outputs.ComputeResourcePolicySnapshotSchedulePolicyRetentionPolicy;
    /**
     * Contains one of an 'hourlySchedule', 'dailySchedule', or 'weeklySchedule'.
     */
    schedule: outputs.ComputeResourcePolicySnapshotSchedulePolicySchedule;
    /**
     * Properties with which the snapshots are created, such as labels.
     */
    snapshotProperties?: outputs.ComputeResourcePolicySnapshotSchedulePolicySnapshotProperties;
}

export interface ComputeResourcePolicySnapshotSchedulePolicyRetentionPolicy {
    /**
     * Maximum age of the snapshot that is allowed to be kept.
     */
    maxRetentionDays: number;
    /**
     * Specifies the behavior to apply to scheduled snapshots when
     * the source disk is deleted. Default value: "KEEP_AUTO_SNAPSHOTS" Possible values: ["KEEP_AUTO_SNAPSHOTS", "APPLY_RETENTION_POLICY"]
     */
    onSourceDiskDelete?: string;
}

export interface ComputeResourcePolicySnapshotSchedulePolicySchedule {
    /**
     * The policy will execute every nth day at the specified time.
     */
    dailySchedule?: outputs.ComputeResourcePolicySnapshotSchedulePolicyScheduleDailySchedule;
    /**
     * The policy will execute every nth hour starting at the specified time.
     */
    hourlySchedule?: outputs.ComputeResourcePolicySnapshotSchedulePolicyScheduleHourlySchedule;
    /**
     * Allows specifying a snapshot time for each day of the week.
     */
    weeklySchedule?: outputs.ComputeResourcePolicySnapshotSchedulePolicyScheduleWeeklySchedule;
}

export interface ComputeResourcePolicySnapshotSchedulePolicyScheduleDailySchedule {
    /**
     * Defines a schedule with units measured in days. The value determines how many days pass between the start of each cycle. Days in cycle for snapshot schedule policy must be 1.
     */
    daysInCycle: number;
    /**
     * This must be in UTC format that resolves to one of
     * 00:00, 04:00, 08:00, 12:00, 16:00, or 20:00. For example,
     * both 13:00-5 and 08:00 are valid.
     */
    startTime: string;
}

export interface ComputeResourcePolicySnapshotSchedulePolicyScheduleHourlySchedule {
    /**
     * The number of hours between snapshots.
     */
    hoursInCycle: number;
    /**
     * Time within the window to start the operations.
     * It must be in an hourly format "HH:MM",
     * where HH : [00-23] and MM : [00] GMT. eg: 21:00
     */
    startTime: string;
}

export interface ComputeResourcePolicySnapshotSchedulePolicyScheduleWeeklySchedule {
    /**
     * May contain up to seven (one for each day of the week) snapshot times.
     */
    dayOfWeeks: outputs.ComputeResourcePolicySnapshotSchedulePolicyScheduleWeeklyScheduleDayOfWeek[];
}

export interface ComputeResourcePolicySnapshotSchedulePolicyScheduleWeeklyScheduleDayOfWeek {
    /**
     * The day of the week to create the snapshot. e.g. MONDAY Possible values: ["MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    day: string;
    /**
     * Time within the window to start the operations.
     * It must be in format "HH:MM", where HH : [00-23] and MM : [00-00] GMT.
     */
    startTime: string;
}

export interface ComputeResourcePolicySnapshotSchedulePolicySnapshotProperties {
    /**
     * Creates the new snapshot in the snapshot chain labeled with the
     * specified name. The chain name must be 1-63 characters long and comply
     * with RFC1035.
     */
    chainName?: string;
    /**
     * Whether to perform a 'guest aware' snapshot.
     */
    guestFlush?: boolean;
    /**
     * A set of key-value pairs.
     */
    labels?: {[key: string]: string};
    /**
     * Cloud Storage bucket location to store the auto snapshot
     * (regional or multi-regional)
     */
    storageLocations?: string[];
}

export interface ComputeResourcePolicyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeRouteTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeRouterBgp {
    /**
     * User-specified flag to indicate which mode to use for advertisement. Default value: "DEFAULT" Possible values: ["DEFAULT", "CUSTOM"]
     */
    advertiseMode?: string;
    /**
     * User-specified list of prefix groups to advertise in custom mode.
     * This field can only be populated if advertiseMode is CUSTOM and
     * is advertised to all peers of the router. These groups will be
     * advertised in addition to any specified prefixes. Leave this field
     * blank to advertise no custom groups.
     *
     * This enum field has the one valid value: ALL_SUBNETS
     */
    advertisedGroups?: string[];
    /**
     * User-specified list of individual IP ranges to advertise in
     * custom mode. This field can only be populated if advertiseMode
     * is CUSTOM and is advertised to all peers of the router. These IP
     * ranges will be advertised in addition to any specified groups.
     * Leave this field blank to advertise no custom IP ranges.
     */
    advertisedIpRanges?: outputs.ComputeRouterBgpAdvertisedIpRange[];
    /**
     * Local BGP Autonomous System Number (ASN). Must be an RFC6996
     * private ASN, either 16-bit or 32-bit. The value will be fixed for
     * this router resource. All VPN tunnels that link to this router
     * will have the same local ASN.
     */
    asn: number;
    /**
     * Explicitly specifies a range of valid BGP Identifiers for this Router.
     * It is provided as a link-local IPv4 range (from 169.254.0.0/16), of
     * size at least /30, even if the BGP sessions are over IPv6. It must
     * not overlap with any IPv4 BGP session ranges. Other vendors commonly
     * call this router ID.
     */
    identifierRange: string;
    /**
     * The interval in seconds between BGP keepalive messages that are sent
     * to the peer. Hold time is three times the interval at which keepalive
     * messages are sent, and the hold time is the maximum number of seconds
     * allowed to elapse between successive keepalive messages that BGP
     * receives from a peer.
     *
     * BGP will use the smaller of either the local hold time value or the
     * peer's hold time value as the hold time for the BGP connection
     * between the two peers. If set, this value must be between 20 and 60.
     * The default is 20.
     */
    keepaliveInterval?: number;
}

export interface ComputeRouterBgpAdvertisedIpRange {
    /**
     * User-specified description for the IP range.
     */
    description?: string;
    /**
     * The IP range to advertise. The value must be a
     * CIDR-formatted string.
     */
    range: string;
}

export interface ComputeRouterInterfaceTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeRouterNatLogConfig {
    /**
     * Indicates whether or not to export logs.
     */
    enable: boolean;
    /**
     * Specifies the desired filtering of logs on this NAT. Possible values: ["ERRORS_ONLY", "TRANSLATIONS_ONLY", "ALL"]
     */
    filter: string;
}

export interface ComputeRouterNatRule {
    /**
     * The action to be enforced for traffic that matches this rule.
     */
    action?: outputs.ComputeRouterNatRuleAction;
    /**
     * An optional description of this rule.
     */
    description?: string;
    /**
     * CEL expression that specifies the match condition that egress traffic from a VM is evaluated against.
     * If it evaluates to true, the corresponding action is enforced.
     *
     * The following examples are valid match expressions for public NAT:
     *
     * "inIpRange(destination.ip, '1.1.0.0/16') || inIpRange(destination.ip, '2.2.0.0/16')"
     *
     * "destination.ip == '1.1.0.1' || destination.ip == '8.8.8.8'"
     *
     * The following example is a valid match expression for private NAT:
     *
     * "nexthop.hub == 'https://networkconnectivity.googleapis.com/v1alpha1/projects/my-project/global/hub/hub-1'"
     */
    match: string;
    /**
     * An integer uniquely identifying a rule in the list.
     * The rule number must be a positive value between 0 and 65000, and must be unique among rules within a NAT.
     */
    ruleNumber: number;
}

export interface ComputeRouterNatRuleAction {
    /**
     * A list of URLs of the IP resources used for this NAT rule.
     * These IP addresses must be valid static external IP addresses assigned to the project.
     * This field is used for public NAT.
     */
    sourceNatActiveIps?: string[];
    /**
     * A list of URLs of the IP resources to be drained.
     * These IPs must be valid static external IPs that have been assigned to the NAT.
     * These IPs should be used for updating/patching a NAT rule only.
     * This field is used for public NAT.
     */
    sourceNatDrainIps?: string[];
}

export interface ComputeRouterNatSubnetwork {
    /**
     * Self-link of subnetwork to NAT
     */
    name: string;
    /**
     * List of the secondary ranges of the subnetwork that are allowed
     * to use NAT. This can be populated only if
     * 'LIST_OF_SECONDARY_IP_RANGES' is one of the values in
     * sourceIpRangesToNat
     */
    secondaryIpRangeNames?: string[];
    /**
     * List of options for which source IPs in the subnetwork
     * should have NAT enabled. Supported values include:
     * 'ALL_IP_RANGES', 'LIST_OF_SECONDARY_IP_RANGES',
     * 'PRIMARY_IP_RANGE'.
     */
    sourceIpRangesToNats: string[];
}

export interface ComputeRouterNatTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeRouterPeerAdvertisedIpRange {
    /**
     * User-specified description for the IP range.
     */
    description?: string;
    /**
     * The IP range to advertise. The value must be a
     * CIDR-formatted string.
     */
    range: string;
}

export interface ComputeRouterPeerBfd {
    /**
     * The minimum interval, in milliseconds, between BFD control packets
     * received from the peer router. The actual value is negotiated
     * between the two routers and is equal to the greater of this value
     * and the transmit interval of the other router. If set, this value
     * must be between 1000 and 30000.
     */
    minReceiveInterval?: number;
    /**
     * The minimum interval, in milliseconds, between BFD control packets
     * transmitted to the peer router. The actual value is negotiated
     * between the two routers and is equal to the greater of this value
     * and the corresponding receive interval of the other router. If set,
     * this value must be between 1000 and 30000.
     */
    minTransmitInterval?: number;
    /**
     * The number of consecutive BFD packets that must be missed before
     * BFD declares that a peer is unavailable. If set, the value must
     * be a value between 5 and 16.
     */
    multiplier?: number;
    /**
     * The BFD session initialization mode for this BGP peer.
     * If set to 'ACTIVE', the Cloud Router will initiate the BFD session
     * for this BGP peer. If set to 'PASSIVE', the Cloud Router will wait
     * for the peer router to initiate the BFD session for this BGP peer.
     * If set to 'DISABLED', BFD is disabled for this BGP peer. Possible values: ["ACTIVE", "DISABLED", "PASSIVE"]
     */
    sessionInitializationMode: string;
}

export interface ComputeRouterPeerCustomLearnedIpRange {
    /**
     * The IP range to advertise. The value must be a
     * CIDR-formatted string.
     */
    range: string;
}

export interface ComputeRouterPeerMd5AuthenticationKey {
    /**
     * Value of the key.
     */
    key: string;
    /**
     * [REQUIRED] Name used to identify the key.
     * Must be unique within a router. Must be referenced by exactly one bgpPeer. Must comply with RFC1035.
     */
    name: string;
}

export interface ComputeRouterPeerTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeRouterTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeSecurityPolicyAdaptiveProtectionConfig {
    /**
     * Layer 7 DDoS Defense Config of this security policy
     */
    layer7DdosDefenseConfig?: outputs.ComputeSecurityPolicyAdaptiveProtectionConfigLayer7DdosDefenseConfig;
}

export interface ComputeSecurityPolicyAdaptiveProtectionConfigLayer7DdosDefenseConfig {
    /**
     * If set to true, enables CAAP for L7 DDoS detection.
     */
    enable?: boolean;
    /**
     * Rule visibility. Supported values include: "STANDARD", "PREMIUM".
     */
    ruleVisibility: string;
}

export interface ComputeSecurityPolicyAdvancedOptionsConfig {
    /**
     * Custom configuration to apply the JSON parsing. Only applicable when JSON parsing is set to STANDARD.
     */
    jsonCustomConfig?: outputs.ComputeSecurityPolicyAdvancedOptionsConfigJsonCustomConfig;
    /**
     * JSON body parsing. Supported values include: "DISABLED", "STANDARD".
     */
    jsonParsing: string;
    /**
     * Logging level. Supported values include: "NORMAL", "VERBOSE".
     */
    logLevel: string;
    /**
     * An optional list of case-insensitive request header names to use for resolving the callers client IP address.
     */
    userIpRequestHeaders?: string[];
}

export interface ComputeSecurityPolicyAdvancedOptionsConfigJsonCustomConfig {
    /**
     * A list of custom Content-Type header values to apply the JSON parsing.
     */
    contentTypes: string[];
}

export interface ComputeSecurityPolicyRecaptchaOptionsConfig {
    /**
     * A field to supply a reCAPTCHA site key to be used for all the rules using the redirect action with the type of GOOGLE_RECAPTCHA under the security policy. The specified site key needs to be created from the reCAPTCHA API. The user is responsible for the validity of the specified site key. If not specified, a Google-managed site key is used.
     */
    redirectSiteKey: string;
}

export interface ComputeSecurityPolicyRule {
    /**
     * Action to take when match matches the request.
     */
    action: string;
    /**
     * An optional description of this rule. Max size is 64.
     */
    description?: string;
    /**
     * Additional actions that are performed on headers.
     */
    headerAction?: outputs.ComputeSecurityPolicyRuleHeaderAction;
    /**
     * A match condition that incoming traffic is evaluated against. If it evaluates to true, the corresponding action is enforced.
     */
    match: outputs.ComputeSecurityPolicyRuleMatch;
    /**
     * When set to true, the action specified above is not enforced. Stackdriver logs for requests that trigger a preview action are annotated as such.
     */
    preview: boolean;
    /**
     * An unique positive integer indicating the priority of evaluation for a rule. Rules are evaluated from highest priority (lowest numerically) to lowest priority (highest numerically) in order.
     */
    priority: number;
    /**
     * Rate limit threshold for this security policy. Must be specified if the action is "rate_based_ban" or "throttle". Cannot be specified for any other actions.
     */
    rateLimitOptions?: outputs.ComputeSecurityPolicyRuleRateLimitOptions;
    /**
     * Parameters defining the redirect action. Cannot be specified for any other actions.
     */
    redirectOptions?: outputs.ComputeSecurityPolicyRuleRedirectOptions;
}

export interface ComputeSecurityPolicyRuleHeaderAction {
    /**
     * The list of request headers to add or overwrite if they're already present.
     */
    requestHeadersToAdds: outputs.ComputeSecurityPolicyRuleHeaderActionRequestHeadersToAdd[];
}

export interface ComputeSecurityPolicyRuleHeaderActionRequestHeadersToAdd {
    /**
     * The name of the header to set.
     */
    headerName: string;
    /**
     * The value to set the named header to.
     */
    headerValue?: string;
}

export interface ComputeSecurityPolicyRuleMatch {
    /**
     * The configuration options available when specifying versionedExpr.
     * This field must be specified if versionedExpr is specified and cannot be specified if versionedExpr is not specified.
     */
    config?: outputs.ComputeSecurityPolicyRuleMatchConfig;
    /**
     * User defined CEVAL expression. A CEVAL expression is used to specify match criteria such as origin.ip, source.region_code and contents in the request header.
     */
    expr?: outputs.ComputeSecurityPolicyRuleMatchExpr;
    /**
     * The configuration options available when specifying a user defined CEVAL expression (i.e., 'expr').
     */
    exprOptions?: outputs.ComputeSecurityPolicyRuleMatchExprOptions;
    /**
     * Preconfigured versioned expression. If this field is specified, config must also be specified.
     * Available preconfigured expressions along with their requirements are: SRC_IPS_V1 - must specify the corresponding srcIpRange field in config. Possible values: ["SRC_IPS_V1"]
     */
    versionedExpr?: string;
}

export interface ComputeSecurityPolicyRuleMatchConfig {
    /**
     * CIDR IP address range. Maximum number of srcIpRanges allowed is 10.
     */
    srcIpRanges?: string[];
}

export interface ComputeSecurityPolicyRuleMatchExpr {
    /**
     * Textual representation of an expression in Common Expression Language syntax. The application context of the containing message determines which well-known feature set of CEL is supported.
     */
    expression: string;
}

export interface ComputeSecurityPolicyRuleMatchExprOptions {
    /**
     * reCAPTCHA configuration options to be applied for the rule. If the rule does not evaluate reCAPTCHA tokens, this field has no effect.
     */
    recaptchaOptions: outputs.ComputeSecurityPolicyRuleMatchExprOptionsRecaptchaOptions;
}

export interface ComputeSecurityPolicyRuleMatchExprOptionsRecaptchaOptions {
    /**
     * A list of site keys to be used during the validation of reCAPTCHA action-tokens. The provided site keys need to be created from reCAPTCHA API under the same project where the security policy is created.
     */
    actionTokenSiteKeys?: string[];
    /**
     * A list of site keys to be used during the validation of reCAPTCHA session-tokens. The provided site keys need to be created from reCAPTCHA API under the same project where the security policy is created.
     */
    sessionTokenSiteKeys?: string[];
}

export interface ComputeSecurityPolicyRulePreconfiguredWafConfig {
    /**
     * An exclusion to apply during preconfigured WAF evaluation.
     */
    exclusions?: outputs.ComputeSecurityPolicyRulePreconfiguredWafConfigExclusion[];
}

export interface ComputeSecurityPolicyRulePreconfiguredWafConfigExclusion {
    /**
     * Request cookie whose value will be excluded from inspection during preconfigured WAF evaluation.
     */
    requestCookies?: outputs.ComputeSecurityPolicyRulePreconfiguredWafConfigExclusionRequestCooky[];
    /**
     * Request header whose value will be excluded from inspection during preconfigured WAF evaluation.
     */
    requestHeaders?: outputs.ComputeSecurityPolicyRulePreconfiguredWafConfigExclusionRequestHeader[];
    /**
     * Request query parameter whose value will be excluded from inspection during preconfigured WAF evaluation.
     * Note that the parameter can be in the query string or in the POST body.
     */
    requestQueryParams?: outputs.ComputeSecurityPolicyRulePreconfiguredWafConfigExclusionRequestQueryParam[];
    /**
     * Request URI from the request line to be excluded from inspection during preconfigured WAF evaluation.
     * When specifying this field, the query or fragment part should be excluded.
     */
    requestUris?: outputs.ComputeSecurityPolicyRulePreconfiguredWafConfigExclusionRequestUri[];
    /**
     * A list of target rule IDs under the WAF rule set to apply the preconfigured WAF exclusion.
     * If omitted, it refers to all the rule IDs under the WAF rule set.
     */
    targetRuleIds?: string[];
    /**
     * Target WAF rule set to apply the preconfigured WAF exclusion.
     */
    targetRuleSet: string;
}

export interface ComputeSecurityPolicyRulePreconfiguredWafConfigExclusionRequestCooky {
    /**
     * You can specify an exact match or a partial match by using a field operator and a field value.
     * Available options:
     * EQUALS: The operator matches if the field value equals the specified value.
     * STARTS_WITH: The operator matches if the field value starts with the specified value.
     * ENDS_WITH: The operator matches if the field value ends with the specified value.
     * CONTAINS: The operator matches if the field value contains the specified value.
     * EQUALS_ANY: The operator matches if the field value is any value.
     */
    operator: string;
    /**
     * A request field matching the specified value will be excluded from inspection during preconfigured WAF evaluation.
     * The field value must be given if the field operator is not EQUALS_ANY, and cannot be given if the field operator is EQUALS_ANY.
     */
    value?: string;
}

export interface ComputeSecurityPolicyRulePreconfiguredWafConfigExclusionRequestHeader {
    /**
     * You can specify an exact match or a partial match by using a field operator and a field value.
     * Available options:
     * EQUALS: The operator matches if the field value equals the specified value.
     * STARTS_WITH: The operator matches if the field value starts with the specified value.
     * ENDS_WITH: The operator matches if the field value ends with the specified value.
     * CONTAINS: The operator matches if the field value contains the specified value.
     * EQUALS_ANY: The operator matches if the field value is any value.
     */
    operator: string;
    /**
     * A request field matching the specified value will be excluded from inspection during preconfigured WAF evaluation.
     * The field value must be given if the field operator is not EQUALS_ANY, and cannot be given if the field operator is EQUALS_ANY.
     */
    value?: string;
}

export interface ComputeSecurityPolicyRulePreconfiguredWafConfigExclusionRequestQueryParam {
    /**
     * You can specify an exact match or a partial match by using a field operator and a field value.
     * Available options:
     * EQUALS: The operator matches if the field value equals the specified value.
     * STARTS_WITH: The operator matches if the field value starts with the specified value.
     * ENDS_WITH: The operator matches if the field value ends with the specified value.
     * CONTAINS: The operator matches if the field value contains the specified value.
     * EQUALS_ANY: The operator matches if the field value is any value.
     */
    operator: string;
    /**
     * A request field matching the specified value will be excluded from inspection during preconfigured WAF evaluation.
     * The field value must be given if the field operator is not EQUALS_ANY, and cannot be given if the field operator is EQUALS_ANY.
     */
    value?: string;
}

export interface ComputeSecurityPolicyRulePreconfiguredWafConfigExclusionRequestUri {
    /**
     * You can specify an exact match or a partial match by using a field operator and a field value.
     * Available options:
     * EQUALS: The operator matches if the field value equals the specified value.
     * STARTS_WITH: The operator matches if the field value starts with the specified value.
     * ENDS_WITH: The operator matches if the field value ends with the specified value.
     * CONTAINS: The operator matches if the field value contains the specified value.
     * EQUALS_ANY: The operator matches if the field value is any value.
     */
    operator: string;
    /**
     * A request field matching the specified value will be excluded from inspection during preconfigured WAF evaluation.
     * The field value must be given if the field operator is not EQUALS_ANY, and cannot be given if the field operator is EQUALS_ANY.
     */
    value?: string;
}

export interface ComputeSecurityPolicyRuleRateLimitOptions {
    /**
     * Can only be specified if the action for the rule is "rate_based_ban".
     * If specified, determines the time (in seconds) the traffic will continue to be banned by the rate limit after the rate falls below the threshold.
     */
    banDurationSec?: number;
    /**
     * Can only be specified if the action for the rule is "rate_based_ban".
     * If specified, the key will be banned for the configured 'banDurationSec' when the number of requests that exceed the 'rateLimitThreshold' also exceed this 'banThreshold'.
     */
    banThreshold?: outputs.ComputeSecurityPolicyRuleRateLimitOptionsBanThreshold;
    /**
     * Action to take for requests that are under the configured rate limit threshold.
     * Valid option is "allow" only.
     */
    conformAction?: string;
    /**
     * Determines the key to enforce the rateLimitThreshold on. Possible values are:
     * * ALL: A single rate limit threshold is applied to all the requests matching this rule. This is the default value if "enforceOnKey" is not configured.
     * * IP: The source IP address of the request is the key. Each IP has this limit enforced separately.
     * * HTTP_HEADER: The value of the HTTP header whose name is configured under "enforceOnKeyName". The key value is truncated to the first 128 bytes of the header value. If no such header is present in the request, the key type defaults to ALL.
     * * XFF_IP: The first IP address (i.e. the originating client IP address) specified in the list of IPs under X-Forwarded-For HTTP header. If no such header is present or the value is not a valid IP, the key defaults to the source IP address of the request i.e. key type IP.
     * * HTTP_COOKIE: The value of the HTTP cookie whose name is configured under "enforceOnKeyName". The key value is truncated to the first 128 bytes of the cookie value. If no such cookie is present in the request, the key type defaults to ALL.
     * * HTTP_PATH: The URL path of the HTTP request. The key value is truncated to the first 128 bytes.
     * * SNI: Server name indication in the TLS session of the HTTPS request. The key value is truncated to the first 128 bytes. The key type defaults to ALL on a HTTP session.
     * * REGION_CODE: The country/region from which the request originates.
     * * TLS_JA3_FINGERPRINT: JA3 TLS/SSL fingerprint if the client connects using HTTPS, HTTP/2 or HTTP/3. If not available, the key type defaults to ALL.
     * * USER_IP: The IP address of the originating client, which is resolved based on "userIpRequestHeaders" configured with the security policy. If there is no "userIpRequestHeaders" configuration or an IP address cannot be resolved from it, the key type defaults to IP. Possible values: ["ALL", "IP", "HTTP_HEADER", "XFF_IP", "HTTP_COOKIE", "HTTP_PATH", "SNI", "REGION_CODE", "TLS_JA3_FINGERPRINT", "USER_IP"]
     */
    enforceOnKey?: string;
    /**
     * If specified, any combination of values of enforceOnKeyType/enforceOnKeyName is treated as the key on which ratelimit threshold/action is enforced.
     * You can specify up to 3 enforceOnKeyConfigs.
     * If enforceOnKeyConfigs is specified, enforceOnKey must not be specified.
     */
    enforceOnKeyConfigs?: outputs.ComputeSecurityPolicyRuleRateLimitOptionsEnforceOnKeyConfig[];
    /**
     * Rate limit key name applicable only for the following key types:
     * HTTP_HEADER -- Name of the HTTP header whose value is taken as the key value.
     * HTTP_COOKIE -- Name of the HTTP cookie whose value is taken as the key value.
     */
    enforceOnKeyName?: string;
    /**
     * Action to take for requests that are above the configured rate limit threshold, to either deny with a specified HTTP response code, or redirect to a different endpoint.
     * Valid options are deny(STATUS), where valid values for STATUS are 403, 404, 429, and 502.
     */
    exceedAction?: string;
    /**
     * Parameters defining the redirect action that is used as the exceed action. Cannot be specified if the exceed action is not redirect. This field is only supported in Global Security Policies of type CLOUD_ARMOR.
     */
    exceedRedirectOptions?: outputs.ComputeSecurityPolicyRuleRateLimitOptionsExceedRedirectOptions;
    /**
     * Threshold at which to begin ratelimiting.
     */
    rateLimitThreshold?: outputs.ComputeSecurityPolicyRuleRateLimitOptionsRateLimitThreshold;
}

export interface ComputeSecurityPolicyRuleRateLimitOptionsBanThreshold {
    /**
     * Number of HTTP(S) requests for calculating the threshold.
     */
    count?: number;
    /**
     * Interval over which the threshold is computed.
     */
    intervalSec?: number;
}

export interface ComputeSecurityPolicyRuleRateLimitOptionsEnforceOnKeyConfig {
    /**
     * Rate limit key name applicable only for the following key types:
     * HTTP_HEADER -- Name of the HTTP header whose value is taken as the key value.
     * HTTP_COOKIE -- Name of the HTTP cookie whose value is taken as the key value.
     */
    enforceOnKeyName?: string;
    /**
     * Determines the key to enforce the rateLimitThreshold on. Possible values are:
     * * ALL: A single rate limit threshold is applied to all the requests matching this rule. This is the default value if "enforceOnKeyConfigs" is not configured.
     * * IP: The source IP address of the request is the key. Each IP has this limit enforced separately.
     * * HTTP_HEADER: The value of the HTTP header whose name is configured under "enforceOnKeyName". The key value is truncated to the first 128 bytes of the header value. If no such header is present in the request, the key type defaults to ALL.
     * * XFF_IP: The first IP address (i.e. the originating client IP address) specified in the list of IPs under X-Forwarded-For HTTP header. If no such header is present or the value is not a valid IP, the key defaults to the source IP address of the request i.e. key type IP.
     * * HTTP_COOKIE: The value of the HTTP cookie whose name is configured under "enforceOnKeyName". The key value is truncated to the first 128 bytes of the cookie value. If no such cookie is present in the request, the key type defaults to ALL.
     * * HTTP_PATH: The URL path of the HTTP request. The key value is truncated to the first 128 bytes.
     * * SNI: Server name indication in the TLS session of the HTTPS request. The key value is truncated to the first 128 bytes. The key type defaults to ALL on a HTTP session.
     * * REGION_CODE: The country/region from which the request originates.
     * * TLS_JA3_FINGERPRINT: JA3 TLS/SSL fingerprint if the client connects using HTTPS, HTTP/2 or HTTP/3. If not available, the key type defaults to ALL.
     * * USER_IP: The IP address of the originating client, which is resolved based on "userIpRequestHeaders" configured with the security policy. If there is no "userIpRequestHeaders" configuration or an IP address cannot be resolved from it, the key type defaults to IP. Possible values: ["ALL", "IP", "HTTP_HEADER", "XFF_IP", "HTTP_COOKIE", "HTTP_PATH", "SNI", "REGION_CODE", "TLS_JA3_FINGERPRINT", "USER_IP"]
     */
    enforceOnKeyType?: string;
}

export interface ComputeSecurityPolicyRuleRateLimitOptionsExceedRedirectOptions {
    /**
     * Target for the redirect action. This is required if the type is EXTERNAL_302 and cannot be specified for GOOGLE_RECAPTCHA.
     */
    target?: string;
    /**
     * Type of the redirect action.
     */
    type?: string;
}

export interface ComputeSecurityPolicyRuleRateLimitOptionsRateLimitThreshold {
    /**
     * Number of HTTP(S) requests for calculating the threshold.
     */
    count?: number;
    /**
     * Interval over which the threshold is computed.
     */
    intervalSec?: number;
}

export interface ComputeSecurityPolicyRuleRedirectOptions {
    /**
     * Target for the redirect action. This is required if the type is EXTERNAL_302 and cannot be specified for GOOGLE_RECAPTCHA.
     */
    target?: string;
    /**
     * Type of the redirect action. Available options: EXTERNAL_302: Must specify the corresponding target field in config. GOOGLE_RECAPTCHA: Cannot specify target field in config.
     */
    type: string;
}

export interface ComputeSecurityPolicyRuleTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeSecurityPolicyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeServiceAttachmentConnectedEndpoint {
    endpoint: string;
    status: string;
}

export interface ComputeServiceAttachmentConsumerAcceptList {
    /**
     * The number of consumer forwarding rules the consumer project can
     * create.
     */
    connectionLimit: number;
    /**
     * The network that is allowed to connect to this service attachment.
     * Only one of project_id_or_num and network_url may be set.
     */
    networkUrl?: string;
    /**
     * A project that is allowed to connect to this service attachment.
     * Only one of project_id_or_num and network_url may be set.
     */
    projectIdOrNum?: string;
}

export interface ComputeServiceAttachmentTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeSharedVpcHostProjectTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeSharedVpcServiceProjectTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeSnapshotIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ComputeSnapshotIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ComputeSnapshotSnapshotEncryptionKey {
    /**
     * The name of the encryption key that is stored in Google Cloud KMS.
     */
    kmsKeySelfLink?: string;
    /**
     * The service account used for the encryption request for the given KMS key.
     * If absent, the Compute Engine Service Agent service account is used.
     */
    kmsKeyServiceAccount?: string;
    /**
     * Specifies a 256-bit customer-supplied encryption key, encoded in
     * RFC 4648 base64 to either encrypt or decrypt this resource.
     */
    rawKey?: string;
    /**
     * The RFC 4648 base64 encoded SHA-256 hash of the customer-supplied
     * encryption key that protects this resource.
     */
    sha256: string;
}

export interface ComputeSnapshotSourceDiskEncryptionKey {
    /**
     * The service account used for the encryption request for the given KMS key.
     * If absent, the Compute Engine Service Agent service account is used.
     */
    kmsKeyServiceAccount?: string;
    /**
     * Specifies a 256-bit customer-supplied encryption key, encoded in
     * RFC 4648 base64 to either encrypt or decrypt this resource.
     */
    rawKey?: string;
}

export interface ComputeSnapshotTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeSslCertificateTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeSslPolicyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeSubnetworkIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ComputeSubnetworkIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ComputeSubnetworkLogConfig {
    /**
     * Can only be specified if VPC flow logging for this subnetwork is enabled.
     * Toggles the aggregation interval for collecting flow logs. Increasing the
     * interval time will reduce the amount of generated flow logs for long
     * lasting connections. Default is an interval of 5 seconds per connection. Default value: "INTERVAL_5_SEC" Possible values: ["INTERVAL_5_SEC", "INTERVAL_30_SEC", "INTERVAL_1_MIN", "INTERVAL_5_MIN", "INTERVAL_10_MIN", "INTERVAL_15_MIN"]
     */
    aggregationInterval?: string;
    /**
     * Export filter used to define which VPC flow logs should be logged, as as CEL expression. See
     * https://cloud.google.com/vpc/docs/flow-logs#filtering for details on how to format this field.
     * The default value is 'true', which evaluates to include everything.
     */
    filterExpr?: string;
    /**
     * Can only be specified if VPC flow logging for this subnetwork is enabled.
     * The value of the field must be in [0, 1]. Set the sampling rate of VPC
     * flow logs within the subnetwork where 1.0 means all collected logs are
     * reported and 0.0 means no logs are reported. Default is 0.5 which means
     * half of all collected logs are reported.
     */
    flowSampling?: number;
    /**
     * Can only be specified if VPC flow logging for this subnetwork is enabled.
     * Configures whether metadata fields should be added to the reported VPC
     * flow logs. Default value: "INCLUDE_ALL_METADATA" Possible values: ["EXCLUDE_ALL_METADATA", "INCLUDE_ALL_METADATA", "CUSTOM_METADATA"]
     */
    metadata?: string;
    /**
     * List of metadata fields that should be added to reported logs.
     * Can only be specified if VPC flow logs for this subnetwork is enabled and "metadata" is set to CUSTOM_METADATA.
     */
    metadataFields?: string[];
}

export interface ComputeSubnetworkSecondaryIpRange {
    /**
     * The range of IP addresses belonging to this subnetwork secondary
     * range. Provide this property when you create the subnetwork.
     * Ranges must be unique and non-overlapping with all primary and
     * secondary IP ranges within a network. Only IPv4 is supported.
     * Field is optional when 'reserved_internal_range' is defined, otherwise required.
     */
    ipCidrRange: string;
    /**
     * The name associated with this subnetwork secondary range, used
     * when adding an alias IP range to a VM instance. The name must
     * be 1-63 characters long, and comply with RFC1035. The name
     * must be unique within the subnetwork.
     */
    rangeName: string;
    /**
     * The ID of the reserved internal range. Must be prefixed with 'networkconnectivity.googleapis.com'
     * E.g. 'networkconnectivity.googleapis.com/projects/{project}/locations/global/internalRanges/{rangeId}'
     */
    reservedInternalRange?: string;
}

export interface ComputeSubnetworkTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeTargetGrpcProxyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeTargetHttpProxyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeTargetHttpsProxyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeTargetInstanceTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeTargetPoolTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeTargetSslProxyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeTargetTcpProxyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeUrlMapDefaultRouteAction {
    /**
     * The specification for allowing client side cross-origin requests. Please see
     * [W3C Recommendation for Cross Origin Resource Sharing](https://www.w3.org/TR/cors/)
     */
    corsPolicy?: outputs.ComputeUrlMapDefaultRouteActionCorsPolicy;
    /**
     * The specification for fault injection introduced into traffic to test the resiliency of clients to backend service failure.
     * As part of fault injection, when clients send requests to a backend service, delays can be introduced by Loadbalancer on a
     * percentage of requests before sending those request to the backend service. Similarly requests from clients can be aborted
     * by the Loadbalancer for a percentage of requests.
     *
     * timeout and retryPolicy will be ignored by clients that are configured with a faultInjectionPolicy.
     */
    faultInjectionPolicy?: outputs.ComputeUrlMapDefaultRouteActionFaultInjectionPolicy;
    /**
     * Specifies the policy on how requests intended for the route's backends are shadowed to a separate mirrored backend service.
     * Loadbalancer does not wait for responses from the shadow service. Prior to sending traffic to the shadow service,
     * the host / authority header is suffixed with -shadow.
     */
    requestMirrorPolicy?: outputs.ComputeUrlMapDefaultRouteActionRequestMirrorPolicy;
    /**
     * Specifies the retry policy associated with this route.
     */
    retryPolicy?: outputs.ComputeUrlMapDefaultRouteActionRetryPolicy;
    /**
     * Specifies the timeout for the selected route. Timeout is computed from the time the request has been
     * fully processed (i.e. end-of-stream) up until the response has been completely processed. Timeout includes all retries.
     *
     * If not specified, will use the largest timeout among all backend services associated with the route.
     */
    timeout?: outputs.ComputeUrlMapDefaultRouteActionTimeout;
    /**
     * The spec to modify the URL of the request, prior to forwarding the request to the matched service.
     */
    urlRewrite?: outputs.ComputeUrlMapDefaultRouteActionUrlRewrite;
    /**
     * A list of weighted backend services to send traffic to when a route match occurs.
     * The weights determine the fraction of traffic that flows to their corresponding backend service.
     * If all traffic needs to go to a single backend service, there must be one weightedBackendService
     * with weight set to a non 0 number.
     *
     * Once a backendService is identified and before forwarding the request to the backend service,
     * advanced routing actions like Url rewrites and header transformations are applied depending on
     * additional settings specified in this HttpRouteAction.
     */
    weightedBackendServices?: outputs.ComputeUrlMapDefaultRouteActionWeightedBackendService[];
}

export interface ComputeUrlMapDefaultRouteActionCorsPolicy {
    /**
     * In response to a preflight request, setting this to true indicates that the actual request can include user credentials.
     * This translates to the Access-Control-Allow-Credentials header.
     */
    allowCredentials?: boolean;
    /**
     * Specifies the content for the Access-Control-Allow-Headers header.
     */
    allowHeaders?: string[];
    /**
     * Specifies the content for the Access-Control-Allow-Methods header.
     */
    allowMethods?: string[];
    /**
     * Specifies the regular expression patterns that match allowed origins. For regular expression grammar
     * please see en.cppreference.com/w/cpp/regex/ecmascript
     * An origin is allowed if it matches either an item in allowOrigins or an item in allowOriginRegexes.
     */
    allowOriginRegexes?: string[];
    /**
     * Specifies the list of origins that will be allowed to do CORS requests.
     * An origin is allowed if it matches either an item in allowOrigins or an item in allowOriginRegexes.
     */
    allowOrigins?: string[];
    /**
     * If true, specifies the CORS policy is disabled. The default value is false, which indicates that the CORS policy is in effect.
     */
    disabled?: boolean;
    /**
     * Specifies the content for the Access-Control-Expose-Headers header.
     */
    exposeHeaders?: string[];
    /**
     * Specifies how long results of a preflight request can be cached in seconds.
     * This translates to the Access-Control-Max-Age header.
     */
    maxAge?: number;
}

export interface ComputeUrlMapDefaultRouteActionFaultInjectionPolicy {
    /**
     * The specification for how client requests are aborted as part of fault injection.
     */
    abort?: outputs.ComputeUrlMapDefaultRouteActionFaultInjectionPolicyAbort;
    /**
     * The specification for how client requests are delayed as part of fault injection, before being sent to a backend service.
     */
    delay?: outputs.ComputeUrlMapDefaultRouteActionFaultInjectionPolicyDelay;
}

export interface ComputeUrlMapDefaultRouteActionFaultInjectionPolicyAbort {
    /**
     * The HTTP status code used to abort the request.
     * The value must be between 200 and 599 inclusive.
     */
    httpStatus?: number;
    /**
     * The percentage of traffic (connections/operations/requests) which will be aborted as part of fault injection.
     * The value must be between 0.0 and 100.0 inclusive.
     */
    percentage?: number;
}

export interface ComputeUrlMapDefaultRouteActionFaultInjectionPolicyDelay {
    /**
     * Specifies the value of the fixed delay interval.
     */
    fixedDelay?: outputs.ComputeUrlMapDefaultRouteActionFaultInjectionPolicyDelayFixedDelay;
    /**
     * The percentage of traffic (connections/operations/requests) on which delay will be introduced as part of fault injection.
     * The value must be between 0.0 and 100.0 inclusive.
     */
    percentage?: number;
}

export interface ComputeUrlMapDefaultRouteActionFaultInjectionPolicyDelayFixedDelay {
    /**
     * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are
     * represented with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive.
     * Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
     */
    seconds?: string;
}

export interface ComputeUrlMapDefaultRouteActionRequestMirrorPolicy {
    /**
     * The full or partial URL to the BackendService resource being mirrored to.
     */
    backendService: string;
}

export interface ComputeUrlMapDefaultRouteActionRetryPolicy {
    /**
     * Specifies the allowed number retries. This number must be > 0. If not specified, defaults to 1.
     */
    numRetries?: number;
    /**
     * Specifies a non-zero timeout per retry attempt.
     *
     * If not specified, will use the timeout set in HttpRouteAction. If timeout in HttpRouteAction is not set,
     * will use the largest timeout among all backend services associated with the route.
     */
    perTryTimeout?: outputs.ComputeUrlMapDefaultRouteActionRetryPolicyPerTryTimeout;
    /**
     * Specfies one or more conditions when this retry rule applies. Valid values are:
     *
     * * 5xx: Loadbalancer will attempt a retry if the backend service responds with any 5xx response code,
     *   or if the backend service does not respond at all, example: disconnects, reset, read timeout,
     * * connection failure, and refused streams.
     * * gateway-error: Similar to 5xx, but only applies to response codes 502, 503 or 504.
     * * connect-failure: Loadbalancer will retry on failures connecting to backend services,
     *   for example due to connection timeouts.
     * * retriable-4xx: Loadbalancer will retry for retriable 4xx response codes.
     *   Currently the only retriable error supported is 409.
     * * refused-stream:Loadbalancer will retry if the backend service resets the stream with a REFUSED_STREAM error code.
     *   This reset type indicates that it is safe to retry.
     * * cancelled: Loadbalancer will retry if the gRPC status code in the response header is set to cancelled
     * * deadline-exceeded: Loadbalancer will retry if the gRPC status code in the response header is set to deadline-exceeded
     * * resource-exhausted: Loadbalancer will retry if the gRPC status code in the response header is set to resource-exhausted
     * * unavailable: Loadbalancer will retry if the gRPC status code in the response header is set to unavailable
     */
    retryConditions?: string[];
}

export interface ComputeUrlMapDefaultRouteActionRetryPolicyPerTryTimeout {
    /**
     * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are
     * represented with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive.
     * Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
     */
    seconds?: string;
}

export interface ComputeUrlMapDefaultRouteActionTimeout {
    /**
     * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are represented
     * with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive.
     * Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
     */
    seconds?: string;
}

export interface ComputeUrlMapDefaultRouteActionUrlRewrite {
    /**
     * Prior to forwarding the request to the selected service, the request's host header is replaced
     * with contents of hostRewrite.
     *
     * The value must be between 1 and 255 characters.
     */
    hostRewrite?: string;
    /**
     * Prior to forwarding the request to the selected backend service, the matching portion of the
     * request's path is replaced by pathPrefixRewrite.
     *
     * The value must be between 1 and 1024 characters.
     */
    pathPrefixRewrite?: string;
}

export interface ComputeUrlMapDefaultRouteActionWeightedBackendService {
    /**
     * The full or partial URL to the default BackendService resource. Before forwarding the
     * request to backendService, the loadbalancer applies any relevant headerActions
     * specified as part of this backendServiceWeight.
     */
    backendService?: string;
    /**
     * Specifies changes to request and response headers that need to take effect for
     * the selected backendService.
     *
     * headerAction specified here take effect before headerAction in the enclosing
     * HttpRouteRule, PathMatcher and UrlMap.
     */
    headerAction?: outputs.ComputeUrlMapDefaultRouteActionWeightedBackendServiceHeaderAction;
    /**
     * Specifies the fraction of traffic sent to backendService, computed as
     * weight / (sum of all weightedBackendService weights in routeAction) .
     *
     * The selection of a backend service is determined only for new traffic. Once a user's request
     * has been directed to a backendService, subsequent requests will be sent to the same backendService
     * as determined by the BackendService's session affinity policy.
     *
     * The value must be between 0 and 1000
     */
    weight?: number;
}

export interface ComputeUrlMapDefaultRouteActionWeightedBackendServiceHeaderAction {
    /**
     * Headers to add to a matching request prior to forwarding the request to the backendService.
     */
    requestHeadersToAdds?: outputs.ComputeUrlMapDefaultRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd[];
    /**
     * A list of header names for headers that need to be removed from the request prior to
     * forwarding the request to the backendService.
     */
    requestHeadersToRemoves?: string[];
    /**
     * Headers to add the response prior to sending the response back to the client.
     */
    responseHeadersToAdds?: outputs.ComputeUrlMapDefaultRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd[];
    /**
     * A list of header names for headers that need to be removed from the response prior to sending the
     * response back to the client.
     */
    responseHeadersToRemoves?: string[];
}

export interface ComputeUrlMapDefaultRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd {
    /**
     * The name of the header to add.
     */
    headerName?: string;
    /**
     * The value of the header to add.
     */
    headerValue?: string;
    /**
     * If false, headerValue is appended to any values that already exist for the header.
     * If true, headerValue is set for the header, discarding any values that were set for that header.
     */
    replace?: boolean;
}

export interface ComputeUrlMapDefaultRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd {
    /**
     * The name of the header to add.
     */
    headerName?: string;
    /**
     * The value of the header to add.
     */
    headerValue?: string;
    /**
     * If false, headerValue is appended to any values that already exist for the header.
     * If true, headerValue is set for the header, discarding any values that were set for that header.
     */
    replace?: boolean;
}

export interface ComputeUrlMapDefaultUrlRedirect {
    /**
     * The host that will be used in the redirect response instead of the one that was
     * supplied in the request. The value must be between 1 and 255 characters.
     */
    hostRedirect?: string;
    /**
     * If set to true, the URL scheme in the redirected request is set to https. If set to
     * false, the URL scheme of the redirected request will remain the same as that of the
     * request. This must only be set for UrlMaps used in TargetHttpProxys. Setting this
     * true for TargetHttpsProxy is not permitted. The default is set to false.
     */
    httpsRedirect?: boolean;
    /**
     * The path that will be used in the redirect response instead of the one that was
     * supplied in the request. pathRedirect cannot be supplied together with
     * prefixRedirect. Supply one alone or neither. If neither is supplied, the path of the
     * original request will be used for the redirect. The value must be between 1 and 1024
     * characters.
     */
    pathRedirect?: string;
    /**
     * The prefix that replaces the prefixMatch specified in the HttpRouteRuleMatch,
     * retaining the remaining portion of the URL before redirecting the request.
     * prefixRedirect cannot be supplied together with pathRedirect. Supply one alone or
     * neither. If neither is supplied, the path of the original request will be used for
     * the redirect. The value must be between 1 and 1024 characters.
     */
    prefixRedirect?: string;
    /**
     * The HTTP Status code to use for this RedirectAction. Supported values are:
     *
     * * MOVED_PERMANENTLY_DEFAULT, which is the default value and corresponds to 301.
     *
     * * FOUND, which corresponds to 302.
     *
     * * SEE_OTHER which corresponds to 303.
     *
     * * TEMPORARY_REDIRECT, which corresponds to 307. In this case, the request method
     * will be retained.
     *
     * * PERMANENT_REDIRECT, which corresponds to 308. In this case,
     * the request method will be retained. Possible values: ["FOUND", "MOVED_PERMANENTLY_DEFAULT", "PERMANENT_REDIRECT", "SEE_OTHER", "TEMPORARY_REDIRECT"]
     */
    redirectResponseCode?: string;
    /**
     * If set to true, any accompanying query portion of the original URL is removed prior
     * to redirecting the request. If set to false, the query portion of the original URL is
     * retained. The default is set to false.
     *  This field is required to ensure an empty block is not set. The normal default value is false.
     */
    stripQuery: boolean;
}

export interface ComputeUrlMapHeaderAction {
    /**
     * Headers to add to a matching request prior to forwarding the request to the
     * backendService.
     */
    requestHeadersToAdds?: outputs.ComputeUrlMapHeaderActionRequestHeadersToAdd[];
    /**
     * A list of header names for headers that need to be removed from the request
     * prior to forwarding the request to the backendService.
     */
    requestHeadersToRemoves?: string[];
    /**
     * Headers to add the response prior to sending the response back to the client.
     */
    responseHeadersToAdds?: outputs.ComputeUrlMapHeaderActionResponseHeadersToAdd[];
    /**
     * A list of header names for headers that need to be removed from the response
     * prior to sending the response back to the client.
     */
    responseHeadersToRemoves?: string[];
}

export interface ComputeUrlMapHeaderActionRequestHeadersToAdd {
    /**
     * The name of the header.
     */
    headerName: string;
    /**
     * The value of the header to add.
     */
    headerValue: string;
    /**
     * If false, headerValue is appended to any values that already exist for the
     * header. If true, headerValue is set for the header, discarding any values that
     * were set for that header.
     */
    replace: boolean;
}

export interface ComputeUrlMapHeaderActionResponseHeadersToAdd {
    /**
     * The name of the header.
     */
    headerName: string;
    /**
     * The value of the header to add.
     */
    headerValue: string;
    /**
     * If false, headerValue is appended to any values that already exist for the
     * header. If true, headerValue is set for the header, discarding any values that
     * were set for that header.
     */
    replace: boolean;
}

export interface ComputeUrlMapHostRule {
    /**
     * An optional description of this resource. Provide this property when you create
     * the resource.
     */
    description?: string;
    /**
     * The list of host patterns to match. They must be valid hostnames, except * will
     * match any string of ([a-z0-9-.]*). In that case, * must be the first character
     * and must be followed in the pattern by either - or ..
     */
    hosts: string[];
    /**
     * The name of the PathMatcher to use to match the path portion of the URL if the
     * hostRule matches the URL's host portion.
     */
    pathMatcher: string;
}

export interface ComputeUrlMapPathMatcher {
    /**
     * defaultRouteAction takes effect when none of the pathRules or routeRules match. The load balancer performs
     * advanced routing actions like URL rewrites, header transformations, etc. prior to forwarding the request
     * to the selected backend. If defaultRouteAction specifies any weightedBackendServices, defaultService must not be set.
     * Conversely if defaultService is set, defaultRouteAction cannot contain any weightedBackendServices.
     *
     * Only one of defaultRouteAction or defaultUrlRedirect must be set.
     */
    defaultRouteAction?: outputs.ComputeUrlMapPathMatcherDefaultRouteAction;
    /**
     * The backend service or backend bucket to use when none of the given paths match.
     */
    defaultService?: string;
    /**
     * When none of the specified hostRules match, the request is redirected to a URL specified
     * by defaultUrlRedirect. If defaultUrlRedirect is specified, defaultService or
     * defaultRouteAction must not be set.
     */
    defaultUrlRedirect?: outputs.ComputeUrlMapPathMatcherDefaultUrlRedirect;
    /**
     * An optional description of this resource. Provide this property when you create
     * the resource.
     */
    description?: string;
    /**
     * Specifies changes to request and response headers that need to take effect for
     * the selected backendService. HeaderAction specified here are applied after the
     * matching HttpRouteRule HeaderAction and before the HeaderAction in the UrlMap
     */
    headerAction?: outputs.ComputeUrlMapPathMatcherHeaderAction;
    /**
     * The name to which this PathMatcher is referred by the HostRule.
     */
    name: string;
    /**
     * The list of path rules. Use this list instead of routeRules when routing based
     * on simple path matching is all that's required. The order by which path rules
     * are specified does not matter. Matches are always done on the longest-path-first
     * basis. For example: a pathRule with a path /a/b/c/* will match before /a/b/*
     * irrespective of the order in which those paths appear in this list. Within a
     * given pathMatcher, only one of pathRules or routeRules must be set.
     */
    pathRules?: outputs.ComputeUrlMapPathMatcherPathRule[];
    /**
     * The list of ordered HTTP route rules. Use this list instead of pathRules when
     * advanced route matching and routing actions are desired. The order of specifying
     * routeRules matters: the first rule that matches will cause its specified routing
     * action to take effect. Within a given pathMatcher, only one of pathRules or
     * routeRules must be set. routeRules are not supported in UrlMaps intended for
     * External load balancers.
     */
    routeRules?: outputs.ComputeUrlMapPathMatcherRouteRule[];
}

export interface ComputeUrlMapPathMatcherDefaultRouteAction {
    /**
     * The specification for allowing client side cross-origin requests. Please see
     * [W3C Recommendation for Cross Origin Resource Sharing](https://www.w3.org/TR/cors/)
     */
    corsPolicy?: outputs.ComputeUrlMapPathMatcherDefaultRouteActionCorsPolicy;
    /**
     * The specification for fault injection introduced into traffic to test the resiliency of clients to backend service failure.
     * As part of fault injection, when clients send requests to a backend service, delays can be introduced by Loadbalancer on a
     * percentage of requests before sending those request to the backend service. Similarly requests from clients can be aborted
     * by the Loadbalancer for a percentage of requests.
     *
     * timeout and retryPolicy will be ignored by clients that are configured with a faultInjectionPolicy.
     */
    faultInjectionPolicy?: outputs.ComputeUrlMapPathMatcherDefaultRouteActionFaultInjectionPolicy;
    /**
     * Specifies the policy on how requests intended for the route's backends are shadowed to a separate mirrored backend service.
     * Loadbalancer does not wait for responses from the shadow service. Prior to sending traffic to the shadow service,
     * the host / authority header is suffixed with -shadow.
     */
    requestMirrorPolicy?: outputs.ComputeUrlMapPathMatcherDefaultRouteActionRequestMirrorPolicy;
    /**
     * Specifies the retry policy associated with this route.
     */
    retryPolicy?: outputs.ComputeUrlMapPathMatcherDefaultRouteActionRetryPolicy;
    /**
     * Specifies the timeout for the selected route. Timeout is computed from the time the request has been
     * fully processed (i.e. end-of-stream) up until the response has been completely processed. Timeout includes all retries.
     *
     * If not specified, will use the largest timeout among all backend services associated with the route.
     */
    timeout?: outputs.ComputeUrlMapPathMatcherDefaultRouteActionTimeout;
    /**
     * The spec to modify the URL of the request, prior to forwarding the request to the matched service.
     */
    urlRewrite?: outputs.ComputeUrlMapPathMatcherDefaultRouteActionUrlRewrite;
    /**
     * A list of weighted backend services to send traffic to when a route match occurs.
     * The weights determine the fraction of traffic that flows to their corresponding backend service.
     * If all traffic needs to go to a single backend service, there must be one weightedBackendService
     * with weight set to a non 0 number.
     *
     * Once a backendService is identified and before forwarding the request to the backend service,
     * advanced routing actions like Url rewrites and header transformations are applied depending on
     * additional settings specified in this HttpRouteAction.
     */
    weightedBackendServices?: outputs.ComputeUrlMapPathMatcherDefaultRouteActionWeightedBackendService[];
}

export interface ComputeUrlMapPathMatcherDefaultRouteActionCorsPolicy {
    /**
     * In response to a preflight request, setting this to true indicates that the actual request can include user credentials.
     * This translates to the Access-Control-Allow-Credentials header.
     */
    allowCredentials?: boolean;
    /**
     * Specifies the content for the Access-Control-Allow-Headers header.
     */
    allowHeaders?: string[];
    /**
     * Specifies the content for the Access-Control-Allow-Methods header.
     */
    allowMethods?: string[];
    /**
     * Specifies the regular expression patterns that match allowed origins. For regular expression grammar
     * please see en.cppreference.com/w/cpp/regex/ecmascript
     * An origin is allowed if it matches either an item in allowOrigins or an item in allowOriginRegexes.
     */
    allowOriginRegexes?: string[];
    /**
     * Specifies the list of origins that will be allowed to do CORS requests.
     * An origin is allowed if it matches either an item in allowOrigins or an item in allowOriginRegexes.
     */
    allowOrigins?: string[];
    /**
     * If true, specifies the CORS policy is disabled. The default value is false, which indicates that the CORS policy is in effect.
     */
    disabled?: boolean;
    /**
     * Specifies the content for the Access-Control-Expose-Headers header.
     */
    exposeHeaders?: string[];
    /**
     * Specifies how long results of a preflight request can be cached in seconds.
     * This translates to the Access-Control-Max-Age header.
     */
    maxAge?: number;
}

export interface ComputeUrlMapPathMatcherDefaultRouteActionFaultInjectionPolicy {
    /**
     * The specification for how client requests are aborted as part of fault injection.
     */
    abort?: outputs.ComputeUrlMapPathMatcherDefaultRouteActionFaultInjectionPolicyAbort;
    /**
     * The specification for how client requests are delayed as part of fault injection, before being sent to a backend service.
     */
    delay?: outputs.ComputeUrlMapPathMatcherDefaultRouteActionFaultInjectionPolicyDelay;
}

export interface ComputeUrlMapPathMatcherDefaultRouteActionFaultInjectionPolicyAbort {
    /**
     * The HTTP status code used to abort the request.
     * The value must be between 200 and 599 inclusive.
     */
    httpStatus?: number;
    /**
     * The percentage of traffic (connections/operations/requests) which will be aborted as part of fault injection.
     * The value must be between 0.0 and 100.0 inclusive.
     */
    percentage?: number;
}

export interface ComputeUrlMapPathMatcherDefaultRouteActionFaultInjectionPolicyDelay {
    /**
     * Specifies the value of the fixed delay interval.
     */
    fixedDelay?: outputs.ComputeUrlMapPathMatcherDefaultRouteActionFaultInjectionPolicyDelayFixedDelay;
    /**
     * The percentage of traffic (connections/operations/requests) on which delay will be introduced as part of fault injection.
     * The value must be between 0.0 and 100.0 inclusive.
     */
    percentage?: number;
}

export interface ComputeUrlMapPathMatcherDefaultRouteActionFaultInjectionPolicyDelayFixedDelay {
    /**
     * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are
     * represented with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive.
     * Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
     */
    seconds?: string;
}

export interface ComputeUrlMapPathMatcherDefaultRouteActionRequestMirrorPolicy {
    /**
     * The full or partial URL to the BackendService resource being mirrored to.
     */
    backendService: string;
}

export interface ComputeUrlMapPathMatcherDefaultRouteActionRetryPolicy {
    /**
     * Specifies the allowed number retries. This number must be > 0. If not specified, defaults to 1.
     */
    numRetries?: number;
    /**
     * Specifies a non-zero timeout per retry attempt.
     *
     * If not specified, will use the timeout set in HttpRouteAction. If timeout in HttpRouteAction is not set,
     * will use the largest timeout among all backend services associated with the route.
     */
    perTryTimeout?: outputs.ComputeUrlMapPathMatcherDefaultRouteActionRetryPolicyPerTryTimeout;
    /**
     * Specfies one or more conditions when this retry rule applies. Valid values are:
     *
     * * 5xx: Loadbalancer will attempt a retry if the backend service responds with any 5xx response code,
     *   or if the backend service does not respond at all, example: disconnects, reset, read timeout,
     * * connection failure, and refused streams.
     * * gateway-error: Similar to 5xx, but only applies to response codes 502, 503 or 504.
     * * connect-failure: Loadbalancer will retry on failures connecting to backend services,
     *   for example due to connection timeouts.
     * * retriable-4xx: Loadbalancer will retry for retriable 4xx response codes.
     *   Currently the only retriable error supported is 409.
     * * refused-stream:Loadbalancer will retry if the backend service resets the stream with a REFUSED_STREAM error code.
     *   This reset type indicates that it is safe to retry.
     * * cancelled: Loadbalancer will retry if the gRPC status code in the response header is set to cancelled
     * * deadline-exceeded: Loadbalancer will retry if the gRPC status code in the response header is set to deadline-exceeded
     * * resource-exhausted: Loadbalancer will retry if the gRPC status code in the response header is set to resource-exhausted
     * * unavailable: Loadbalancer will retry if the gRPC status code in the response header is set to unavailable
     */
    retryConditions?: string[];
}

export interface ComputeUrlMapPathMatcherDefaultRouteActionRetryPolicyPerTryTimeout {
    /**
     * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are
     * represented with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive.
     * Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
     */
    seconds?: string;
}

export interface ComputeUrlMapPathMatcherDefaultRouteActionTimeout {
    /**
     * Span of time that's a fraction of a second at nanosecond resolution. Durations less than one second are represented
     * with a 0 seconds field and a positive nanos field. Must be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000 inclusive.
     * Note: these bounds are computed from: 60 sec/min * 60 min/hr * 24 hr/day * 365.25 days/year * 10000 years
     */
    seconds?: string;
}

export interface ComputeUrlMapPathMatcherDefaultRouteActionUrlRewrite {
    /**
     * Prior to forwarding the request to the selected service, the request's host header is replaced
     * with contents of hostRewrite.
     *
     * The value must be between 1 and 255 characters.
     */
    hostRewrite?: string;
    /**
     * Prior to forwarding the request to the selected backend service, the matching portion of the
     * request's path is replaced by pathPrefixRewrite.
     *
     * The value must be between 1 and 1024 characters.
     */
    pathPrefixRewrite?: string;
}

export interface ComputeUrlMapPathMatcherDefaultRouteActionWeightedBackendService {
    /**
     * The full or partial URL to the default BackendService resource. Before forwarding the
     * request to backendService, the loadbalancer applies any relevant headerActions
     * specified as part of this backendServiceWeight.
     */
    backendService?: string;
    /**
     * Specifies changes to request and response headers that need to take effect for
     * the selected backendService.
     *
     * headerAction specified here take effect before headerAction in the enclosing
     * HttpRouteRule, PathMatcher and UrlMap.
     */
    headerAction?: outputs.ComputeUrlMapPathMatcherDefaultRouteActionWeightedBackendServiceHeaderAction;
    /**
     * Specifies the fraction of traffic sent to backendService, computed as
     * weight / (sum of all weightedBackendService weights in routeAction) .
     *
     * The selection of a backend service is determined only for new traffic. Once a user's request
     * has been directed to a backendService, subsequent requests will be sent to the same backendService
     * as determined by the BackendService's session affinity policy.
     *
     * The value must be between 0 and 1000
     */
    weight?: number;
}

export interface ComputeUrlMapPathMatcherDefaultRouteActionWeightedBackendServiceHeaderAction {
    /**
     * Headers to add to a matching request prior to forwarding the request to the backendService.
     */
    requestHeadersToAdds?: outputs.ComputeUrlMapPathMatcherDefaultRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd[];
    /**
     * A list of header names for headers that need to be removed from the request prior to
     * forwarding the request to the backendService.
     */
    requestHeadersToRemoves?: string[];
    /**
     * Headers to add the response prior to sending the response back to the client.
     */
    responseHeadersToAdds?: outputs.ComputeUrlMapPathMatcherDefaultRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd[];
    /**
     * A list of header names for headers that need to be removed from the response prior to sending the
     * response back to the client.
     */
    responseHeadersToRemoves?: string[];
}

export interface ComputeUrlMapPathMatcherDefaultRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd {
    /**
     * The name of the header to add.
     */
    headerName?: string;
    /**
     * The value of the header to add.
     */
    headerValue?: string;
    /**
     * If false, headerValue is appended to any values that already exist for the header.
     * If true, headerValue is set for the header, discarding any values that were set for that header.
     */
    replace?: boolean;
}

export interface ComputeUrlMapPathMatcherDefaultRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd {
    /**
     * The name of the header to add.
     */
    headerName?: string;
    /**
     * The value of the header to add.
     */
    headerValue?: string;
    /**
     * If false, headerValue is appended to any values that already exist for the header.
     * If true, headerValue is set for the header, discarding any values that were set for that header.
     */
    replace?: boolean;
}

export interface ComputeUrlMapPathMatcherDefaultUrlRedirect {
    /**
     * The host that will be used in the redirect response instead of the one that was
     * supplied in the request. The value must be between 1 and 255 characters.
     */
    hostRedirect?: string;
    /**
     * If set to true, the URL scheme in the redirected request is set to https. If set to
     * false, the URL scheme of the redirected request will remain the same as that of the
     * request. This must only be set for UrlMaps used in TargetHttpProxys. Setting this
     * true for TargetHttpsProxy is not permitted. The default is set to false.
     */
    httpsRedirect?: boolean;
    /**
     * The path that will be used in the redirect response instead of the one that was
     * supplied in the request. pathRedirect cannot be supplied together with
     * prefixRedirect. Supply one alone or neither. If neither is supplied, the path of the
     * original request will be used for the redirect. The value must be between 1 and 1024
     * characters.
     */
    pathRedirect?: string;
    /**
     * The prefix that replaces the prefixMatch specified in the HttpRouteRuleMatch,
     * retaining the remaining portion of the URL before redirecting the request.
     * prefixRedirect cannot be supplied together with pathRedirect. Supply one alone or
     * neither. If neither is supplied, the path of the original request will be used for
     * the redirect. The value must be between 1 and 1024 characters.
     */
    prefixRedirect?: string;
    /**
     * The HTTP Status code to use for this RedirectAction. Supported values are:
     *
     * * MOVED_PERMANENTLY_DEFAULT, which is the default value and corresponds to 301.
     *
     * * FOUND, which corresponds to 302.
     *
     * * SEE_OTHER which corresponds to 303.
     *
     * * TEMPORARY_REDIRECT, which corresponds to 307. In this case, the request method
     * will be retained.
     *
     * * PERMANENT_REDIRECT, which corresponds to 308. In this case,
     * the request method will be retained. Possible values: ["FOUND", "MOVED_PERMANENTLY_DEFAULT", "PERMANENT_REDIRECT", "SEE_OTHER", "TEMPORARY_REDIRECT"]
     */
    redirectResponseCode?: string;
    /**
     * If set to true, any accompanying query portion of the original URL is removed prior
     * to redirecting the request. If set to false, the query portion of the original URL is
     * retained.
     *  This field is required to ensure an empty block is not set. The normal default value is false.
     */
    stripQuery: boolean;
}

export interface ComputeUrlMapPathMatcherHeaderAction {
    /**
     * Headers to add to a matching request prior to forwarding the request to the
     * backendService.
     */
    requestHeadersToAdds?: outputs.ComputeUrlMapPathMatcherHeaderActionRequestHeadersToAdd[];
    /**
     * A list of header names for headers that need to be removed from the request
     * prior to forwarding the request to the backendService.
     */
    requestHeadersToRemoves?: string[];
    /**
     * Headers to add the response prior to sending the response back to the client.
     */
    responseHeadersToAdds?: outputs.ComputeUrlMapPathMatcherHeaderActionResponseHeadersToAdd[];
    /**
     * A list of header names for headers that need to be removed from the response
     * prior to sending the response back to the client.
     */
    responseHeadersToRemoves?: string[];
}

export interface ComputeUrlMapPathMatcherHeaderActionRequestHeadersToAdd {
    /**
     * The name of the header.
     */
    headerName: string;
    /**
     * The value of the header to add.
     */
    headerValue: string;
    /**
     * If false, headerValue is appended to any values that already exist for the
     * header. If true, headerValue is set for the header, discarding any values that
     * were set for that header.
     */
    replace: boolean;
}

export interface ComputeUrlMapPathMatcherHeaderActionResponseHeadersToAdd {
    /**
     * The name of the header.
     */
    headerName: string;
    /**
     * The value of the header to add.
     */
    headerValue: string;
    /**
     * If false, headerValue is appended to any values that already exist for the
     * header. If true, headerValue is set for the header, discarding any values that
     * were set for that header.
     */
    replace: boolean;
}

export interface ComputeUrlMapPathMatcherPathRule {
    /**
     * The list of path patterns to match. Each must start with / and the only place a
     * \* is allowed is at the end following a /. The string fed to the path matcher
     * does not include any text after the first ? or #, and those chars are not
     * allowed here.
     */
    paths: string[];
    /**
     * In response to a matching path, the load balancer performs advanced routing
     * actions like URL rewrites, header transformations, etc. prior to forwarding the
     * request to the selected backend. If routeAction specifies any
     * weightedBackendServices, service must not be set. Conversely if service is set,
     * routeAction cannot contain any  weightedBackendServices. Only one of routeAction
     * or urlRedirect must be set.
     */
    routeAction?: outputs.ComputeUrlMapPathMatcherPathRuleRouteAction;
    /**
     * The backend service or backend bucket to use if any of the given paths match.
     */
    service?: string;
    /**
     * When a path pattern is matched, the request is redirected to a URL specified
     * by urlRedirect. If urlRedirect is specified, service or routeAction must not
     * be set.
     */
    urlRedirect?: outputs.ComputeUrlMapPathMatcherPathRuleUrlRedirect;
}

export interface ComputeUrlMapPathMatcherPathRuleRouteAction {
    /**
     * The specification for allowing client side cross-origin requests. Please see W3C
     * Recommendation for Cross Origin Resource Sharing
     */
    corsPolicy?: outputs.ComputeUrlMapPathMatcherPathRuleRouteActionCorsPolicy;
    /**
     * The specification for fault injection introduced into traffic to test the
     * resiliency of clients to backend service failure. As part of fault injection,
     * when clients send requests to a backend service, delays can be introduced by
     * Loadbalancer on a percentage of requests before sending those request to the
     * backend service. Similarly requests from clients can be aborted by the
     * Loadbalancer for a percentage of requests. timeout and retry_policy will be
     * ignored by clients that are configured with a fault_injection_policy.
     */
    faultInjectionPolicy?: outputs.ComputeUrlMapPathMatcherPathRuleRouteActionFaultInjectionPolicy;
    /**
     * Specifies the policy on how requests intended for the route's backends are
     * shadowed to a separate mirrored backend service. Loadbalancer does not wait for
     * responses from the shadow service. Prior to sending traffic to the shadow
     * service, the host / authority header is suffixed with -shadow.
     */
    requestMirrorPolicy?: outputs.ComputeUrlMapPathMatcherPathRuleRouteActionRequestMirrorPolicy;
    /**
     * Specifies the retry policy associated with this route.
     */
    retryPolicy?: outputs.ComputeUrlMapPathMatcherPathRuleRouteActionRetryPolicy;
    /**
     * Specifies the timeout for the selected route. Timeout is computed from the time
     * the request is has been fully processed (i.e. end-of-stream) up until the
     * response has been completely processed. Timeout includes all retries. If not
     * specified, the default value is 15 seconds.
     */
    timeout?: outputs.ComputeUrlMapPathMatcherPathRuleRouteActionTimeout;
    /**
     * The spec to modify the URL of the request, prior to forwarding the request to
     * the matched service
     */
    urlRewrite?: outputs.ComputeUrlMapPathMatcherPathRuleRouteActionUrlRewrite;
    /**
     * A list of weighted backend services to send traffic to when a route match
     * occurs. The weights determine the fraction of traffic that flows to their
     * corresponding backend service. If all traffic needs to go to a single backend
     * service, there must be one  weightedBackendService with weight set to a non 0
     * number. Once a backendService is identified and before forwarding the request to
     * the backend service, advanced routing actions like Url rewrites and header
     * transformations are applied depending on additional settings specified in this
     * HttpRouteAction.
     */
    weightedBackendServices?: outputs.ComputeUrlMapPathMatcherPathRuleRouteActionWeightedBackendService[];
}

export interface ComputeUrlMapPathMatcherPathRuleRouteActionCorsPolicy {
    /**
     * In response to a preflight request, setting this to true indicates that the
     * actual request can include user credentials. This translates to the Access-
     * Control-Allow-Credentials header. Defaults to false.
     */
    allowCredentials?: boolean;
    /**
     * Specifies the content for the Access-Control-Allow-Headers header.
     */
    allowHeaders?: string[];
    /**
     * Specifies the content for the Access-Control-Allow-Methods header.
     */
    allowMethods?: string[];
    /**
     * Specifies the regular expression patterns that match allowed origins. For
     * regular expression grammar please see en.cppreference.com/w/cpp/regex/ecmascript
     * An origin is allowed if it matches either allow_origins or allow_origin_regex.
     */
    allowOriginRegexes?: string[];
    /**
     * Specifies the list of origins that will be allowed to do CORS requests. An
     * origin is allowed if it matches either allow_origins or allow_origin_regex.
     */
    allowOrigins?: string[];
    /**
     * If true, specifies the CORS policy is disabled.
     */
    disabled: boolean;
    /**
     * Specifies the content for the Access-Control-Expose-Headers header.
     */
    exposeHeaders?: string[];
    /**
     * Specifies how long the results of a preflight request can be cached. This
     * translates to the content for the Access-Control-Max-Age header.
     */
    maxAge?: number;
}

export interface ComputeUrlMapPathMatcherPathRuleRouteActionFaultInjectionPolicy {
    /**
     * The specification for how client requests are aborted as part of fault
     * injection.
     */
    abort?: outputs.ComputeUrlMapPathMatcherPathRuleRouteActionFaultInjectionPolicyAbort;
    /**
     * The specification for how client requests are delayed as part of fault
     * injection, before being sent to a backend service.
     */
    delay?: outputs.ComputeUrlMapPathMatcherPathRuleRouteActionFaultInjectionPolicyDelay;
}

export interface ComputeUrlMapPathMatcherPathRuleRouteActionFaultInjectionPolicyAbort {
    /**
     * The HTTP status code used to abort the request. The value must be between 200
     * and 599 inclusive.
     */
    httpStatus: number;
    /**
     * The percentage of traffic (connections/operations/requests) which will be
     * aborted as part of fault injection. The value must be between 0.0 and 100.0
     * inclusive.
     */
    percentage: number;
}

export interface ComputeUrlMapPathMatcherPathRuleRouteActionFaultInjectionPolicyDelay {
    /**
     * Specifies the value of the fixed delay interval.
     */
    fixedDelay: outputs.ComputeUrlMapPathMatcherPathRuleRouteActionFaultInjectionPolicyDelayFixedDelay;
    /**
     * The percentage of traffic (connections/operations/requests) on which delay will
     * be introduced as part of fault injection. The value must be between 0.0 and
     * 100.0 inclusive.
     */
    percentage: number;
}

export interface ComputeUrlMapPathMatcherPathRuleRouteActionFaultInjectionPolicyDelayFixedDelay {
    /**
     * Span of time that's a fraction of a second at nanosecond resolution. Durations
     * less than one second are represented with a 0 'seconds' field and a positive
     * 'nanos' field. Must be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000
     * inclusive.
     */
    seconds: string;
}

export interface ComputeUrlMapPathMatcherPathRuleRouteActionRequestMirrorPolicy {
    /**
     * The BackendService resource being mirrored to.
     */
    backendService: string;
}

export interface ComputeUrlMapPathMatcherPathRuleRouteActionRetryPolicy {
    /**
     * Specifies the allowed number retries. This number must be > 0.
     */
    numRetries?: number;
    /**
     * Specifies a non-zero timeout per retry attempt.
     */
    perTryTimeout?: outputs.ComputeUrlMapPathMatcherPathRuleRouteActionRetryPolicyPerTryTimeout;
    /**
     * Specifies one or more conditions when this retry rule applies. Valid values are:
     *
     * * 5xx: Loadbalancer will attempt a retry if the backend service responds with
     * any 5xx response code, or if the backend service does not respond at all,
     * for example: disconnects, reset, read timeout, connection failure, and refused
     * streams.
     * * gateway-error: Similar to 5xx, but only applies to response codes
     * 502, 503 or 504.
     * * connect-failure: Loadbalancer will retry on failures
     * connecting to backend services, for example due to connection timeouts.
     * * retriable-4xx: Loadbalancer will retry for retriable 4xx response codes.
     * Currently the only retriable error supported is 409.
     * * refused-stream: Loadbalancer will retry if the backend service resets the stream with a
     * REFUSED_STREAM error code. This reset type indicates that it is safe to retry.
     * * cancelled: Loadbalancer will retry if the gRPC status code in the response
     * header is set to cancelled
     * * deadline-exceeded: Loadbalancer will retry if the
     * gRPC status code in the response header is set to deadline-exceeded
     * * resource-exhausted: Loadbalancer will retry if the gRPC status code in the response
     * header is set to resource-exhausted
     * * unavailable: Loadbalancer will retry if
     * the gRPC status code in the response header is set to unavailable
     */
    retryConditions?: string[];
}

export interface ComputeUrlMapPathMatcherPathRuleRouteActionRetryPolicyPerTryTimeout {
    /**
     * Span of time that's a fraction of a second at nanosecond resolution. Durations
     * less than one second are represented with a 0 'seconds' field and a positive
     * 'nanos' field. Must be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000
     * inclusive.
     */
    seconds: string;
}

export interface ComputeUrlMapPathMatcherPathRuleRouteActionTimeout {
    /**
     * Span of time that's a fraction of a second at nanosecond resolution. Durations
     * less than one second are represented with a 0 'seconds' field and a positive
     * 'nanos' field. Must be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000
     * inclusive.
     */
    seconds: string;
}

export interface ComputeUrlMapPathMatcherPathRuleRouteActionUrlRewrite {
    /**
     * Prior to forwarding the request to the selected service, the request's host
     * header is replaced with contents of hostRewrite. The value must be between 1 and
     * 255 characters.
     */
    hostRewrite?: string;
    /**
     * Prior to forwarding the request to the selected backend service, the matching
     * portion of the request's path is replaced by pathPrefixRewrite. The value must
     * be between 1 and 1024 characters.
     */
    pathPrefixRewrite?: string;
}

export interface ComputeUrlMapPathMatcherPathRuleRouteActionWeightedBackendService {
    /**
     * The default BackendService resource. Before
     * forwarding the request to backendService, the loadbalancer applies any relevant
     * headerActions specified as part of this backendServiceWeight.
     */
    backendService: string;
    /**
     * Specifies changes to request and response headers that need to take effect for
     * the selected backendService. headerAction specified here take effect before
     * headerAction in the enclosing HttpRouteRule, PathMatcher and UrlMap.
     */
    headerAction?: outputs.ComputeUrlMapPathMatcherPathRuleRouteActionWeightedBackendServiceHeaderAction;
    /**
     * Specifies the fraction of traffic sent to backendService, computed as weight /
     * (sum of all weightedBackendService weights in routeAction) . The selection of a
     * backend service is determined only for new traffic. Once a user's request has
     * been directed to a backendService, subsequent requests will be sent to the same
     * backendService as determined by the BackendService's session affinity policy.
     * The value must be between 0 and 1000
     */
    weight: number;
}

export interface ComputeUrlMapPathMatcherPathRuleRouteActionWeightedBackendServiceHeaderAction {
    /**
     * Headers to add to a matching request prior to forwarding the request to the
     * backendService.
     */
    requestHeadersToAdds?: outputs.ComputeUrlMapPathMatcherPathRuleRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd[];
    /**
     * A list of header names for headers that need to be removed from the request
     * prior to forwarding the request to the backendService.
     */
    requestHeadersToRemoves?: string[];
    /**
     * Headers to add the response prior to sending the response back to the client.
     */
    responseHeadersToAdds?: outputs.ComputeUrlMapPathMatcherPathRuleRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd[];
    /**
     * A list of header names for headers that need to be removed from the response
     * prior to sending the response back to the client.
     */
    responseHeadersToRemoves?: string[];
}

export interface ComputeUrlMapPathMatcherPathRuleRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd {
    /**
     * The name of the header.
     */
    headerName: string;
    /**
     * The value of the header to add.
     */
    headerValue: string;
    /**
     * If false, headerValue is appended to any values that already exist for the
     * header. If true, headerValue is set for the header, discarding any values that
     * were set for that header.
     */
    replace: boolean;
}

export interface ComputeUrlMapPathMatcherPathRuleRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd {
    /**
     * The name of the header.
     */
    headerName: string;
    /**
     * The value of the header to add.
     */
    headerValue: string;
    /**
     * If false, headerValue is appended to any values that already exist for the
     * header. If true, headerValue is set for the header, discarding any values that
     * were set for that header.
     */
    replace: boolean;
}

export interface ComputeUrlMapPathMatcherPathRuleUrlRedirect {
    /**
     * The host that will be used in the redirect response instead of the one
     * that was supplied in the request. The value must be between 1 and 255
     * characters.
     */
    hostRedirect?: string;
    /**
     * If set to true, the URL scheme in the redirected request is set to https.
     * If set to false, the URL scheme of the redirected request will remain the
     * same as that of the request. This must only be set for UrlMaps used in
     * TargetHttpProxys. Setting this true for TargetHttpsProxy is not
     * permitted. The default is set to false.
     */
    httpsRedirect?: boolean;
    /**
     * The path that will be used in the redirect response instead of the one
     * that was supplied in the request. pathRedirect cannot be supplied
     * together with prefixRedirect. Supply one alone or neither. If neither is
     * supplied, the path of the original request will be used for the redirect.
     * The value must be between 1 and 1024 characters.
     */
    pathRedirect?: string;
    /**
     * The prefix that replaces the prefixMatch specified in the
     * HttpRouteRuleMatch, retaining the remaining portion of the URL before
     * redirecting the request. prefixRedirect cannot be supplied together with
     * pathRedirect. Supply one alone or neither. If neither is supplied, the
     * path of the original request will be used for the redirect. The value
     * must be between 1 and 1024 characters.
     */
    prefixRedirect?: string;
    /**
     * The HTTP Status code to use for this RedirectAction. Supported values are:
     *
     * * MOVED_PERMANENTLY_DEFAULT, which is the default value and corresponds to 301.
     *
     * * FOUND, which corresponds to 302.
     *
     * * SEE_OTHER which corresponds to 303.
     *
     * * TEMPORARY_REDIRECT, which corresponds to 307. In this case, the request method
     * will be retained.
     *
     * * PERMANENT_REDIRECT, which corresponds to 308. In this case,
     * the request method will be retained. Possible values: ["FOUND", "MOVED_PERMANENTLY_DEFAULT", "PERMANENT_REDIRECT", "SEE_OTHER", "TEMPORARY_REDIRECT"]
     */
    redirectResponseCode?: string;
    /**
     * If set to true, any accompanying query portion of the original URL is
     * removed prior to redirecting the request. If set to false, the query
     * portion of the original URL is retained.
     *  This field is required to ensure an empty block is not set. The normal default value is false.
     */
    stripQuery: boolean;
}

export interface ComputeUrlMapPathMatcherRouteRule {
    /**
     * Specifies changes to request and response headers that need to take effect for
     * the selected backendService. The headerAction specified here are applied before
     * the matching pathMatchers[].headerAction and after pathMatchers[].routeRules[].r
     * outeAction.weightedBackendService.backendServiceWeightAction[].headerAction
     */
    headerAction?: outputs.ComputeUrlMapPathMatcherRouteRuleHeaderAction;
    /**
     * The rules for determining a match.
     */
    matchRules?: outputs.ComputeUrlMapPathMatcherRouteRuleMatchRule[];
    /**
     * For routeRules within a given pathMatcher, priority determines the order
     * in which load balancer will interpret routeRules. RouteRules are evaluated
     * in order of priority, from the lowest to highest number. The priority of
     * a rule decreases as its number increases (1, 2, 3, N+1). The first rule
     * that matches the request is applied.
     *
     * You cannot configure two or more routeRules with the same priority.
     * Priority for each rule must be set to a number between 0 and
     * 2147483647 inclusive.
     *
     * Priority numbers can have gaps, which enable you to add or remove rules
     * in the future without affecting the rest of the rules. For example,
     * 1, 2, 3, 4, 5, 9, 12, 16 is a valid series of priority numbers to which
     * you could add rules numbered from 6 to 8, 10 to 11, and 13 to 15 in the
     * future without any impact on existing rules.
     */
    priority: number;
    /**
     * In response to a matching matchRule, the load balancer performs advanced routing
     * actions like URL rewrites, header transformations, etc. prior to forwarding the
     * request to the selected backend. If  routeAction specifies any
     * weightedBackendServices, service must not be set. Conversely if service is set,
     * routeAction cannot contain any  weightedBackendServices. Only one of routeAction
     * or urlRedirect must be set.
     */
    routeAction?: outputs.ComputeUrlMapPathMatcherRouteRuleRouteAction;
    /**
     * The backend service resource to which traffic is
     * directed if this rule is matched. If routeAction is additionally specified,
     * advanced routing actions like URL Rewrites, etc. take effect prior to sending
     * the request to the backend. However, if service is specified, routeAction cannot
     * contain any weightedBackendService s. Conversely, if routeAction specifies any
     * weightedBackendServices, service must not be specified. Only one of urlRedirect,
     * service or routeAction.weightedBackendService must be set.
     */
    service?: string;
    /**
     * When this rule is matched, the request is redirected to a URL specified by
     * urlRedirect. If urlRedirect is specified, service or routeAction must not be
     * set.
     */
    urlRedirect?: outputs.ComputeUrlMapPathMatcherRouteRuleUrlRedirect;
}

export interface ComputeUrlMapPathMatcherRouteRuleHeaderAction {
    /**
     * Headers to add to a matching request prior to forwarding the request to the
     * backendService.
     */
    requestHeadersToAdds?: outputs.ComputeUrlMapPathMatcherRouteRuleHeaderActionRequestHeadersToAdd[];
    /**
     * A list of header names for headers that need to be removed from the request
     * prior to forwarding the request to the backendService.
     */
    requestHeadersToRemoves?: string[];
    /**
     * Headers to add the response prior to sending the response back to the client.
     */
    responseHeadersToAdds?: outputs.ComputeUrlMapPathMatcherRouteRuleHeaderActionResponseHeadersToAdd[];
    /**
     * A list of header names for headers that need to be removed from the response
     * prior to sending the response back to the client.
     */
    responseHeadersToRemoves?: string[];
}

export interface ComputeUrlMapPathMatcherRouteRuleHeaderActionRequestHeadersToAdd {
    /**
     * The name of the header.
     */
    headerName: string;
    /**
     * The value of the header to add.
     */
    headerValue: string;
    /**
     * If false, headerValue is appended to any values that already exist for the
     * header. If true, headerValue is set for the header, discarding any values that
     * were set for that header.
     */
    replace: boolean;
}

export interface ComputeUrlMapPathMatcherRouteRuleHeaderActionResponseHeadersToAdd {
    /**
     * The name of the header.
     */
    headerName: string;
    /**
     * The value of the header to add.
     */
    headerValue: string;
    /**
     * If false, headerValue is appended to any values that already exist for the
     * header. If true, headerValue is set for the header, discarding any values that
     * were set for that header.
     */
    replace: boolean;
}

export interface ComputeUrlMapPathMatcherRouteRuleMatchRule {
    /**
     * For satisfying the matchRule condition, the path of the request must exactly
     * match the value specified in fullPathMatch after removing any query parameters
     * and anchor that may be part of the original URL. FullPathMatch must be between 1
     * and 1024 characters. Only one of prefixMatch, fullPathMatch or regexMatch must
     * be specified.
     */
    fullPathMatch?: string;
    /**
     * Specifies a list of header match criteria, all of which must match corresponding
     * headers in the request.
     */
    headerMatches?: outputs.ComputeUrlMapPathMatcherRouteRuleMatchRuleHeaderMatch[];
    /**
     * Specifies that prefixMatch and fullPathMatch matches are case sensitive.
     * Defaults to false.
     */
    ignoreCase?: boolean;
    /**
     * Opaque filter criteria used by Loadbalancer to restrict routing configuration to
     * a limited set xDS compliant clients. In their xDS requests to Loadbalancer, xDS
     * clients present node metadata. If a match takes place, the relevant routing
     * configuration is made available to those proxies. For each metadataFilter in
     * this list, if its filterMatchCriteria is set to MATCH_ANY, at least one of the
     * filterLabels must match the corresponding label provided in the metadata. If its
     * filterMatchCriteria is set to MATCH_ALL, then all of its filterLabels must match
     * with corresponding labels in the provided metadata. metadataFilters specified
     * here can be overrides those specified in ForwardingRule that refers to this
     * UrlMap. metadataFilters only applies to Loadbalancers that have their
     * loadBalancingScheme set to INTERNAL_SELF_MANAGED.
     */
    metadataFilters?: outputs.ComputeUrlMapPathMatcherRouteRuleMatchRuleMetadataFilter[];
    /**
     * For satisfying the matchRule condition, the path of the request
     * must match the wildcard pattern specified in pathTemplateMatch
     * after removing any query parameters and anchor that may be part
     * of the original URL.
     *
     * pathTemplateMatch must be between 1 and 255 characters
     * (inclusive).  The pattern specified by pathTemplateMatch may
     * have at most 5 wildcard operators and at most 5 variable
     * captures in total.
     */
    pathTemplateMatch?: string;
    /**
     * For satisfying the matchRule condition, the request's path must begin with the
     * specified prefixMatch. prefixMatch must begin with a /. The value must be
     * between 1 and 1024 characters. Only one of prefixMatch, fullPathMatch or
     * regexMatch must be specified.
     */
    prefixMatch?: string;
    /**
     * Specifies a list of query parameter match criteria, all of which must match
     * corresponding query parameters in the request.
     */
    queryParameterMatches?: outputs.ComputeUrlMapPathMatcherRouteRuleMatchRuleQueryParameterMatch[];
    /**
     * For satisfying the matchRule condition, the path of the request must satisfy the
     * regular expression specified in regexMatch after removing any query parameters
     * and anchor supplied with the original URL. For regular expression grammar please
     * see en.cppreference.com/w/cpp/regex/ecmascript  Only one of prefixMatch,
     * fullPathMatch or regexMatch must be specified.
     */
    regexMatch?: string;
}

export interface ComputeUrlMapPathMatcherRouteRuleMatchRuleHeaderMatch {
    /**
     * The value should exactly match contents of exactMatch. Only one of exactMatch,
     * prefixMatch, suffixMatch, regexMatch, presentMatch or rangeMatch must be set.
     */
    exactMatch?: string;
    /**
     * The name of the HTTP header to match. For matching against the HTTP request's
     * authority, use a headerMatch with the header name ":authority". For matching a
     * request's method, use the headerName ":method".
     */
    headerName: string;
    /**
     * If set to false, the headerMatch is considered a match if the match criteria
     * above are met. If set to true, the headerMatch is considered a match if the
     * match criteria above are NOT met. Defaults to false.
     */
    invertMatch?: boolean;
    /**
     * The value of the header must start with the contents of prefixMatch. Only one of
     * exactMatch, prefixMatch, suffixMatch, regexMatch, presentMatch or rangeMatch
     * must be set.
     */
    prefixMatch?: string;
    /**
     * A header with the contents of headerName must exist. The match takes place
     * whether or not the request's header has a value or not. Only one of exactMatch,
     * prefixMatch, suffixMatch, regexMatch, presentMatch or rangeMatch must be set.
     */
    presentMatch?: boolean;
    /**
     * The header value must be an integer and its value must be in the range specified
     * in rangeMatch. If the header does not contain an integer, number or is empty,
     * the match fails. For example for a range [-5, 0]   - -3 will match.  - 0 will
     * not match.  - 0.25 will not match.  - -3someString will not match.   Only one of
     * exactMatch, prefixMatch, suffixMatch, regexMatch, presentMatch or rangeMatch
     * must be set.
     */
    rangeMatch?: outputs.ComputeUrlMapPathMatcherRouteRuleMatchRuleHeaderMatchRangeMatch;
    /**
     * The value of the header must match the regular expression specified in
     * regexMatch. For regular expression grammar, please see:
     * en.cppreference.com/w/cpp/regex/ecmascript  For matching against a port
     * specified in the HTTP request, use a headerMatch with headerName set to PORT and
     * a regular expression that satisfies the RFC2616 Host header's port specifier.
     * Only one of exactMatch, prefixMatch, suffixMatch, regexMatch, presentMatch or
     * rangeMatch must be set.
     */
    regexMatch?: string;
    /**
     * The value of the header must end with the contents of suffixMatch. Only one of
     * exactMatch, prefixMatch, suffixMatch, regexMatch, presentMatch or rangeMatch
     * must be set.
     */
    suffixMatch?: string;
}

export interface ComputeUrlMapPathMatcherRouteRuleMatchRuleHeaderMatchRangeMatch {
    /**
     * The end of the range (exclusive).
     */
    rangeEnd: number;
    /**
     * The start of the range (inclusive).
     */
    rangeStart: number;
}

export interface ComputeUrlMapPathMatcherRouteRuleMatchRuleMetadataFilter {
    /**
     * The list of label value pairs that must match labels in the provided metadata
     * based on filterMatchCriteria  This list must not be empty and can have at the
     * most 64 entries.
     */
    filterLabels: outputs.ComputeUrlMapPathMatcherRouteRuleMatchRuleMetadataFilterFilterLabel[];
    /**
     * Specifies how individual filterLabel matches within the list of filterLabels
     * contribute towards the overall metadataFilter match. Supported values are:
     *   - MATCH_ANY: At least one of the filterLabels must have a matching label in the
     * provided metadata.
     *   - MATCH_ALL: All filterLabels must have matching labels in
     * the provided metadata. Possible values: ["MATCH_ALL", "MATCH_ANY"]
     */
    filterMatchCriteria: string;
}

export interface ComputeUrlMapPathMatcherRouteRuleMatchRuleMetadataFilterFilterLabel {
    /**
     * Name of metadata label. The name can have a maximum length of 1024 characters
     * and must be at least 1 character long.
     */
    name: string;
    /**
     * The value of the label must match the specified value. value can have a maximum
     * length of 1024 characters.
     */
    value: string;
}

export interface ComputeUrlMapPathMatcherRouteRuleMatchRuleQueryParameterMatch {
    /**
     * The queryParameterMatch matches if the value of the parameter exactly matches
     * the contents of exactMatch. Only one of presentMatch, exactMatch and regexMatch
     * must be set.
     */
    exactMatch?: string;
    /**
     * The name of the query parameter to match. The query parameter must exist in the
     * request, in the absence of which the request match fails.
     */
    name: string;
    /**
     * Specifies that the queryParameterMatch matches if the request contains the query
     * parameter, irrespective of whether the parameter has a value or not. Only one of
     * presentMatch, exactMatch and regexMatch must be set.
     */
    presentMatch?: boolean;
    /**
     * The queryParameterMatch matches if the value of the parameter matches the
     * regular expression specified by regexMatch. For the regular expression grammar,
     * please see en.cppreference.com/w/cpp/regex/ecmascript  Only one of presentMatch,
     * exactMatch and regexMatch must be set.
     */
    regexMatch?: string;
}

export interface ComputeUrlMapPathMatcherRouteRuleRouteAction {
    /**
     * The specification for allowing client side cross-origin requests. Please see W3C
     * Recommendation for Cross Origin Resource Sharing
     */
    corsPolicy?: outputs.ComputeUrlMapPathMatcherRouteRuleRouteActionCorsPolicy;
    /**
     * The specification for fault injection introduced into traffic to test the
     * resiliency of clients to backend service failure. As part of fault injection,
     * when clients send requests to a backend service, delays can be introduced by
     * Loadbalancer on a percentage of requests before sending those request to the
     * backend service. Similarly requests from clients can be aborted by the
     * Loadbalancer for a percentage of requests. timeout and retry_policy will be
     * ignored by clients that are configured with a fault_injection_policy.
     */
    faultInjectionPolicy?: outputs.ComputeUrlMapPathMatcherRouteRuleRouteActionFaultInjectionPolicy;
    /**
     * Specifies the policy on how requests intended for the route's backends are
     * shadowed to a separate mirrored backend service. Loadbalancer does not wait for
     * responses from the shadow service. Prior to sending traffic to the shadow
     * service, the host / authority header is suffixed with -shadow.
     */
    requestMirrorPolicy?: outputs.ComputeUrlMapPathMatcherRouteRuleRouteActionRequestMirrorPolicy;
    /**
     * Specifies the retry policy associated with this route.
     */
    retryPolicy?: outputs.ComputeUrlMapPathMatcherRouteRuleRouteActionRetryPolicy;
    /**
     * Specifies the timeout for the selected route. Timeout is computed from the time
     * the request is has been fully processed (i.e. end-of-stream) up until the
     * response has been completely processed. Timeout includes all retries. If not
     * specified, the default value is 15 seconds.
     */
    timeout?: outputs.ComputeUrlMapPathMatcherRouteRuleRouteActionTimeout;
    /**
     * The spec to modify the URL of the request, prior to forwarding the request to
     * the matched service
     */
    urlRewrite?: outputs.ComputeUrlMapPathMatcherRouteRuleRouteActionUrlRewrite;
    /**
     * A list of weighted backend services to send traffic to when a route match
     * occurs. The weights determine the fraction of traffic that flows to their
     * corresponding backend service. If all traffic needs to go to a single backend
     * service, there must be one  weightedBackendService with weight set to a non 0
     * number. Once a backendService is identified and before forwarding the request to
     * the backend service, advanced routing actions like Url rewrites and header
     * transformations are applied depending on additional settings specified in this
     * HttpRouteAction.
     */
    weightedBackendServices?: outputs.ComputeUrlMapPathMatcherRouteRuleRouteActionWeightedBackendService[];
}

export interface ComputeUrlMapPathMatcherRouteRuleRouteActionCorsPolicy {
    /**
     * In response to a preflight request, setting this to true indicates that the
     * actual request can include user credentials. This translates to the Access-
     * Control-Allow-Credentials header. Defaults to false.
     */
    allowCredentials?: boolean;
    /**
     * Specifies the content for the Access-Control-Allow-Headers header.
     */
    allowHeaders?: string[];
    /**
     * Specifies the content for the Access-Control-Allow-Methods header.
     */
    allowMethods?: string[];
    /**
     * Specifies the regular expression patterns that match allowed origins. For
     * regular expression grammar please see en.cppreference.com/w/cpp/regex/ecmascript
     * An origin is allowed if it matches either allow_origins or allow_origin_regex.
     */
    allowOriginRegexes?: string[];
    /**
     * Specifies the list of origins that will be allowed to do CORS requests. An
     * origin is allowed if it matches either allow_origins or allow_origin_regex.
     */
    allowOrigins?: string[];
    /**
     * If true, specifies the CORS policy is disabled.
     * which indicates that the CORS policy is in effect. Defaults to false.
     */
    disabled?: boolean;
    /**
     * Specifies the content for the Access-Control-Expose-Headers header.
     */
    exposeHeaders?: string[];
    /**
     * Specifies how long the results of a preflight request can be cached. This
     * translates to the content for the Access-Control-Max-Age header.
     */
    maxAge?: number;
}

export interface ComputeUrlMapPathMatcherRouteRuleRouteActionFaultInjectionPolicy {
    /**
     * The specification for how client requests are aborted as part of fault
     * injection.
     */
    abort?: outputs.ComputeUrlMapPathMatcherRouteRuleRouteActionFaultInjectionPolicyAbort;
    /**
     * The specification for how client requests are delayed as part of fault
     * injection, before being sent to a backend service.
     */
    delay?: outputs.ComputeUrlMapPathMatcherRouteRuleRouteActionFaultInjectionPolicyDelay;
}

export interface ComputeUrlMapPathMatcherRouteRuleRouteActionFaultInjectionPolicyAbort {
    /**
     * The HTTP status code used to abort the request. The value must be between 200
     * and 599 inclusive.
     */
    httpStatus?: number;
    /**
     * The percentage of traffic (connections/operations/requests) which will be
     * aborted as part of fault injection. The value must be between 0.0 and 100.0
     * inclusive.
     */
    percentage?: number;
}

export interface ComputeUrlMapPathMatcherRouteRuleRouteActionFaultInjectionPolicyDelay {
    /**
     * Specifies the value of the fixed delay interval.
     */
    fixedDelay?: outputs.ComputeUrlMapPathMatcherRouteRuleRouteActionFaultInjectionPolicyDelayFixedDelay;
    /**
     * The percentage of traffic (connections/operations/requests) on which delay will
     * be introduced as part of fault injection. The value must be between 0.0 and
     * 100.0 inclusive.
     */
    percentage?: number;
}

export interface ComputeUrlMapPathMatcherRouteRuleRouteActionFaultInjectionPolicyDelayFixedDelay {
    /**
     * Span of time that's a fraction of a second at nanosecond resolution. Durations
     * less than one second are represented with a 0 'seconds' field and a positive
     * 'nanos' field. Must be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000
     * inclusive.
     */
    seconds: string;
}

export interface ComputeUrlMapPathMatcherRouteRuleRouteActionRequestMirrorPolicy {
    /**
     * The BackendService resource being mirrored to.
     */
    backendService: string;
}

export interface ComputeUrlMapPathMatcherRouteRuleRouteActionRetryPolicy {
    /**
     * Specifies the allowed number retries. This number must be > 0.
     */
    numRetries: number;
    /**
     * Specifies a non-zero timeout per retry attempt.
     * If not specified, will use the timeout set in HttpRouteAction. If timeout in HttpRouteAction
     * is not set, will use the largest timeout among all backend services associated with the route.
     */
    perTryTimeout?: outputs.ComputeUrlMapPathMatcherRouteRuleRouteActionRetryPolicyPerTryTimeout;
    /**
     * Specfies one or more conditions when this retry rule applies. Valid values are:
     *
     * * 5xx: Loadbalancer will attempt a retry if the backend service responds with
     *   any 5xx response code, or if the backend service does not respond at all,
     *   for example: disconnects, reset, read timeout, connection failure, and refused
     *   streams.
     * * gateway-error: Similar to 5xx, but only applies to response codes
     *   502, 503 or 504.
     * * connect-failure: Loadbalancer will retry on failures
     *   connecting to backend services, for example due to connection timeouts.
     * * retriable-4xx: Loadbalancer will retry for retriable 4xx response codes.
     *   Currently the only retriable error supported is 409.
     * * refused-stream: Loadbalancer will retry if the backend service resets the stream with a
     *   REFUSED_STREAM error code. This reset type indicates that it is safe to retry.
     * * cancelled: Loadbalancer will retry if the gRPC status code in the response
     *   header is set to cancelled
     * * deadline-exceeded: Loadbalancer will retry if the
     *   gRPC status code in the response header is set to deadline-exceeded
     * * resource-exhausted: Loadbalancer will retry if the gRPC status code in the response
     *   header is set to resource-exhausted
     * * unavailable: Loadbalancer will retry if the gRPC status code in
     *   the response header is set to unavailable
     */
    retryConditions?: string[];
}

export interface ComputeUrlMapPathMatcherRouteRuleRouteActionRetryPolicyPerTryTimeout {
    /**
     * Span of time that's a fraction of a second at nanosecond resolution. Durations
     * less than one second are represented with a 0 'seconds' field and a positive
     * 'nanos' field. Must be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000
     * inclusive.
     */
    seconds: string;
}

export interface ComputeUrlMapPathMatcherRouteRuleRouteActionTimeout {
    /**
     * Span of time that's a fraction of a second at nanosecond resolution. Durations
     * less than one second are represented with a 0 'seconds' field and a positive
     * 'nanos' field. Must be from 0 to 999,999,999 inclusive.
     */
    nanos?: number;
    /**
     * Span of time at a resolution of a second. Must be from 0 to 315,576,000,000
     * inclusive.
     */
    seconds: string;
}

export interface ComputeUrlMapPathMatcherRouteRuleRouteActionUrlRewrite {
    /**
     * Prior to forwarding the request to the selected service, the request's host
     * header is replaced with contents of hostRewrite. The value must be between 1 and
     * 255 characters.
     */
    hostRewrite?: string;
    /**
     * Prior to forwarding the request to the selected backend service, the matching
     * portion of the request's path is replaced by pathPrefixRewrite. The value must
     * be between 1 and 1024 characters.
     */
    pathPrefixRewrite?: string;
    /**
     * Prior to forwarding the request to the selected origin, if the
     * request matched a pathTemplateMatch, the matching portion of the
     * request's path is replaced re-written using the pattern specified
     * by pathTemplateRewrite.
     *
     * pathTemplateRewrite must be between 1 and 255 characters
     * (inclusive), must start with a '/', and must only use variables
     * captured by the route's pathTemplate matchers.
     *
     * pathTemplateRewrite may only be used when all of a route's
     * MatchRules specify pathTemplate.
     *
     * Only one of pathPrefixRewrite and pathTemplateRewrite may be
     * specified.
     */
    pathTemplateRewrite?: string;
}

export interface ComputeUrlMapPathMatcherRouteRuleRouteActionWeightedBackendService {
    /**
     * The default BackendService resource. Before
     * forwarding the request to backendService, the loadbalancer applies any relevant
     * headerActions specified as part of this backendServiceWeight.
     */
    backendService: string;
    /**
     * Specifies changes to request and response headers that need to take effect for
     * the selected backendService. headerAction specified here take effect before
     * headerAction in the enclosing HttpRouteRule, PathMatcher and UrlMap.
     */
    headerAction?: outputs.ComputeUrlMapPathMatcherRouteRuleRouteActionWeightedBackendServiceHeaderAction;
    /**
     * Specifies the fraction of traffic sent to backendService, computed as weight /
     * (sum of all weightedBackendService weights in routeAction) . The selection of a
     * backend service is determined only for new traffic. Once a user's request has
     * been directed to a backendService, subsequent requests will be sent to the same
     * backendService as determined by the BackendService's session affinity policy.
     * The value must be between 0 and 1000
     */
    weight: number;
}

export interface ComputeUrlMapPathMatcherRouteRuleRouteActionWeightedBackendServiceHeaderAction {
    /**
     * Headers to add to a matching request prior to forwarding the request to the
     * backendService.
     */
    requestHeadersToAdds?: outputs.ComputeUrlMapPathMatcherRouteRuleRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd[];
    /**
     * A list of header names for headers that need to be removed from the request
     * prior to forwarding the request to the backendService.
     */
    requestHeadersToRemoves?: string[];
    /**
     * Headers to add the response prior to sending the response back to the client.
     */
    responseHeadersToAdds?: outputs.ComputeUrlMapPathMatcherRouteRuleRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd[];
    /**
     * A list of header names for headers that need to be removed from the response
     * prior to sending the response back to the client.
     */
    responseHeadersToRemoves?: string[];
}

export interface ComputeUrlMapPathMatcherRouteRuleRouteActionWeightedBackendServiceHeaderActionRequestHeadersToAdd {
    /**
     * The name of the header.
     */
    headerName: string;
    /**
     * The value of the header to add.
     */
    headerValue: string;
    /**
     * If false, headerValue is appended to any values that already exist for the
     * header. If true, headerValue is set for the header, discarding any values that
     * were set for that header.
     */
    replace: boolean;
}

export interface ComputeUrlMapPathMatcherRouteRuleRouteActionWeightedBackendServiceHeaderActionResponseHeadersToAdd {
    /**
     * The name of the header.
     */
    headerName: string;
    /**
     * The value of the header to add.
     */
    headerValue: string;
    /**
     * If false, headerValue is appended to any values that already exist for the
     * header. If true, headerValue is set for the header, discarding any values that
     * were set for that header.
     */
    replace: boolean;
}

export interface ComputeUrlMapPathMatcherRouteRuleUrlRedirect {
    /**
     * The host that will be used in the redirect response instead of the one that was
     * supplied in the request. The value must be between 1 and 255 characters.
     */
    hostRedirect?: string;
    /**
     * If set to true, the URL scheme in the redirected request is set to https. If set
     * to false, the URL scheme of the redirected request will remain the same as that
     * of the request. This must only be set for UrlMaps used in TargetHttpProxys.
     * Setting this true for TargetHttpsProxy is not permitted. Defaults to false.
     */
    httpsRedirect?: boolean;
    /**
     * The path that will be used in the redirect response instead of the one that was
     * supplied in the request. Only one of pathRedirect or prefixRedirect must be
     * specified. The value must be between 1 and 1024 characters.
     */
    pathRedirect?: string;
    /**
     * The prefix that replaces the prefixMatch specified in the HttpRouteRuleMatch,
     * retaining the remaining portion of the URL before redirecting the request.
     */
    prefixRedirect?: string;
    /**
     * The HTTP Status code to use for this RedirectAction. Supported values are:
     *
     * * MOVED_PERMANENTLY_DEFAULT, which is the default value and corresponds to 301.
     *
     * * FOUND, which corresponds to 302.
     *
     * * SEE_OTHER which corresponds to 303.
     *
     * * TEMPORARY_REDIRECT, which corresponds to 307. In this case, the request method will be retained.
     *
     * * PERMANENT_REDIRECT, which corresponds to 308. In this case, the request method will be retained. Possible values: ["FOUND", "MOVED_PERMANENTLY_DEFAULT", "PERMANENT_REDIRECT", "SEE_OTHER", "TEMPORARY_REDIRECT"]
     */
    redirectResponseCode?: string;
    /**
     * If set to true, any accompanying query portion of the original URL is removed
     * prior to redirecting the request. If set to false, the query portion of the
     * original URL is retained. Defaults to false.
     */
    stripQuery?: boolean;
}

export interface ComputeUrlMapTest {
    /**
     * Description of this test case.
     */
    description?: string;
    /**
     * Host portion of the URL.
     */
    host: string;
    /**
     * Path portion of the URL.
     */
    path: string;
    /**
     * The backend service or backend bucket link that should be matched by this test.
     */
    service: string;
}

export interface ComputeUrlMapTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ComputeVpnGatewayTimeouts {
    create?: string;
    delete?: string;
}

export interface ComputeVpnTunnelTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ContainerAnalysisNoteAttestationAuthority {
    /**
     * This submessage provides human-readable hints about the purpose of
     * the AttestationAuthority. Because the name of a Note acts as its
     * resource reference, it is important to disambiguate the canonical
     * name of the Note (which might be a UUID for security purposes)
     * from "readable" names more suitable for debug output. Note that
     * these hints should NOT be used to look up AttestationAuthorities
     * in security sensitive contexts, such as when looking up
     * Attestations to verify.
     */
    hint: outputs.ContainerAnalysisNoteAttestationAuthorityHint;
}

export interface ContainerAnalysisNoteAttestationAuthorityHint {
    /**
     * The human readable name of this Attestation Authority, for
     * example "qa".
     */
    humanReadableName: string;
}

export interface ContainerAnalysisNoteIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ContainerAnalysisNoteIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ContainerAnalysisNoteRelatedUrl {
    /**
     * Label to describe usage of the URL
     */
    label?: string;
    /**
     * Specific URL associated with the resource.
     */
    url: string;
}

export interface ContainerAnalysisNoteTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ContainerAnalysisOccurrenceAttestation {
    /**
     * The serialized payload that is verified by one or
     * more signatures. A base64-encoded string.
     */
    serializedPayload: string;
    /**
     * One or more signatures over serializedPayload.
     * Verifier implementations should consider this attestation
     * message verified if at least one signature verifies
     * serializedPayload. See Signature in common.proto for more
     * details on signature structure and verification.
     */
    signatures: outputs.ContainerAnalysisOccurrenceAttestationSignature[];
}

export interface ContainerAnalysisOccurrenceAttestationSignature {
    /**
     * The identifier for the public key that verifies this
     * signature. MUST be an RFC3986 conformant
     * URI. * When possible, the key id should be an
     * immutable reference, such as a cryptographic digest.
     * Examples of valid values:
     *
     * * OpenPGP V4 public key fingerprint. See https://www.iana.org/assignments/uri-schemes/prov/openpgp4fpr
     *   for more details on this scheme.
     *     * 'openpgp4fpr:74FAF3B861BDA0870C7B6DEF607E48D2A663AEEA'
     * * RFC6920 digest-named SubjectPublicKeyInfo (digest of the DER serialization):
     *     * "ni:///sha-256;cD9o9Cq6LG3jD0iKXqEi_vdjJGecm_iXkbqVoScViaU"
     */
    publicKeyId: string;
    /**
     * The content of the signature, an opaque bytestring.
     * The payload that this signature verifies MUST be
     * unambiguously provided with the Signature during
     * verification. A wrapper message might provide the
     * payload explicitly. Alternatively, a message might
     * have a canonical serialization that can always be
     * unambiguously computed to derive the payload.
     */
    signature?: string;
}

export interface ContainerAnalysisOccurrenceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ContainerAttachedClusterAuthorization {
    /**
     * Groups that can perform operations as a cluster admin. A managed
     * ClusterRoleBinding will be created to grant the 'cluster-admin' ClusterRole
     * to the groups. Up to ten admin groups can be provided.
     *
     * For more info on RBAC, see
     * https://kubernetes.io/docs/reference/access-authn-authz/rbac/#user-facing-roles
     */
    adminGroups?: string[];
    /**
     * Users that can perform operations as a cluster admin. A managed
     * ClusterRoleBinding will be created to grant the 'cluster-admin' ClusterRole
     * to the users. Up to ten admin users can be provided.
     *
     * For more info on RBAC, see
     * https://kubernetes.io/docs/reference/access-authn-authz/rbac/#user-facing-roles
     */
    adminUsers?: string[];
}

export interface ContainerAttachedClusterBinaryAuthorization {
    /**
     * Configure Binary Authorization evaluation mode. Possible values: ["DISABLED", "PROJECT_SINGLETON_POLICY_ENFORCE"]
     */
    evaluationMode?: string;
}

export interface ContainerAttachedClusterError {
    message: string;
}

export interface ContainerAttachedClusterFleet {
    /**
     * The name of the managed Hub Membership resource associated to this
     * cluster. Membership names are formatted as
     * projects/<project-number>/locations/global/membership/<cluster-id>.
     */
    membership: string;
    /**
     * The number of the Fleet host project where this cluster will be registered.
     */
    project: string;
}

export interface ContainerAttachedClusterLoggingConfig {
    /**
     * The configuration of the logging components
     */
    componentConfig?: outputs.ContainerAttachedClusterLoggingConfigComponentConfig;
}

export interface ContainerAttachedClusterLoggingConfigComponentConfig {
    /**
     * The components to be enabled. Possible values: ["SYSTEM_COMPONENTS", "WORKLOADS"]
     */
    enableComponents?: string[];
}

export interface ContainerAttachedClusterMonitoringConfig {
    /**
     * Enable Google Cloud Managed Service for Prometheus in the cluster.
     */
    managedPrometheusConfig?: outputs.ContainerAttachedClusterMonitoringConfigManagedPrometheusConfig;
}

export interface ContainerAttachedClusterMonitoringConfigManagedPrometheusConfig {
    /**
     * Enable Managed Collection.
     */
    enabled?: boolean;
}

export interface ContainerAttachedClusterOidcConfig {
    /**
     * A JSON Web Token (JWT) issuer URI. 'issuer' must start with 'https://'
     */
    issuerUrl: string;
    /**
     * OIDC verification keys in JWKS format (RFC 7517).
     */
    jwks?: string;
}

export interface ContainerAttachedClusterProxyConfig {
    /**
     * The Kubernetes Secret resource that contains the HTTP(S) proxy configuration.
     */
    kubernetesSecret?: outputs.ContainerAttachedClusterProxyConfigKubernetesSecret;
}

export interface ContainerAttachedClusterProxyConfigKubernetesSecret {
    /**
     * Name of the kubernetes secret containing the proxy config.
     */
    name: string;
    /**
     * Namespace of the kubernetes secret containing the proxy config.
     */
    namespace: string;
}

export interface ContainerAttachedClusterTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ContainerAttachedClusterWorkloadIdentityConfig {
    identityProvider: string;
    issuerUri: string;
    workloadPool: string;
}

export interface ContainerAwsClusterAuthorization {
    /**
     * Groups of users that can perform operations as a cluster admin. A managed ClusterRoleBinding will be created to grant the `cluster-admin` ClusterRole to the groups. Up to ten admin groups can be provided. For more info on RBAC, see https://kubernetes.io/docs/reference/access-authn-authz/rbac/#user-facing-roles
     */
    adminGroups?: outputs.ContainerAwsClusterAuthorizationAdminGroup[];
    /**
     * Users to perform operations as a cluster admin. A managed ClusterRoleBinding will be created to grant the `cluster-admin` ClusterRole to the users. Up to ten admin users can be provided. For more info on RBAC, see https://kubernetes.io/docs/reference/access-authn-authz/rbac/#user-facing-roles
     */
    adminUsers: outputs.ContainerAwsClusterAuthorizationAdminUser[];
}

export interface ContainerAwsClusterAuthorizationAdminGroup {
    /**
     * The name of the group, e.g. `my-group@domain.com`.
     */
    group: string;
}

export interface ContainerAwsClusterAuthorizationAdminUser {
    /**
     * The name of the user, e.g. `my-gcp-id@gmail.com`.
     */
    username: string;
}

export interface ContainerAwsClusterBinaryAuthorization {
    /**
     * Mode of operation for Binary Authorization policy evaluation. Possible values: DISABLED, PROJECT_SINGLETON_POLICY_ENFORCE
     */
    evaluationMode: string;
}

export interface ContainerAwsClusterControlPlane {
    /**
     * Authentication configuration for management of AWS resources.
     */
    awsServicesAuthentication: outputs.ContainerAwsClusterControlPlaneAwsServicesAuthentication;
    /**
     * The ARN of the AWS KMS key used to encrypt cluster configuration.
     */
    configEncryption: outputs.ContainerAwsClusterControlPlaneConfigEncryption;
    /**
     * The ARN of the AWS KMS key used to encrypt cluster secrets.
     */
    databaseEncryption: outputs.ContainerAwsClusterControlPlaneDatabaseEncryption;
    /**
     * The name of the AWS IAM instance pofile to assign to each control plane replica.
     */
    iamInstanceProfile: string;
    /**
     * Optional. The AWS instance type. When unspecified, it defaults to `m5.large`.
     */
    instanceType: string;
    /**
     * Optional. Configuration related to the main volume provisioned for each control plane replica. The main volume is in charge of storing all of the cluster's etcd state. Volumes will be provisioned in the availability zone associated with the corresponding subnet. When unspecified, it defaults to 8 GiB with the GP2 volume type.
     */
    mainVolume?: outputs.ContainerAwsClusterControlPlaneMainVolume;
    /**
     * Proxy configuration for outbound HTTP(S) traffic.
     */
    proxyConfig?: outputs.ContainerAwsClusterControlPlaneProxyConfig;
    /**
     * Optional. Configuration related to the root volume provisioned for each control plane replica. Volumes will be provisioned in the availability zone associated with the corresponding subnet. When unspecified, it defaults to 32 GiB with the GP2 volume type.
     */
    rootVolume?: outputs.ContainerAwsClusterControlPlaneRootVolume;
    /**
     * Optional. The IDs of additional security groups to add to control plane replicas. The Anthos Multi-Cloud API will automatically create and manage security groups with the minimum rules needed for a functioning cluster.
     */
    securityGroupIds?: string[];
    /**
     * Optional. SSH configuration for how to access the underlying control plane machines.
     */
    sshConfig?: outputs.ContainerAwsClusterControlPlaneSshConfig;
    /**
     * The list of subnets where control plane replicas will run. A replica will be provisioned on each subnet and up to three values can be provided. Each subnet must be in a different AWS Availability Zone (AZ).
     */
    subnetIds: string[];
    /**
     * Optional. A set of AWS resource tags to propagate to all underlying managed AWS resources. Specify at most 50 pairs containing alphanumerics, spaces, and symbols (.+-=_:@/). Keys can be up to 127 Unicode characters. Values can be up to 255 Unicode characters.
     */
    tags?: {[key: string]: string};
    /**
     * The Kubernetes version to run on control plane replicas (e.g. `1.19.10-gke.1000`). You can list all supported versions on a given Google Cloud region by calling .
     */
    version: string;
}

export interface ContainerAwsClusterControlPlaneAwsServicesAuthentication {
    /**
     * The Amazon Resource Name (ARN) of the role that the Anthos Multi-Cloud API will assume when managing AWS resources on your account.
     */
    roleArn: string;
    /**
     * Optional. An identifier for the assumed role session. When unspecified, it defaults to `multicloud-service-agent`.
     */
    roleSessionName: string;
}

export interface ContainerAwsClusterControlPlaneConfigEncryption {
    /**
     * The ARN of the AWS KMS key used to encrypt cluster configuration.
     */
    kmsKeyArn: string;
}

export interface ContainerAwsClusterControlPlaneDatabaseEncryption {
    /**
     * The ARN of the AWS KMS key used to encrypt cluster secrets.
     */
    kmsKeyArn: string;
}

export interface ContainerAwsClusterControlPlaneMainVolume {
    /**
     * Optional. The number of I/O operations per second (IOPS) to provision for GP3 volume.
     */
    iops: number;
    /**
     * Optional. The Amazon Resource Name (ARN) of the Customer Managed Key (CMK) used to encrypt AWS EBS volumes. If not specified, the default Amazon managed key associated to the AWS region where this cluster runs will be used.
     */
    kmsKeyArn?: string;
    /**
     * Optional. The size of the volume, in GiBs. When unspecified, a default value is provided. See the specific reference in the parent resource.
     */
    sizeGib: number;
    /**
     * Optional. The throughput to provision for the volume, in MiB/s. Only valid if the volume type is GP3. If volume type is gp3 and throughput is not specified, the throughput will defaults to 125.
     */
    throughput: number;
    /**
     * Optional. Type of the EBS volume. When unspecified, it defaults to GP2 volume. Possible values: VOLUME_TYPE_UNSPECIFIED, GP2, GP3
     */
    volumeType: string;
}

export interface ContainerAwsClusterControlPlaneProxyConfig {
    /**
     * The ARN of the AWS Secret Manager secret that contains the HTTP(S) proxy configuration.
     */
    secretArn: string;
    /**
     * The version string of the AWS Secret Manager secret that contains the HTTP(S) proxy configuration.
     */
    secretVersion: string;
}

export interface ContainerAwsClusterControlPlaneRootVolume {
    /**
     * Optional. The number of I/O operations per second (IOPS) to provision for GP3 volume.
     */
    iops: number;
    /**
     * Optional. The Amazon Resource Name (ARN) of the Customer Managed Key (CMK) used to encrypt AWS EBS volumes. If not specified, the default Amazon managed key associated to the AWS region where this cluster runs will be used.
     */
    kmsKeyArn?: string;
    /**
     * Optional. The size of the volume, in GiBs. When unspecified, a default value is provided. See the specific reference in the parent resource.
     */
    sizeGib: number;
    /**
     * Optional. The throughput to provision for the volume, in MiB/s. Only valid if the volume type is GP3. If volume type is gp3 and throughput is not specified, the throughput will defaults to 125.
     */
    throughput: number;
    /**
     * Optional. Type of the EBS volume. When unspecified, it defaults to GP2 volume. Possible values: VOLUME_TYPE_UNSPECIFIED, GP2, GP3
     */
    volumeType: string;
}

export interface ContainerAwsClusterControlPlaneSshConfig {
    /**
     * The name of the EC2 key pair used to login into cluster machines.
     */
    ec2KeyPair: string;
}

export interface ContainerAwsClusterFleet {
    /**
     * The name of the managed Hub Membership resource associated to this cluster. Membership names are formatted as projects/<project-number>/locations/global/membership/<cluster-id>.
     */
    membership: string;
    /**
     * The number of the Fleet host project where this cluster will be registered.
     */
    project: string;
}

export interface ContainerAwsClusterNetworking {
    /**
     * Disable the per node pool subnet security group rules on the control plane security group. When set to true, you must also provide one or more security groups that ensure node pools are able to send requests to the control plane on TCP/443 and TCP/8132. Failure to do so may result in unavailable node pools.
     */
    perNodePoolSgRulesDisabled?: boolean;
    /**
     * All pods in the cluster are assigned an RFC1918 IPv4 address from these ranges. Only a single range is supported. This field cannot be changed after creation.
     */
    podAddressCidrBlocks: string[];
    /**
     * All services in the cluster are assigned an RFC1918 IPv4 address from these ranges. Only a single range is supported. This field cannot be changed after creation.
     */
    serviceAddressCidrBlocks: string[];
    /**
     * The VPC associated with the cluster. All component clusters (i.e. control plane and node pools) run on a single VPC. This field cannot be changed after creation.
     */
    vpcId: string;
}

export interface ContainerAwsClusterTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ContainerAwsClusterWorkloadIdentityConfig {
    identityProvider: string;
    issuerUri: string;
    workloadPool: string;
}

export interface ContainerAwsNodePoolAutoscaling {
    /**
     * Maximum number of nodes in the NodePool. Must be >= min_node_count.
     */
    maxNodeCount: number;
    /**
     * Minimum number of nodes in the NodePool. Must be >= 1 and <= max_node_count.
     */
    minNodeCount: number;
}

export interface ContainerAwsNodePoolConfig {
    /**
     * Optional. Configuration related to CloudWatch metrics collection on the Auto Scaling group of the node pool. When unspecified, metrics collection is disabled.
     */
    autoscalingMetricsCollection?: outputs.ContainerAwsNodePoolConfigAutoscalingMetricsCollection;
    /**
     * The ARN of the AWS KMS key used to encrypt node pool configuration.
     */
    configEncryption: outputs.ContainerAwsNodePoolConfigConfigEncryption;
    /**
     * The name of the AWS IAM role assigned to nodes in the pool.
     */
    iamInstanceProfile: string;
    /**
     * Optional. The AWS instance type. When unspecified, it defaults to `m5.large`.
     */
    instanceType: string;
    /**
     * Optional. The initial labels assigned to nodes of this node pool. An object containing a list of "key": value pairs. Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
     */
    labels?: {[key: string]: string};
    /**
     * Proxy configuration for outbound HTTP(S) traffic.
     */
    proxyConfig?: outputs.ContainerAwsNodePoolConfigProxyConfig;
    /**
     * Optional. Template for the root volume provisioned for node pool nodes. Volumes will be provisioned in the availability zone assigned to the node pool subnet. When unspecified, it defaults to 32 GiB with the GP2 volume type.
     */
    rootVolume?: outputs.ContainerAwsNodePoolConfigRootVolume;
    /**
     * Optional. The IDs of additional security groups to add to nodes in this pool. The manager will automatically create security groups with minimum rules needed for a functioning cluster.
     */
    securityGroupIds?: string[];
    /**
     * Optional. The SSH configuration.
     */
    sshConfig?: outputs.ContainerAwsNodePoolConfigSshConfig;
    /**
     * Optional. Key/value metadata to assign to each underlying AWS resource. Specify at most 50 pairs containing alphanumerics, spaces, and symbols (.+-=_:@/). Keys can be up to 127 Unicode characters. Values can be up to 255 Unicode characters.
     */
    tags?: {[key: string]: string};
    /**
     * Optional. The initial taints assigned to nodes of this node pool.
     */
    taints?: outputs.ContainerAwsNodePoolConfigTaint[];
}

export interface ContainerAwsNodePoolConfigAutoscalingMetricsCollection {
    /**
     * The frequency at which EC2 Auto Scaling sends aggregated data to AWS CloudWatch. The only valid value is "1Minute".
     */
    granularity: string;
    /**
     * The metrics to enable. For a list of valid metrics, see https://docs.aws.amazon.com/autoscaling/ec2/APIReference/API_EnableMetricsCollection.html. If you specify granularity and don't specify any metrics, all metrics are enabled.
     */
    metrics?: string[];
}

export interface ContainerAwsNodePoolConfigConfigEncryption {
    /**
     * The ARN of the AWS KMS key used to encrypt node pool configuration.
     */
    kmsKeyArn: string;
}

export interface ContainerAwsNodePoolConfigProxyConfig {
    /**
     * The ARN of the AWS Secret Manager secret that contains the HTTP(S) proxy configuration.
     */
    secretArn: string;
    /**
     * The version string of the AWS Secret Manager secret that contains the HTTP(S) proxy configuration.
     */
    secretVersion: string;
}

export interface ContainerAwsNodePoolConfigRootVolume {
    /**
     * Optional. The number of I/O operations per second (IOPS) to provision for GP3 volume.
     */
    iops: number;
    /**
     * Optional. The Amazon Resource Name (ARN) of the Customer Managed Key (CMK) used to encrypt AWS EBS volumes. If not specified, the default Amazon managed key associated to the AWS region where this cluster runs will be used.
     */
    kmsKeyArn?: string;
    /**
     * Optional. The size of the volume, in GiBs. When unspecified, a default value is provided. See the specific reference in the parent resource.
     */
    sizeGib: number;
    /**
     * Optional. The throughput to provision for the volume, in MiB/s. Only valid if the volume type is GP3. If volume type is gp3 and throughput is not specified, the throughput will defaults to 125.
     */
    throughput: number;
    /**
     * Optional. Type of the EBS volume. When unspecified, it defaults to GP2 volume. Possible values: VOLUME_TYPE_UNSPECIFIED, GP2, GP3
     */
    volumeType: string;
}

export interface ContainerAwsNodePoolConfigSshConfig {
    /**
     * The name of the EC2 key pair used to login into cluster machines.
     */
    ec2KeyPair: string;
}

export interface ContainerAwsNodePoolConfigTaint {
    /**
     * The taint effect. Possible values: EFFECT_UNSPECIFIED, NO_SCHEDULE, PREFER_NO_SCHEDULE, NO_EXECUTE
     */
    effect: string;
    /**
     * Key for the taint.
     */
    key: string;
    /**
     * Value for the taint.
     */
    value: string;
}

export interface ContainerAwsNodePoolManagement {
    /**
     * Optional. Whether or not the nodes will be automatically repaired.
     */
    autoRepair: boolean;
}

export interface ContainerAwsNodePoolMaxPodsConstraint {
    /**
     * The maximum number of pods to schedule on a single node.
     */
    maxPodsPerNode: number;
}

export interface ContainerAwsNodePoolTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ContainerAwsNodePoolUpdateSettings {
    /**
     * Optional. Settings for surge update.
     */
    surgeSettings?: outputs.ContainerAwsNodePoolUpdateSettingsSurgeSettings;
}

export interface ContainerAwsNodePoolUpdateSettingsSurgeSettings {
    /**
     * Optional. The maximum number of nodes that can be created beyond the current size of the node pool during the update process.
     */
    maxSurge: number;
    /**
     * Optional. The maximum number of nodes that can be simultaneously unavailable during the update process. A node is considered unavailable if its status is not Ready.
     */
    maxUnavailable: number;
}

export interface ContainerAzureClientTimeouts {
    create?: string;
    delete?: string;
}

export interface ContainerAzureClusterAuthorization {
    /**
     * Groups of users that can perform operations as a cluster admin. A managed ClusterRoleBinding will be created to grant the `cluster-admin` ClusterRole to the groups. Up to ten admin groups can be provided. For more info on RBAC, see https://kubernetes.io/docs/reference/access-authn-authz/rbac/#user-facing-roles
     */
    adminGroups?: outputs.ContainerAzureClusterAuthorizationAdminGroup[];
    /**
     * Users that can perform operations as a cluster admin. A new ClusterRoleBinding will be created to grant the cluster-admin ClusterRole to the users. Up to ten admin users can be provided. For more info on RBAC, see https://kubernetes.io/docs/reference/access-authn-authz/rbac/#user-facing-roles
     */
    adminUsers: outputs.ContainerAzureClusterAuthorizationAdminUser[];
}

export interface ContainerAzureClusterAuthorizationAdminGroup {
    /**
     * The name of the group, e.g. `my-group@domain.com`.
     */
    group: string;
}

export interface ContainerAzureClusterAuthorizationAdminUser {
    /**
     * The name of the user, e.g. `my-gcp-id@gmail.com`.
     */
    username: string;
}

export interface ContainerAzureClusterAzureServicesAuthentication {
    /**
     * The Azure Active Directory Application ID for Authentication configuration.
     */
    applicationId: string;
    /**
     * The Azure Active Directory Tenant ID for Authentication configuration.
     */
    tenantId: string;
}

export interface ContainerAzureClusterControlPlane {
    /**
     * Optional. Configuration related to application-layer secrets encryption.
     */
    databaseEncryption?: outputs.ContainerAzureClusterControlPlaneDatabaseEncryption;
    /**
     * Optional. Configuration related to the main volume provisioned for each control plane replica. The main volume is in charge of storing all of the cluster's etcd state. When unspecified, it defaults to a 8-GiB Azure Disk.
     */
    mainVolume?: outputs.ContainerAzureClusterControlPlaneMainVolume;
    /**
     * Proxy configuration for outbound HTTP(S) traffic.
     */
    proxyConfig?: outputs.ContainerAzureClusterControlPlaneProxyConfig;
    /**
     * Configuration for where to place the control plane replicas. Up to three replica placement instances can be specified. If replica_placements is set, the replica placement instances will be applied to the three control plane replicas as evenly as possible.
     */
    replicaPlacements?: outputs.ContainerAzureClusterControlPlaneReplicaPlacement[];
    /**
     * Optional. Configuration related to the root volume provisioned for each control plane replica. When unspecified, it defaults to 32-GiB Azure Disk.
     */
    rootVolume?: outputs.ContainerAzureClusterControlPlaneRootVolume;
    /**
     * SSH configuration for how to access the underlying control plane machines.
     */
    sshConfig: outputs.ContainerAzureClusterControlPlaneSshConfig;
    /**
     * The ARM ID of the subnet where the control plane VMs are deployed. Example: `/subscriptions//resourceGroups//providers/Microsoft.Network/virtualNetworks//subnets/default`.
     */
    subnetId: string;
    /**
     * Optional. A set of tags to apply to all underlying control plane Azure resources.
     */
    tags?: {[key: string]: string};
    /**
     * The Kubernetes version to run on control plane replicas (e.g. `1.19.10-gke.1000`). You can list all supported versions on a given Google Cloud region by calling GetAzureServerConfig.
     */
    version: string;
    /**
     * Optional. The Azure VM size name. Example: `Standard_DS2_v2`. For available VM sizes, see https://docs.microsoft.com/en-us/azure/virtual-machines/vm-naming-conventions. When unspecified, it defaults to `Standard_DS2_v2`.
     */
    vmSize: string;
}

export interface ContainerAzureClusterControlPlaneDatabaseEncryption {
    /**
     * The ARM ID of the Azure Key Vault key to encrypt / decrypt data. For example: `/subscriptions/<subscription-id>/resourceGroups/<resource-group-id>/providers/Microsoft.KeyVault/vaults/<key-vault-id>/keys/<key-name>` Encryption will always take the latest version of the key and hence specific version is not supported.
     */
    keyId: string;
}

export interface ContainerAzureClusterControlPlaneMainVolume {
    /**
     * Optional. The size of the disk, in GiBs. When unspecified, a default value is provided. See the specific reference in the parent resource.
     */
    sizeGib: number;
}

export interface ContainerAzureClusterControlPlaneProxyConfig {
    /**
     * The ARM ID the of the resource group containing proxy keyvault. Resource group ids are formatted as `/subscriptions/<subscription-id>/resourceGroups/<resource-group-name>`
     */
    resourceGroupId: string;
    /**
     * The URL the of the proxy setting secret with its version. Secret ids are formatted as `https:<key-vault-name>.vault.azure.net/secrets/<secret-name>/<secret-version>`.
     */
    secretId: string;
}

export interface ContainerAzureClusterControlPlaneReplicaPlacement {
    /**
     * For a given replica, the Azure availability zone where to provision the control plane VM and the ETCD disk.
     */
    azureAvailabilityZone: string;
    /**
     * For a given replica, the ARM ID of the subnet where the control plane VM is deployed. Make sure it's a subnet under the virtual network in the cluster configuration.
     */
    subnetId: string;
}

export interface ContainerAzureClusterControlPlaneRootVolume {
    /**
     * Optional. The size of the disk, in GiBs. When unspecified, a default value is provided. See the specific reference in the parent resource.
     */
    sizeGib: number;
}

export interface ContainerAzureClusterControlPlaneSshConfig {
    /**
     * The SSH public key data for VMs managed by Anthos. This accepts the authorized_keys file format used in OpenSSH according to the sshd(8) manual page.
     */
    authorizedKey: string;
}

export interface ContainerAzureClusterFleet {
    /**
     * The name of the managed Hub Membership resource associated to this cluster. Membership names are formatted as projects/<project-number>/locations/global/membership/<cluster-id>.
     */
    membership: string;
    /**
     * The number of the Fleet host project where this cluster will be registered.
     */
    project: string;
}

export interface ContainerAzureClusterNetworking {
    /**
     * The IP address range of the pods in this cluster, in CIDR notation (e.g. `10.96.0.0/14`). All pods in the cluster get assigned a unique RFC1918 IPv4 address from these ranges. Only a single range is supported. This field cannot be changed after creation.
     */
    podAddressCidrBlocks: string[];
    /**
     * The IP address range for services in this cluster, in CIDR notation (e.g. `10.96.0.0/14`). All services in the cluster get assigned a unique RFC1918 IPv4 address from these ranges. Only a single range is supported. This field cannot be changed after creating a cluster.
     */
    serviceAddressCidrBlocks: string[];
    /**
     * The Azure Resource Manager (ARM) ID of the VNet associated with your cluster. All components in the cluster (i.e. control plane and node pools) run on a single VNet. Example: `/subscriptions/*&#47;resourceGroups/*&#47;providers/Microsoft.Network/virtualNetworks/*` This field cannot be changed after creation.
     */
    virtualNetworkId: string;
}

export interface ContainerAzureClusterTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ContainerAzureClusterWorkloadIdentityConfig {
    identityProvider: string;
    issuerUri: string;
    workloadPool: string;
}

export interface ContainerAzureNodePoolAutoscaling {
    /**
     * Maximum number of nodes in the node pool. Must be >= min_node_count.
     */
    maxNodeCount: number;
    /**
     * Minimum number of nodes in the node pool. Must be >= 1 and <= max_node_count.
     */
    minNodeCount: number;
}

export interface ContainerAzureNodePoolConfig {
    /**
     * Optional. The initial labels assigned to nodes of this node pool. An object containing a list of "key": value pairs. Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
     */
    labels?: {[key: string]: string};
    /**
     * Proxy configuration for outbound HTTP(S) traffic.
     */
    proxyConfig?: outputs.ContainerAzureNodePoolConfigProxyConfig;
    /**
     * Optional. Configuration related to the root volume provisioned for each node pool machine. When unspecified, it defaults to a 32-GiB Azure Disk.
     */
    rootVolume?: outputs.ContainerAzureNodePoolConfigRootVolume;
    /**
     * SSH configuration for how to access the node pool machines.
     */
    sshConfig: outputs.ContainerAzureNodePoolConfigSshConfig;
    /**
     * Optional. A set of tags to apply to all underlying Azure resources for this node pool. This currently only includes Virtual Machine Scale Sets. Specify at most 50 pairs containing alphanumerics, spaces, and symbols (.+-=_:@/). Keys can be up to 127 Unicode characters. Values can be up to 255 Unicode characters.
     */
    tags?: {[key: string]: string};
    /**
     * Optional. The Azure VM size name. Example: `Standard_DS2_v2`. See (/anthos/clusters/docs/azure/reference/supported-vms) for options. When unspecified, it defaults to `Standard_DS2_v2`.
     */
    vmSize: string;
}

export interface ContainerAzureNodePoolConfigProxyConfig {
    /**
     * The ARM ID the of the resource group containing proxy keyvault. Resource group ids are formatted as `/subscriptions/<subscription-id>/resourceGroups/<resource-group-name>`
     */
    resourceGroupId: string;
    /**
     * The URL the of the proxy setting secret with its version. Secret ids are formatted as `https:<key-vault-name>.vault.azure.net/secrets/<secret-name>/<secret-version>`.
     */
    secretId: string;
}

export interface ContainerAzureNodePoolConfigRootVolume {
    /**
     * Optional. The size of the disk, in GiBs. When unspecified, a default value is provided. See the specific reference in the parent resource.
     */
    sizeGib: number;
}

export interface ContainerAzureNodePoolConfigSshConfig {
    /**
     * The SSH public key data for VMs managed by Anthos. This accepts the authorized_keys file format used in OpenSSH according to the sshd(8) manual page.
     */
    authorizedKey: string;
}

export interface ContainerAzureNodePoolManagement {
    /**
     * Optional. Whether or not the nodes will be automatically repaired.
     */
    autoRepair: boolean;
}

export interface ContainerAzureNodePoolMaxPodsConstraint {
    /**
     * The maximum number of pods to schedule on a single node.
     */
    maxPodsPerNode: number;
}

export interface ContainerAzureNodePoolTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ContainerClusterAddonsConfig {
    /**
     * The status of the CloudRun addon. It is disabled by default. Set disabled = false to enable.
     */
    cloudrunConfig?: outputs.ContainerClusterAddonsConfigCloudrunConfig;
    /**
     * The of the Config Connector addon.
     */
    configConnectorConfig?: outputs.ContainerClusterAddonsConfigConfigConnectorConfig;
    /**
     * The status of the NodeLocal DNSCache addon. It is disabled by default. Set enabled = true to enable.
     */
    dnsCacheConfig?: outputs.ContainerClusterAddonsConfigDnsCacheConfig;
    /**
     * Whether this cluster should enable the Google Compute Engine Persistent Disk Container Storage Interface (CSI) Driver. Set enabled = true to enable. The Compute Engine persistent disk CSI Driver is enabled by default on newly created clusters for the following versions: Linux clusters: GKE version 1.18.10-gke.2100 or later, or 1.19.3-gke.2100 or later.
     */
    gcePersistentDiskCsiDriverConfig?: outputs.ContainerClusterAddonsConfigGcePersistentDiskCsiDriverConfig;
    /**
     * The status of the Filestore CSI driver addon, which allows the usage of filestore instance as volumes. Defaults to disabled; set enabled = true to enable.
     */
    gcpFilestoreCsiDriverConfig?: outputs.ContainerClusterAddonsConfigGcpFilestoreCsiDriverConfig;
    /**
     * The status of the GCS Fuse CSI driver addon, which allows the usage of gcs bucket as volumes. Defaults to disabled; set enabled = true to enable.
     */
    gcsFuseCsiDriverConfig?: outputs.ContainerClusterAddonsConfigGcsFuseCsiDriverConfig;
    /**
     * The status of the Backup for GKE Agent addon. It is disabled by default. Set enabled = true to enable.
     */
    gkeBackupAgentConfig?: outputs.ContainerClusterAddonsConfigGkeBackupAgentConfig;
    /**
     * The status of the Horizontal Pod Autoscaling addon, which increases or decreases the number of replica pods a replication controller has based on the resource usage of the existing pods. It ensures that a Heapster pod is running in the cluster, which is also used by the Cloud Monitoring service. It is enabled by default; set disabled = true to disable.
     */
    horizontalPodAutoscaling?: outputs.ContainerClusterAddonsConfigHorizontalPodAutoscaling;
    /**
     * The status of the HTTP (L7) load balancing controller addon, which makes it easy to set up HTTP load balancers for services in a cluster. It is enabled by default; set disabled = true to disable.
     */
    httpLoadBalancing?: outputs.ContainerClusterAddonsConfigHttpLoadBalancing;
    /**
     * Whether we should enable the network policy addon for the master. This must be enabled in order to enable network policy for the nodes. To enable this, you must also define a network_policy block, otherwise nothing will happen. It can only be disabled if the nodes already do not have network policies enabled. Defaults to disabled; set disabled = false to enable.
     */
    networkPolicyConfig?: outputs.ContainerClusterAddonsConfigNetworkPolicyConfig;
    /**
     * The status of the Ray Operator addon, which enabled management of Ray AI/ML jobs on GKE. Defaults to disabled; set enabled = true to enable.
     */
    rayOperatorConfigs?: outputs.ContainerClusterAddonsConfigRayOperatorConfig[];
    /**
     * The status of the Stateful HA addon, which provides automatic configurable failover for stateful applications. Defaults to disabled; set enabled = true to enable.
     */
    statefulHaConfig?: outputs.ContainerClusterAddonsConfigStatefulHaConfig;
}

export interface ContainerClusterAddonsConfigCloudrunConfig {
    disabled: boolean;
    loadBalancerType?: string;
}

export interface ContainerClusterAddonsConfigConfigConnectorConfig {
    enabled: boolean;
}

export interface ContainerClusterAddonsConfigDnsCacheConfig {
    enabled: boolean;
}

export interface ContainerClusterAddonsConfigGcePersistentDiskCsiDriverConfig {
    enabled: boolean;
}

export interface ContainerClusterAddonsConfigGcpFilestoreCsiDriverConfig {
    enabled: boolean;
}

export interface ContainerClusterAddonsConfigGcsFuseCsiDriverConfig {
    enabled: boolean;
}

export interface ContainerClusterAddonsConfigGkeBackupAgentConfig {
    enabled: boolean;
}

export interface ContainerClusterAddonsConfigHorizontalPodAutoscaling {
    disabled: boolean;
}

export interface ContainerClusterAddonsConfigHttpLoadBalancing {
    disabled: boolean;
}

export interface ContainerClusterAddonsConfigNetworkPolicyConfig {
    disabled: boolean;
}

export interface ContainerClusterAddonsConfigRayOperatorConfig {
    enabled: boolean;
    /**
     * The status of Ray Logging, which scrapes Ray cluster logs to Cloud Logging. Defaults to disabled; set enabled = true to enable.
     */
    rayClusterLoggingConfig?: outputs.ContainerClusterAddonsConfigRayOperatorConfigRayClusterLoggingConfig;
    /**
     * The status of Ray Cluster monitoring, which shows Ray cluster metrics in Cloud Console. Defaults to disabled; set enabled = true to enable.
     */
    rayClusterMonitoringConfig?: outputs.ContainerClusterAddonsConfigRayOperatorConfigRayClusterMonitoringConfig;
}

export interface ContainerClusterAddonsConfigRayOperatorConfigRayClusterLoggingConfig {
    enabled: boolean;
}

export interface ContainerClusterAddonsConfigRayOperatorConfigRayClusterMonitoringConfig {
    enabled: boolean;
}

export interface ContainerClusterAddonsConfigStatefulHaConfig {
    enabled: boolean;
}

export interface ContainerClusterAuthenticatorGroupsConfig {
    /**
     * The name of the RBAC security group for use with Google security groups in Kubernetes RBAC. Group name must be in format gke-security-groups@yourdomain.com.
     */
    securityGroup: string;
}

export interface ContainerClusterBinaryAuthorization {
    /**
     * Enable Binary Authorization for this cluster.
     *
     * @deprecated Deprecated
     */
    enabled?: boolean;
    /**
     * Mode of operation for Binary Authorization policy evaluation.
     */
    evaluationMode: string;
}

export interface ContainerClusterClusterAutoscaling {
    /**
     * Contains defaults for a node pool created by NAP.
     */
    autoProvisioningDefaults?: outputs.ContainerClusterClusterAutoscalingAutoProvisioningDefaults;
    /**
     * The list of Google Compute Engine zones in which the NodePool's nodes can be created by NAP.
     */
    autoProvisioningLocations: string[];
    /**
     * Configuration options for the Autoscaling profile feature, which lets you choose whether the cluster autoscaler should optimize for resource utilization or resource availability when deciding to remove nodes from a cluster. Can be BALANCED or OPTIMIZE_UTILIZATION. Defaults to BALANCED.
     */
    autoscalingProfile?: string;
    /**
     * Whether node auto-provisioning is enabled. Resource limits for cpu and memory must be defined to enable node auto-provisioning.
     */
    enabled: boolean;
    /**
     * Global constraints for machine resources in the cluster. Configuring the cpu and memory types is required if node auto-provisioning is enabled. These limits will apply to node pool autoscaling in addition to node auto-provisioning.
     */
    resourceLimits?: outputs.ContainerClusterClusterAutoscalingResourceLimit[];
}

export interface ContainerClusterClusterAutoscalingAutoProvisioningDefaults {
    /**
     * The Customer Managed Encryption Key used to encrypt the boot disk attached to each node in the node pool.
     */
    bootDiskKmsKey?: string;
    /**
     * Size of the disk attached to each node, specified in GB. The smallest allowed disk size is 10GB.
     */
    diskSize?: number;
    /**
     * Type of the disk attached to each node.
     */
    diskType?: string;
    /**
     * The default image type used by NAP once a new node pool is being created.
     */
    imageType?: string;
    /**
     * NodeManagement configuration for this NodePool.
     */
    management?: outputs.ContainerClusterClusterAutoscalingAutoProvisioningDefaultsManagement;
    /**
     * Minimum CPU platform to be used by this instance. The instance may be scheduled on the specified or newer CPU platform. Applicable values are the friendly names of CPU platforms, such as Intel Haswell.
     */
    minCpuPlatform?: string;
    /**
     * Scopes that are used by NAP when creating node pools.
     */
    oauthScopes: string[];
    /**
     * The Google Cloud Platform Service Account to be used by the node VMs.
     */
    serviceAccount?: string;
    /**
     * Shielded Instance options.
     */
    shieldedInstanceConfig?: outputs.ContainerClusterClusterAutoscalingAutoProvisioningDefaultsShieldedInstanceConfig;
    /**
     * Specifies the upgrade settings for NAP created node pools
     */
    upgradeSettings?: outputs.ContainerClusterClusterAutoscalingAutoProvisioningDefaultsUpgradeSettings;
}

export interface ContainerClusterClusterAutoscalingAutoProvisioningDefaultsManagement {
    /**
     * Specifies whether the node auto-repair is enabled for the node pool. If enabled, the nodes in this node pool will be monitored and, if they fail health checks too many times, an automatic repair action will be triggered.
     */
    autoRepair: boolean;
    /**
     * Specifies whether node auto-upgrade is enabled for the node pool. If enabled, node auto-upgrade helps keep the nodes in your node pool up to date with the latest release version of Kubernetes.
     */
    autoUpgrade: boolean;
    /**
     * Specifies the Auto Upgrade knobs for the node pool.
     */
    upgradeOptions: outputs.ContainerClusterClusterAutoscalingAutoProvisioningDefaultsManagementUpgradeOption[];
}

export interface ContainerClusterClusterAutoscalingAutoProvisioningDefaultsManagementUpgradeOption {
    autoUpgradeStartTime: string;
    description: string;
}

export interface ContainerClusterClusterAutoscalingAutoProvisioningDefaultsShieldedInstanceConfig {
    /**
     * Defines whether the instance has integrity monitoring enabled.
     */
    enableIntegrityMonitoring?: boolean;
    /**
     * Defines whether the instance has Secure Boot enabled.
     */
    enableSecureBoot?: boolean;
}

export interface ContainerClusterClusterAutoscalingAutoProvisioningDefaultsUpgradeSettings {
    /**
     * Settings for blue-green upgrade strategy.
     */
    blueGreenSettings?: outputs.ContainerClusterClusterAutoscalingAutoProvisioningDefaultsUpgradeSettingsBlueGreenSettings;
    /**
     * The maximum number of nodes that can be created beyond the current size of the node pool during the upgrade process.
     */
    maxSurge?: number;
    /**
     * The maximum number of nodes that can be simultaneously unavailable during the upgrade process.
     */
    maxUnavailable?: number;
    /**
     * Update strategy of the node pool.
     */
    strategy: string;
}

export interface ContainerClusterClusterAutoscalingAutoProvisioningDefaultsUpgradeSettingsBlueGreenSettings {
    /**
     * Time needed after draining entire blue pool. After this period, blue pool will be cleaned up.
     *
     * 																A duration in seconds with up to nine fractional digits, ending with 's'. Example: "3.5s".
     */
    nodePoolSoakDuration: string;
    /**
     * Standard policy for the blue-green upgrade.
     */
    standardRolloutPolicy?: outputs.ContainerClusterClusterAutoscalingAutoProvisioningDefaultsUpgradeSettingsBlueGreenSettingsStandardRolloutPolicy;
}

export interface ContainerClusterClusterAutoscalingAutoProvisioningDefaultsUpgradeSettingsBlueGreenSettingsStandardRolloutPolicy {
    /**
     * Number of blue nodes to drain in a batch.
     */
    batchNodeCount: number;
    /**
     * Percentage of the bool pool nodes to drain in a batch. The range of this field should be (0.0, 1.0].
     */
    batchPercentage: number;
    /**
     * Soak time after each batch gets drained.
     *
     * 																			A duration in seconds with up to nine fractional digits, ending with 's'. Example: "3.5s".
     */
    batchSoakDuration?: string;
}

export interface ContainerClusterClusterAutoscalingResourceLimit {
    /**
     * Maximum amount of the resource in the cluster.
     */
    maximum?: number;
    /**
     * Minimum amount of the resource in the cluster.
     */
    minimum?: number;
    /**
     * The type of the resource. For example, cpu and memory. See the guide to using Node Auto-Provisioning for a list of types.
     */
    resourceType: string;
}

export interface ContainerClusterConfidentialNodes {
    /**
     * Whether Confidential Nodes feature is enabled for all nodes in this cluster.
     */
    enabled: boolean;
}

export interface ContainerClusterCostManagementConfig {
    /**
     * Whether to enable GKE cost allocation. When you enable GKE cost allocation, the cluster name and namespace of your GKE workloads appear in the labels field of the billing export to BigQuery. Defaults to false.
     */
    enabled: boolean;
}

export interface ContainerClusterDatabaseEncryption {
    /**
     * The key to use to encrypt/decrypt secrets.
     */
    keyName?: string;
    /**
     * ENCRYPTED or DECRYPTED.
     */
    state: string;
}

export interface ContainerClusterDefaultSnatStatus {
    /**
     * When disabled is set to false, default IP masquerade rules will be applied to the nodes to prevent sNAT on cluster internal traffic.
     */
    disabled: boolean;
}

export interface ContainerClusterDnsConfig {
    /**
     * Which in-cluster DNS provider should be used.
     */
    clusterDns?: string;
    /**
     * The suffix used for all cluster service records.
     */
    clusterDnsDomain?: string;
    /**
     * The scope of access to cluster DNS records.
     */
    clusterDnsScope?: string;
}

export interface ContainerClusterEnableK8sBetaApis {
    /**
     * Enabled Kubernetes Beta APIs.
     */
    enabledApis: string[];
}

export interface ContainerClusterFleet {
    /**
     * Full resource name of the registered fleet membership of the cluster.
     */
    membership: string;
    /**
     * Short name of the fleet membership, for example "member-1".
     */
    membershipId: string;
    /**
     * Location of the fleet membership, for example "us-central1".
     */
    membershipLocation: string;
    /**
     * Whether the cluster has been registered via the fleet API.
     */
    preRegistered: boolean;
    /**
     * The Fleet host project of the cluster.
     */
    project?: string;
}

export interface ContainerClusterGatewayApiConfig {
    /**
     * The Gateway API release channel to use for Gateway API.
     */
    channel: string;
}

export interface ContainerClusterIdentityServiceConfig {
    /**
     * Whether to enable the Identity Service component.
     */
    enabled?: boolean;
}

export interface ContainerClusterIpAllocationPolicy {
    /**
     * AdditionalPodRangesConfig is the configuration for additional pod secondary ranges supporting the ClusterUpdate message.
     */
    additionalPodRangesConfig?: outputs.ContainerClusterIpAllocationPolicyAdditionalPodRangesConfig;
    /**
     * The IP address range for the cluster pod IPs. Set to blank to have a range chosen with the default size. Set to /netmask (e.g. /14) to have a range chosen with a specific netmask. Set to a CIDR notation (e.g. 10.96.0.0/14) from the RFC-1918 private networks (e.g. 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16) to pick a specific range to use.
     */
    clusterIpv4CidrBlock: string;
    /**
     * The name of the existing secondary range in the cluster's subnetwork to use for pod IP addresses. Alternatively, cluster_ipv4_cidr_block can be used to automatically create a GKE-managed one.
     */
    clusterSecondaryRangeName: string;
    /**
     * Configuration for cluster level pod cidr overprovision. Default is disabled=false.
     */
    podCidrOverprovisionConfig?: outputs.ContainerClusterIpAllocationPolicyPodCidrOverprovisionConfig;
    /**
     * The IP address range of the services IPs in this cluster. Set to blank to have a range chosen with the default size. Set to /netmask (e.g. /14) to have a range chosen with a specific netmask. Set to a CIDR notation (e.g. 10.96.0.0/14) from the RFC-1918 private networks (e.g. 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16) to pick a specific range to use.
     */
    servicesIpv4CidrBlock: string;
    /**
     * The name of the existing secondary range in the cluster's subnetwork to use for service ClusterIPs. Alternatively, services_ipv4_cidr_block can be used to automatically create a GKE-managed one.
     */
    servicesSecondaryRangeName: string;
    /**
     * The IP Stack type of the cluster. Choose between IPV4 and IPV4_IPV6. Default type is IPV4 Only if not set
     */
    stackType?: string;
}

export interface ContainerClusterIpAllocationPolicyAdditionalPodRangesConfig {
    /**
     * Name for pod secondary ipv4 range which has the actual range defined ahead.
     */
    podRangeNames: string[];
}

export interface ContainerClusterIpAllocationPolicyPodCidrOverprovisionConfig {
    disabled: boolean;
}

export interface ContainerClusterLoggingConfig {
    /**
     * GKE components exposing logs. Valid values include SYSTEM_COMPONENTS, APISERVER, CONTROLLER_MANAGER, SCHEDULER, and WORKLOADS.
     */
    enableComponents: string[];
}

export interface ContainerClusterMaintenancePolicy {
    /**
     * Time window specified for daily maintenance operations. Specify start_time in RFC3339 format "HH:MM”, where HH : [00-23] and MM : [00-59] GMT.
     */
    dailyMaintenanceWindow?: outputs.ContainerClusterMaintenancePolicyDailyMaintenanceWindow;
    /**
     * Exceptions to maintenance window. Non-emergency maintenance should not occur in these windows.
     */
    maintenanceExclusions?: outputs.ContainerClusterMaintenancePolicyMaintenanceExclusion[];
    /**
     * Time window for recurring maintenance operations.
     */
    recurringWindow?: outputs.ContainerClusterMaintenancePolicyRecurringWindow;
}

export interface ContainerClusterMaintenancePolicyDailyMaintenanceWindow {
    duration: string;
    startTime: string;
}

export interface ContainerClusterMaintenancePolicyMaintenanceExclusion {
    endTime: string;
    exclusionName: string;
    /**
     * Maintenance exclusion related options.
     */
    exclusionOptions?: outputs.ContainerClusterMaintenancePolicyMaintenanceExclusionExclusionOptions;
    startTime: string;
}

export interface ContainerClusterMaintenancePolicyMaintenanceExclusionExclusionOptions {
    /**
     * The scope of automatic upgrades to restrict in the exclusion window.
     */
    scope: string;
}

export interface ContainerClusterMaintenancePolicyRecurringWindow {
    endTime: string;
    recurrence: string;
    startTime: string;
}

export interface ContainerClusterMasterAuth {
    /**
     * Base64 encoded public certificate used by clients to authenticate to the cluster endpoint.
     */
    clientCertificate: string;
    /**
     * Whether client certificate authorization is enabled for this cluster.
     */
    clientCertificateConfig: outputs.ContainerClusterMasterAuthClientCertificateConfig;
    /**
     * Base64 encoded private key used by clients to authenticate to the cluster endpoint.
     */
    clientKey: string;
    /**
     * Base64 encoded public certificate that is the root of trust for the cluster.
     */
    clusterCaCertificate: string;
}

export interface ContainerClusterMasterAuthClientCertificateConfig {
    /**
     * Whether client certificate authorization is enabled for this cluster.
     */
    issueClientCertificate: boolean;
}

export interface ContainerClusterMasterAuthorizedNetworksConfig {
    /**
     * External networks that can access the Kubernetes cluster master through HTTPS.
     */
    cidrBlocks?: outputs.ContainerClusterMasterAuthorizedNetworksConfigCidrBlock[];
    /**
     * Whether Kubernetes master is accessible via Google Compute Engine Public IPs.
     */
    gcpPublicCidrsAccessEnabled: boolean;
}

export interface ContainerClusterMasterAuthorizedNetworksConfigCidrBlock {
    /**
     * External network that can access Kubernetes master through HTTPS. Must be specified in CIDR notation.
     */
    cidrBlock: string;
    /**
     * Field for users to identify CIDR blocks.
     */
    displayName?: string;
}

export interface ContainerClusterMeshCertificates {
    /**
     * When enabled the GKE Workload Identity Certificates controller and node agent will be deployed in the cluster.
     */
    enableCertificates: boolean;
}

export interface ContainerClusterMonitoringConfig {
    /**
     * Configuration of Advanced Datapath Observability features.
     */
    advancedDatapathObservabilityConfig?: outputs.ContainerClusterMonitoringConfigAdvancedDatapathObservabilityConfig;
    /**
     * GKE components exposing metrics. Valid values include SYSTEM_COMPONENTS, APISERVER, SCHEDULER, CONTROLLER_MANAGER, STORAGE, HPA, POD, DAEMONSET, DEPLOYMENT, STATEFULSET, KUBELET, CADVISOR and DCGM.
     */
    enableComponents: string[];
    /**
     * Configuration for Google Cloud Managed Services for Prometheus.
     */
    managedPrometheus?: outputs.ContainerClusterMonitoringConfigManagedPrometheus;
}

export interface ContainerClusterMonitoringConfigAdvancedDatapathObservabilityConfig {
    /**
     * Whether or not the advanced datapath metrics are enabled.
     */
    enableMetrics: boolean;
    /**
     * Whether or not Relay is enabled.
     */
    enableRelay: boolean;
}

export interface ContainerClusterMonitoringConfigManagedPrometheus {
    /**
     * Whether or not the managed collection is enabled.
     */
    enabled: boolean;
}

export interface ContainerClusterNetworkPolicy {
    /**
     * Whether network policy is enabled on the cluster.
     */
    enabled: boolean;
    /**
     * The selected network policy provider.
     */
    provider?: string;
}

export interface ContainerClusterNodeConfig {
    /**
     * Specifies options for controlling advanced machine features.
     */
    advancedMachineFeatures?: outputs.ContainerClusterNodeConfigAdvancedMachineFeatures;
    /**
     * The Customer Managed Encryption Key used to encrypt the boot disk attached to each node in the node pool.
     */
    bootDiskKmsKey?: string;
    /**
     * Configuration for the confidential nodes feature, which makes nodes run on confidential VMs. Warning: This configuration can't be changed (or added/removed) after pool creation without deleting and recreating the entire pool.
     */
    confidentialNodes?: outputs.ContainerClusterNodeConfigConfidentialNodes;
    /**
     * Parameters for containerd configuration.
     */
    containerdConfig?: outputs.ContainerClusterNodeConfigContainerdConfig;
    /**
     * Size of the disk attached to each node, specified in GB. The smallest allowed disk size is 10GB.
     */
    diskSizeGb: number;
    /**
     * Type of the disk attached to each node. Such as pd-standard, pd-balanced or pd-ssd
     */
    diskType: string;
    /**
     * List of kubernetes taints applied to each node.
     */
    effectiveTaints: outputs.ContainerClusterNodeConfigEffectiveTaint[];
    /**
     * If enabled boot disks are configured with confidential mode.
     */
    enableConfidentialStorage?: boolean;
    /**
     * Parameters for the ephemeral storage filesystem. If unspecified, ephemeral storage is backed by the boot disk.
     */
    ephemeralStorageLocalSsdConfig?: outputs.ContainerClusterNodeConfigEphemeralStorageLocalSsdConfig;
    /**
     * Enable or disable NCCL Fast Socket in the node pool.
     */
    fastSocket?: outputs.ContainerClusterNodeConfigFastSocket;
    /**
     * GCFS configuration for this node.
     */
    gcfsConfig?: outputs.ContainerClusterNodeConfigGcfsConfig;
    /**
     * List of the type and count of accelerator cards attached to the instance.
     */
    guestAccelerators?: outputs.ContainerClusterNodeConfigGuestAccelerator[];
    /**
     * Enable or disable gvnic in the node pool.
     */
    gvnic?: outputs.ContainerClusterNodeConfigGvnic;
    /**
     * The maintenance policy for the hosts on which the GKE VMs run on.
     */
    hostMaintenancePolicy?: outputs.ContainerClusterNodeConfigHostMaintenancePolicy;
    /**
     * The image type to use for this node. Note that for a given image type, the latest version of it will be used.
     */
    imageType: string;
    /**
     * Node kubelet configs.
     */
    kubeletConfig?: outputs.ContainerClusterNodeConfigKubeletConfig;
    /**
     * The map of Kubernetes labels (key/value pairs) to be applied to each node. These will added in addition to any default label(s) that Kubernetes may apply to the node.
     */
    labels: {[key: string]: string};
    /**
     * Parameters that can be configured on Linux nodes.
     */
    linuxNodeConfig?: outputs.ContainerClusterNodeConfigLinuxNodeConfig;
    /**
     * Parameters for raw-block local NVMe SSDs.
     */
    localNvmeSsdBlockConfig?: outputs.ContainerClusterNodeConfigLocalNvmeSsdBlockConfig;
    /**
     * The number of local SSD disks to be attached to the node.
     */
    localSsdCount: number;
    /**
     * Type of logging agent that is used as the default value for node pools in the cluster. Valid values include DEFAULT and MAX_THROUGHPUT.
     */
    loggingVariant: string;
    /**
     * The name of a Google Compute Engine machine type.
     */
    machineType: string;
    /**
     * The metadata key/value pairs assigned to instances in the cluster.
     */
    metadata: {[key: string]: string};
    /**
     * Minimum CPU platform to be used by this instance. The instance may be scheduled on the specified or newer CPU platform.
     */
    minCpuPlatform: string;
    /**
     * Setting this field will assign instances of this pool to run on the specified node group. This is useful for running workloads on sole tenant nodes.
     */
    nodeGroup?: string;
    /**
     * The set of Google API scopes to be made available on all of the node VMs.
     */
    oauthScopes: string[];
    /**
     * Whether the nodes are created as preemptible VM instances.
     */
    preemptible?: boolean;
    /**
     * The reservation affinity configuration for the node pool.
     */
    reservationAffinity?: outputs.ContainerClusterNodeConfigReservationAffinity;
    /**
     * The GCE resource labels (a map of key/value pairs) to be applied to the node pool.
     */
    resourceLabels?: {[key: string]: string};
    /**
     * A map of resource manager tags. Resource manager tag keys and values have the same definition as resource manager tags. Keys must be in the format tagKeys/{tag_key_id}, and values are in the format tagValues/456. The field is ignored (both PUT & PATCH) when empty.
     */
    resourceManagerTags?: {[key: string]: string};
    /**
     * Secondary boot disks for preloading data or container images.
     */
    secondaryBootDisks?: outputs.ContainerClusterNodeConfigSecondaryBootDisk[];
    /**
     * The Google Cloud Platform Service Account to be used by the node VMs.
     */
    serviceAccount: string;
    /**
     * Shielded Instance options.
     */
    shieldedInstanceConfig?: outputs.ContainerClusterNodeConfigShieldedInstanceConfig;
    /**
     * Node affinity options for sole tenant node pools.
     */
    soleTenantConfig?: outputs.ContainerClusterNodeConfigSoleTenantConfig;
    /**
     * Whether the nodes are created as spot VM instances.
     */
    spot?: boolean;
    /**
     * The list of instance tags applied to all nodes.
     */
    tags?: string[];
    /**
     * List of Kubernetes taints to be applied to each node.
     */
    taints?: outputs.ContainerClusterNodeConfigTaint[];
    /**
     * The workload metadata configuration for this node.
     */
    workloadMetadataConfig?: outputs.ContainerClusterNodeConfigWorkloadMetadataConfig;
}

export interface ContainerClusterNodeConfigAdvancedMachineFeatures {
    /**
     * Whether the node should have nested virtualization enabled.
     */
    enableNestedVirtualization?: boolean;
    /**
     * The number of threads per physical core. To disable simultaneous multithreading (SMT) set this to 1. If unset, the maximum number of threads supported per core by the underlying processor is assumed.
     */
    threadsPerCore: number;
}

export interface ContainerClusterNodeConfigConfidentialNodes {
    /**
     * Whether Confidential Nodes feature is enabled for all nodes in this pool.
     */
    enabled: boolean;
}

export interface ContainerClusterNodeConfigContainerdConfig {
    /**
     * Parameters for private container registries configuration.
     */
    privateRegistryAccessConfig?: outputs.ContainerClusterNodeConfigContainerdConfigPrivateRegistryAccessConfig;
}

export interface ContainerClusterNodeConfigContainerdConfigPrivateRegistryAccessConfig {
    /**
     * Parameters for configuring CA certificate and domains.
     */
    certificateAuthorityDomainConfigs?: outputs.ContainerClusterNodeConfigContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfig[];
    /**
     * Whether or not private registries are configured.
     */
    enabled: boolean;
}

export interface ContainerClusterNodeConfigContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfig {
    /**
     * List of fully-qualified-domain-names. IPv4s and port specification are supported.
     */
    fqdns: string[];
    /**
     * Parameters for configuring a certificate hosted in GCP SecretManager.
     */
    gcpSecretManagerCertificateConfig: outputs.ContainerClusterNodeConfigContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfigGcpSecretManagerCertificateConfig;
}

export interface ContainerClusterNodeConfigContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfigGcpSecretManagerCertificateConfig {
    /**
     * URI for the secret that hosts a certificate. Must be in the format 'projects/PROJECT_NUM/secrets/SECRET_NAME/versions/VERSION_OR_LATEST'.
     */
    secretUri: string;
}

export interface ContainerClusterNodeConfigEffectiveTaint {
    effect: string;
    key: string;
    value: string;
}

export interface ContainerClusterNodeConfigEphemeralStorageLocalSsdConfig {
    /**
     * Number of local SSDs to use to back ephemeral storage. Uses NVMe interfaces. Each local SSD must be 375 or 3000 GB in size, and all local SSDs must share the same size.
     */
    localSsdCount: number;
}

export interface ContainerClusterNodeConfigFastSocket {
    /**
     * Whether or not NCCL Fast Socket is enabled
     */
    enabled: boolean;
}

export interface ContainerClusterNodeConfigGcfsConfig {
    /**
     * Whether or not GCFS is enabled
     */
    enabled: boolean;
}

export interface ContainerClusterNodeConfigGuestAccelerator {
    /**
     * The number of the accelerator cards exposed to an instance.
     */
    count: number;
    /**
     * Configuration for auto installation of GPU driver.
     */
    gpuDriverInstallationConfig?: outputs.ContainerClusterNodeConfigGuestAcceleratorGpuDriverInstallationConfig;
    /**
     * Size of partitions to create on the GPU. Valid values are described in the NVIDIA mig user guide (https://docs.nvidia.com/datacenter/tesla/mig-user-guide/#partitioning)
     */
    gpuPartitionSize?: string;
    /**
     * Configuration for GPU sharing.
     */
    gpuSharingConfig?: outputs.ContainerClusterNodeConfigGuestAcceleratorGpuSharingConfig;
    /**
     * The accelerator type resource name.
     */
    type: string;
}

export interface ContainerClusterNodeConfigGuestAcceleratorGpuDriverInstallationConfig {
    /**
     * Mode for how the GPU driver is installed.
     */
    gpuDriverVersion: string;
}

export interface ContainerClusterNodeConfigGuestAcceleratorGpuSharingConfig {
    /**
     * The type of GPU sharing strategy to enable on the GPU node. Possible values are described in the API package (https://pkg.go.dev/google.golang.org/api/container/v1#GPUSharingConfig)
     */
    gpuSharingStrategy: string;
    /**
     * The maximum number of containers that can share a GPU.
     */
    maxSharedClientsPerGpu: number;
}

export interface ContainerClusterNodeConfigGvnic {
    /**
     * Whether or not gvnic is enabled
     */
    enabled: boolean;
}

export interface ContainerClusterNodeConfigHostMaintenancePolicy {
    /**
     * .
     */
    maintenanceInterval: string;
}

export interface ContainerClusterNodeConfigKubeletConfig {
    /**
     * Enable CPU CFS quota enforcement for containers that specify CPU limits.
     */
    cpuCfsQuota?: boolean;
    /**
     * Set the CPU CFS quota period value 'cpu.cfs_period_us'.
     */
    cpuCfsQuotaPeriod?: string;
    /**
     * Control the CPU management policy on the node.
     */
    cpuManagerPolicy: string;
    /**
     * Controls the maximum number of processes allowed to run in a pod.
     */
    podPidsLimit?: number;
}

export interface ContainerClusterNodeConfigLinuxNodeConfig {
    /**
     * cgroupMode specifies the cgroup mode to be used on the node.
     */
    cgroupMode: string;
    /**
     * The Linux kernel parameters to be applied to the nodes and all pods running on the nodes.
     */
    sysctls?: {[key: string]: string};
}

export interface ContainerClusterNodeConfigLocalNvmeSsdBlockConfig {
    /**
     * Number of raw-block local NVMe SSD disks to be attached to the node. Each local SSD is 375 GB in size.
     */
    localSsdCount: number;
}

export interface ContainerClusterNodeConfigReservationAffinity {
    /**
     * Corresponds to the type of reservation consumption.
     */
    consumeReservationType: string;
    /**
     * The label key of a reservation resource.
     */
    key?: string;
    /**
     * The label values of the reservation resource.
     */
    values?: string[];
}

export interface ContainerClusterNodeConfigSecondaryBootDisk {
    /**
     * Disk image to create the secondary boot disk from
     */
    diskImage: string;
    /**
     * Mode for how the secondary boot disk is used.
     */
    mode?: string;
}

export interface ContainerClusterNodeConfigShieldedInstanceConfig {
    /**
     * Defines whether the instance has integrity monitoring enabled.
     */
    enableIntegrityMonitoring?: boolean;
    /**
     * Defines whether the instance has Secure Boot enabled.
     */
    enableSecureBoot?: boolean;
}

export interface ContainerClusterNodeConfigSoleTenantConfig {
    /**
     * .
     */
    nodeAffinities: outputs.ContainerClusterNodeConfigSoleTenantConfigNodeAffinity[];
}

export interface ContainerClusterNodeConfigSoleTenantConfigNodeAffinity {
    /**
     * .
     */
    key: string;
    /**
     * .
     */
    operator: string;
    /**
     * .
     */
    values: string[];
}

export interface ContainerClusterNodeConfigTaint {
    /**
     * Effect for taint.
     */
    effect: string;
    /**
     * Key for taint.
     */
    key: string;
    /**
     * Value for taint.
     */
    value: string;
}

export interface ContainerClusterNodeConfigWorkloadMetadataConfig {
    /**
     * Mode is the configuration for how to expose metadata to workloads running on the node.
     */
    mode: string;
}

export interface ContainerClusterNodePool {
    /**
     * Configuration required by cluster autoscaler to adjust the size of the node pool to the current cluster usage.
     */
    autoscaling?: outputs.ContainerClusterNodePoolAutoscaling;
    /**
     * The initial number of nodes for the pool. In regional or multi-zonal clusters, this is the number of nodes per zone. Changing this will force recreation of the resource.
     */
    initialNodeCount: number;
    /**
     * The resource URLs of the managed instance groups associated with this node pool.
     */
    instanceGroupUrls: string[];
    /**
     * List of instance group URLs which have been assigned to this node pool.
     */
    managedInstanceGroupUrls: string[];
    /**
     * Node management configuration, wherein auto-repair and auto-upgrade is configured.
     */
    management?: outputs.ContainerClusterNodePoolManagement;
    /**
     * The maximum number of pods per node in this node pool. Note that this does not work on node pools which are "route-based" - that is, node pools belonging to clusters that do not have IP Aliasing enabled.
     */
    maxPodsPerNode: number;
    name: string;
    /**
     * Creates a unique name for the node pool beginning with the specified prefix. Conflicts with name.
     */
    namePrefix: string;
    /**
     * Networking configuration for this NodePool. If specified, it overrides the cluster-level defaults.
     */
    networkConfig?: outputs.ContainerClusterNodePoolNetworkConfig;
    /**
     * The configuration of the nodepool
     */
    nodeConfig?: outputs.ContainerClusterNodePoolNodeConfig;
    /**
     * The number of nodes per instance group. This field can be used to update the number of nodes per instance group but should not be used alongside autoscaling.
     */
    nodeCount: number;
    /**
     * The list of zones in which the node pool's nodes should be located. Nodes must be in the region of their regional cluster or in the same region as their cluster's zone for zonal clusters. If unspecified, the cluster-level node_locations will be used.
     */
    nodeLocations: string[];
    /**
     * Specifies the node placement policy
     */
    placementPolicy?: outputs.ContainerClusterNodePoolPlacementPolicy;
    /**
     * Specifies the configuration of queued provisioning
     */
    queuedProvisioning?: outputs.ContainerClusterNodePoolQueuedProvisioning;
    /**
     * Specify node upgrade settings to change how many nodes GKE attempts to upgrade at once. The number of nodes upgraded simultaneously is the sum of max_surge and max_unavailable. The maximum number of nodes upgraded simultaneously is limited to 20.
     */
    upgradeSettings?: outputs.ContainerClusterNodePoolUpgradeSettings;
    version: string;
}

export interface ContainerClusterNodePoolAutoConfig {
    /**
     * Collection of Compute Engine network tags that can be applied to a node's underlying VM instance.
     */
    networkTags?: outputs.ContainerClusterNodePoolAutoConfigNetworkTags;
    /**
     * A map of resource manager tags. Resource manager tag keys and values have the same definition as resource manager tags. Keys must be in the format tagKeys/{tag_key_id}, and values are in the format tagValues/456. The field is ignored (both PUT & PATCH) when empty.
     */
    resourceManagerTags?: {[key: string]: string};
}

export interface ContainerClusterNodePoolAutoConfigNetworkTags {
    /**
     * List of network tags applied to auto-provisioned node pools.
     */
    tags?: string[];
}

export interface ContainerClusterNodePoolAutoscaling {
    /**
     * Location policy specifies the algorithm used when scaling-up the node pool. "BALANCED" - Is a best effort policy that aims to balance the sizes of available zones. "ANY" - Instructs the cluster autoscaler to prioritize utilization of unused reservations, and reduces preemption risk for Spot VMs.
     */
    locationPolicy: string;
    /**
     * Maximum number of nodes per zone in the node pool. Must be >= min_node_count. Cannot be used with total limits.
     */
    maxNodeCount?: number;
    /**
     * Minimum number of nodes per zone in the node pool. Must be >=0 and <= max_node_count. Cannot be used with total limits.
     */
    minNodeCount?: number;
    /**
     * Maximum number of all nodes in the node pool. Must be >= total_min_node_count. Cannot be used with per zone limits.
     */
    totalMaxNodeCount?: number;
    /**
     * Minimum number of all nodes in the node pool. Must be >=0 and <= total_max_node_count. Cannot be used with per zone limits.
     */
    totalMinNodeCount?: number;
}

export interface ContainerClusterNodePoolDefaults {
    /**
     * Subset of NodeConfig message that has defaults.
     */
    nodeConfigDefaults?: outputs.ContainerClusterNodePoolDefaultsNodeConfigDefaults;
}

export interface ContainerClusterNodePoolDefaultsNodeConfigDefaults {
    /**
     * Parameters for containerd configuration.
     */
    containerdConfig?: outputs.ContainerClusterNodePoolDefaultsNodeConfigDefaultsContainerdConfig;
    /**
     * Type of logging agent that is used as the default value for node pools in the cluster. Valid values include DEFAULT and MAX_THROUGHPUT.
     */
    loggingVariant: string;
}

export interface ContainerClusterNodePoolDefaultsNodeConfigDefaultsContainerdConfig {
    /**
     * Parameters for private container registries configuration.
     */
    privateRegistryAccessConfig?: outputs.ContainerClusterNodePoolDefaultsNodeConfigDefaultsContainerdConfigPrivateRegistryAccessConfig;
}

export interface ContainerClusterNodePoolDefaultsNodeConfigDefaultsContainerdConfigPrivateRegistryAccessConfig {
    /**
     * Parameters for configuring CA certificate and domains.
     */
    certificateAuthorityDomainConfigs?: outputs.ContainerClusterNodePoolDefaultsNodeConfigDefaultsContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfig[];
    /**
     * Whether or not private registries are configured.
     */
    enabled: boolean;
}

export interface ContainerClusterNodePoolDefaultsNodeConfigDefaultsContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfig {
    /**
     * List of fully-qualified-domain-names. IPv4s and port specification are supported.
     */
    fqdns: string[];
    /**
     * Parameters for configuring a certificate hosted in GCP SecretManager.
     */
    gcpSecretManagerCertificateConfig: outputs.ContainerClusterNodePoolDefaultsNodeConfigDefaultsContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfigGcpSecretManagerCertificateConfig;
}

export interface ContainerClusterNodePoolDefaultsNodeConfigDefaultsContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfigGcpSecretManagerCertificateConfig {
    /**
     * URI for the secret that hosts a certificate. Must be in the format 'projects/PROJECT_NUM/secrets/SECRET_NAME/versions/VERSION_OR_LATEST'.
     */
    secretUri: string;
}

export interface ContainerClusterNodePoolManagement {
    /**
     * Whether the nodes will be automatically repaired. Enabled by default.
     */
    autoRepair?: boolean;
    /**
     * Whether the nodes will be automatically upgraded. Enabled by default.
     */
    autoUpgrade?: boolean;
}

export interface ContainerClusterNodePoolNetworkConfig {
    /**
     * We specify the additional node networks for this node pool using this list. Each node network corresponds to an additional interface
     */
    additionalNodeNetworkConfigs?: outputs.ContainerClusterNodePoolNetworkConfigAdditionalNodeNetworkConfig[];
    /**
     * We specify the additional pod networks for this node pool using this list. Each pod network corresponds to an additional alias IP range for the node
     */
    additionalPodNetworkConfigs?: outputs.ContainerClusterNodePoolNetworkConfigAdditionalPodNetworkConfig[];
    /**
     * Whether to create a new range for pod IPs in this node pool. Defaults are provided for pod_range and pod_ipv4_cidr_block if they are not specified.
     */
    createPodRange?: boolean;
    /**
     * Whether nodes have internal IP addresses only.
     */
    enablePrivateNodes: boolean;
    /**
     * Network bandwidth tier configuration.
     */
    networkPerformanceConfig?: outputs.ContainerClusterNodePoolNetworkConfigNetworkPerformanceConfig;
    /**
     * Configuration for node-pool level pod cidr overprovision. If not set, the cluster level setting will be inherited
     */
    podCidrOverprovisionConfig?: outputs.ContainerClusterNodePoolNetworkConfigPodCidrOverprovisionConfig;
    /**
     * The IP address range for pod IPs in this node pool. Only applicable if create_pod_range is true. Set to blank to have a range chosen with the default size. Set to /netmask (e.g. /14) to have a range chosen with a specific netmask. Set to a CIDR notation (e.g. 10.96.0.0/14) to pick a specific range to use.
     */
    podIpv4CidrBlock: string;
    /**
     * The ID of the secondary range for pod IPs. If create_pod_range is true, this ID is used for the new range. If create_pod_range is false, uses an existing secondary range with this ID.
     */
    podRange: string;
}

export interface ContainerClusterNodePoolNetworkConfigAdditionalNodeNetworkConfig {
    /**
     * Name of the VPC where the additional interface belongs.
     */
    network?: string;
    /**
     * Name of the subnetwork where the additional interface belongs.
     */
    subnetwork?: string;
}

export interface ContainerClusterNodePoolNetworkConfigAdditionalPodNetworkConfig {
    /**
     * The maximum number of pods per node which use this pod network.
     */
    maxPodsPerNode: number;
    /**
     * The name of the secondary range on the subnet which provides IP address for this pod range.
     */
    secondaryPodRange?: string;
    /**
     * Name of the subnetwork where the additional pod network belongs.
     */
    subnetwork?: string;
}

export interface ContainerClusterNodePoolNetworkConfigNetworkPerformanceConfig {
    /**
     * Specifies the total network bandwidth tier for the NodePool.
     */
    totalEgressBandwidthTier: string;
}

export interface ContainerClusterNodePoolNetworkConfigPodCidrOverprovisionConfig {
    disabled: boolean;
}

export interface ContainerClusterNodePoolNodeConfig {
    /**
     * Specifies options for controlling advanced machine features.
     */
    advancedMachineFeatures?: outputs.ContainerClusterNodePoolNodeConfigAdvancedMachineFeatures;
    /**
     * The Customer Managed Encryption Key used to encrypt the boot disk attached to each node in the node pool.
     */
    bootDiskKmsKey?: string;
    /**
     * Configuration for the confidential nodes feature, which makes nodes run on confidential VMs. Warning: This configuration can't be changed (or added/removed) after pool creation without deleting and recreating the entire pool.
     */
    confidentialNodes?: outputs.ContainerClusterNodePoolNodeConfigConfidentialNodes;
    /**
     * Parameters for containerd configuration.
     */
    containerdConfig?: outputs.ContainerClusterNodePoolNodeConfigContainerdConfig;
    /**
     * Size of the disk attached to each node, specified in GB. The smallest allowed disk size is 10GB.
     */
    diskSizeGb: number;
    /**
     * Type of the disk attached to each node. Such as pd-standard, pd-balanced or pd-ssd
     */
    diskType: string;
    /**
     * List of kubernetes taints applied to each node.
     */
    effectiveTaints: outputs.ContainerClusterNodePoolNodeConfigEffectiveTaint[];
    /**
     * If enabled boot disks are configured with confidential mode.
     */
    enableConfidentialStorage?: boolean;
    /**
     * Parameters for the ephemeral storage filesystem. If unspecified, ephemeral storage is backed by the boot disk.
     */
    ephemeralStorageLocalSsdConfig?: outputs.ContainerClusterNodePoolNodeConfigEphemeralStorageLocalSsdConfig;
    /**
     * Enable or disable NCCL Fast Socket in the node pool.
     */
    fastSocket?: outputs.ContainerClusterNodePoolNodeConfigFastSocket;
    /**
     * GCFS configuration for this node.
     */
    gcfsConfig?: outputs.ContainerClusterNodePoolNodeConfigGcfsConfig;
    /**
     * List of the type and count of accelerator cards attached to the instance.
     */
    guestAccelerators?: outputs.ContainerClusterNodePoolNodeConfigGuestAccelerator[];
    /**
     * Enable or disable gvnic in the node pool.
     */
    gvnic?: outputs.ContainerClusterNodePoolNodeConfigGvnic;
    /**
     * The maintenance policy for the hosts on which the GKE VMs run on.
     */
    hostMaintenancePolicy?: outputs.ContainerClusterNodePoolNodeConfigHostMaintenancePolicy;
    /**
     * The image type to use for this node. Note that for a given image type, the latest version of it will be used.
     */
    imageType: string;
    /**
     * Node kubelet configs.
     */
    kubeletConfig?: outputs.ContainerClusterNodePoolNodeConfigKubeletConfig;
    /**
     * The map of Kubernetes labels (key/value pairs) to be applied to each node. These will added in addition to any default label(s) that Kubernetes may apply to the node.
     */
    labels: {[key: string]: string};
    /**
     * Parameters that can be configured on Linux nodes.
     */
    linuxNodeConfig?: outputs.ContainerClusterNodePoolNodeConfigLinuxNodeConfig;
    /**
     * Parameters for raw-block local NVMe SSDs.
     */
    localNvmeSsdBlockConfig?: outputs.ContainerClusterNodePoolNodeConfigLocalNvmeSsdBlockConfig;
    /**
     * The number of local SSD disks to be attached to the node.
     */
    localSsdCount: number;
    /**
     * Type of logging agent that is used as the default value for node pools in the cluster. Valid values include DEFAULT and MAX_THROUGHPUT.
     */
    loggingVariant: string;
    /**
     * The name of a Google Compute Engine machine type.
     */
    machineType: string;
    /**
     * The metadata key/value pairs assigned to instances in the cluster.
     */
    metadata: {[key: string]: string};
    /**
     * Minimum CPU platform to be used by this instance. The instance may be scheduled on the specified or newer CPU platform.
     */
    minCpuPlatform: string;
    /**
     * Setting this field will assign instances of this pool to run on the specified node group. This is useful for running workloads on sole tenant nodes.
     */
    nodeGroup?: string;
    /**
     * The set of Google API scopes to be made available on all of the node VMs.
     */
    oauthScopes: string[];
    /**
     * Whether the nodes are created as preemptible VM instances.
     */
    preemptible?: boolean;
    /**
     * The reservation affinity configuration for the node pool.
     */
    reservationAffinity?: outputs.ContainerClusterNodePoolNodeConfigReservationAffinity;
    /**
     * The GCE resource labels (a map of key/value pairs) to be applied to the node pool.
     */
    resourceLabels?: {[key: string]: string};
    /**
     * A map of resource manager tags. Resource manager tag keys and values have the same definition as resource manager tags. Keys must be in the format tagKeys/{tag_key_id}, and values are in the format tagValues/456. The field is ignored (both PUT & PATCH) when empty.
     */
    resourceManagerTags?: {[key: string]: string};
    /**
     * Secondary boot disks for preloading data or container images.
     */
    secondaryBootDisks?: outputs.ContainerClusterNodePoolNodeConfigSecondaryBootDisk[];
    /**
     * The Google Cloud Platform Service Account to be used by the node VMs.
     */
    serviceAccount: string;
    /**
     * Shielded Instance options.
     */
    shieldedInstanceConfig?: outputs.ContainerClusterNodePoolNodeConfigShieldedInstanceConfig;
    /**
     * Node affinity options for sole tenant node pools.
     */
    soleTenantConfig?: outputs.ContainerClusterNodePoolNodeConfigSoleTenantConfig;
    /**
     * Whether the nodes are created as spot VM instances.
     */
    spot?: boolean;
    /**
     * The list of instance tags applied to all nodes.
     */
    tags?: string[];
    /**
     * List of Kubernetes taints to be applied to each node.
     */
    taints?: outputs.ContainerClusterNodePoolNodeConfigTaint[];
    /**
     * The workload metadata configuration for this node.
     */
    workloadMetadataConfig?: outputs.ContainerClusterNodePoolNodeConfigWorkloadMetadataConfig;
}

export interface ContainerClusterNodePoolNodeConfigAdvancedMachineFeatures {
    /**
     * Whether the node should have nested virtualization enabled.
     */
    enableNestedVirtualization?: boolean;
    /**
     * The number of threads per physical core. To disable simultaneous multithreading (SMT) set this to 1. If unset, the maximum number of threads supported per core by the underlying processor is assumed.
     */
    threadsPerCore: number;
}

export interface ContainerClusterNodePoolNodeConfigConfidentialNodes {
    /**
     * Whether Confidential Nodes feature is enabled for all nodes in this pool.
     */
    enabled: boolean;
}

export interface ContainerClusterNodePoolNodeConfigContainerdConfig {
    /**
     * Parameters for private container registries configuration.
     */
    privateRegistryAccessConfig?: outputs.ContainerClusterNodePoolNodeConfigContainerdConfigPrivateRegistryAccessConfig;
}

export interface ContainerClusterNodePoolNodeConfigContainerdConfigPrivateRegistryAccessConfig {
    /**
     * Parameters for configuring CA certificate and domains.
     */
    certificateAuthorityDomainConfigs?: outputs.ContainerClusterNodePoolNodeConfigContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfig[];
    /**
     * Whether or not private registries are configured.
     */
    enabled: boolean;
}

export interface ContainerClusterNodePoolNodeConfigContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfig {
    /**
     * List of fully-qualified-domain-names. IPv4s and port specification are supported.
     */
    fqdns: string[];
    /**
     * Parameters for configuring a certificate hosted in GCP SecretManager.
     */
    gcpSecretManagerCertificateConfig: outputs.ContainerClusterNodePoolNodeConfigContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfigGcpSecretManagerCertificateConfig;
}

export interface ContainerClusterNodePoolNodeConfigContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfigGcpSecretManagerCertificateConfig {
    /**
     * URI for the secret that hosts a certificate. Must be in the format 'projects/PROJECT_NUM/secrets/SECRET_NAME/versions/VERSION_OR_LATEST'.
     */
    secretUri: string;
}

export interface ContainerClusterNodePoolNodeConfigEffectiveTaint {
    effect: string;
    key: string;
    value: string;
}

export interface ContainerClusterNodePoolNodeConfigEphemeralStorageLocalSsdConfig {
    /**
     * Number of local SSDs to use to back ephemeral storage. Uses NVMe interfaces. Each local SSD must be 375 or 3000 GB in size, and all local SSDs must share the same size.
     */
    localSsdCount: number;
}

export interface ContainerClusterNodePoolNodeConfigFastSocket {
    /**
     * Whether or not NCCL Fast Socket is enabled
     */
    enabled: boolean;
}

export interface ContainerClusterNodePoolNodeConfigGcfsConfig {
    /**
     * Whether or not GCFS is enabled
     */
    enabled: boolean;
}

export interface ContainerClusterNodePoolNodeConfigGuestAccelerator {
    /**
     * The number of the accelerator cards exposed to an instance.
     */
    count: number;
    /**
     * Configuration for auto installation of GPU driver.
     */
    gpuDriverInstallationConfig?: outputs.ContainerClusterNodePoolNodeConfigGuestAcceleratorGpuDriverInstallationConfig;
    /**
     * Size of partitions to create on the GPU. Valid values are described in the NVIDIA mig user guide (https://docs.nvidia.com/datacenter/tesla/mig-user-guide/#partitioning)
     */
    gpuPartitionSize?: string;
    /**
     * Configuration for GPU sharing.
     */
    gpuSharingConfig?: outputs.ContainerClusterNodePoolNodeConfigGuestAcceleratorGpuSharingConfig;
    /**
     * The accelerator type resource name.
     */
    type: string;
}

export interface ContainerClusterNodePoolNodeConfigGuestAcceleratorGpuDriverInstallationConfig {
    /**
     * Mode for how the GPU driver is installed.
     */
    gpuDriverVersion: string;
}

export interface ContainerClusterNodePoolNodeConfigGuestAcceleratorGpuSharingConfig {
    /**
     * The type of GPU sharing strategy to enable on the GPU node. Possible values are described in the API package (https://pkg.go.dev/google.golang.org/api/container/v1#GPUSharingConfig)
     */
    gpuSharingStrategy: string;
    /**
     * The maximum number of containers that can share a GPU.
     */
    maxSharedClientsPerGpu: number;
}

export interface ContainerClusterNodePoolNodeConfigGvnic {
    /**
     * Whether or not gvnic is enabled
     */
    enabled: boolean;
}

export interface ContainerClusterNodePoolNodeConfigHostMaintenancePolicy {
    /**
     * .
     */
    maintenanceInterval: string;
}

export interface ContainerClusterNodePoolNodeConfigKubeletConfig {
    /**
     * Enable CPU CFS quota enforcement for containers that specify CPU limits.
     */
    cpuCfsQuota?: boolean;
    /**
     * Set the CPU CFS quota period value 'cpu.cfs_period_us'.
     */
    cpuCfsQuotaPeriod?: string;
    /**
     * Control the CPU management policy on the node.
     */
    cpuManagerPolicy: string;
    /**
     * Controls the maximum number of processes allowed to run in a pod.
     */
    podPidsLimit?: number;
}

export interface ContainerClusterNodePoolNodeConfigLinuxNodeConfig {
    /**
     * cgroupMode specifies the cgroup mode to be used on the node.
     */
    cgroupMode: string;
    /**
     * The Linux kernel parameters to be applied to the nodes and all pods running on the nodes.
     */
    sysctls?: {[key: string]: string};
}

export interface ContainerClusterNodePoolNodeConfigLocalNvmeSsdBlockConfig {
    /**
     * Number of raw-block local NVMe SSD disks to be attached to the node. Each local SSD is 375 GB in size.
     */
    localSsdCount: number;
}

export interface ContainerClusterNodePoolNodeConfigReservationAffinity {
    /**
     * Corresponds to the type of reservation consumption.
     */
    consumeReservationType: string;
    /**
     * The label key of a reservation resource.
     */
    key?: string;
    /**
     * The label values of the reservation resource.
     */
    values?: string[];
}

export interface ContainerClusterNodePoolNodeConfigSecondaryBootDisk {
    /**
     * Disk image to create the secondary boot disk from
     */
    diskImage: string;
    /**
     * Mode for how the secondary boot disk is used.
     */
    mode?: string;
}

export interface ContainerClusterNodePoolNodeConfigShieldedInstanceConfig {
    /**
     * Defines whether the instance has integrity monitoring enabled.
     */
    enableIntegrityMonitoring?: boolean;
    /**
     * Defines whether the instance has Secure Boot enabled.
     */
    enableSecureBoot?: boolean;
}

export interface ContainerClusterNodePoolNodeConfigSoleTenantConfig {
    /**
     * .
     */
    nodeAffinities: outputs.ContainerClusterNodePoolNodeConfigSoleTenantConfigNodeAffinity[];
}

export interface ContainerClusterNodePoolNodeConfigSoleTenantConfigNodeAffinity {
    /**
     * .
     */
    key: string;
    /**
     * .
     */
    operator: string;
    /**
     * .
     */
    values: string[];
}

export interface ContainerClusterNodePoolNodeConfigTaint {
    /**
     * Effect for taint.
     */
    effect: string;
    /**
     * Key for taint.
     */
    key: string;
    /**
     * Value for taint.
     */
    value: string;
}

export interface ContainerClusterNodePoolNodeConfigWorkloadMetadataConfig {
    /**
     * Mode is the configuration for how to expose metadata to workloads running on the node.
     */
    mode: string;
}

export interface ContainerClusterNodePoolPlacementPolicy {
    /**
     * If set, refers to the name of a custom resource policy supplied by the user. The resource policy must be in the same project and region as the node pool. If not found, InvalidArgument error is returned.
     */
    policyName?: string;
    /**
     * TPU placement topology for pod slice node pool. https://cloud.google.com/tpu/docs/types-topologies#tpu_topologies
     */
    tpuTopology?: string;
    /**
     * Type defines the type of placement policy
     */
    type: string;
}

export interface ContainerClusterNodePoolQueuedProvisioning {
    /**
     * Whether nodes in this node pool are obtainable solely through the ProvisioningRequest API
     */
    enabled: boolean;
}

export interface ContainerClusterNodePoolUpgradeSettings {
    /**
     * Settings for BlueGreen node pool upgrade.
     */
    blueGreenSettings?: outputs.ContainerClusterNodePoolUpgradeSettingsBlueGreenSettings;
    /**
     * The number of additional nodes that can be added to the node pool during an upgrade. Increasing max_surge raises the number of nodes that can be upgraded simultaneously. Can be set to 0 or greater.
     */
    maxSurge: number;
    /**
     * The number of nodes that can be simultaneously unavailable during an upgrade. Increasing max_unavailable raises the number of nodes that can be upgraded in parallel. Can be set to 0 or greater.
     */
    maxUnavailable: number;
    /**
     * Update strategy for the given nodepool.
     */
    strategy?: string;
}

export interface ContainerClusterNodePoolUpgradeSettingsBlueGreenSettings {
    /**
     * Time needed after draining entire blue pool. After this period, blue pool will be cleaned up.
     */
    nodePoolSoakDuration: string;
    /**
     * Standard rollout policy is the default policy for blue-green.
     */
    standardRolloutPolicy: outputs.ContainerClusterNodePoolUpgradeSettingsBlueGreenSettingsStandardRolloutPolicy;
}

export interface ContainerClusterNodePoolUpgradeSettingsBlueGreenSettingsStandardRolloutPolicy {
    /**
     * Number of blue nodes to drain in a batch.
     */
    batchNodeCount: number;
    /**
     * Percentage of the blue pool nodes to drain in a batch.
     */
    batchPercentage: number;
    /**
     * Soak time after each batch gets drained.
     */
    batchSoakDuration: string;
}

export interface ContainerClusterNotificationConfig {
    /**
     * Notification config for Cloud Pub/Sub
     */
    pubsub: outputs.ContainerClusterNotificationConfigPubsub;
}

export interface ContainerClusterNotificationConfigPubsub {
    /**
     * Whether or not the notification config is enabled
     */
    enabled: boolean;
    /**
     * Allows filtering to one or more specific event types. If event types are present, those and only those event types will be transmitted to the cluster. Other types will be skipped. If no filter is specified, or no event types are present, all event types will be sent
     */
    filter?: outputs.ContainerClusterNotificationConfigPubsubFilter;
    /**
     * The pubsub topic to push upgrade notifications to. Must be in the same project as the cluster. Must be in the format: projects/{project}/topics/{topic}.
     */
    topic?: string;
}

export interface ContainerClusterNotificationConfigPubsubFilter {
    /**
     * Can be used to filter what notifications are sent. Valid values include include UPGRADE_AVAILABLE_EVENT, UPGRADE_EVENT and SECURITY_BULLETIN_EVENT
     */
    eventTypes: string[];
}

export interface ContainerClusterPrivateClusterConfig {
    /**
     * When true, the cluster's private endpoint is used as the cluster endpoint and access through the public endpoint is disabled. When false, either endpoint can be used.
     */
    enablePrivateEndpoint?: boolean;
    /**
     * Enables the private cluster feature, creating a private endpoint on the cluster. In a private cluster, nodes only have RFC 1918 private addresses and communicate with the master's private endpoint via private networking.
     */
    enablePrivateNodes?: boolean;
    /**
     * Controls cluster master global access settings.
     */
    masterGlobalAccessConfig?: outputs.ContainerClusterPrivateClusterConfigMasterGlobalAccessConfig;
    /**
     * The IP range in CIDR notation to use for the hosted master network. This range will be used for assigning private IP addresses to the cluster master(s) and the ILB VIP. This range must not overlap with any other ranges in use within the cluster's network, and it must be a /28 subnet. See Private Cluster Limitations for more details. This field only applies to private clusters, when enable_private_nodes is true.
     */
    masterIpv4CidrBlock: string;
    /**
     * The name of the peering between this cluster and the Google owned VPC.
     */
    peeringName: string;
    /**
     * The internal IP address of this cluster's master endpoint.
     */
    privateEndpoint: string;
    /**
     * Subnetwork in cluster's network where master's endpoint will be provisioned.
     */
    privateEndpointSubnetwork?: string;
    /**
     * The external IP address of this cluster's master endpoint.
     */
    publicEndpoint: string;
}

export interface ContainerClusterPrivateClusterConfigMasterGlobalAccessConfig {
    /**
     * Whether the cluster master is accessible globally or not.
     */
    enabled: boolean;
}

export interface ContainerClusterReleaseChannel {
    /**
     * The selected release channel. Accepted values are:
     * * UNSPECIFIED: Not set.
     * * RAPID: Weekly upgrade cadence; Early testers and developers who requires new features.
     * * REGULAR: Multiple per month upgrade cadence; Production users who need features not yet offered in the Stable channel.
     * * STABLE: Every few months upgrade cadence; Production users who need stability above all else, and for whom frequent upgrades are too risky.
     * * EXTENDED: GKE provides extended support for Kubernetes minor versions through the Extended channel. With this channel, you can stay on a minor version for up to 24 months.
     */
    channel: string;
}

export interface ContainerClusterResourceUsageExportConfig {
    /**
     * Parameters for using BigQuery as the destination of resource usage export.
     */
    bigqueryDestination: outputs.ContainerClusterResourceUsageExportConfigBigqueryDestination;
    /**
     * Whether to enable network egress metering for this cluster. If enabled, a daemonset will be created in the cluster to meter network egress traffic.
     */
    enableNetworkEgressMetering?: boolean;
    /**
     * Whether to enable resource consumption metering on this cluster. When enabled, a table will be created in the resource export BigQuery dataset to store resource consumption data. The resulting table can be joined with the resource usage table or with BigQuery billing export. Defaults to true.
     */
    enableResourceConsumptionMetering?: boolean;
}

export interface ContainerClusterResourceUsageExportConfigBigqueryDestination {
    /**
     * The ID of a BigQuery Dataset.
     */
    datasetId: string;
}

export interface ContainerClusterSecretManagerConfig {
    /**
     * Enable the Secret manager csi component.
     */
    enabled: boolean;
}

export interface ContainerClusterSecurityPostureConfig {
    /**
     * Sets the mode of the Kubernetes security posture API's off-cluster features. Available options include DISABLED, BASIC, and ENTERPRISE.
     */
    mode: string;
    /**
     * Sets the mode of the Kubernetes security posture API's workload vulnerability scanning. Available options include VULNERABILITY_DISABLED, VULNERABILITY_BASIC and VULNERABILITY_ENTERPRISE.
     */
    vulnerabilityMode: string;
}

export interface ContainerClusterServiceExternalIpsConfig {
    /**
     * When enabled, services with external ips specified will be allowed.
     */
    enabled: boolean;
}

export interface ContainerClusterTimeouts {
    create?: string;
    delete?: string;
    read?: string;
    update?: string;
}

export interface ContainerClusterVerticalPodAutoscaling {
    /**
     * Enables vertical pod autoscaling.
     */
    enabled: boolean;
}

export interface ContainerClusterWorkloadIdentityConfig {
    /**
     * The workload pool to attach all Kubernetes service accounts to.
     */
    workloadPool?: string;
}

export interface ContainerNodePoolAutoscaling {
    /**
     * Location policy specifies the algorithm used when scaling-up the node pool. "BALANCED" - Is a best effort policy that aims to balance the sizes of available zones. "ANY" - Instructs the cluster autoscaler to prioritize utilization of unused reservations, and reduces preemption risk for Spot VMs.
     */
    locationPolicy: string;
    /**
     * Maximum number of nodes per zone in the node pool. Must be >= min_node_count. Cannot be used with total limits.
     */
    maxNodeCount?: number;
    /**
     * Minimum number of nodes per zone in the node pool. Must be >=0 and <= max_node_count. Cannot be used with total limits.
     */
    minNodeCount?: number;
    /**
     * Maximum number of all nodes in the node pool. Must be >= total_min_node_count. Cannot be used with per zone limits.
     */
    totalMaxNodeCount?: number;
    /**
     * Minimum number of all nodes in the node pool. Must be >=0 and <= total_max_node_count. Cannot be used with per zone limits.
     */
    totalMinNodeCount?: number;
}

export interface ContainerNodePoolManagement {
    /**
     * Whether the nodes will be automatically repaired. Enabled by default.
     */
    autoRepair?: boolean;
    /**
     * Whether the nodes will be automatically upgraded. Enabled by default.
     */
    autoUpgrade?: boolean;
}

export interface ContainerNodePoolNetworkConfig {
    /**
     * We specify the additional node networks for this node pool using this list. Each node network corresponds to an additional interface
     */
    additionalNodeNetworkConfigs?: outputs.ContainerNodePoolNetworkConfigAdditionalNodeNetworkConfig[];
    /**
     * We specify the additional pod networks for this node pool using this list. Each pod network corresponds to an additional alias IP range for the node
     */
    additionalPodNetworkConfigs?: outputs.ContainerNodePoolNetworkConfigAdditionalPodNetworkConfig[];
    /**
     * Whether to create a new range for pod IPs in this node pool. Defaults are provided for pod_range and pod_ipv4_cidr_block if they are not specified.
     */
    createPodRange?: boolean;
    /**
     * Whether nodes have internal IP addresses only.
     */
    enablePrivateNodes: boolean;
    /**
     * Network bandwidth tier configuration.
     */
    networkPerformanceConfig?: outputs.ContainerNodePoolNetworkConfigNetworkPerformanceConfig;
    /**
     * Configuration for node-pool level pod cidr overprovision. If not set, the cluster level setting will be inherited
     */
    podCidrOverprovisionConfig?: outputs.ContainerNodePoolNetworkConfigPodCidrOverprovisionConfig;
    /**
     * The IP address range for pod IPs in this node pool. Only applicable if create_pod_range is true. Set to blank to have a range chosen with the default size. Set to /netmask (e.g. /14) to have a range chosen with a specific netmask. Set to a CIDR notation (e.g. 10.96.0.0/14) to pick a specific range to use.
     */
    podIpv4CidrBlock: string;
    /**
     * The ID of the secondary range for pod IPs. If create_pod_range is true, this ID is used for the new range. If create_pod_range is false, uses an existing secondary range with this ID.
     */
    podRange: string;
}

export interface ContainerNodePoolNetworkConfigAdditionalNodeNetworkConfig {
    /**
     * Name of the VPC where the additional interface belongs.
     */
    network?: string;
    /**
     * Name of the subnetwork where the additional interface belongs.
     */
    subnetwork?: string;
}

export interface ContainerNodePoolNetworkConfigAdditionalPodNetworkConfig {
    /**
     * The maximum number of pods per node which use this pod network.
     */
    maxPodsPerNode: number;
    /**
     * The name of the secondary range on the subnet which provides IP address for this pod range.
     */
    secondaryPodRange?: string;
    /**
     * Name of the subnetwork where the additional pod network belongs.
     */
    subnetwork?: string;
}

export interface ContainerNodePoolNetworkConfigNetworkPerformanceConfig {
    /**
     * Specifies the total network bandwidth tier for the NodePool.
     */
    totalEgressBandwidthTier: string;
}

export interface ContainerNodePoolNetworkConfigPodCidrOverprovisionConfig {
    disabled: boolean;
}

export interface ContainerNodePoolNodeConfig {
    /**
     * Specifies options for controlling advanced machine features.
     */
    advancedMachineFeatures?: outputs.ContainerNodePoolNodeConfigAdvancedMachineFeatures;
    /**
     * The Customer Managed Encryption Key used to encrypt the boot disk attached to each node in the node pool.
     */
    bootDiskKmsKey?: string;
    /**
     * Configuration for the confidential nodes feature, which makes nodes run on confidential VMs. Warning: This configuration can't be changed (or added/removed) after pool creation without deleting and recreating the entire pool.
     */
    confidentialNodes?: outputs.ContainerNodePoolNodeConfigConfidentialNodes;
    /**
     * Parameters for containerd configuration.
     */
    containerdConfig?: outputs.ContainerNodePoolNodeConfigContainerdConfig;
    /**
     * Size of the disk attached to each node, specified in GB. The smallest allowed disk size is 10GB.
     */
    diskSizeGb: number;
    /**
     * Type of the disk attached to each node. Such as pd-standard, pd-balanced or pd-ssd
     */
    diskType: string;
    /**
     * List of kubernetes taints applied to each node.
     */
    effectiveTaints: outputs.ContainerNodePoolNodeConfigEffectiveTaint[];
    /**
     * If enabled boot disks are configured with confidential mode.
     */
    enableConfidentialStorage?: boolean;
    /**
     * Parameters for the ephemeral storage filesystem. If unspecified, ephemeral storage is backed by the boot disk.
     */
    ephemeralStorageLocalSsdConfig?: outputs.ContainerNodePoolNodeConfigEphemeralStorageLocalSsdConfig;
    /**
     * Enable or disable NCCL Fast Socket in the node pool.
     */
    fastSocket?: outputs.ContainerNodePoolNodeConfigFastSocket;
    /**
     * GCFS configuration for this node.
     */
    gcfsConfig?: outputs.ContainerNodePoolNodeConfigGcfsConfig;
    /**
     * List of the type and count of accelerator cards attached to the instance.
     */
    guestAccelerators?: outputs.ContainerNodePoolNodeConfigGuestAccelerator[];
    /**
     * Enable or disable gvnic in the node pool.
     */
    gvnic?: outputs.ContainerNodePoolNodeConfigGvnic;
    /**
     * The maintenance policy for the hosts on which the GKE VMs run on.
     */
    hostMaintenancePolicy?: outputs.ContainerNodePoolNodeConfigHostMaintenancePolicy;
    /**
     * The image type to use for this node. Note that for a given image type, the latest version of it will be used.
     */
    imageType: string;
    /**
     * Node kubelet configs.
     */
    kubeletConfig?: outputs.ContainerNodePoolNodeConfigKubeletConfig;
    /**
     * The map of Kubernetes labels (key/value pairs) to be applied to each node. These will added in addition to any default label(s) that Kubernetes may apply to the node.
     */
    labels: {[key: string]: string};
    /**
     * Parameters that can be configured on Linux nodes.
     */
    linuxNodeConfig?: outputs.ContainerNodePoolNodeConfigLinuxNodeConfig;
    /**
     * Parameters for raw-block local NVMe SSDs.
     */
    localNvmeSsdBlockConfig?: outputs.ContainerNodePoolNodeConfigLocalNvmeSsdBlockConfig;
    /**
     * The number of local SSD disks to be attached to the node.
     */
    localSsdCount: number;
    /**
     * Type of logging agent that is used as the default value for node pools in the cluster. Valid values include DEFAULT and MAX_THROUGHPUT.
     */
    loggingVariant: string;
    /**
     * The name of a Google Compute Engine machine type.
     */
    machineType: string;
    /**
     * The metadata key/value pairs assigned to instances in the cluster.
     */
    metadata: {[key: string]: string};
    /**
     * Minimum CPU platform to be used by this instance. The instance may be scheduled on the specified or newer CPU platform.
     */
    minCpuPlatform: string;
    /**
     * Setting this field will assign instances of this pool to run on the specified node group. This is useful for running workloads on sole tenant nodes.
     */
    nodeGroup?: string;
    /**
     * The set of Google API scopes to be made available on all of the node VMs.
     */
    oauthScopes: string[];
    /**
     * Whether the nodes are created as preemptible VM instances.
     */
    preemptible?: boolean;
    /**
     * The reservation affinity configuration for the node pool.
     */
    reservationAffinity?: outputs.ContainerNodePoolNodeConfigReservationAffinity;
    /**
     * The GCE resource labels (a map of key/value pairs) to be applied to the node pool.
     */
    resourceLabels?: {[key: string]: string};
    /**
     * A map of resource manager tags. Resource manager tag keys and values have the same definition as resource manager tags. Keys must be in the format tagKeys/{tag_key_id}, and values are in the format tagValues/456. The field is ignored (both PUT & PATCH) when empty.
     */
    resourceManagerTags?: {[key: string]: string};
    /**
     * Secondary boot disks for preloading data or container images.
     */
    secondaryBootDisks?: outputs.ContainerNodePoolNodeConfigSecondaryBootDisk[];
    /**
     * The Google Cloud Platform Service Account to be used by the node VMs.
     */
    serviceAccount: string;
    /**
     * Shielded Instance options.
     */
    shieldedInstanceConfig?: outputs.ContainerNodePoolNodeConfigShieldedInstanceConfig;
    /**
     * Node affinity options for sole tenant node pools.
     */
    soleTenantConfig?: outputs.ContainerNodePoolNodeConfigSoleTenantConfig;
    /**
     * Whether the nodes are created as spot VM instances.
     */
    spot?: boolean;
    /**
     * The list of instance tags applied to all nodes.
     */
    tags?: string[];
    /**
     * List of Kubernetes taints to be applied to each node.
     */
    taints?: outputs.ContainerNodePoolNodeConfigTaint[];
    /**
     * The workload metadata configuration for this node.
     */
    workloadMetadataConfig?: outputs.ContainerNodePoolNodeConfigWorkloadMetadataConfig;
}

export interface ContainerNodePoolNodeConfigAdvancedMachineFeatures {
    /**
     * Whether the node should have nested virtualization enabled.
     */
    enableNestedVirtualization?: boolean;
    /**
     * The number of threads per physical core. To disable simultaneous multithreading (SMT) set this to 1. If unset, the maximum number of threads supported per core by the underlying processor is assumed.
     */
    threadsPerCore: number;
}

export interface ContainerNodePoolNodeConfigConfidentialNodes {
    /**
     * Whether Confidential Nodes feature is enabled for all nodes in this pool.
     */
    enabled: boolean;
}

export interface ContainerNodePoolNodeConfigContainerdConfig {
    /**
     * Parameters for private container registries configuration.
     */
    privateRegistryAccessConfig?: outputs.ContainerNodePoolNodeConfigContainerdConfigPrivateRegistryAccessConfig;
}

export interface ContainerNodePoolNodeConfigContainerdConfigPrivateRegistryAccessConfig {
    /**
     * Parameters for configuring CA certificate and domains.
     */
    certificateAuthorityDomainConfigs?: outputs.ContainerNodePoolNodeConfigContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfig[];
    /**
     * Whether or not private registries are configured.
     */
    enabled: boolean;
}

export interface ContainerNodePoolNodeConfigContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfig {
    /**
     * List of fully-qualified-domain-names. IPv4s and port specification are supported.
     */
    fqdns: string[];
    /**
     * Parameters for configuring a certificate hosted in GCP SecretManager.
     */
    gcpSecretManagerCertificateConfig: outputs.ContainerNodePoolNodeConfigContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfigGcpSecretManagerCertificateConfig;
}

export interface ContainerNodePoolNodeConfigContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfigGcpSecretManagerCertificateConfig {
    /**
     * URI for the secret that hosts a certificate. Must be in the format 'projects/PROJECT_NUM/secrets/SECRET_NAME/versions/VERSION_OR_LATEST'.
     */
    secretUri: string;
}

export interface ContainerNodePoolNodeConfigEffectiveTaint {
    effect: string;
    key: string;
    value: string;
}

export interface ContainerNodePoolNodeConfigEphemeralStorageLocalSsdConfig {
    /**
     * Number of local SSDs to use to back ephemeral storage. Uses NVMe interfaces. Each local SSD must be 375 or 3000 GB in size, and all local SSDs must share the same size.
     */
    localSsdCount: number;
}

export interface ContainerNodePoolNodeConfigFastSocket {
    /**
     * Whether or not NCCL Fast Socket is enabled
     */
    enabled: boolean;
}

export interface ContainerNodePoolNodeConfigGcfsConfig {
    /**
     * Whether or not GCFS is enabled
     */
    enabled: boolean;
}

export interface ContainerNodePoolNodeConfigGuestAccelerator {
    /**
     * The number of the accelerator cards exposed to an instance.
     */
    count: number;
    /**
     * Configuration for auto installation of GPU driver.
     */
    gpuDriverInstallationConfig?: outputs.ContainerNodePoolNodeConfigGuestAcceleratorGpuDriverInstallationConfig;
    /**
     * Size of partitions to create on the GPU. Valid values are described in the NVIDIA mig user guide (https://docs.nvidia.com/datacenter/tesla/mig-user-guide/#partitioning)
     */
    gpuPartitionSize?: string;
    /**
     * Configuration for GPU sharing.
     */
    gpuSharingConfig?: outputs.ContainerNodePoolNodeConfigGuestAcceleratorGpuSharingConfig;
    /**
     * The accelerator type resource name.
     */
    type: string;
}

export interface ContainerNodePoolNodeConfigGuestAcceleratorGpuDriverInstallationConfig {
    /**
     * Mode for how the GPU driver is installed.
     */
    gpuDriverVersion: string;
}

export interface ContainerNodePoolNodeConfigGuestAcceleratorGpuSharingConfig {
    /**
     * The type of GPU sharing strategy to enable on the GPU node. Possible values are described in the API package (https://pkg.go.dev/google.golang.org/api/container/v1#GPUSharingConfig)
     */
    gpuSharingStrategy: string;
    /**
     * The maximum number of containers that can share a GPU.
     */
    maxSharedClientsPerGpu: number;
}

export interface ContainerNodePoolNodeConfigGvnic {
    /**
     * Whether or not gvnic is enabled
     */
    enabled: boolean;
}

export interface ContainerNodePoolNodeConfigHostMaintenancePolicy {
    /**
     * .
     */
    maintenanceInterval: string;
}

export interface ContainerNodePoolNodeConfigKubeletConfig {
    /**
     * Enable CPU CFS quota enforcement for containers that specify CPU limits.
     */
    cpuCfsQuota?: boolean;
    /**
     * Set the CPU CFS quota period value 'cpu.cfs_period_us'.
     */
    cpuCfsQuotaPeriod?: string;
    /**
     * Control the CPU management policy on the node.
     */
    cpuManagerPolicy: string;
    /**
     * Controls the maximum number of processes allowed to run in a pod.
     */
    podPidsLimit?: number;
}

export interface ContainerNodePoolNodeConfigLinuxNodeConfig {
    /**
     * cgroupMode specifies the cgroup mode to be used on the node.
     */
    cgroupMode: string;
    /**
     * The Linux kernel parameters to be applied to the nodes and all pods running on the nodes.
     */
    sysctls?: {[key: string]: string};
}

export interface ContainerNodePoolNodeConfigLocalNvmeSsdBlockConfig {
    /**
     * Number of raw-block local NVMe SSD disks to be attached to the node. Each local SSD is 375 GB in size.
     */
    localSsdCount: number;
}

export interface ContainerNodePoolNodeConfigReservationAffinity {
    /**
     * Corresponds to the type of reservation consumption.
     */
    consumeReservationType: string;
    /**
     * The label key of a reservation resource.
     */
    key?: string;
    /**
     * The label values of the reservation resource.
     */
    values?: string[];
}

export interface ContainerNodePoolNodeConfigSecondaryBootDisk {
    /**
     * Disk image to create the secondary boot disk from
     */
    diskImage: string;
    /**
     * Mode for how the secondary boot disk is used.
     */
    mode?: string;
}

export interface ContainerNodePoolNodeConfigShieldedInstanceConfig {
    /**
     * Defines whether the instance has integrity monitoring enabled.
     */
    enableIntegrityMonitoring?: boolean;
    /**
     * Defines whether the instance has Secure Boot enabled.
     */
    enableSecureBoot?: boolean;
}

export interface ContainerNodePoolNodeConfigSoleTenantConfig {
    /**
     * .
     */
    nodeAffinities: outputs.ContainerNodePoolNodeConfigSoleTenantConfigNodeAffinity[];
}

export interface ContainerNodePoolNodeConfigSoleTenantConfigNodeAffinity {
    /**
     * .
     */
    key: string;
    /**
     * .
     */
    operator: string;
    /**
     * .
     */
    values: string[];
}

export interface ContainerNodePoolNodeConfigTaint {
    /**
     * Effect for taint.
     */
    effect: string;
    /**
     * Key for taint.
     */
    key: string;
    /**
     * Value for taint.
     */
    value: string;
}

export interface ContainerNodePoolNodeConfigWorkloadMetadataConfig {
    /**
     * Mode is the configuration for how to expose metadata to workloads running on the node.
     */
    mode: string;
}

export interface ContainerNodePoolPlacementPolicy {
    /**
     * If set, refers to the name of a custom resource policy supplied by the user. The resource policy must be in the same project and region as the node pool. If not found, InvalidArgument error is returned.
     */
    policyName?: string;
    /**
     * TPU placement topology for pod slice node pool. https://cloud.google.com/tpu/docs/types-topologies#tpu_topologies
     */
    tpuTopology?: string;
    /**
     * Type defines the type of placement policy
     */
    type: string;
}

export interface ContainerNodePoolQueuedProvisioning {
    /**
     * Whether nodes in this node pool are obtainable solely through the ProvisioningRequest API
     */
    enabled: boolean;
}

export interface ContainerNodePoolTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ContainerNodePoolUpgradeSettings {
    /**
     * Settings for BlueGreen node pool upgrade.
     */
    blueGreenSettings?: outputs.ContainerNodePoolUpgradeSettingsBlueGreenSettings;
    /**
     * The number of additional nodes that can be added to the node pool during an upgrade. Increasing max_surge raises the number of nodes that can be upgraded simultaneously. Can be set to 0 or greater.
     */
    maxSurge: number;
    /**
     * The number of nodes that can be simultaneously unavailable during an upgrade. Increasing max_unavailable raises the number of nodes that can be upgraded in parallel. Can be set to 0 or greater.
     */
    maxUnavailable: number;
    /**
     * Update strategy for the given nodepool.
     */
    strategy?: string;
}

export interface ContainerNodePoolUpgradeSettingsBlueGreenSettings {
    /**
     * Time needed after draining entire blue pool. After this period, blue pool will be cleaned up.
     */
    nodePoolSoakDuration: string;
    /**
     * Standard rollout policy is the default policy for blue-green.
     */
    standardRolloutPolicy: outputs.ContainerNodePoolUpgradeSettingsBlueGreenSettingsStandardRolloutPolicy;
}

export interface ContainerNodePoolUpgradeSettingsBlueGreenSettingsStandardRolloutPolicy {
    /**
     * Number of blue nodes to drain in a batch.
     */
    batchNodeCount: number;
    /**
     * Percentage of the blue pool nodes to drain in a batch.
     */
    batchPercentage: number;
    /**
     * Soak time after each batch gets drained.
     */
    batchSoakDuration: string;
}

export interface DataCatalogEntryBigqueryDateShardedSpec {
    dataset: string;
    shardCount: number;
    tablePrefix: string;
}

export interface DataCatalogEntryBigqueryTableSpec {
    tableSourceType: string;
    tableSpecs: outputs.DataCatalogEntryBigqueryTableSpecTableSpec[];
    viewSpecs: outputs.DataCatalogEntryBigqueryTableSpecViewSpec[];
}

export interface DataCatalogEntryBigqueryTableSpecTableSpec {
    groupedEntry: string;
}

export interface DataCatalogEntryBigqueryTableSpecViewSpec {
    viewQuery: string;
}

export interface DataCatalogEntryGcsFilesetSpec {
    /**
     * Patterns to identify a set of files in Google Cloud Storage.
     * See [Cloud Storage documentation](https://cloud.google.com/storage/docs/gsutil/addlhelp/WildcardNames)
     * for more information. Note that bucket wildcards are currently not supported. Examples of valid filePatterns:
     *
     * * gs://bucket_name/dir/*: matches all files within bucket_name/dir directory.
     * * gs://bucket_name/dir/**: matches all files in bucket_name/dir spanning all subdirectories.
     * * gs://bucket_name/file*: matches files prefixed by file in bucket_name
     * * gs://bucket_name/??.txt: matches files with two characters followed by .txt in bucket_name
     * * gs://bucket_name/[aeiou].txt: matches files that contain a single vowel character followed by .txt in bucket_name
     * * gs://bucket_name/[a-m].txt: matches files that contain a, b, ... or m followed by .txt in bucket_name
     * * gs://bucket_name/a/*&#47;b: matches all files in bucket_name that match a/*&#47;b pattern, such as a/c/b, a/d/b
     * * gs://another_bucket/a.txt: matches gs://another_bucket/a.txt
     */
    filePatterns: string[];
    /**
     * Sample files contained in this fileset, not all files contained in this fileset are represented here.
     */
    sampleGcsFileSpecs: outputs.DataCatalogEntryGcsFilesetSpecSampleGcsFileSpec[];
}

export interface DataCatalogEntryGcsFilesetSpecSampleGcsFileSpec {
    filePath: string;
    sizeBytes: number;
}

export interface DataCatalogEntryGroupIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataCatalogEntryGroupIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataCatalogEntryGroupTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DataCatalogEntryTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DataCatalogPolicyTagIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataCatalogPolicyTagIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataCatalogPolicyTagTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DataCatalogTagField {
    /**
     * Holds the value for a tag field with boolean type.
     */
    boolValue?: boolean;
    /**
     * The display name of this field
     */
    displayName: string;
    /**
     * Holds the value for a tag field with double type.
     */
    doubleValue?: number;
    /**
     * The display name of the enum value.
     */
    enumValue?: string;
    fieldName: string;
    /**
     * The order of this field with respect to other fields in this tag. For example, a higher value can indicate
     * a more important field. The value can be negative. Multiple fields can have the same order, and field orders
     * within a tag do not have to be sequential.
     */
    order: number;
    /**
     * Holds the value for a tag field with string type.
     */
    stringValue?: string;
    /**
     * Holds the value for a tag field with timestamp type.
     */
    timestampValue?: string;
}

export interface DataCatalogTagTemplateField {
    /**
     * A description for this field.
     */
    description: string;
    /**
     * The display name for this field.
     */
    displayName: string;
    fieldId: string;
    /**
     * Whether this is a required field. Defaults to false.
     */
    isRequired: boolean;
    /**
     * The resource name of the tag template field in URL format. Example: projects/{project_id}/locations/{location}/tagTemplates/{tagTemplateId}/fields/{field}
     */
    name: string;
    /**
     * The order of this field with respect to other fields in this tag template.
     * A higher value indicates a more important field. The value can be negative.
     * Multiple fields can have the same order, and field orders within a tag do not have to be sequential.
     */
    order: number;
    /**
     * The type of value this tag field can contain.
     */
    type: outputs.DataCatalogTagTemplateFieldType;
}

export interface DataCatalogTagTemplateFieldType {
    /**
     * Represents an enum type.
     *  Exactly one of 'primitive_type' or 'enum_type' must be set
     */
    enumType?: outputs.DataCatalogTagTemplateFieldTypeEnumType;
    /**
     * Represents primitive types - string, bool etc.
     *  Exactly one of 'primitive_type' or 'enum_type' must be set Possible values: ["DOUBLE", "STRING", "BOOL", "TIMESTAMP"]
     */
    primitiveType: string;
}

export interface DataCatalogTagTemplateFieldTypeEnumType {
    /**
     * The set of allowed values for this enum. The display names of the
     * values must be case-insensitively unique within this set. Currently,
     * enum values can only be added to the list of allowed values. Deletion
     * and renaming of enum values are not supported.
     * Can have up to 500 allowed values.
     */
    allowedValues: outputs.DataCatalogTagTemplateFieldTypeEnumTypeAllowedValue[];
}

export interface DataCatalogTagTemplateFieldTypeEnumTypeAllowedValue {
    /**
     * The display name of the enum value.
     */
    displayName: string;
}

export interface DataCatalogTagTemplateIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataCatalogTagTemplateIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataCatalogTagTemplateTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DataCatalogTagTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DataCatalogTaxonomyIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataCatalogTaxonomyIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataCatalogTaxonomyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DataFusionInstanceAccelerator {
    /**
     * The type of an accelator for a CDF instance. Possible values: ["CDC", "HEALTHCARE", "CCAI_INSIGHTS"]
     */
    acceleratorType: string;
    /**
     * The type of an accelator for a CDF instance. Possible values: ["ENABLED", "DISABLED"]
     */
    state: string;
}

export interface DataFusionInstanceCryptoKeyConfig {
    /**
     * The name of the key which is used to encrypt/decrypt customer data. For key in Cloud KMS, the key should be in the format of projects/*&#47;locations/*&#47;keyRings/*&#47;cryptoKeys/*.
     */
    keyReference: string;
}

export interface DataFusionInstanceEventPublishConfig {
    /**
     * Option to enable Event Publishing.
     */
    enabled: boolean;
    /**
     * The resource name of the Pub/Sub topic. Format: projects/{projectId}/topics/{topic_id}
     */
    topic: string;
}

export interface DataFusionInstanceIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataFusionInstanceIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataFusionInstanceNetworkConfig {
    /**
     * Optional. Type of connection for establishing private IP connectivity between the Data Fusion customer project VPC and
     * the corresponding tenant project from a predefined list of available connection modes.
     * If this field is unspecified for a private instance, VPC peering is used. Possible values: ["VPC_PEERING", "PRIVATE_SERVICE_CONNECT_INTERFACES"]
     */
    connectionType?: string;
    /**
     * The IP range in CIDR notation to use for the managed Data Fusion instance
     * nodes. This range must not overlap with any other ranges used in the Data Fusion instance network.
     */
    ipAllocation?: string;
    /**
     * Name of the network in the project with which the tenant project
     * will be peered for executing pipelines. In case of shared VPC where the network resides in another host
     * project the network should specified in the form of projects/{host-project-id}/global/networks/{network}
     */
    network?: string;
    /**
     * Optional. Configuration for Private Service Connect.
     * This is required only when using connection type PRIVATE_SERVICE_CONNECT_INTERFACES.
     */
    privateServiceConnectConfig?: outputs.DataFusionInstanceNetworkConfigPrivateServiceConnectConfig;
}

export interface DataFusionInstanceNetworkConfigPrivateServiceConnectConfig {
    /**
     * Output only. The CIDR block to which the CDF instance can't route traffic to in the consumer project VPC.
     * The size of this block is /25. The format of this field is governed by RFC 4632.
     */
    effectiveUnreachableCidrBlock: string;
    /**
     * Optional. The reference to the network attachment used to establish private connectivity.
     * It will be of the form projects/{project-id}/regions/{region}/networkAttachments/{network-attachment-id}.
     * This is required only when using connection type PRIVATE_SERVICE_CONNECT_INTERFACES.
     */
    networkAttachment?: string;
    /**
     * Optional. Input only. The CIDR block to which the CDF instance can't route traffic to in the consumer project VPC.
     * The size of this block should be at least /25. This range should not overlap with the primary address range of any subnetwork used by the network attachment.
     * This range can be used for other purposes in the consumer VPC as long as there is no requirement for CDF to reach destinations using these addresses.
     * If this value is not provided, the server chooses a non RFC 1918 address range. The format of this field is governed by RFC 4632.
     */
    unreachableCidrBlock?: string;
}

export interface DataFusionInstanceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfig {
    /**
     * Treat the dataset as an image and redact.
     */
    imageTransformations?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigImageTransformations;
    /**
     * Treat the dataset as free-form text and apply the same free text transformation everywhere
     */
    infoTypeTransformations?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformations;
    /**
     * Treat the dataset as structured. Transformations can be applied to specific locations within structured datasets, such as transforming a column within a table.
     */
    recordTransformations?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformations;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigImageTransformations {
    /**
     * For determination of how redaction of images should occur.
     */
    transforms: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransform[];
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransform {
    /**
     * Apply transformation to all findings not specified in other ImageTransformation's selectedInfoTypes.
     */
    allInfoTypes?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransformAllInfoTypes;
    /**
     * Apply transformation to all text that doesn't match an infoType.
     */
    allText?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransformAllText;
    /**
     * The color to use when redacting content from an image. If not specified, the default is black.
     */
    redactionColor?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransformRedactionColor;
    /**
     * Apply transformation to the selected infoTypes.
     */
    selectedInfoTypes?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransformSelectedInfoTypes;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransformAllInfoTypes {
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransformAllText {
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransformRedactionColor {
    /**
     * The amount of blue in the color as a value in the interval [0, 1].
     */
    blue?: number;
    /**
     * The amount of green in the color as a value in the interval [0, 1].
     */
    green?: number;
    /**
     * The amount of red in the color as a value in the interval [0, 1].
     */
    red?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransformSelectedInfoTypes {
    /**
     * InfoTypes to apply the transformation to. Leaving this empty will apply the transformation to apply to
     * all findings that correspond to infoTypes that were requested in InspectConfig.
     */
    infoTypes: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransformSelectedInfoTypesInfoType[];
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransformSelectedInfoTypesInfoType {
    /**
     * Name of the information type.
     */
    name: string;
    /**
     * Optional custom sensitivity for this InfoType. This only applies to data profiling.
     */
    sensitivityScore?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransformSelectedInfoTypesInfoTypeSensitivityScore;
    /**
     * Version name for this InfoType.
     */
    version?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigImageTransformationsTransformSelectedInfoTypesInfoTypeSensitivityScore {
    /**
     * The sensitivity score applied to the resource. Possible values: ["SENSITIVITY_LOW", "SENSITIVITY_MODERATE", "SENSITIVITY_HIGH"]
     */
    score: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformations {
    /**
     * Transformation for each infoType. Cannot specify more than one for a given infoType.
     */
    transformations: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformation[];
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformation {
    /**
     * InfoTypes to apply the transformation to. Leaving this empty will apply the transformation to apply to
     * all findings that correspond to infoTypes that were requested in InspectConfig.
     */
    infoTypes?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationInfoType[];
    /**
     * Primitive transformation to apply to the infoType.
     * The 'primitive_transformation' block must only contain one argument, corresponding to the type of transformation.
     */
    primitiveTransformation: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformation;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationInfoType {
    /**
     * Name of the information type.
     */
    name: string;
    /**
     * Optional custom sensitivity for this InfoType. This only applies to data profiling.
     */
    sensitivityScore?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationInfoTypeSensitivityScore;
    /**
     * Version name for this InfoType.
     */
    version?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationInfoTypeSensitivityScore {
    /**
     * The sensitivity score applied to the resource. Possible values: ["SENSITIVITY_LOW", "SENSITIVITY_MODERATE", "SENSITIVITY_HIGH"]
     */
    score: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformation {
    /**
     * Generalization function that buckets values based on ranges. The ranges and replacement values are dynamically provided by the user for custom behavior, such as 1-30 > LOW 31-65 > MEDIUM 66-100 > HIGH
     * This can be used on data of type: number, long, string, timestamp.
     * If the provided value type differs from the type of data being transformed, we will first attempt converting the type of the data to be transformed to match the type of the bound before comparing.
     * See https://cloud.google.com/dlp/docs/concepts-bucketing to learn more.
     */
    bucketingConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfig;
    /**
     * Partially mask a string by replacing a given number of characters with a fixed character.
     * Masking can start from the beginning or end of the string.
     */
    characterMaskConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCharacterMaskConfig;
    /**
     * Pseudonymization method that generates deterministic encryption for the given input. Outputs a base64 encoded representation of the encrypted output. Uses AES-SIV based on the RFC [https://tools.ietf.org/html/rfc5297](https://tools.ietf.org/html/rfc5297).
     */
    cryptoDeterministicConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfig;
    /**
     * Pseudonymization method that generates surrogates via cryptographic hashing. Uses SHA-256. The key size must be either 32 or 64 bytes.
     * Outputs a base64 encoded representation of the hashed output (for example, L7k0BHmF1ha5U3NfGykjro4xWi1MPVQPjhMAZbSV9mM=).
     * Currently, only string and integer values can be hashed.
     * See https://cloud.google.com/dlp/docs/pseudonymization to learn more.
     */
    cryptoHashConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfig;
    /**
     * Replaces an identifier with a surrogate using Format Preserving Encryption (FPE) with the FFX mode of operation; however when used in the 'content.reidentify' API method, it serves the opposite function by reversing the surrogate back into the original identifier. The identifier must be encoded as ASCII. For a given crypto key and context, the same identifier will be replaced with the same surrogate. Identifiers must be at least two characters long. In the case that the identifier is the empty string, it will be skipped. See [https://cloud.google.com/dlp/docs/pseudonymization](https://cloud.google.com/dlp/docs/pseudonymization) to learn more.
     *
     * Note: We recommend using CryptoDeterministicConfig for all use cases which do not require preserving the input alphabet space and size, plus warrant referential integrity.
     */
    cryptoReplaceFfxFpeConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfig;
    /**
     * Shifts dates by random number of days, with option to be consistent for the same context.
     */
    dateShiftConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfig;
    /**
     * Buckets values based on fixed size ranges. The Bucketing transformation can provide all of this functionality, but requires more configuration. This message is provided as a convenience to the user for simple bucketing strategies.
     *
     * The transformed value will be a hyphenated string of {lower_bound}-{upper_bound}. For example, if lower_bound = 10 and upper_bound = 20, all values that are within this bucket will be replaced with "10-20".
     *
     * This can be used on data of type: double, long.
     *
     * If the bound Value type differs from the type of data being transformed, we will first attempt converting the type of the data to be transformed to match the type of the bound before comparing.
     *
     * See https://cloud.google.com/dlp/docs/concepts-bucketing to learn more.
     */
    fixedSizeBucketingConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationFixedSizeBucketingConfig;
    /**
     * Redact a given value. For example, if used with an InfoTypeTransformation transforming PHONE_NUMBER, and input 'My phone number is 206-555-0123', the output would be 'My phone number is '.
     */
    redactConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationRedactConfig;
    /**
     * Replace each input value with a given value.
     */
    replaceConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfig;
    /**
     * Replace with a value randomly drawn (with replacement) from a dictionary.
     */
    replaceDictionaryConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationReplaceDictionaryConfig;
    /**
     * Replace each matching finding with the name of the info type.
     */
    replaceWithInfoTypeConfig?: boolean;
    /**
     * For use with Date, Timestamp, and TimeOfDay, extract or preserve a portion of the value.
     */
    timePartConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationTimePartConfig;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfig {
    /**
     * Set of buckets. Ranges must be non-overlapping.
     * Bucket is represented as a range, along with replacement values.
     */
    buckets?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucket[];
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucket {
    /**
     * Upper bound of the range, exclusive; type must match min.
     * The 'max' block must only contain one argument. See the 'bucketing_config' block description for more information about choosing a data type.
     */
    max?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMax;
    /**
     * Lower bound of the range, inclusive. Type should be the same as max if used.
     * The 'min' block must only contain one argument. See the 'bucketing_config' block description for more information about choosing a data type.
     */
    min?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMin;
    /**
     * Replacement value for this bucket.
     * The 'replacement_value' block must only contain one argument.
     */
    replacementValue: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketReplacementValue;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMax {
    /**
     * Represents a whole or partial calendar date.
     */
    dateValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMaxDateValue;
    /**
     * Represents a day of the week. Possible values: ["MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    dayOfWeekValue?: string;
    /**
     * A float value.
     */
    floatValue?: number;
    /**
     * An integer value (int64 format)
     */
    integerValue?: string;
    /**
     * A string value.
     */
    stringValue?: string;
    /**
     * Represents a time of day.
     */
    timeValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMaxTimeValue;
    /**
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
     */
    timestampValue?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMaxDateValue {
    /**
     * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
     */
    day?: number;
    /**
     * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
     */
    month?: number;
    /**
     * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
     */
    year?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMaxTimeValue {
    /**
     * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
     */
    hours?: number;
    /**
     * Minutes of hour of day. Must be from 0 to 59.
     */
    minutes?: number;
    /**
     * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
     */
    nanos?: number;
    /**
     * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
     */
    seconds?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMin {
    /**
     * Represents a whole or partial calendar date.
     */
    dateValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMinDateValue;
    /**
     * Represents a day of the week. Possible values: ["MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    dayOfWeekValue?: string;
    /**
     * A float value.
     */
    floatValue?: number;
    /**
     * An integer value (int64 format)
     */
    integerValue?: string;
    /**
     * A string value.
     */
    stringValue?: string;
    /**
     * Represents a time of day.
     */
    timeValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMinTimeValue;
    /**
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
     */
    timestampValue?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMinDateValue {
    /**
     * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
     */
    day?: number;
    /**
     * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
     */
    month?: number;
    /**
     * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
     */
    year?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMinTimeValue {
    /**
     * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
     */
    hours?: number;
    /**
     * Minutes of hour of day. Must be from 0 to 59.
     */
    minutes?: number;
    /**
     * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
     */
    nanos?: number;
    /**
     * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
     */
    seconds?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketReplacementValue {
    /**
     * Represents a whole or partial calendar date.
     */
    dateValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketReplacementValueDateValue;
    /**
     * Represents a day of the week. Possible values: ["MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    dayOfWeekValue?: string;
    /**
     * A float value.
     */
    floatValue?: number;
    /**
     * An integer value (int64 format)
     */
    integerValue?: string;
    /**
     * A string value.
     */
    stringValue?: string;
    /**
     * Represents a time of day.
     */
    timeValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketReplacementValueTimeValue;
    /**
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
     */
    timestampValue?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketReplacementValueDateValue {
    /**
     * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
     */
    day?: number;
    /**
     * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
     */
    month?: number;
    /**
     * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
     */
    year?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketReplacementValueTimeValue {
    /**
     * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
     */
    hours?: number;
    /**
     * Minutes of hour of day. Must be from 0 to 59.
     */
    minutes?: number;
    /**
     * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
     */
    nanos?: number;
    /**
     * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
     */
    seconds?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCharacterMaskConfig {
    /**
     * Characters to skip when doing de-identification of a value. These will be left alone and skipped.
     */
    charactersToIgnores?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCharacterMaskConfigCharactersToIgnore[];
    /**
     * Character to use to mask the sensitive values—for example, * for an alphabetic string such as a name, or 0 for a numeric string
     * such as ZIP code or credit card number. This string must have a length of 1. If not supplied, this value defaults to * for
     * strings, and 0 for digits.
     */
    maskingCharacter?: string;
    /**
     * Number of characters to mask. If not set, all matching chars will be masked. Skipped characters do not count towards this tally.
     */
    numberToMask?: number;
    /**
     * Mask characters in reverse order. For example, if masking_character is 0, number_to_mask is 14, and reverse_order is 'false', then the
     * input string '1234-5678-9012-3456' is masked as '00000000000000-3456'.
     */
    reverseOrder?: boolean;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCharacterMaskConfigCharactersToIgnore {
    /**
     * Characters to not transform when masking.
     */
    charactersToSkip?: string;
    /**
     * Common characters to not transform when masking. Useful to avoid removing punctuation. Possible values: ["NUMERIC", "ALPHA_UPPER_CASE", "ALPHA_LOWER_CASE", "PUNCTUATION", "WHITESPACE"]
     */
    commonCharactersToIgnore?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfig {
    /**
     * A context may be used for higher security and maintaining referential integrity such that the same identifier in two different contexts will be given a distinct surrogate. The context is appended to plaintext value being encrypted. On decryption the provided context is validated against the value used during encryption. If a context was provided during encryption, same context must be provided during decryption as well.
     *
     * If the context is not set, plaintext would be used as is for encryption. If the context is set but:
     *
     * 1.  there is no record present when transforming a given value or
     * 2.  the field is not present when transforming a given value,
     *
     * plaintext would be used as is for encryption.
     *
     * Note that case (1) is expected when an 'InfoTypeTransformation' is applied to both structured and non-structured 'ContentItem's.
     */
    context?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigContext;
    /**
     * The key used by the encryption function.
     */
    cryptoKey?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKey;
    /**
     * The custom info type to annotate the surrogate with. This annotation will be applied to the surrogate by prefixing it with the name of the custom info type followed by the number of characters comprising the surrogate. The following scheme defines the format: {info type name}({surrogate character count}):{surrogate}
     *
     * For example, if the name of custom info type is 'MY\_TOKEN\_INFO\_TYPE' and the surrogate is 'abc', the full replacement value will be: 'MY\_TOKEN\_INFO\_TYPE(3):abc'
     *
     * This annotation identifies the surrogate when inspecting content using the custom info type 'Surrogate'. This facilitates reversal of the surrogate when it occurs in free text.
     *
     * Note: For record transformations where the entire cell in a table is being transformed, surrogates are not mandatory. Surrogates are used to denote the location of the token and are necessary for re-identification in free form text.
     *
     * In order for inspection to work properly, the name of this info type must not occur naturally anywhere in your data; otherwise, inspection may either
     *
     * *   reverse a surrogate that does not correspond to an actual identifier
     * *   be unable to parse the surrogate and result in an error
     *
     * Therefore, choose your custom info type name carefully after considering what your data looks like. One way to select a name that has a high chance of yielding reliable detection is to include one or more unicode characters that are highly improbable to exist in your data. For example, assuming your data is entered from a regular ASCII keyboard, the symbol with the hex code point 29DD might be used like so: ⧝MY\_TOKEN\_TYPE.
     */
    surrogateInfoType?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigSurrogateInfoType;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigContext {
    /**
     * Name describing the field.
     */
    name?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKey {
    /**
     * KMS wrapped key.
     * Include to use an existing data crypto key wrapped by KMS. The wrapped key must be a 128-, 192-, or 256-bit key. Authorization requires the following IAM permissions when sending a request to perform a crypto transformation using a KMS-wrapped crypto key: dlp.kms.encrypt
     * For more information, see [Creating a wrapped key](https://cloud.google.com/dlp/docs/create-wrapped-key).
     * Note: When you use Cloud KMS for cryptographic operations, [charges apply](https://cloud.google.com/kms/pricing).
     */
    kmsWrapped?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyKmsWrapped;
    /**
     * Transient crypto key. Use this to have a random data crypto key generated. It will be discarded after the request finishes.
     */
    transient?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyTransient;
    /**
     * Unwrapped crypto key. Using raw keys is prone to security risks due to accidentally leaking the key. Choose another type of key if possible.
     */
    unwrapped?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyUnwrapped;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyKmsWrapped {
    /**
     * The resource name of the KMS CryptoKey to use for unwrapping.
     */
    cryptoKeyName: string;
    /**
     * The wrapped data crypto key.
     *
     * A base64-encoded string.
     */
    wrappedKey: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyTransient {
    /**
     * Name of the key. This is an arbitrary string used to differentiate different keys. A unique key is generated per name: two separate 'TransientCryptoKey' protos share the same generated key if their names are the same. When the data crypto key is generated, this name is not used in any way (repeating the api call will result in a different key being generated).
     */
    name: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyUnwrapped {
    /**
     * A 128/192/256 bit key.
     *
     * A base64-encoded string.
     */
    key: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigSurrogateInfoType {
    /**
     * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names listed at [https://cloud.google.com/dlp/docs/infotypes-reference](https://cloud.google.com/dlp/docs/infotypes-reference) when specifying a built-in type. When sending Cloud DLP results to Data Catalog, infoType names should conform to the pattern '[A-Za-z0-9$-_]{1,64}'.
     */
    name?: string;
    /**
     * Optional custom sensitivity for this InfoType. This only applies to data profiling.
     */
    sensitivityScore?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigSurrogateInfoTypeSensitivityScore;
    /**
     * Optional version name for this InfoType.
     */
    version?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigSurrogateInfoTypeSensitivityScore {
    /**
     * The sensitivity score applied to the resource. Possible values: ["SENSITIVITY_LOW", "SENSITIVITY_MODERATE", "SENSITIVITY_HIGH"]
     */
    score: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfig {
    /**
     * The key used by the encryption function.
     */
    cryptoKey?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKey;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKey {
    /**
     * KMS wrapped key.
     * Include to use an existing data crypto key wrapped by KMS. The wrapped key must be a 128-, 192-, or 256-bit key. Authorization requires the following IAM permissions when sending a request to perform a crypto transformation using a KMS-wrapped crypto key: dlp.kms.encrypt
     * For more information, see [Creating a wrapped key](https://cloud.google.com/dlp/docs/create-wrapped-key).
     * Note: When you use Cloud KMS for cryptographic operations, [charges apply](https://cloud.google.com/kms/pricing).
     */
    kmsWrapped?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyKmsWrapped;
    /**
     * Transient crypto key. Use this to have a random data crypto key generated. It will be discarded after the request finishes.
     */
    transient?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyTransient;
    /**
     * Unwrapped crypto key. Using raw keys is prone to security risks due to accidentally leaking the key. Choose another type of key if possible.
     */
    unwrapped?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyUnwrapped;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyKmsWrapped {
    /**
     * The resource name of the KMS CryptoKey to use for unwrapping.
     */
    cryptoKeyName: string;
    /**
     * The wrapped data crypto key.
     *
     * A base64-encoded string.
     */
    wrappedKey: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyTransient {
    /**
     * Name of the key. This is an arbitrary string used to differentiate different keys. A unique key is generated per name: two separate 'TransientCryptoKey' protos share the same generated key if their names are the same. When the data crypto key is generated, this name is not used in any way (repeating the api call will result in a different key being generated).
     */
    name: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyUnwrapped {
    /**
     * A 128/192/256 bit key.
     *
     * A base64-encoded string.
     */
    key: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfig {
    /**
     * Common alphabets. Possible values: ["FFX_COMMON_NATIVE_ALPHABET_UNSPECIFIED", "NUMERIC", "HEXADECIMAL", "UPPER_CASE_ALPHA_NUMERIC", "ALPHA_NUMERIC"]
     */
    commonAlphabet?: string;
    /**
     * The 'tweak', a context may be used for higher security since the same identifier in two different contexts won't be given the same surrogate. If the context is not set, a default tweak will be used.
     *
     * If the context is set but:
     *
     * 1.  there is no record present when transforming a given value or
     * 2.  the field is not present when transforming a given value,
     *
     * a default tweak will be used.
     *
     * Note that case (1) is expected when an 'InfoTypeTransformation' is applied to both structured and non-structured 'ContentItem's. Currently, the referenced field may be of value type integer or string.
     *
     * The tweak is constructed as a sequence of bytes in big endian byte order such that:
     *
     * *   a 64 bit integer is encoded followed by a single byte of value 1
     * *   a string is encoded in UTF-8 format followed by a single byte of value 2
     */
    context?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigContext;
    /**
     * The key used by the encryption algorithm.
     */
    cryptoKey?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKey;
    /**
     * This is supported by mapping these to the alphanumeric characters that the FFX mode natively supports. This happens before/after encryption/decryption. Each character listed must appear only once. Number of characters must be in the range \[2, 95\]. This must be encoded as ASCII. The order of characters does not matter. The full list of allowed characters is:
     *
     * ''0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz ~'!@#$%^&*()_-+={[}]|:;"'<,>.?/''
     */
    customAlphabet?: string;
    /**
     * The native way to select the alphabet. Must be in the range \[2, 95\].
     */
    radix?: number;
    /**
     * The custom infoType to annotate the surrogate with. This annotation will be applied to the surrogate by prefixing it with the name of the custom infoType followed by the number of characters comprising the surrogate. The following scheme defines the format: info\_type\_name(surrogate\_character\_count):surrogate
     *
     * For example, if the name of custom infoType is 'MY\_TOKEN\_INFO\_TYPE' and the surrogate is 'abc', the full replacement value will be: 'MY\_TOKEN\_INFO\_TYPE(3):abc'
     *
     * This annotation identifies the surrogate when inspecting content using the custom infoType ['SurrogateType'](https://cloud.google.com/dlp/docs/reference/rest/v2/InspectConfig#surrogatetype). This facilitates reversal of the surrogate when it occurs in free text.
     *
     * In order for inspection to work properly, the name of this infoType must not occur naturally anywhere in your data; otherwise, inspection may find a surrogate that does not correspond to an actual identifier. Therefore, choose your custom infoType name carefully after considering what your data looks like. One way to select a name that has a high chance of yielding reliable detection is to include one or more unicode characters that are highly improbable to exist in your data. For example, assuming your data is entered from a regular ASCII keyboard, the symbol with the hex code point 29DD might be used like so: ⧝MY\_TOKEN\_TYPE
     */
    surrogateInfoType?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigSurrogateInfoType;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigContext {
    /**
     * Name describing the field.
     */
    name?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKey {
    /**
     * KMS wrapped key.
     * Include to use an existing data crypto key wrapped by KMS. The wrapped key must be a 128-, 192-, or 256-bit key. Authorization requires the following IAM permissions when sending a request to perform a crypto transformation using a KMS-wrapped crypto key: dlp.kms.encrypt
     * For more information, see [Creating a wrapped key](https://cloud.google.com/dlp/docs/create-wrapped-key).
     * Note: When you use Cloud KMS for cryptographic operations, [charges apply](https://cloud.google.com/kms/pricing).
     */
    kmsWrapped?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyKmsWrapped;
    /**
     * Transient crypto key. Use this to have a random data crypto key generated. It will be discarded after the request finishes.
     */
    transient?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyTransient;
    /**
     * Unwrapped crypto key. Using raw keys is prone to security risks due to accidentally leaking the key. Choose another type of key if possible.
     */
    unwrapped?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyUnwrapped;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyKmsWrapped {
    /**
     * The resource name of the KMS CryptoKey to use for unwrapping.
     */
    cryptoKeyName: string;
    /**
     * The wrapped data crypto key.
     *
     * A base64-encoded string.
     */
    wrappedKey: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyTransient {
    /**
     * Name of the key. This is an arbitrary string used to differentiate different keys. A unique key is generated per name: two separate 'TransientCryptoKey' protos share the same generated key if their names are the same. When the data crypto key is generated, this name is not used in any way (repeating the api call will result in a different key being generated).
     */
    name: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyUnwrapped {
    /**
     * A 128/192/256 bit key.
     *
     * A base64-encoded string.
     */
    key: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigSurrogateInfoType {
    /**
     * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names listed at [https://cloud.google.com/dlp/docs/infotypes-reference](https://cloud.google.com/dlp/docs/infotypes-reference) when specifying a built-in type. When sending Cloud DLP results to Data Catalog, infoType names should conform to the pattern '[A-Za-z0-9$-_]{1,64}'.
     */
    name?: string;
    /**
     * Optional custom sensitivity for this InfoType. This only applies to data profiling.
     */
    sensitivityScore?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigSurrogateInfoTypeSensitivityScore;
    /**
     * Optional version name for this InfoType.
     */
    version?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigSurrogateInfoTypeSensitivityScore {
    /**
     * The sensitivity score applied to the resource. Possible values: ["SENSITIVITY_LOW", "SENSITIVITY_MODERATE", "SENSITIVITY_HIGH"]
     */
    score: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfig {
    /**
     * Points to the field that contains the context, for example, an entity id.
     * If set, must also set cryptoKey. If set, shift will be consistent for the given context.
     */
    context?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigContext;
    /**
     * The key used by the encryption function.
     */
    cryptoKey?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKey;
    /**
     * Range of shift in days. Negative means shift to earlier in time.
     */
    lowerBoundDays: number;
    /**
     * Range of shift in days. Actual shift will be selected at random within this range (inclusive ends).
     * Negative means shift to earlier in time. Must not be more than 365250 days (1000 years) each direction.
     */
    upperBoundDays: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigContext {
    /**
     * Name describing the field.
     */
    name: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKey {
    /**
     * KMS wrapped key.
     * Include to use an existing data crypto key wrapped by KMS. The wrapped key must be a 128-, 192-, or 256-bit key. Authorization requires the following IAM permissions when sending a request to perform a crypto transformation using a KMS-wrapped crypto key: dlp.kms.encrypt
     * For more information, see [Creating a wrapped key](https://cloud.google.com/dlp/docs/create-wrapped-key).
     * Note: When you use Cloud KMS for cryptographic operations, [charges apply](https://cloud.google.com/kms/pricing).
     */
    kmsWrapped?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKeyKmsWrapped;
    /**
     * Transient crypto key. Use this to have a random data crypto key generated. It will be discarded after the request finishes.
     */
    transient?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKeyTransient;
    /**
     * Unwrapped crypto key. Using raw keys is prone to security risks due to accidentally leaking the key. Choose another type of key if possible.
     */
    unwrapped?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKeyUnwrapped;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKeyKmsWrapped {
    /**
     * The resource name of the KMS CryptoKey to use for unwrapping.
     */
    cryptoKeyName: string;
    /**
     * The wrapped data crypto key.
     * A base64-encoded string.
     */
    wrappedKey: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKeyTransient {
    /**
     * Name of the key. This is an arbitrary string used to differentiate different keys. A unique key is generated per name: two separate 'TransientCryptoKey' protos share the same generated key if their names are the same. When the data crypto key is generated, this name is not used in any way (repeating the api call will result in a different key being generated).
     */
    name: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKeyUnwrapped {
    /**
     * A 128/192/256 bit key.
     * A base64-encoded string.
     */
    key: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationFixedSizeBucketingConfig {
    /**
     * Size of each bucket (except for minimum and maximum buckets).
     * So if lower_bound = 10, upper_bound = 89, and bucketSize = 10, then the following buckets would be used: -10, 10-20, 20-30, 30-40, 40-50, 50-60, 60-70, 70-80, 80-89, 89+.
     * Precision up to 2 decimals works.
     */
    bucketSize: number;
    /**
     * Lower bound value of buckets.
     * All values less than lower_bound are grouped together into a single bucket; for example if lower_bound = 10, then all values less than 10 are replaced with the value "-10".
     * The 'lower_bound' block must only contain one argument. See the 'fixed_size_bucketing_config' block description for more information about choosing a data type.
     */
    lowerBound: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationFixedSizeBucketingConfigLowerBound;
    /**
     * Upper bound value of buckets.
     * All values greater than upper_bound are grouped together into a single bucket; for example if upper_bound = 89, then all values greater than 89 are replaced with the value "89+".
     * The 'upper_bound' block must only contain one argument. See the 'fixed_size_bucketing_config' block description for more information about choosing a data type.
     */
    upperBound: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationFixedSizeBucketingConfigUpperBound;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationFixedSizeBucketingConfigLowerBound {
    /**
     * A float value.
     */
    floatValue?: number;
    /**
     * An integer value (int64 format)
     */
    integerValue?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationFixedSizeBucketingConfigUpperBound {
    /**
     * A float value.
     */
    floatValue?: number;
    /**
     * An integer value (int64 format)
     */
    integerValue?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationRedactConfig {
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfig {
    /**
     * Replace each input value with a given value.
     * The 'new_value' block must only contain one argument. For example when replacing the contents of a string-type field, only 'string_value' should be set.
     */
    newValue: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfigNewValue;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfigNewValue {
    /**
     * A boolean value.
     */
    booleanValue?: boolean;
    /**
     * Represents a whole or partial calendar date.
     */
    dateValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfigNewValueDateValue;
    /**
     * Represents a day of the week. Possible values: ["MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    dayOfWeekValue?: string;
    /**
     * A float value.
     */
    floatValue?: number;
    /**
     * An integer value.
     */
    integerValue?: number;
    /**
     * A string value.
     */
    stringValue?: string;
    /**
     * Represents a time of day.
     */
    timeValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfigNewValueTimeValue;
    /**
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits.
     * Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
     */
    timestampValue?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfigNewValueDateValue {
    /**
     * Day of month. Must be from 1 to 31 and valid for the year and month, or 0 if specifying a
     * year by itself or a year and month where the day is not significant.
     */
    day?: number;
    /**
     * Month of year. Must be from 1 to 12, or 0 if specifying a year without a month and day.
     */
    month?: number;
    /**
     * Year of date. Must be from 1 to 9999, or 0 if specifying a date without a year.
     */
    year?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfigNewValueTimeValue {
    /**
     * Hours of day in 24 hour format. Should be from 0 to 23.
     */
    hours?: number;
    /**
     * Minutes of hour of day. Must be from 0 to 59.
     */
    minutes?: number;
    /**
     * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
     */
    nanos?: number;
    /**
     * Seconds of minutes of the time. Must normally be from 0 to 59.
     */
    seconds?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationReplaceDictionaryConfig {
    /**
     * A list of words to select from for random replacement. The [limits](https://cloud.google.com/dlp/limits) page contains details about the size limits of dictionaries.
     */
    wordList: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationReplaceDictionaryConfigWordList;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationReplaceDictionaryConfigWordList {
    /**
     * Words or phrases defining the dictionary. The dictionary must contain at least one phrase and every phrase must contain at least 2 characters that are letters or digits.
     */
    words: string[];
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigInfoTypeTransformationsTransformationPrimitiveTransformationTimePartConfig {
    /**
     * The part of the time to keep. Possible values: ["YEAR", "MONTH", "DAY_OF_MONTH", "DAY_OF_WEEK", "WEEK_OF_YEAR", "HOUR_OF_DAY"]
     */
    partToExtract?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformations {
    /**
     * Transform the record by applying various field transformations.
     */
    fieldTransformations?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformation[];
    /**
     * Configuration defining which records get suppressed entirely. Records that match any suppression rule are omitted from the output.
     */
    recordSuppressions?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppression[];
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformation {
    /**
     * Only apply the transformation if the condition evaluates to true for the given RecordCondition. The conditions are allowed to reference fields that are not used in the actual transformation.
     * Example Use Cases:
     * - Apply a different bucket transformation to an age column if the zip code column for the same record is within a specific range.
     * - Redact a field if the date of birth field is greater than 85.
     */
    condition?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationCondition;
    /**
     * Input field(s) to apply the transformation to. When you have columns that reference their position within a list, omit the index from the FieldId.
     * FieldId name matching ignores the index. For example, instead of "contact.nums[0].type", use "contact.nums.type".
     */
    fields: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationField[];
    /**
     * Treat the contents of the field as free text, and selectively transform content that matches an InfoType.
     * Only one of 'primitive_transformation' or 'info_type_transformations' must be specified.
     */
    infoTypeTransformations?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformations;
    /**
     * Apply the transformation to the entire field.
     * The 'primitive_transformation' block must only contain one argument, corresponding to the type of transformation.
     * Only one of 'primitive_transformation' or 'info_type_transformations' must be specified.
     */
    primitiveTransformation?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformation;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationCondition {
    /**
     * An expression.
     */
    expressions?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressions;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressions {
    /**
     * Conditions to apply to the expression.
     */
    conditions?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressionsConditions;
    /**
     * The operator to apply to the result of conditions. Default and currently only supported value is AND Default value: "AND" Possible values: ["AND"]
     */
    logicalOperator?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressionsConditions {
    /**
     * A collection of conditions.
     */
    conditions?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressionsConditionsCondition[];
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressionsConditionsCondition {
    /**
     * Field within the record this condition is evaluated against.
     */
    field: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressionsConditionsConditionField;
    /**
     * Operator used to compare the field or infoType to the value. Possible values: ["EQUAL_TO", "NOT_EQUAL_TO", "GREATER_THAN", "LESS_THAN", "GREATER_THAN_OR_EQUALS", "LESS_THAN_OR_EQUALS", "EXISTS"]
     */
    operator: string;
    /**
     * Value to compare against.
     * The 'value' block must only contain one argument. For example when a condition is evaluated against a string-type field, only 'string_value' should be set.
     * This argument is mandatory, except for conditions using the 'EXISTS' operator.
     */
    value?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressionsConditionsConditionValue;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressionsConditionsConditionField {
    /**
     * Name describing the field.
     */
    name?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressionsConditionsConditionValue {
    /**
     * A boolean value.
     */
    booleanValue?: boolean;
    /**
     * Represents a whole or partial calendar date.
     */
    dateValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressionsConditionsConditionValueDateValue;
    /**
     * Represents a day of the week. Possible values: ["MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    dayOfWeekValue?: string;
    /**
     * A float value.
     */
    floatValue?: number;
    /**
     * An integer value (int64 format)
     */
    integerValue?: string;
    /**
     * A string value.
     */
    stringValue?: string;
    /**
     * Represents a time of day.
     */
    timeValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressionsConditionsConditionValueTimeValue;
    /**
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
     */
    timestampValue?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressionsConditionsConditionValueDateValue {
    /**
     * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
     */
    day?: number;
    /**
     * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
     */
    month?: number;
    /**
     * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
     */
    year?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationConditionExpressionsConditionsConditionValueTimeValue {
    /**
     * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
     */
    hours?: number;
    /**
     * Minutes of hour of day. Must be from 0 to 59.
     */
    minutes?: number;
    /**
     * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
     */
    nanos?: number;
    /**
     * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
     */
    seconds?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationField {
    /**
     * Name describing the field.
     */
    name?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformations {
    /**
     * Transformation for each infoType. Cannot specify more than one for a given infoType.
     */
    transformations: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformation[];
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformation {
    /**
     * InfoTypes to apply the transformation to. Leaving this empty will apply the transformation to apply to
     * all findings that correspond to infoTypes that were requested in InspectConfig.
     */
    infoTypes?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationInfoType[];
    /**
     * Apply the transformation to the entire field.
     * The 'primitive_transformation' block must only contain one argument, corresponding to the type of transformation.
     */
    primitiveTransformation: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformation;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationInfoType {
    /**
     * Name of the information type.
     */
    name: string;
    /**
     * Optional custom sensitivity for this InfoType. This only applies to data profiling.
     */
    sensitivityScore?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationInfoTypeSensitivityScore;
    /**
     * Version name for this InfoType.
     */
    version?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationInfoTypeSensitivityScore {
    /**
     * The sensitivity score applied to the resource. Possible values: ["SENSITIVITY_LOW", "SENSITIVITY_MODERATE", "SENSITIVITY_HIGH"]
     */
    score: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformation {
    /**
     * Generalization function that buckets values based on ranges. The ranges and replacement values are dynamically provided by the user for custom behavior, such as 1-30 > LOW 31-65 > MEDIUM 66-100 > HIGH
     * This can be used on data of type: number, long, string, timestamp.
     * If the provided value type differs from the type of data being transformed, we will first attempt converting the type of the data to be transformed to match the type of the bound before comparing.
     * See https://cloud.google.com/dlp/docs/concepts-bucketing to learn more.
     */
    bucketingConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfig;
    /**
     * Partially mask a string by replacing a given number of characters with a fixed character. Masking can start from the beginning or end of the string. This can be used on data of any type (numbers, longs, and so on) and when de-identifying structured data we'll attempt to preserve the original data's type. (This allows you to take a long like 123 and modify it to a string like **3).
     */
    characterMaskConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCharacterMaskConfig;
    /**
     * Pseudonymization method that generates deterministic encryption for the given input. Outputs a base64 encoded representation of the encrypted output. Uses AES-SIV based on the RFC [https://tools.ietf.org/html/rfc5297](https://tools.ietf.org/html/rfc5297).
     */
    cryptoDeterministicConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfig;
    /**
     * Pseudonymization method that generates surrogates via cryptographic hashing. Uses SHA-256. The key size must be either 32 or 64 bytes.
     * Outputs a base64 encoded representation of the hashed output (for example, L7k0BHmF1ha5U3NfGykjro4xWi1MPVQPjhMAZbSV9mM=).
     * Currently, only string and integer values can be hashed.
     * See https://cloud.google.com/dlp/docs/pseudonymization to learn more.
     */
    cryptoHashConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfig;
    /**
     * Replaces an identifier with a surrogate using Format Preserving Encryption (FPE) with the FFX mode of operation; however when used in the 'content.reidentify' API method, it serves the opposite function by reversing the surrogate back into the original identifier. The identifier must be encoded as ASCII. For a given crypto key and context, the same identifier will be replaced with the same surrogate. Identifiers must be at least two characters long. In the case that the identifier is the empty string, it will be skipped. See [https://cloud.google.com/dlp/docs/pseudonymization](https://cloud.google.com/dlp/docs/pseudonymization) to learn more.
     *
     * Note: We recommend using CryptoDeterministicConfig for all use cases which do not require preserving the input alphabet space and size, plus warrant referential integrity.
     */
    cryptoReplaceFfxFpeConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfig;
    /**
     * Shifts dates by random number of days, with option to be consistent for the same context. See https://cloud.google.com/dlp/docs/concepts-date-shifting to learn more.
     */
    dateShiftConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfig;
    /**
     * Buckets values based on fixed size ranges. The Bucketing transformation can provide all of this functionality, but requires more configuration. This message is provided as a convenience to the user for simple bucketing strategies.
     *
     * The transformed value will be a hyphenated string of {lower_bound}-{upper_bound}. For example, if lower_bound = 10 and upper_bound = 20, all values that are within this bucket will be replaced with "10-20".
     *
     * This can be used on data of type: double, long.
     *
     * If the bound Value type differs from the type of data being transformed, we will first attempt converting the type of the data to be transformed to match the type of the bound before comparing.
     *
     * See https://cloud.google.com/dlp/docs/concepts-bucketing to learn more.
     */
    fixedSizeBucketingConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationFixedSizeBucketingConfig;
    /**
     * Redact a given value. For example, if used with an InfoTypeTransformation transforming PHONE_NUMBER, and input 'My phone number is 206-555-0123', the output would be 'My phone number is '.
     */
    redactConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationRedactConfig;
    /**
     * Replace each input value with a given value.
     */
    replaceConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfig;
    /**
     * Replace with a value randomly drawn (with replacement) from a dictionary.
     */
    replaceDictionaryConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceDictionaryConfig;
    /**
     * Replace each matching finding with the name of the info type.
     */
    replaceWithInfoTypeConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceWithInfoTypeConfig;
    /**
     * For use with Date, Timestamp, and TimeOfDay, extract or preserve a portion of the value.
     */
    timePartConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationTimePartConfig;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfig {
    /**
     * Set of buckets. Ranges must be non-overlapping.
     * Bucket is represented as a range, along with replacement values.
     */
    buckets: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucket[];
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucket {
    /**
     * Upper bound of the range, exclusive; type must match min.
     * The 'max' block must only contain one argument. See the 'bucketing_config' block description for more information about choosing a data type.
     */
    max?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMax;
    /**
     * Lower bound of the range, inclusive. Type should be the same as max if used.
     * The 'min' block must only contain one argument. See the 'bucketing_config' block description for more information about choosing a data type.
     */
    min?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMin;
    /**
     * Replacement value for this bucket.
     * The 'replacement_value' block must only contain one argument.
     */
    replacementValue: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketReplacementValue;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMax {
    /**
     * Represents a whole or partial calendar date.
     */
    dateValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMaxDateValue;
    /**
     * Represents a day of the week. Possible values: ["MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    dayOfWeekValue?: string;
    /**
     * A float value.
     */
    floatValue?: number;
    /**
     * An integer value (int64 format)
     */
    integerValue?: string;
    /**
     * A string value.
     */
    stringValue?: string;
    /**
     * Represents a time of day.
     */
    timeValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMaxTimeValue;
    /**
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
     */
    timestampValue?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMaxDateValue {
    /**
     * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
     */
    day?: number;
    /**
     * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
     */
    month?: number;
    /**
     * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
     */
    year?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMaxTimeValue {
    /**
     * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
     */
    hours?: number;
    /**
     * Minutes of hour of day. Must be from 0 to 59.
     */
    minutes?: number;
    /**
     * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
     */
    nanos?: number;
    /**
     * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
     */
    seconds?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMin {
    /**
     * Represents a whole or partial calendar date.
     */
    dateValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMinDateValue;
    /**
     * Represents a day of the week. Possible values: ["MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    dayOfWeekValue?: string;
    /**
     * A float value.
     */
    floatValue?: number;
    /**
     * An integer value (int64 format)
     */
    integerValue?: string;
    /**
     * A string value.
     */
    stringValue?: string;
    /**
     * Represents a time of day.
     */
    timeValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMinTimeValue;
    /**
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
     */
    timestampValue?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMinDateValue {
    /**
     * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
     */
    day?: number;
    /**
     * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
     */
    month?: number;
    /**
     * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
     */
    year?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketMinTimeValue {
    /**
     * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
     */
    hours?: number;
    /**
     * Minutes of hour of day. Must be from 0 to 59.
     */
    minutes?: number;
    /**
     * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
     */
    nanos?: number;
    /**
     * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
     */
    seconds?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketReplacementValue {
    /**
     * Represents a whole or partial calendar date.
     */
    dateValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketReplacementValueDateValue;
    /**
     * Represents a day of the week. Possible values: ["MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    dayOfWeekValue?: string;
    /**
     * A float value.
     */
    floatValue?: number;
    /**
     * An integer value (int64 format)
     */
    integerValue?: string;
    /**
     * A string value.
     */
    stringValue?: string;
    /**
     * Represents a time of day.
     */
    timeValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketReplacementValueTimeValue;
    /**
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
     */
    timestampValue?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketReplacementValueDateValue {
    /**
     * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
     */
    day?: number;
    /**
     * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
     */
    month?: number;
    /**
     * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
     */
    year?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationBucketingConfigBucketReplacementValueTimeValue {
    /**
     * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
     */
    hours?: number;
    /**
     * Minutes of hour of day. Must be from 0 to 59.
     */
    minutes?: number;
    /**
     * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
     */
    nanos?: number;
    /**
     * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
     */
    seconds?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCharacterMaskConfig {
    /**
     * Characters to skip when doing de-identification of a value. These will be left alone and skipped.
     */
    charactersToIgnores?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCharacterMaskConfigCharactersToIgnore[];
    /**
     * Character to use to mask the sensitive values—for example, * for an alphabetic string such as a name, or 0 for a numeric string
     * such as ZIP code or credit card number. This string must have a length of 1. If not supplied, this value defaults to * for
     * strings, and 0 for digits.
     */
    maskingCharacter?: string;
    /**
     * Number of characters to mask. If not set, all matching chars will be masked. Skipped characters do not count towards this tally.
     * If number_to_mask is negative, this denotes inverse masking. Cloud DLP masks all but a number of characters. For example, suppose you have the following values:
     * - 'masking_character' is *
     * - 'number_to_mask' is -4
     * - 'reverse_order' is false
     * - 'characters_to_ignore' includes -
     * - Input string is 1234-5678-9012-3456
     *
     * The resulting de-identified string is ****-****-****-3456. Cloud DLP masks all but the last four characters. If reverseOrder is true, all but the first four characters are masked as 1234-****-****-****.
     */
    numberToMask?: number;
    /**
     * Mask characters in reverse order. For example, if masking_character is 0, number_to_mask is 14, and reverse_order is 'false', then the
     * input string '1234-5678-9012-3456' is masked as '00000000000000-3456'.
     */
    reverseOrder?: boolean;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCharacterMaskConfigCharactersToIgnore {
    /**
     * Characters to not transform when masking. Only one of this or 'common_characters_to_ignore' must be specified.
     */
    charactersToSkip?: string;
    /**
     * Common characters to not transform when masking. Useful to avoid removing punctuation. Only one of this or 'characters_to_skip' must be specified. Possible values: ["NUMERIC", "ALPHA_UPPER_CASE", "ALPHA_LOWER_CASE", "PUNCTUATION", "WHITESPACE"]
     */
    commonCharactersToIgnore?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfig {
    /**
     * A context may be used for higher security and maintaining referential integrity such that the same identifier in two different contexts will be given a distinct surrogate. The context is appended to plaintext value being encrypted. On decryption the provided context is validated against the value used during encryption. If a context was provided during encryption, same context must be provided during decryption as well.
     *
     * If the context is not set, plaintext would be used as is for encryption. If the context is set but:
     *
     * 1. there is no record present when transforming a given value or
     * 2. the field is not present when transforming a given value,
     *
     * plaintext would be used as is for encryption.
     *
     * Note that case (1) is expected when an InfoTypeTransformation is applied to both structured and unstructured ContentItems.
     */
    context?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigContext;
    /**
     * The key used by the encryption function. For deterministic encryption using AES-SIV, the provided key is internally expanded to 64 bytes prior to use.
     */
    cryptoKey: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKey;
    /**
     * The custom info type to annotate the surrogate with. This annotation will be applied to the surrogate by prefixing it with the name of the custom info type followed by the number of characters comprising the surrogate. The following scheme defines the format: {info type name}({surrogate character count}):{surrogate}
     *
     * For example, if the name of custom info type is 'MY\_TOKEN\_INFO\_TYPE' and the surrogate is 'abc', the full replacement value will be: 'MY\_TOKEN\_INFO\_TYPE(3):abc'
     *
     * This annotation identifies the surrogate when inspecting content using the custom info type 'Surrogate'. This facilitates reversal of the surrogate when it occurs in free text.
     *
     * Note: For record transformations where the entire cell in a table is being transformed, surrogates are not mandatory. Surrogates are used to denote the location of the token and are necessary for re-identification in free form text.
     *
     * In order for inspection to work properly, the name of this info type must not occur naturally anywhere in your data; otherwise, inspection may either
     *
     * *   reverse a surrogate that does not correspond to an actual identifier
     * *   be unable to parse the surrogate and result in an error
     *
     * Therefore, choose your custom info type name carefully after considering what your data looks like. One way to select a name that has a high chance of yielding reliable detection is to include one or more unicode characters that are highly improbable to exist in your data. For example, assuming your data is entered from a regular ASCII keyboard, the symbol with the hex code point 29DD might be used like so: ⧝MY\_TOKEN\_TYPE.
     */
    surrogateInfoType: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigSurrogateInfoType;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigContext {
    /**
     * Name describing the field.
     */
    name: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKey {
    /**
     * KMS wrapped key.
     * Include to use an existing data crypto key wrapped by KMS. The wrapped key must be a 128-, 192-, or 256-bit key. Authorization requires the following IAM permissions when sending a request to perform a crypto transformation using a KMS-wrapped crypto key: dlp.kms.encrypt
     * For more information, see [Creating a wrapped key](https://cloud.google.com/dlp/docs/create-wrapped-key). Only one of this, 'transient' or 'unwrapped' must be specified.
     * Note: When you use Cloud KMS for cryptographic operations, [charges apply](https://cloud.google.com/kms/pricing).
     */
    kmsWrapped?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyKmsWrapped;
    /**
     * Transient crypto key. Use this to have a random data crypto key generated. It will be discarded after the request finishes. Only one of this, 'unwrapped' or 'kms_wrapped' must be specified.
     */
    transient?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyTransient;
    /**
     * Unwrapped crypto key. Using raw keys is prone to security risks due to accidentally leaking the key. Choose another type of key if possible. Only one of this, 'transient' or 'kms_wrapped' must be specified.
     */
    unwrapped?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyUnwrapped;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyKmsWrapped {
    /**
     * The resource name of the KMS CryptoKey to use for unwrapping.
     */
    cryptoKeyName: string;
    /**
     * The wrapped data crypto key.
     *
     * A base64-encoded string.
     */
    wrappedKey: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyTransient {
    /**
     * Name of the key. This is an arbitrary string used to differentiate different keys. A unique key is generated per name: two separate 'TransientCryptoKey' protos share the same generated key if their names are the same. When the data crypto key is generated, this name is not used in any way (repeating the api call will result in a different key being generated).
     */
    name: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyUnwrapped {
    /**
     * A 128/192/256 bit key.
     *
     * A base64-encoded string.
     */
    key: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigSurrogateInfoType {
    /**
     * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names listed at [https://cloud.google.com/dlp/docs/infotypes-reference](https://cloud.google.com/dlp/docs/infotypes-reference) when specifying a built-in type. When sending Cloud DLP results to Data Catalog, infoType names should conform to the pattern '[A-Za-z0-9$-_]{1,64}'.
     */
    name: string;
    /**
     * Optional custom sensitivity for this InfoType. This only applies to data profiling.
     */
    sensitivityScore?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigSurrogateInfoTypeSensitivityScore;
    /**
     * Optional version name for this InfoType.
     */
    version?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoDeterministicConfigSurrogateInfoTypeSensitivityScore {
    /**
     * The sensitivity score applied to the resource. Possible values: ["SENSITIVITY_LOW", "SENSITIVITY_MODERATE", "SENSITIVITY_HIGH"]
     */
    score: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfig {
    /**
     * The key used by the encryption function.
     */
    cryptoKey: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKey;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKey {
    /**
     * KMS wrapped key.
     * Include to use an existing data crypto key wrapped by KMS. The wrapped key must be a 128-, 192-, or 256-bit key. Authorization requires the following IAM permissions when sending a request to perform a crypto transformation using a KMS-wrapped crypto key: dlp.kms.encrypt
     * For more information, see [Creating a wrapped key](https://cloud.google.com/dlp/docs/create-wrapped-key). Only one of this, 'transient' or 'unwrapped' must be specified.
     * Note: When you use Cloud KMS for cryptographic operations, [charges apply](https://cloud.google.com/kms/pricing).
     */
    kmsWrapped?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyKmsWrapped;
    /**
     * Transient crypto key. Use this to have a random data crypto key generated. It will be discarded after the request finishes. Only one of this, 'unwrapped' or 'kms_wrapped' must be specified.
     */
    transient?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyTransient;
    /**
     * Unwrapped crypto key. Using raw keys is prone to security risks due to accidentally leaking the key. Choose another type of key if possible. Only one of this, 'transient' or 'kms_wrapped' must be specified.
     */
    unwrapped?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyUnwrapped;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyKmsWrapped {
    /**
     * The resource name of the KMS CryptoKey to use for unwrapping.
     */
    cryptoKeyName: string;
    /**
     * The wrapped data crypto key.
     *
     * A base64-encoded string.
     */
    wrappedKey: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyTransient {
    /**
     * Name of the key. This is an arbitrary string used to differentiate different keys. A unique key is generated per name: two separate 'TransientCryptoKey' protos share the same generated key if their names are the same. When the data crypto key is generated, this name is not used in any way (repeating the api call will result in a different key being generated).
     */
    name: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyUnwrapped {
    /**
     * A 128/192/256 bit key.
     *
     * A base64-encoded string.
     */
    key: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfig {
    /**
     * Common alphabets. Only one of this, 'custom_alphabet' or 'radix' must be specified. Possible values: ["NUMERIC", "HEXADECIMAL", "UPPER_CASE_ALPHA_NUMERIC", "ALPHA_NUMERIC"]
     */
    commonAlphabet?: string;
    /**
     * The 'tweak', a context may be used for higher security since the same identifier in two different contexts won't be given the same surrogate. If the context is not set, a default tweak will be used.
     *
     * If the context is set but:
     *
     * 1.  there is no record present when transforming a given value or
     * 2.  the field is not present when transforming a given value,
     *
     * a default tweak will be used.
     *
     * Note that case (1) is expected when an 'InfoTypeTransformation' is applied to both structured and non-structured 'ContentItem's. Currently, the referenced field may be of value type integer or string.
     *
     * The tweak is constructed as a sequence of bytes in big endian byte order such that:
     *
     * *   a 64 bit integer is encoded followed by a single byte of value 1
     * *   a string is encoded in UTF-8 format followed by a single byte of value 2
     */
    context?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigContext;
    /**
     * The key used by the encryption algorithm.
     */
    cryptoKey: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKey;
    /**
     * This is supported by mapping these to the alphanumeric characters that the FFX mode natively supports. This happens before/after encryption/decryption. Each character listed must appear only once. Number of characters must be in the range \[2, 95\]. This must be encoded as ASCII. The order of characters does not matter. The full list of allowed characters is:
     *
     * ''0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz ~'!@#$%^&*()_-+={[}]|:;"'<,>.?/''. Only one of this, 'common_alphabet' or 'radix' must be specified.
     */
    customAlphabet?: string;
    /**
     * The native way to select the alphabet. Must be in the range \[2, 95\]. Only one of this, 'custom_alphabet' or 'common_alphabet' must be specified.
     */
    radix?: number;
    /**
     * The custom infoType to annotate the surrogate with. This annotation will be applied to the surrogate by prefixing it with the name of the custom infoType followed by the number of characters comprising the surrogate. The following scheme defines the format: info\_type\_name(surrogate\_character\_count):surrogate
     *
     * For example, if the name of custom infoType is 'MY\_TOKEN\_INFO\_TYPE' and the surrogate is 'abc', the full replacement value will be: 'MY\_TOKEN\_INFO\_TYPE(3):abc'
     *
     * This annotation identifies the surrogate when inspecting content using the custom infoType ['SurrogateType'](https://cloud.google.com/dlp/docs/reference/rest/v2/InspectConfig#surrogatetype). This facilitates reversal of the surrogate when it occurs in free text.
     *
     * In order for inspection to work properly, the name of this infoType must not occur naturally anywhere in your data; otherwise, inspection may find a surrogate that does not correspond to an actual identifier. Therefore, choose your custom infoType name carefully after considering what your data looks like. One way to select a name that has a high chance of yielding reliable detection is to include one or more unicode characters that are highly improbable to exist in your data. For example, assuming your data is entered from a regular ASCII keyboard, the symbol with the hex code point 29DD might be used like so: ⧝MY\_TOKEN\_TYPE
     */
    surrogateInfoType?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigSurrogateInfoType;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigContext {
    /**
     * Name describing the field.
     */
    name: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKey {
    /**
     * KMS wrapped key.
     * Include to use an existing data crypto key wrapped by KMS. The wrapped key must be a 128-, 192-, or 256-bit key. Authorization requires the following IAM permissions when sending a request to perform a crypto transformation using a KMS-wrapped crypto key: dlp.kms.encrypt
     * For more information, see [Creating a wrapped key](https://cloud.google.com/dlp/docs/create-wrapped-key). Only one of this, 'transient' or 'unwrapped' must be specified.
     * Note: When you use Cloud KMS for cryptographic operations, [charges apply](https://cloud.google.com/kms/pricing).
     */
    kmsWrapped?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyKmsWrapped;
    /**
     * Transient crypto key. Use this to have a random data crypto key generated. It will be discarded after the request finishes. Only one of this, 'unwrapped' or 'kms_wrapped' must be specified.
     */
    transient?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyTransient;
    /**
     * Unwrapped crypto key. Using raw keys is prone to security risks due to accidentally leaking the key. Choose another type of key if possible. Only one of this, 'transient' or 'kms_wrapped' must be specified.
     */
    unwrapped?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyUnwrapped;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyKmsWrapped {
    /**
     * The resource name of the KMS CryptoKey to use for unwrapping.
     */
    cryptoKeyName: string;
    /**
     * The wrapped data crypto key.
     *
     * A base64-encoded string.
     */
    wrappedKey: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyTransient {
    /**
     * Name of the key. This is an arbitrary string used to differentiate different keys. A unique key is generated per name: two separate 'TransientCryptoKey' protos share the same generated key if their names are the same. When the data crypto key is generated, this name is not used in any way (repeating the api call will result in a different key being generated).
     */
    name: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyUnwrapped {
    /**
     * A 128/192/256 bit key.
     *
     * A base64-encoded string.
     */
    key: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigSurrogateInfoType {
    /**
     * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names listed at [https://cloud.google.com/dlp/docs/infotypes-reference](https://cloud.google.com/dlp/docs/infotypes-reference) when specifying a built-in type. When sending Cloud DLP results to Data Catalog, infoType names should conform to the pattern '[A-Za-z0-9$-_]{1,64}'.
     */
    name: string;
    /**
     * Optional custom sensitivity for this InfoType. This only applies to data profiling.
     */
    sensitivityScore?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigSurrogateInfoTypeSensitivityScore;
    /**
     * Optional version name for this InfoType.
     */
    version?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigSurrogateInfoTypeSensitivityScore {
    /**
     * The sensitivity score applied to the resource. Possible values: ["SENSITIVITY_LOW", "SENSITIVITY_MODERATE", "SENSITIVITY_HIGH"]
     */
    score: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfig {
    /**
     * Points to the field that contains the context, for example, an entity id.
     * If set, must also set cryptoKey. If set, shift will be consistent for the given context.
     */
    context?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigContext;
    /**
     * Causes the shift to be computed based on this key and the context. This results in the same shift for the same context and cryptoKey. If set, must also set context. Can only be applied to table items.
     */
    cryptoKey?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKey;
    /**
     * For example, -5 means shift date to at most 5 days back in the past.
     */
    lowerBoundDays: number;
    /**
     * Range of shift in days. Actual shift will be selected at random within this range (inclusive ends). Negative means shift to earlier in time. Must not be more than 365250 days (1000 years) each direction.
     *
     * For example, 3 means shift date to at most 3 days into the future.
     */
    upperBoundDays: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigContext {
    /**
     * Name describing the field.
     */
    name: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKey {
    /**
     * KMS wrapped key.
     * Include to use an existing data crypto key wrapped by KMS. The wrapped key must be a 128-, 192-, or 256-bit key. Authorization requires the following IAM permissions when sending a request to perform a crypto transformation using a KMS-wrapped crypto key: dlp.kms.encrypt
     * For more information, see [Creating a wrapped key](https://cloud.google.com/dlp/docs/create-wrapped-key). Only one of this, 'transient' or 'unwrapped' must be specified.
     * Note: When you use Cloud KMS for cryptographic operations, [charges apply](https://cloud.google.com/kms/pricing).
     */
    kmsWrapped?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKeyKmsWrapped;
    /**
     * Transient crypto key. Use this to have a random data crypto key generated. It will be discarded after the request finishes. Only one of this, 'unwrapped' or 'kms_wrapped' must be specified.
     */
    transient?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKeyTransient;
    /**
     * Unwrapped crypto key. Using raw keys is prone to security risks due to accidentally leaking the key. Choose another type of key if possible. Only one of this, 'transient' or 'kms_wrapped' must be specified.
     */
    unwrapped?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKeyUnwrapped;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKeyKmsWrapped {
    /**
     * The resource name of the KMS CryptoKey to use for unwrapping.
     */
    cryptoKeyName: string;
    /**
     * The wrapped data crypto key.
     *
     * A base64-encoded string.
     */
    wrappedKey: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKeyTransient {
    /**
     * Name of the key. This is an arbitrary string used to differentiate different keys. A unique key is generated per name: two separate 'TransientCryptoKey' protos share the same generated key if their names are the same. When the data crypto key is generated, this name is not used in any way (repeating the api call will result in a different key being generated).
     */
    name: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationDateShiftConfigCryptoKeyUnwrapped {
    /**
     * A 128/192/256 bit key.
     *
     * A base64-encoded string.
     */
    key: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationFixedSizeBucketingConfig {
    /**
     * Size of each bucket (except for minimum and maximum buckets).
     * So if lower_bound = 10, upper_bound = 89, and bucketSize = 10, then the following buckets would be used: -10, 10-20, 20-30, 30-40, 40-50, 50-60, 60-70, 70-80, 80-89, 89+.
     * Precision up to 2 decimals works.
     */
    bucketSize: number;
    /**
     * Lower bound value of buckets.
     * All values less than lower_bound are grouped together into a single bucket; for example if lower_bound = 10, then all values less than 10 are replaced with the value "-10".
     * The 'lower_bound' block must only contain one argument. See the 'fixed_size_bucketing_config' block description for more information about choosing a data type.
     */
    lowerBound: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationFixedSizeBucketingConfigLowerBound;
    /**
     * Upper bound value of buckets.
     * All values greater than upper_bound are grouped together into a single bucket; for example if upper_bound = 89, then all values greater than 89 are replaced with the value "89+".
     * The 'upper_bound' block must only contain one argument. See the 'fixed_size_bucketing_config' block description for more information about choosing a data type.
     */
    upperBound: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationFixedSizeBucketingConfigUpperBound;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationFixedSizeBucketingConfigLowerBound {
    /**
     * A float value.
     */
    floatValue?: number;
    /**
     * An integer value (int64 format)
     */
    integerValue?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationFixedSizeBucketingConfigUpperBound {
    /**
     * A float value.
     */
    floatValue?: number;
    /**
     * An integer value (int64 format)
     */
    integerValue?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationRedactConfig {
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfig {
    /**
     * Replace each input value with a given value.
     * The 'new_value' block must only contain one argument. For example when replacing the contents of a string-type field, only 'string_value' should be set.
     */
    newValue: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfigNewValue;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfigNewValue {
    /**
     * A boolean value.
     */
    booleanValue?: boolean;
    /**
     * Represents a whole or partial calendar date.
     */
    dateValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfigNewValueDateValue;
    /**
     * Represents a day of the week. Possible values: ["MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    dayOfWeekValue?: string;
    /**
     * A float value.
     */
    floatValue?: number;
    /**
     * An integer value (int64 format)
     */
    integerValue?: string;
    /**
     * A string value.
     */
    stringValue?: string;
    /**
     * Represents a time of day.
     */
    timeValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfigNewValueTimeValue;
    /**
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits.
     * Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
     */
    timestampValue?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfigNewValueDateValue {
    /**
     * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
     */
    day?: number;
    /**
     * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
     */
    month?: number;
    /**
     * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
     */
    year?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceConfigNewValueTimeValue {
    /**
     * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
     */
    hours?: number;
    /**
     * Minutes of hour of day. Must be from 0 to 59.
     */
    minutes?: number;
    /**
     * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
     */
    nanos?: number;
    /**
     * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
     */
    seconds?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceDictionaryConfig {
    /**
     * A list of words to select from for random replacement. The [limits](https://cloud.google.com/dlp/limits) page contains details about the size limits of dictionaries.
     */
    wordList: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceDictionaryConfigWordList;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceDictionaryConfigWordList {
    /**
     * Words or phrases defining the dictionary. The dictionary must contain at least one phrase and every phrase must contain at least 2 characters that are letters or digits.
     */
    words: string[];
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationReplaceWithInfoTypeConfig {
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationInfoTypeTransformationsTransformationPrimitiveTransformationTimePartConfig {
    /**
     * The part of the time to keep. Possible values: ["YEAR", "MONTH", "DAY_OF_MONTH", "DAY_OF_WEEK", "WEEK_OF_YEAR", "HOUR_OF_DAY"]
     */
    partToExtract: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformation {
    /**
     * Generalization function that buckets values based on ranges. The ranges and replacement values are dynamically provided by the user for custom behavior, such as 1-30 > LOW 31-65 > MEDIUM 66-100 > HIGH
     * This can be used on data of type: number, long, string, timestamp.
     * If the provided value type differs from the type of data being transformed, we will first attempt converting the type of the data to be transformed to match the type of the bound before comparing.
     * See https://cloud.google.com/dlp/docs/concepts-bucketing to learn more.
     */
    bucketingConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfig;
    /**
     * Partially mask a string by replacing a given number of characters with a fixed character. Masking can start from the beginning or end of the string. This can be used on data of any type (numbers, longs, and so on) and when de-identifying structured data we'll attempt to preserve the original data's type. (This allows you to take a long like 123 and modify it to a string like **3).
     */
    characterMaskConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCharacterMaskConfig;
    /**
     * Pseudonymization method that generates deterministic encryption for the given input. Outputs a base64 encoded representation of the encrypted output. Uses AES-SIV based on the RFC [https://tools.ietf.org/html/rfc5297](https://tools.ietf.org/html/rfc5297).
     */
    cryptoDeterministicConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfig;
    /**
     * Pseudonymization method that generates surrogates via cryptographic hashing. Uses SHA-256. The key size must be either 32 or 64 bytes.
     * Outputs a base64 encoded representation of the hashed output (for example, L7k0BHmF1ha5U3NfGykjro4xWi1MPVQPjhMAZbSV9mM=).
     * Currently, only string and integer values can be hashed.
     * See https://cloud.google.com/dlp/docs/pseudonymization to learn more.
     */
    cryptoHashConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoHashConfig;
    /**
     * Replaces an identifier with a surrogate using Format Preserving Encryption (FPE) with the FFX mode of operation; however when used in the 'content.reidentify' API method, it serves the opposite function by reversing the surrogate back into the original identifier. The identifier must be encoded as ASCII. For a given crypto key and context, the same identifier will be replaced with the same surrogate. Identifiers must be at least two characters long. In the case that the identifier is the empty string, it will be skipped. See [https://cloud.google.com/dlp/docs/pseudonymization](https://cloud.google.com/dlp/docs/pseudonymization) to learn more.
     *
     * Note: We recommend using CryptoDeterministicConfig for all use cases which do not require preserving the input alphabet space and size, plus warrant referential integrity.
     */
    cryptoReplaceFfxFpeConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfig;
    /**
     * Shifts dates by random number of days, with option to be consistent for the same context. See https://cloud.google.com/dlp/docs/concepts-date-shifting to learn more.
     */
    dateShiftConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationDateShiftConfig;
    /**
     * Buckets values based on fixed size ranges. The Bucketing transformation can provide all of this functionality, but requires more configuration. This message is provided as a convenience to the user for simple bucketing strategies.
     *
     * The transformed value will be a hyphenated string of {lower_bound}-{upper_bound}. For example, if lower_bound = 10 and upper_bound = 20, all values that are within this bucket will be replaced with "10-20".
     *
     * This can be used on data of type: double, long.
     *
     * If the bound Value type differs from the type of data being transformed, we will first attempt converting the type of the data to be transformed to match the type of the bound before comparing.
     *
     * See https://cloud.google.com/dlp/docs/concepts-bucketing to learn more.
     */
    fixedSizeBucketingConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfig;
    /**
     * Redact a given value. For example, if used with an InfoTypeTransformation transforming PHONE_NUMBER, and input 'My phone number is 206-555-0123', the output would be 'My phone number is '.
     */
    redactConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationRedactConfig;
    /**
     * Replace with a specified value.
     */
    replaceConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationReplaceConfig;
    /**
     * Replace with a value randomly drawn (with replacement) from a dictionary.
     */
    replaceDictionaryConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationReplaceDictionaryConfig;
    /**
     * For use with Date, Timestamp, and TimeOfDay, extract or preserve a portion of the value.
     */
    timePartConfig?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationTimePartConfig;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfig {
    /**
     * Set of buckets. Ranges must be non-overlapping.
     * Bucket is represented as a range, along with replacement values.
     */
    buckets?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucket[];
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucket {
    /**
     * Upper bound of the range, exclusive; type must match min.
     * The 'max' block must only contain one argument. See the 'bucketing_config' block description for more information about choosing a data type.
     */
    max?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketMax;
    /**
     * Lower bound of the range, inclusive. Type should be the same as max if used.
     * The 'min' block must only contain one argument. See the 'bucketing_config' block description for more information about choosing a data type.
     */
    min?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketMin;
    /**
     * Replacement value for this bucket.
     * The 'replacement_value' block must only contain one argument.
     */
    replacementValue: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketReplacementValue;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketMax {
    /**
     * A boolean value.
     */
    booleanValue?: boolean;
    /**
     * Represents a whole or partial calendar date.
     */
    dateValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketMaxDateValue;
    /**
     * Represents a day of the week. Possible values: ["MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    dayOfWeekValue?: string;
    /**
     * A float value.
     */
    floatValue?: number;
    /**
     * An integer value (int64 format)
     */
    integerValue?: string;
    /**
     * A string value.
     */
    stringValue?: string;
    /**
     * Represents a time of day.
     */
    timeValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketMaxTimeValue;
    /**
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
     */
    timestampValue?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketMaxDateValue {
    /**
     * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
     */
    day?: number;
    /**
     * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
     */
    month?: number;
    /**
     * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
     */
    year?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketMaxTimeValue {
    /**
     * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
     */
    hours?: number;
    /**
     * Minutes of hour of day. Must be from 0 to 59.
     */
    minutes?: number;
    /**
     * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
     */
    nanos?: number;
    /**
     * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
     */
    seconds?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketMin {
    /**
     * A boolean value.
     */
    booleanValue?: boolean;
    /**
     * Represents a whole or partial calendar date.
     */
    dateValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketMinDateValue;
    /**
     * Represents a day of the week. Possible values: ["MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    dayOfWeekValue?: string;
    /**
     * A float value.
     */
    floatValue?: number;
    /**
     * An integer value (int64 format)
     */
    integerValue?: string;
    /**
     * A string value.
     */
    stringValue?: string;
    /**
     * Represents a time of day.
     */
    timeValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketMinTimeValue;
    /**
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
     */
    timestampValue?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketMinDateValue {
    /**
     * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
     */
    day?: number;
    /**
     * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
     */
    month?: number;
    /**
     * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
     */
    year?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketMinTimeValue {
    /**
     * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
     */
    hours?: number;
    /**
     * Minutes of hour of day. Must be from 0 to 59.
     */
    minutes?: number;
    /**
     * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
     */
    nanos?: number;
    /**
     * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
     */
    seconds?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketReplacementValue {
    /**
     * A boolean value.
     */
    booleanValue?: boolean;
    /**
     * Represents a whole or partial calendar date.
     */
    dateValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketReplacementValueDateValue;
    /**
     * Represents a day of the week. Possible values: ["MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    dayOfWeekValue?: string;
    /**
     * A float value.
     */
    floatValue?: number;
    /**
     * An integer value (int64 format)
     */
    integerValue?: string;
    /**
     * A string value.
     */
    stringValue?: string;
    /**
     * Represents a time of day.
     */
    timeValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketReplacementValueTimeValue;
    /**
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
     */
    timestampValue?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketReplacementValueDateValue {
    /**
     * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
     */
    day?: number;
    /**
     * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
     */
    month?: number;
    /**
     * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
     */
    year?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationBucketingConfigBucketReplacementValueTimeValue {
    /**
     * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
     */
    hours?: number;
    /**
     * Minutes of hour of day. Must be from 0 to 59.
     */
    minutes?: number;
    /**
     * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
     */
    nanos?: number;
    /**
     * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
     */
    seconds?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCharacterMaskConfig {
    /**
     * Characters to skip when doing de-identification of a value. These will be left alone and skipped.
     */
    charactersToIgnores?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCharacterMaskConfigCharactersToIgnore[];
    /**
     * Character to use to mask the sensitive values—for example, * for an alphabetic string such as a name, or 0 for a numeric string
     * such as ZIP code or credit card number. This string must have a length of 1. If not supplied, this value defaults to * for
     * strings, and 0 for digits.
     */
    maskingCharacter?: string;
    /**
     * Number of characters to mask. If not set, all matching chars will be masked. Skipped characters do not count towards this tally.
     * If number_to_mask is negative, this denotes inverse masking. Cloud DLP masks all but a number of characters. For example, suppose you have the following values:
     * - 'masking_character' is *
     * - 'number_to_mask' is -4
     * - 'reverse_order' is false
     * - 'characters_to_ignore' includes -
     * - Input string is 1234-5678-9012-3456
     *
     * The resulting de-identified string is ****-****-****-3456. Cloud DLP masks all but the last four characters. If reverseOrder is true, all but the first four characters are masked as 1234-****-****-****.
     */
    numberToMask?: number;
    /**
     * Mask characters in reverse order. For example, if masking_character is 0, number_to_mask is 14, and reverse_order is 'false', then the
     * input string '1234-5678-9012-3456' is masked as '00000000000000-3456'.
     */
    reverseOrder?: boolean;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCharacterMaskConfigCharactersToIgnore {
    /**
     * Characters to not transform when masking.
     */
    charactersToSkip?: string;
    /**
     * Common characters to not transform when masking. Useful to avoid removing punctuation. Possible values: ["NUMERIC", "ALPHA_UPPER_CASE", "ALPHA_LOWER_CASE", "PUNCTUATION", "WHITESPACE"]
     */
    commonCharactersToIgnore?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfig {
    /**
     * A context may be used for higher security and maintaining referential integrity such that the same identifier in two different contexts will be given a distinct surrogate. The context is appended to plaintext value being encrypted. On decryption the provided context is validated against the value used during encryption. If a context was provided during encryption, same context must be provided during decryption as well.
     *
     * If the context is not set, plaintext would be used as is for encryption. If the context is set but:
     *
     * 1. there is no record present when transforming a given value or
     * 2. the field is not present when transforming a given value,
     *
     * plaintext would be used as is for encryption.
     *
     * Note that case (1) is expected when an InfoTypeTransformation is applied to both structured and unstructured ContentItems.
     */
    context?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigContext;
    /**
     * The key used by the encryption function. For deterministic encryption using AES-SIV, the provided key is internally expanded to 64 bytes prior to use.
     */
    cryptoKey?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKey;
    /**
     * The custom info type to annotate the surrogate with. This annotation will be applied to the surrogate by prefixing it with the name of the custom info type followed by the number of characters comprising the surrogate. The following scheme defines the format: {info type name}({surrogate character count}):{surrogate}
     *
     * For example, if the name of custom info type is 'MY\_TOKEN\_INFO\_TYPE' and the surrogate is 'abc', the full replacement value will be: 'MY\_TOKEN\_INFO\_TYPE(3):abc'
     *
     * This annotation identifies the surrogate when inspecting content using the custom info type 'Surrogate'. This facilitates reversal of the surrogate when it occurs in free text.
     *
     * Note: For record transformations where the entire cell in a table is being transformed, surrogates are not mandatory. Surrogates are used to denote the location of the token and are necessary for re-identification in free form text.
     *
     * In order for inspection to work properly, the name of this info type must not occur naturally anywhere in your data; otherwise, inspection may either
     *
     * *   reverse a surrogate that does not correspond to an actual identifier
     * *   be unable to parse the surrogate and result in an error
     *
     * Therefore, choose your custom info type name carefully after considering what your data looks like. One way to select a name that has a high chance of yielding reliable detection is to include one or more unicode characters that are highly improbable to exist in your data. For example, assuming your data is entered from a regular ASCII keyboard, the symbol with the hex code point 29DD might be used like so: ⧝MY\_TOKEN\_TYPE.
     */
    surrogateInfoType?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigSurrogateInfoType;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigContext {
    /**
     * Name describing the field.
     */
    name?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKey {
    /**
     * KMS wrapped key.
     * Include to use an existing data crypto key wrapped by KMS. The wrapped key must be a 128-, 192-, or 256-bit key. Authorization requires the following IAM permissions when sending a request to perform a crypto transformation using a KMS-wrapped crypto key: dlp.kms.encrypt
     * For more information, see [Creating a wrapped key](https://cloud.google.com/dlp/docs/create-wrapped-key).
     * Note: When you use Cloud KMS for cryptographic operations, [charges apply](https://cloud.google.com/kms/pricing).
     */
    kmsWrapped?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyKmsWrapped;
    /**
     * Transient crypto key. Use this to have a random data crypto key generated. It will be discarded after the request finishes.
     */
    transient?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyTransient;
    /**
     * Unwrapped crypto key. Using raw keys is prone to security risks due to accidentally leaking the key. Choose another type of key if possible.
     */
    unwrapped?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyUnwrapped;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyKmsWrapped {
    /**
     * The resource name of the KMS CryptoKey to use for unwrapping.
     */
    cryptoKeyName: string;
    /**
     * The wrapped data crypto key.
     *
     * A base64-encoded string.
     */
    wrappedKey: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyTransient {
    /**
     * Name of the key. This is an arbitrary string used to differentiate different keys. A unique key is generated per name: two separate 'TransientCryptoKey' protos share the same generated key if their names are the same. When the data crypto key is generated, this name is not used in any way (repeating the api call will result in a different key being generated).
     */
    name: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigCryptoKeyUnwrapped {
    /**
     * A 128/192/256 bit key.
     *
     * A base64-encoded string.
     */
    key: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigSurrogateInfoType {
    /**
     * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names listed at [https://cloud.google.com/dlp/docs/infotypes-reference](https://cloud.google.com/dlp/docs/infotypes-reference) when specifying a built-in type. When sending Cloud DLP results to Data Catalog, infoType names should conform to the pattern '[A-Za-z0-9$-_]{1,64}'.
     */
    name?: string;
    /**
     * Optional custom sensitivity for this InfoType. This only applies to data profiling.
     */
    sensitivityScore?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigSurrogateInfoTypeSensitivityScore;
    /**
     * Optional version name for this InfoType.
     */
    version?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoDeterministicConfigSurrogateInfoTypeSensitivityScore {
    /**
     * The sensitivity score applied to the resource. Possible values: ["SENSITIVITY_LOW", "SENSITIVITY_MODERATE", "SENSITIVITY_HIGH"]
     */
    score: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoHashConfig {
    /**
     * The key used by the encryption function.
     */
    cryptoKey?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoHashConfigCryptoKey;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoHashConfigCryptoKey {
    /**
     * KMS wrapped key.
     * Include to use an existing data crypto key wrapped by KMS. The wrapped key must be a 128-, 192-, or 256-bit key. Authorization requires the following IAM permissions when sending a request to perform a crypto transformation using a KMS-wrapped crypto key: dlp.kms.encrypt
     * For more information, see [Creating a wrapped key](https://cloud.google.com/dlp/docs/create-wrapped-key).
     * Note: When you use Cloud KMS for cryptographic operations, [charges apply](https://cloud.google.com/kms/pricing).
     */
    kmsWrapped?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyKmsWrapped;
    /**
     * Transient crypto key. Use this to have a random data crypto key generated. It will be discarded after the request finishes.
     */
    transient?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyTransient;
    /**
     * Unwrapped crypto key. Using raw keys is prone to security risks due to accidentally leaking the key. Choose another type of key if possible.
     */
    unwrapped?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyUnwrapped;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyKmsWrapped {
    /**
     * The resource name of the KMS CryptoKey to use for unwrapping.
     */
    cryptoKeyName: string;
    /**
     * The wrapped data crypto key.
     *
     * A base64-encoded string.
     */
    wrappedKey: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyTransient {
    /**
     * Name of the key. This is an arbitrary string used to differentiate different keys. A unique key is generated per name: two separate 'TransientCryptoKey' protos share the same generated key if their names are the same. When the data crypto key is generated, this name is not used in any way (repeating the api call will result in a different key being generated).
     */
    name: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoHashConfigCryptoKeyUnwrapped {
    /**
     * A 128/192/256 bit key.
     *
     * A base64-encoded string.
     */
    key: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfig {
    /**
     * Common alphabets. Possible values: ["FFX_COMMON_NATIVE_ALPHABET_UNSPECIFIED", "NUMERIC", "HEXADECIMAL", "UPPER_CASE_ALPHA_NUMERIC", "ALPHA_NUMERIC"]
     */
    commonAlphabet?: string;
    /**
     * The 'tweak', a context may be used for higher security since the same identifier in two different contexts won't be given the same surrogate. If the context is not set, a default tweak will be used.
     *
     * If the context is set but:
     *
     * 1.  there is no record present when transforming a given value or
     * 2.  the field is not present when transforming a given value,
     *
     * a default tweak will be used.
     *
     * Note that case (1) is expected when an 'InfoTypeTransformation' is applied to both structured and non-structured 'ContentItem's. Currently, the referenced field may be of value type integer or string.
     *
     * The tweak is constructed as a sequence of bytes in big endian byte order such that:
     *
     * *   a 64 bit integer is encoded followed by a single byte of value 1
     * *   a string is encoded in UTF-8 format followed by a single byte of value 2
     */
    context?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigContext;
    /**
     * The key used by the encryption algorithm.
     */
    cryptoKey?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKey;
    /**
     * This is supported by mapping these to the alphanumeric characters that the FFX mode natively supports. This happens before/after encryption/decryption. Each character listed must appear only once. Number of characters must be in the range \[2, 95\]. This must be encoded as ASCII. The order of characters does not matter. The full list of allowed characters is:
     *
     * ''0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz ~'!@#$%^&*()_-+={[}]|:;"'<,>.?/''
     */
    customAlphabet?: string;
    /**
     * The native way to select the alphabet. Must be in the range \[2, 95\].
     */
    radix?: number;
    /**
     * The custom infoType to annotate the surrogate with. This annotation will be applied to the surrogate by prefixing it with the name of the custom infoType followed by the number of characters comprising the surrogate. The following scheme defines the format: info\_type\_name(surrogate\_character\_count):surrogate
     *
     * For example, if the name of custom infoType is 'MY\_TOKEN\_INFO\_TYPE' and the surrogate is 'abc', the full replacement value will be: 'MY\_TOKEN\_INFO\_TYPE(3):abc'
     *
     * This annotation identifies the surrogate when inspecting content using the custom infoType ['SurrogateType'](https://cloud.google.com/dlp/docs/reference/rest/v2/InspectConfig#surrogatetype). This facilitates reversal of the surrogate when it occurs in free text.
     *
     * In order for inspection to work properly, the name of this infoType must not occur naturally anywhere in your data; otherwise, inspection may find a surrogate that does not correspond to an actual identifier. Therefore, choose your custom infoType name carefully after considering what your data looks like. One way to select a name that has a high chance of yielding reliable detection is to include one or more unicode characters that are highly improbable to exist in your data. For example, assuming your data is entered from a regular ASCII keyboard, the symbol with the hex code point 29DD might be used like so: ⧝MY\_TOKEN\_TYPE
     */
    surrogateInfoType?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigSurrogateInfoType;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigContext {
    /**
     * Name describing the field.
     */
    name?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKey {
    /**
     * KMS wrapped key.
     * Include to use an existing data crypto key wrapped by KMS. The wrapped key must be a 128-, 192-, or 256-bit key. Authorization requires the following IAM permissions when sending a request to perform a crypto transformation using a KMS-wrapped crypto key: dlp.kms.encrypt
     * For more information, see [Creating a wrapped key](https://cloud.google.com/dlp/docs/create-wrapped-key).
     * Note: When you use Cloud KMS for cryptographic operations, [charges apply](https://cloud.google.com/kms/pricing).
     */
    kmsWrapped?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyKmsWrapped;
    /**
     * Transient crypto key. Use this to have a random data crypto key generated. It will be discarded after the request finishes.
     */
    transient?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyTransient;
    /**
     * Unwrapped crypto key. Using raw keys is prone to security risks due to accidentally leaking the key. Choose another type of key if possible.
     */
    unwrapped?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyUnwrapped;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyKmsWrapped {
    /**
     * The resource name of the KMS CryptoKey to use for unwrapping.
     */
    cryptoKeyName: string;
    /**
     * The wrapped data crypto key.
     *
     * A base64-encoded string.
     */
    wrappedKey: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyTransient {
    /**
     * Name of the key. This is an arbitrary string used to differentiate different keys. A unique key is generated per name: two separate 'TransientCryptoKey' protos share the same generated key if their names are the same. When the data crypto key is generated, this name is not used in any way (repeating the api call will result in a different key being generated).
     */
    name: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigCryptoKeyUnwrapped {
    /**
     * A 128/192/256 bit key.
     *
     * A base64-encoded string.
     */
    key: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigSurrogateInfoType {
    /**
     * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names listed at [https://cloud.google.com/dlp/docs/infotypes-reference](https://cloud.google.com/dlp/docs/infotypes-reference) when specifying a built-in type. When sending Cloud DLP results to Data Catalog, infoType names should conform to the pattern '[A-Za-z0-9$-_]{1,64}'.
     */
    name?: string;
    /**
     * Optional custom sensitivity for this InfoType. This only applies to data profiling.
     */
    sensitivityScore?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigSurrogateInfoTypeSensitivityScore;
    /**
     * Optional version name for this InfoType.
     */
    version?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationCryptoReplaceFfxFpeConfigSurrogateInfoTypeSensitivityScore {
    /**
     * The sensitivity score applied to the resource. Possible values: ["SENSITIVITY_LOW", "SENSITIVITY_MODERATE", "SENSITIVITY_HIGH"]
     */
    score: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationDateShiftConfig {
    /**
     * Points to the field that contains the context, for example, an entity id.
     * If set, must also set cryptoKey. If set, shift will be consistent for the given context.
     */
    context?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationDateShiftConfigContext;
    /**
     * Causes the shift to be computed based on this key and the context. This results in the same shift for the same context and cryptoKey. If set, must also set context. Can only be applied to table items.
     */
    cryptoKey?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationDateShiftConfigCryptoKey;
    /**
     * For example, -5 means shift date to at most 5 days back in the past.
     */
    lowerBoundDays: number;
    /**
     * Range of shift in days. Actual shift will be selected at random within this range (inclusive ends). Negative means shift to earlier in time. Must not be more than 365250 days (1000 years) each direction.
     *
     * For example, 3 means shift date to at most 3 days into the future.
     */
    upperBoundDays: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationDateShiftConfigContext {
    /**
     * Name describing the field.
     */
    name?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationDateShiftConfigCryptoKey {
    /**
     * KMS wrapped key.
     * Include to use an existing data crypto key wrapped by KMS. The wrapped key must be a 128-, 192-, or 256-bit key. Authorization requires the following IAM permissions when sending a request to perform a crypto transformation using a KMS-wrapped crypto key: dlp.kms.encrypt
     * For more information, see [Creating a wrapped key](https://cloud.google.com/dlp/docs/create-wrapped-key).
     * Note: When you use Cloud KMS for cryptographic operations, [charges apply](https://cloud.google.com/kms/pricing).
     */
    kmsWrapped?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationDateShiftConfigCryptoKeyKmsWrapped;
    /**
     * Transient crypto key. Use this to have a random data crypto key generated. It will be discarded after the request finishes.
     */
    transient?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationDateShiftConfigCryptoKeyTransient;
    /**
     * Unwrapped crypto key. Using raw keys is prone to security risks due to accidentally leaking the key. Choose another type of key if possible.
     */
    unwrapped?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationDateShiftConfigCryptoKeyUnwrapped;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationDateShiftConfigCryptoKeyKmsWrapped {
    /**
     * The resource name of the KMS CryptoKey to use for unwrapping.
     */
    cryptoKeyName: string;
    /**
     * The wrapped data crypto key.
     *
     * A base64-encoded string.
     */
    wrappedKey: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationDateShiftConfigCryptoKeyTransient {
    /**
     * Name of the key. This is an arbitrary string used to differentiate different keys. A unique key is generated per name: two separate 'TransientCryptoKey' protos share the same generated key if their names are the same. When the data crypto key is generated, this name is not used in any way (repeating the api call will result in a different key being generated).
     */
    name: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationDateShiftConfigCryptoKeyUnwrapped {
    /**
     * A 128/192/256 bit key.
     *
     * A base64-encoded string.
     */
    key: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfig {
    /**
     * Size of each bucket (except for minimum and maximum buckets).
     * So if lower_bound = 10, upper_bound = 89, and bucketSize = 10, then the following buckets would be used: -10, 10-20, 20-30, 30-40, 40-50, 50-60, 60-70, 70-80, 80-89, 89+.
     * Precision up to 2 decimals works.
     */
    bucketSize: number;
    /**
     * Lower bound value of buckets.
     * All values less than lower_bound are grouped together into a single bucket; for example if lower_bound = 10, then all values less than 10 are replaced with the value "-10".
     * The 'lower_bound' block must only contain one argument. See the 'fixed_size_bucketing_config' block description for more information about choosing a data type.
     */
    lowerBound: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfigLowerBound;
    /**
     * Upper bound value of buckets.
     * All values greater than upper_bound are grouped together into a single bucket; for example if upper_bound = 89, then all values greater than 89 are replaced with the value "89+".
     * The 'upper_bound' block must only contain one argument. See the 'fixed_size_bucketing_config' block description for more information about choosing a data type.
     */
    upperBound: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfigUpperBound;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfigLowerBound {
    /**
     * A boolean value.
     */
    booleanValue?: boolean;
    /**
     * Represents a whole or partial calendar date.
     */
    dateValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfigLowerBoundDateValue;
    /**
     * Represents a day of the week. Possible values: ["MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    dayOfWeekValue?: string;
    /**
     * A float value.
     */
    floatValue?: number;
    /**
     * An integer value (int64 format)
     */
    integerValue?: string;
    /**
     * A string value.
     */
    stringValue?: string;
    /**
     * Represents a time of day.
     */
    timeValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfigLowerBoundTimeValue;
    /**
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
     */
    timestampValue?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfigLowerBoundDateValue {
    /**
     * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
     */
    day?: number;
    /**
     * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
     */
    month?: number;
    /**
     * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
     */
    year?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfigLowerBoundTimeValue {
    /**
     * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
     */
    hours?: number;
    /**
     * Minutes of hour of day. Must be from 0 to 59.
     */
    minutes?: number;
    /**
     * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
     */
    nanos?: number;
    /**
     * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
     */
    seconds?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfigUpperBound {
    /**
     * A boolean value.
     */
    booleanValue?: boolean;
    /**
     * Represents a whole or partial calendar date.
     */
    dateValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfigUpperBoundDateValue;
    /**
     * Represents a day of the week. Possible values: ["MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    dayOfWeekValue?: string;
    /**
     * A float value.
     */
    floatValue?: number;
    /**
     * An integer value (int64 format)
     */
    integerValue?: string;
    /**
     * A string value.
     */
    stringValue?: string;
    /**
     * Represents a time of day.
     */
    timeValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfigUpperBoundTimeValue;
    /**
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
     */
    timestampValue?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfigUpperBoundDateValue {
    /**
     * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
     */
    day?: number;
    /**
     * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
     */
    month?: number;
    /**
     * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
     */
    year?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationFixedSizeBucketingConfigUpperBoundTimeValue {
    /**
     * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
     */
    hours?: number;
    /**
     * Minutes of hour of day. Must be from 0 to 59.
     */
    minutes?: number;
    /**
     * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
     */
    nanos?: number;
    /**
     * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
     */
    seconds?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationRedactConfig {
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationReplaceConfig {
    /**
     * Replace each input value with a given value.
     * The 'new_value' block must only contain one argument. For example when replacing the contents of a string-type field, only 'string_value' should be set.
     */
    newValue: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationReplaceConfigNewValue;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationReplaceConfigNewValue {
    /**
     * A boolean value.
     */
    booleanValue?: boolean;
    /**
     * Represents a whole or partial calendar date.
     */
    dateValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationReplaceConfigNewValueDateValue;
    /**
     * Represents a day of the week. Possible values: ["MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    dayOfWeekValue?: string;
    /**
     * A float value.
     */
    floatValue?: number;
    /**
     * An integer value (int64 format)
     */
    integerValue?: string;
    /**
     * A string value.
     */
    stringValue?: string;
    /**
     * Represents a time of day.
     */
    timeValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationReplaceConfigNewValueTimeValue;
    /**
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
     */
    timestampValue?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationReplaceConfigNewValueDateValue {
    /**
     * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
     */
    day?: number;
    /**
     * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
     */
    month?: number;
    /**
     * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
     */
    year?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationReplaceConfigNewValueTimeValue {
    /**
     * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
     */
    hours?: number;
    /**
     * Minutes of hour of day. Must be from 0 to 59.
     */
    minutes?: number;
    /**
     * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
     */
    nanos?: number;
    /**
     * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
     */
    seconds?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationReplaceDictionaryConfig {
    /**
     * A list of words to select from for random replacement. The [limits](https://cloud.google.com/dlp/limits) page contains details about the size limits of dictionaries.
     */
    wordList?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationReplaceDictionaryConfigWordList;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationReplaceDictionaryConfigWordList {
    /**
     * Words or phrases defining the dictionary. The dictionary must contain at least one phrase and every phrase must contain at least 2 characters that are letters or digits.
     */
    words: string[];
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsFieldTransformationPrimitiveTransformationTimePartConfig {
    /**
     * The part of the time to keep. Possible values: ["YEAR", "MONTH", "DAY_OF_MONTH", "DAY_OF_WEEK", "WEEK_OF_YEAR", "HOUR_OF_DAY"]
     */
    partToExtract?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppression {
    /**
     * A condition that when it evaluates to true will result in the record being evaluated to be suppressed from the transformed content.
     */
    condition?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionCondition;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionCondition {
    /**
     * An expression, consisting of an operator and conditions.
     */
    expressions?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressions;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressions {
    /**
     * Conditions to apply to the expression.
     */
    conditions?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressionsConditions;
    /**
     * The operator to apply to the result of conditions. Default and currently only supported value is AND. Default value: "AND" Possible values: ["AND"]
     */
    logicalOperator?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressionsConditions {
    /**
     * A collection of conditions.
     */
    conditions?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressionsConditionsCondition[];
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressionsConditionsCondition {
    /**
     * Field within the record this condition is evaluated against.
     */
    field: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressionsConditionsConditionField;
    /**
     * Operator used to compare the field or infoType to the value. Possible values: ["EQUAL_TO", "NOT_EQUAL_TO", "GREATER_THAN", "LESS_THAN", "GREATER_THAN_OR_EQUALS", "LESS_THAN_OR_EQUALS", "EXISTS"]
     */
    operator: string;
    /**
     * Value to compare against. [Mandatory, except for EXISTS tests.]
     */
    value?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressionsConditionsConditionValue;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressionsConditionsConditionField {
    /**
     * Name describing the field.
     */
    name?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressionsConditionsConditionValue {
    /**
     * A boolean value.
     */
    booleanValue?: boolean;
    /**
     * Represents a whole or partial calendar date.
     */
    dateValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressionsConditionsConditionValueDateValue;
    /**
     * Represents a day of the week. Possible values: ["MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    dayOfWeekValue?: string;
    /**
     * A float value.
     */
    floatValue?: number;
    /**
     * An integer value (int64 format)
     */
    integerValue?: string;
    /**
     * A string value.
     */
    stringValue?: string;
    /**
     * Represents a time of day.
     */
    timeValue?: outputs.DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressionsConditionsConditionValueTimeValue;
    /**
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
     */
    timestampValue?: string;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressionsConditionsConditionValueDateValue {
    /**
     * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0 to specify a year by itself or a year and month where the day isn't significant.
     */
    day?: number;
    /**
     * Month of a year. Must be from 1 to 12, or 0 to specify a year without a month and day.
     */
    month?: number;
    /**
     * Year of the date. Must be from 1 to 9999, or 0 to specify a date without a year.
     */
    year?: number;
}

export interface DataLossPreventionDeidentifyTemplateDeidentifyConfigRecordTransformationsRecordSuppressionConditionExpressionsConditionsConditionValueTimeValue {
    /**
     * Hours of day in 24 hour format. Should be from 0 to 23. An API may choose to allow the value "24:00:00" for scenarios like business closing time.
     */
    hours?: number;
    /**
     * Minutes of hour of day. Must be from 0 to 59.
     */
    minutes?: number;
    /**
     * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
     */
    nanos?: number;
    /**
     * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
     */
    seconds?: number;
}

export interface DataLossPreventionDeidentifyTemplateTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DataLossPreventionDiscoveryConfigAction {
    /**
     * Export data profiles into a provided location
     */
    exportData?: outputs.DataLossPreventionDiscoveryConfigActionExportData;
    /**
     * Publish a message into the Pub/Sub topic.
     */
    pubSubNotification?: outputs.DataLossPreventionDiscoveryConfigActionPubSubNotification;
    /**
     * Publish a message into the Pub/Sub topic.
     */
    tagResources?: outputs.DataLossPreventionDiscoveryConfigActionTagResources;
}

export interface DataLossPreventionDiscoveryConfigActionExportData {
    /**
     * Store all table and column profiles in an existing table or a new table in an existing dataset. Each re-generation will result in a new row in BigQuery
     */
    profileTable?: outputs.DataLossPreventionDiscoveryConfigActionExportDataProfileTable;
}

export interface DataLossPreventionDiscoveryConfigActionExportDataProfileTable {
    /**
     * Dataset Id of the table
     */
    datasetId?: string;
    /**
     * The Google Cloud Platform project ID of the project containing the table. If omitted, the project ID is inferred from the API call.
     */
    projectId?: string;
    /**
     * Name of the table
     */
    tableId?: string;
}

export interface DataLossPreventionDiscoveryConfigActionPubSubNotification {
    /**
     * How much data to include in the pub/sub message. Possible values: ["TABLE_PROFILE", "RESOURCE_NAME"]
     */
    detailOfMessage?: string;
    /**
     * The type of event that triggers a Pub/Sub. At most one PubSubNotification per EventType is permitted. Possible values: ["NEW_PROFILE", "CHANGED_PROFILE", "SCORE_INCREASED", "ERROR_CHANGED"]
     */
    event?: string;
    /**
     * Conditions for triggering pubsub
     */
    pubsubCondition?: outputs.DataLossPreventionDiscoveryConfigActionPubSubNotificationPubsubCondition;
    /**
     * Cloud Pub/Sub topic to send notifications to. Format is projects/{project}/topics/{topic}.
     */
    topic?: string;
}

export interface DataLossPreventionDiscoveryConfigActionPubSubNotificationPubsubCondition {
    /**
     * An expression
     */
    expressions?: outputs.DataLossPreventionDiscoveryConfigActionPubSubNotificationPubsubConditionExpressions;
}

export interface DataLossPreventionDiscoveryConfigActionPubSubNotificationPubsubConditionExpressions {
    /**
     * Conditions to apply to the expression
     */
    conditions?: outputs.DataLossPreventionDiscoveryConfigActionPubSubNotificationPubsubConditionExpressionsCondition[];
    /**
     * The operator to apply to the collection of conditions Possible values: ["OR", "AND"]
     */
    logicalOperator?: string;
}

export interface DataLossPreventionDiscoveryConfigActionPubSubNotificationPubsubConditionExpressionsCondition {
    /**
     * The minimum data risk score that triggers the condition. Possible values: ["HIGH", "MEDIUM_OR_HIGH"]
     */
    minimumRiskScore?: string;
    /**
     * The minimum sensitivity level that triggers the condition. Possible values: ["HIGH", "MEDIUM_OR_HIGH"]
     */
    minimumSensitivityScore?: string;
}

export interface DataLossPreventionDiscoveryConfigActionTagResources {
    /**
     * Whether applying a tag to a resource should lower the risk of the profile for that resource. For example, in conjunction with an [IAM deny policy](https://cloud.google.com/iam/docs/deny-overview), you can deny all principals a permission if a tag value is present, mitigating the risk of the resource. This also lowers the data risk of resources at the lower levels of the resource hierarchy. For example, reducing the data risk of a table data profile also reduces the data risk of the constituent column data profiles.
     */
    lowerDataRiskToLow?: boolean;
    /**
     * The profile generations for which the tag should be attached to resources. If you attach a tag to only new profiles, then if the sensitivity score of a profile subsequently changes, its tag doesn't change. By default, this field includes only new profiles. To include both new and updated profiles for tagging, this field should explicitly include both 'PROFILE_GENERATION_NEW' and 'PROFILE_GENERATION_UPDATE'. Possible values: ["PROFILE_GENERATION_NEW", "PROFILE_GENERATION_UPDATE"]
     */
    profileGenerationsToTags?: string[];
    /**
     * The tags to associate with different conditions.
     */
    tagConditions?: outputs.DataLossPreventionDiscoveryConfigActionTagResourcesTagCondition[];
}

export interface DataLossPreventionDiscoveryConfigActionTagResourcesTagCondition {
    /**
     * Conditions attaching the tag to a resource on its profile having this sensitivity score.
     */
    sensitivityScore?: outputs.DataLossPreventionDiscoveryConfigActionTagResourcesTagConditionSensitivityScore;
    /**
     * The tag value to attach to resources.
     */
    tag?: outputs.DataLossPreventionDiscoveryConfigActionTagResourcesTagConditionTag;
}

export interface DataLossPreventionDiscoveryConfigActionTagResourcesTagConditionSensitivityScore {
    /**
     * The sensitivity score applied to the resource. Possible values: ["SENSITIVITY_LOW", "SENSITIVITY_MODERATE", "SENSITIVITY_HIGH"]
     */
    score: string;
}

export interface DataLossPreventionDiscoveryConfigActionTagResourcesTagConditionTag {
    /**
     * The namespaced name for the tag value to attach to resources. Must be in the format '{parent_id}/{tag_key_short_name}/{short_name}', for example, "123456/environment/prod".
     */
    namespacedValue?: string;
}

export interface DataLossPreventionDiscoveryConfigError {
    details: outputs.DataLossPreventionDiscoveryConfigErrorDetail[];
    timestamp: string;
}

export interface DataLossPreventionDiscoveryConfigErrorDetail {
    code: number;
    details: {[key: string]: string}[];
    message: string;
}

export interface DataLossPreventionDiscoveryConfigOrgConfig {
    /**
     * The data to scan folder org or project
     */
    location?: outputs.DataLossPreventionDiscoveryConfigOrgConfigLocation;
    /**
     * The project that will run the scan. The DLP service account that exists within this project must have access to all resources that are profiled, and the cloud DLP API must be enabled.
     */
    projectId?: string;
}

export interface DataLossPreventionDiscoveryConfigOrgConfigLocation {
    /**
     * The ID for the folder within an organization to scan
     */
    folderId?: string;
    /**
     * The ID of an organization to scan
     */
    organizationId?: string;
}

export interface DataLossPreventionDiscoveryConfigTarget {
    /**
     * BigQuery target for Discovery. The first target to match a table will be the one applied.
     */
    bigQueryTarget?: outputs.DataLossPreventionDiscoveryConfigTargetBigQueryTarget;
    /**
     * Cloud SQL target for Discovery. The first target to match a table will be the one applied.
     */
    cloudSqlTarget?: outputs.DataLossPreventionDiscoveryConfigTargetCloudSqlTarget;
    /**
     * Cloud Storage target for Discovery. The first target to match a bucket will be the one applied.
     */
    cloudStorageTarget?: outputs.DataLossPreventionDiscoveryConfigTargetCloudStorageTarget;
    /**
     * Discovery target that looks for credentials and secrets stored in cloud resource metadata and reports them as vulnerabilities to Security Command Center. Only one target of this type is allowed.
     */
    secretsTarget?: outputs.DataLossPreventionDiscoveryConfigTargetSecretsTarget;
}

export interface DataLossPreventionDiscoveryConfigTargetBigQueryTarget {
    /**
     * How often and when to update profiles. New tables that match both the fiter and conditions are scanned as quickly as possible depending on system capacity.
     */
    cadence?: outputs.DataLossPreventionDiscoveryConfigTargetBigQueryTargetCadence;
    /**
     * In addition to matching the filter, these conditions must be true before a profile is generated
     */
    conditions?: outputs.DataLossPreventionDiscoveryConfigTargetBigQueryTargetConditions;
    /**
     * Tables that match this filter will not have profiles created.
     */
    disabled?: outputs.DataLossPreventionDiscoveryConfigTargetBigQueryTargetDisabled;
    /**
     * Required. The tables the discovery cadence applies to. The first target with a matching filter will be the one to apply to a table
     */
    filter?: outputs.DataLossPreventionDiscoveryConfigTargetBigQueryTargetFilter;
}

export interface DataLossPreventionDiscoveryConfigTargetBigQueryTargetCadence {
    /**
     * Governs when to update data profiles when the inspection rules defined by the 'InspectTemplate' change. If not set, changing the template will not cause a data profile to update.
     */
    inspectTemplateModifiedCadence?: outputs.DataLossPreventionDiscoveryConfigTargetBigQueryTargetCadenceInspectTemplateModifiedCadence;
    /**
     * Governs when to update data profiles when a schema is modified
     */
    schemaModifiedCadence?: outputs.DataLossPreventionDiscoveryConfigTargetBigQueryTargetCadenceSchemaModifiedCadence;
    /**
     * Governs when to update profile when a table is modified.
     */
    tableModifiedCadence?: outputs.DataLossPreventionDiscoveryConfigTargetBigQueryTargetCadenceTableModifiedCadence;
}

export interface DataLossPreventionDiscoveryConfigTargetBigQueryTargetCadenceInspectTemplateModifiedCadence {
    /**
     * How frequently data profiles can be updated when the template is modified. Defaults to never. Possible values: ["UPDATE_FREQUENCY_NEVER", "UPDATE_FREQUENCY_DAILY", "UPDATE_FREQUENCY_MONTHLY"]
     */
    frequency?: string;
}

export interface DataLossPreventionDiscoveryConfigTargetBigQueryTargetCadenceSchemaModifiedCadence {
    /**
     * How frequently profiles may be updated when schemas are modified. Default to monthly Possible values: ["UPDATE_FREQUENCY_NEVER", "UPDATE_FREQUENCY_DAILY", "UPDATE_FREQUENCY_MONTHLY"]
     */
    frequency?: string;
    /**
     * The type of events to consider when deciding if the table's schema has been modified and should have the profile updated. Defaults to NEW_COLUMN. Possible values: ["SCHEMA_NEW_COLUMNS", "SCHEMA_REMOVED_COLUMNS"]
     */
    types?: string[];
}

export interface DataLossPreventionDiscoveryConfigTargetBigQueryTargetCadenceTableModifiedCadence {
    /**
     * How frequently data profiles can be updated when tables are modified. Defaults to never. Possible values: ["UPDATE_FREQUENCY_NEVER", "UPDATE_FREQUENCY_DAILY", "UPDATE_FREQUENCY_MONTHLY"]
     */
    frequency?: string;
    /**
     * The type of events to consider when deciding if the table has been modified and should have the profile updated. Defaults to MODIFIED_TIMESTAMP Possible values: ["TABLE_MODIFIED_TIMESTAMP"]
     */
    types?: string[];
}

export interface DataLossPreventionDiscoveryConfigTargetBigQueryTargetConditions {
    /**
     * A timestamp in RFC3339 UTC "Zulu" format with nanosecond resolution and upto nine fractional digits.
     */
    createdAfter?: string;
    /**
     * At least one of the conditions must be true for a table to be scanned.
     */
    orConditions?: outputs.DataLossPreventionDiscoveryConfigTargetBigQueryTargetConditionsOrConditions;
    /**
     * Restrict discovery to categories of table types. Currently view, materialized view, snapshot and non-biglake external tables are supported. Possible values: ["BIG_QUERY_COLLECTION_ALL_TYPES", "BIG_QUERY_COLLECTION_ONLY_SUPPORTED_TYPES"]
     */
    typeCollection?: string;
    /**
     * Restrict discovery to specific table type
     */
    types?: outputs.DataLossPreventionDiscoveryConfigTargetBigQueryTargetConditionsTypes;
}

export interface DataLossPreventionDiscoveryConfigTargetBigQueryTargetConditionsOrConditions {
    /**
     * Duration format. The minimum age a table must have before Cloud DLP can profile it. Value greater than 1.
     */
    minAge?: string;
    /**
     * Minimum number of rows that should be present before Cloud DLP profiles as a table.
     */
    minRowCount?: number;
}

export interface DataLossPreventionDiscoveryConfigTargetBigQueryTargetConditionsTypes {
    /**
     * A set of BiqQuery table types Possible values: ["BIG_QUERY_TABLE_TYPE_TABLE", "BIG_QUERY_TABLE_TYPE_EXTERNAL_BIG_LAKE"]
     */
    types?: string[];
}

export interface DataLossPreventionDiscoveryConfigTargetBigQueryTargetDisabled {
}

export interface DataLossPreventionDiscoveryConfigTargetBigQueryTargetFilter {
    /**
     * Catch-all. This should always be the last filter in the list because anything above it will apply first.
     */
    otherTables?: outputs.DataLossPreventionDiscoveryConfigTargetBigQueryTargetFilterOtherTables;
    /**
     * The table to scan. Discovery configurations including this can only include one DiscoveryTarget (the DiscoveryTarget with this TableReference).
     */
    tableReference?: outputs.DataLossPreventionDiscoveryConfigTargetBigQueryTargetFilterTableReference;
    /**
     * A specific set of tables for this filter to apply to. A table collection must be specified in only one filter per config.
     */
    tables?: outputs.DataLossPreventionDiscoveryConfigTargetBigQueryTargetFilterTables;
}

export interface DataLossPreventionDiscoveryConfigTargetBigQueryTargetFilterOtherTables {
}

export interface DataLossPreventionDiscoveryConfigTargetBigQueryTargetFilterTableReference {
    /**
     * Dataset ID of the table.
     */
    datasetId: string;
    /**
     * Name of the table.
     */
    tableId: string;
}

export interface DataLossPreventionDiscoveryConfigTargetBigQueryTargetFilterTables {
    /**
     * A collection of regular expressions to match a BQ table against.
     */
    includeRegexes?: outputs.DataLossPreventionDiscoveryConfigTargetBigQueryTargetFilterTablesIncludeRegexes;
}

export interface DataLossPreventionDiscoveryConfigTargetBigQueryTargetFilterTablesIncludeRegexes {
    /**
     * A single BigQuery regular expression pattern to match against one or more tables, datasets, or projects that contain BigQuery tables.
     */
    patterns?: outputs.DataLossPreventionDiscoveryConfigTargetBigQueryTargetFilterTablesIncludeRegexesPattern[];
}

export interface DataLossPreventionDiscoveryConfigTargetBigQueryTargetFilterTablesIncludeRegexesPattern {
    /**
     * if unset, this property matches all datasets
     */
    datasetIdRegex?: string;
    /**
     * For organizations, if unset, will match all projects. Has no effect for data profile configurations created within a project.
     */
    projectIdRegex?: string;
    /**
     * if unset, this property matches all tables
     */
    tableIdRegex?: string;
}

export interface DataLossPreventionDiscoveryConfigTargetCloudSqlTarget {
    /**
     * In addition to matching the filter, these conditions must be true before a profile is generated.
     */
    conditions?: outputs.DataLossPreventionDiscoveryConfigTargetCloudSqlTargetConditions;
    /**
     * Disable profiling for database resources that match this filter.
     */
    disabled?: outputs.DataLossPreventionDiscoveryConfigTargetCloudSqlTargetDisabled;
    /**
     * Required. The tables the discovery cadence applies to. The first target with a matching filter will be the one to apply to a table.
     */
    filter: outputs.DataLossPreventionDiscoveryConfigTargetCloudSqlTargetFilter;
    /**
     * How often and when to update profiles. New tables that match both the filter and conditions are scanned as quickly as possible depending on system capacity.
     */
    generationCadence?: outputs.DataLossPreventionDiscoveryConfigTargetCloudSqlTargetGenerationCadence;
}

export interface DataLossPreventionDiscoveryConfigTargetCloudSqlTargetConditions {
    /**
     * Database engines that should be profiled. Optional. Defaults to ALL_SUPPORTED_DATABASE_ENGINES if unspecified. Possible values: ["ALL_SUPPORTED_DATABASE_ENGINES", "MYSQL", "POSTGRES"]
     */
    databaseEngines?: string[];
    /**
     * Data profiles will only be generated for the database resource types specified in this field. If not specified, defaults to [DATABASE_RESOURCE_TYPE_ALL_SUPPORTED_TYPES]. Possible values: ["DATABASE_RESOURCE_TYPE_ALL_SUPPORTED_TYPES", "DATABASE_RESOURCE_TYPE_TABLE"]
     */
    types?: string[];
}

export interface DataLossPreventionDiscoveryConfigTargetCloudSqlTargetDisabled {
}

export interface DataLossPreventionDiscoveryConfigTargetCloudSqlTargetFilter {
    /**
     * A specific set of database resources for this filter to apply to.
     */
    collection?: outputs.DataLossPreventionDiscoveryConfigTargetCloudSqlTargetFilterCollection;
    /**
     * The database resource to scan. Targets including this can only include one target (the target with this database resource reference).
     */
    databaseResourceReference?: outputs.DataLossPreventionDiscoveryConfigTargetCloudSqlTargetFilterDatabaseResourceReference;
    /**
     * Catch-all. This should always be the last target in the list because anything above it will apply first. Should only appear once in a configuration. If none is specified, a default one will be added automatically.
     */
    others?: outputs.DataLossPreventionDiscoveryConfigTargetCloudSqlTargetFilterOthers;
}

export interface DataLossPreventionDiscoveryConfigTargetCloudSqlTargetFilterCollection {
    /**
     * A collection of regular expressions to match a database resource against.
     */
    includeRegexes?: outputs.DataLossPreventionDiscoveryConfigTargetCloudSqlTargetFilterCollectionIncludeRegexes;
}

export interface DataLossPreventionDiscoveryConfigTargetCloudSqlTargetFilterCollectionIncludeRegexes {
    /**
     * A group of regular expression patterns to match against one or more database resources. Maximum of 100 entries. The sum of all regular expressions' length can't exceed 10 KiB.
     */
    patterns?: outputs.DataLossPreventionDiscoveryConfigTargetCloudSqlTargetFilterCollectionIncludeRegexesPattern[];
}

export interface DataLossPreventionDiscoveryConfigTargetCloudSqlTargetFilterCollectionIncludeRegexesPattern {
    /**
     * Regex to test the database name against. If empty, all databases match.
     */
    databaseRegex?: string;
    /**
     * Regex to test the database resource's name against. An example of a database resource name is a table's name. Other database resource names like view names could be included in the future. If empty, all database resources match.'
     */
    databaseResourceNameRegex?: string;
    /**
     * Regex to test the instance name against. If empty, all instances match.
     */
    instanceRegex?: string;
    /**
     * For organizations, if unset, will match all projects. Has no effect for data profile configurations created within a project.
     */
    projectIdRegex?: string;
}

export interface DataLossPreventionDiscoveryConfigTargetCloudSqlTargetFilterDatabaseResourceReference {
    /**
     * Required. Name of a database within the instance.
     */
    database: string;
    /**
     * Required. Name of a database resource, for example, a table within the database.
     */
    databaseResource: string;
    /**
     * Required. The instance where this resource is located. For example: Cloud SQL instance ID.
     */
    instance: string;
    /**
     * Required. If within a project-level config, then this must match the config's project ID.
     */
    projectId: string;
}

export interface DataLossPreventionDiscoveryConfigTargetCloudSqlTargetFilterOthers {
}

export interface DataLossPreventionDiscoveryConfigTargetCloudSqlTargetGenerationCadence {
    /**
     * Governs when to update data profiles when the inspection rules defined by the 'InspectTemplate' change. If not set, changing the template will not cause a data profile to update.
     */
    inspectTemplateModifiedCadence?: outputs.DataLossPreventionDiscoveryConfigTargetCloudSqlTargetGenerationCadenceInspectTemplateModifiedCadence;
    /**
     * Data changes (non-schema changes) in Cloud SQL tables can't trigger reprofiling. If you set this field, profiles are refreshed at this frequency regardless of whether the underlying tables have changes. Defaults to never. Possible values: ["UPDATE_FREQUENCY_NEVER", "UPDATE_FREQUENCY_DAILY", "UPDATE_FREQUENCY_MONTHLY"]
     */
    refreshFrequency?: string;
    /**
     * Governs when to update data profiles when a schema is modified
     */
    schemaModifiedCadence?: outputs.DataLossPreventionDiscoveryConfigTargetCloudSqlTargetGenerationCadenceSchemaModifiedCadence;
}

export interface DataLossPreventionDiscoveryConfigTargetCloudSqlTargetGenerationCadenceInspectTemplateModifiedCadence {
    /**
     * How frequently data profiles can be updated when the template is modified. Defaults to never. Possible values: ["UPDATE_FREQUENCY_NEVER", "UPDATE_FREQUENCY_DAILY", "UPDATE_FREQUENCY_MONTHLY"]
     */
    frequency: string;
}

export interface DataLossPreventionDiscoveryConfigTargetCloudSqlTargetGenerationCadenceSchemaModifiedCadence {
    /**
     * Frequency to regenerate data profiles when the schema is modified. Defaults to monthly. Possible values: ["UPDATE_FREQUENCY_NEVER", "UPDATE_FREQUENCY_DAILY", "UPDATE_FREQUENCY_MONTHLY"]
     */
    frequency?: string;
    /**
     * The types of schema modifications to consider. Defaults to NEW_COLUMNS. Possible values: ["NEW_COLUMNS", "REMOVED_COLUMNS"]
     */
    types?: string[];
}

export interface DataLossPreventionDiscoveryConfigTargetCloudStorageTarget {
    /**
     * In addition to matching the filter, these conditions must be true before a profile is generated.
     */
    conditions?: outputs.DataLossPreventionDiscoveryConfigTargetCloudStorageTargetConditions;
    /**
     * Disable profiling for buckets that match this filter.
     */
    disabled?: outputs.DataLossPreventionDiscoveryConfigTargetCloudStorageTargetDisabled;
    /**
     * The buckets the generation_cadence applies to. The first target with a matching filter will be the one to apply to a bucket.
     */
    filter: outputs.DataLossPreventionDiscoveryConfigTargetCloudStorageTargetFilter;
    /**
     * How often and when to update profiles. New buckets that match both the filter and conditions are scanned as quickly as possible depending on system capacity.
     */
    generationCadence?: outputs.DataLossPreventionDiscoveryConfigTargetCloudStorageTargetGenerationCadence;
}

export interface DataLossPreventionDiscoveryConfigTargetCloudStorageTargetConditions {
    /**
     * Cloud Storage conditions.
     */
    cloudStorageConditions?: outputs.DataLossPreventionDiscoveryConfigTargetCloudStorageTargetConditionsCloudStorageConditions;
    /**
     * File store must have been created after this date. Used to avoid backfilling. A timestamp in RFC3339 UTC "Zulu" format with nanosecond resolution and upto nine fractional digits.
     */
    createdAfter?: string;
    /**
     * Duration format. Minimum age a file store must have. If set, the value must be 1 hour or greater.
     */
    minAge?: string;
}

export interface DataLossPreventionDiscoveryConfigTargetCloudStorageTargetConditionsCloudStorageConditions {
    /**
     * Only objects with the specified attributes will be scanned. Defaults to [ALL_SUPPORTED_BUCKETS] if unset. Possible values: ["ALL_SUPPORTED_BUCKETS", "AUTOCLASS_DISABLED", "AUTOCLASS_ENABLED"]
     */
    includedBucketAttributes?: string[];
    /**
     * Only objects with the specified attributes will be scanned. If an object has one of the specified attributes but is inside an excluded bucket, it will not be scanned. Defaults to [ALL_SUPPORTED_OBJECTS]. A profile will be created even if no objects match the included_object_attributes. Possible values: ["ALL_SUPPORTED_OBJECTS", "STANDARD", "NEARLINE", "COLDLINE", "ARCHIVE", "REGIONAL", "MULTI_REGIONAL", "DURABLE_REDUCED_AVAILABILITY"]
     */
    includedObjectAttributes?: string[];
}

export interface DataLossPreventionDiscoveryConfigTargetCloudStorageTargetDisabled {
}

export interface DataLossPreventionDiscoveryConfigTargetCloudStorageTargetFilter {
    /**
     * The bucket to scan. Targets including this can only include one target (the target with this bucket). This enables profiling the contents of a single bucket, while the other options allow for easy profiling of many buckets within a project or an organization.
     */
    cloudStorageResourceReference?: outputs.DataLossPreventionDiscoveryConfigTargetCloudStorageTargetFilterCloudStorageResourceReference;
    /**
     * A specific set of buckets for this filter to apply to.
     */
    collection?: outputs.DataLossPreventionDiscoveryConfigTargetCloudStorageTargetFilterCollection;
    /**
     * Match discovery resources not covered by any other filter.
     */
    others?: outputs.DataLossPreventionDiscoveryConfigTargetCloudStorageTargetFilterOthers;
}

export interface DataLossPreventionDiscoveryConfigTargetCloudStorageTargetFilterCloudStorageResourceReference {
    /**
     * The bucket to scan.
     */
    bucketName?: string;
    /**
     * If within a project-level config, then this must match the config's project id.
     */
    projectId?: string;
}

export interface DataLossPreventionDiscoveryConfigTargetCloudStorageTargetFilterCollection {
    /**
     * A collection of regular expressions to match a file store against.
     */
    includeRegexes?: outputs.DataLossPreventionDiscoveryConfigTargetCloudStorageTargetFilterCollectionIncludeRegexes;
}

export interface DataLossPreventionDiscoveryConfigTargetCloudStorageTargetFilterCollectionIncludeRegexes {
    /**
     * The group of regular expression patterns to match against one or more file stores. Maximum of 100 entries. The sum of all lengths of regular expressions can't exceed 10 KiB.
     */
    patterns?: outputs.DataLossPreventionDiscoveryConfigTargetCloudStorageTargetFilterCollectionIncludeRegexesPattern[];
}

export interface DataLossPreventionDiscoveryConfigTargetCloudStorageTargetFilterCollectionIncludeRegexesPattern {
    /**
     * Regex for Cloud Storage.
     */
    cloudStorageRegex?: outputs.DataLossPreventionDiscoveryConfigTargetCloudStorageTargetFilterCollectionIncludeRegexesPatternCloudStorageRegex;
}

export interface DataLossPreventionDiscoveryConfigTargetCloudStorageTargetFilterCollectionIncludeRegexesPatternCloudStorageRegex {
    /**
     * Regex to test the bucket name against. If empty, all buckets match. Example: "marketing2021" or "(marketing)\d{4}" will both match the bucket gs://marketing2021
     */
    bucketNameRegex?: string;
    /**
     * For organizations, if unset, will match all projects.
     */
    projectIdRegex?: string;
}

export interface DataLossPreventionDiscoveryConfigTargetCloudStorageTargetFilterOthers {
}

export interface DataLossPreventionDiscoveryConfigTargetCloudStorageTargetGenerationCadence {
    /**
     * Governs when to update data profiles when the inspection rules defined by the 'InspectTemplate' change. If not set, changing the template will not cause a data profile to update.
     */
    inspectTemplateModifiedCadence?: outputs.DataLossPreventionDiscoveryConfigTargetCloudStorageTargetGenerationCadenceInspectTemplateModifiedCadence;
    /**
     * Data changes in Cloud Storage can't trigger reprofiling. If you set this field, profiles are refreshed at this frequency regardless of whether the underlying buckets have changes. Defaults to never. Possible values: ["UPDATE_FREQUENCY_NEVER", "UPDATE_FREQUENCY_DAILY", "UPDATE_FREQUENCY_MONTHLY"]
     */
    refreshFrequency?: string;
}

export interface DataLossPreventionDiscoveryConfigTargetCloudStorageTargetGenerationCadenceInspectTemplateModifiedCadence {
    /**
     * How frequently data profiles can be updated when the template is modified. Defaults to never. Possible values: ["UPDATE_FREQUENCY_NEVER", "UPDATE_FREQUENCY_DAILY", "UPDATE_FREQUENCY_MONTHLY"]
     */
    frequency?: string;
}

export interface DataLossPreventionDiscoveryConfigTargetSecretsTarget {
}

export interface DataLossPreventionDiscoveryConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DataLossPreventionInspectTemplateInspectConfig {
    /**
     * List of options defining data content to scan. If empty, text, images, and other content will be included. Possible values: ["CONTENT_TEXT", "CONTENT_IMAGE"]
     */
    contentOptions?: string[];
    /**
     * Custom info types to be used. See https://cloud.google.com/dlp/docs/creating-custom-infotypes to learn more.
     */
    customInfoTypes?: outputs.DataLossPreventionInspectTemplateInspectConfigCustomInfoType[];
    /**
     * When true, excludes type information of the findings.
     */
    excludeInfoTypes?: boolean;
    /**
     * When true, a contextual quote from the data that triggered a finding is included in the response.
     */
    includeQuote?: boolean;
    /**
     * Restricts what infoTypes to look for. The values must correspond to InfoType values returned by infoTypes.list
     * or listed at https://cloud.google.com/dlp/docs/infotypes-reference.
     *
     * When no InfoTypes or CustomInfoTypes are specified in a request, the system may automatically choose what detectors to run.
     * By default this may be all types, but may change over time as detectors are updated.
     */
    infoTypes?: outputs.DataLossPreventionInspectTemplateInspectConfigInfoType[];
    /**
     * Configuration to control the number of findings returned.
     */
    limits?: outputs.DataLossPreventionInspectTemplateInspectConfigLimits;
    /**
     * Only returns findings equal or above this threshold. See https://cloud.google.com/dlp/docs/likelihood for more info Default value: "POSSIBLE" Possible values: ["VERY_UNLIKELY", "UNLIKELY", "POSSIBLE", "LIKELY", "VERY_LIKELY"]
     */
    minLikelihood?: string;
    /**
     * Set of rules to apply to the findings for this InspectConfig. Exclusion rules, contained in the set are executed in the end,
     * other rules are executed in the order they are specified for each info type.
     */
    ruleSets?: outputs.DataLossPreventionInspectTemplateInspectConfigRuleSet[];
}

export interface DataLossPreventionInspectTemplateInspectConfigCustomInfoType {
    /**
     * Dictionary which defines the rule.
     */
    dictionary?: outputs.DataLossPreventionInspectTemplateInspectConfigCustomInfoTypeDictionary;
    /**
     * If set to EXCLUSION_TYPE_EXCLUDE this infoType will not cause a finding to be returned. It still can be used for rules matching. Possible values: ["EXCLUSION_TYPE_EXCLUDE"]
     */
    exclusionType?: string;
    /**
     * CustomInfoType can either be a new infoType, or an extension of built-in infoType, when the name matches one of existing
     * infoTypes and that infoType is specified in 'info_types' field. Specifying the latter adds findings to the
     * one detected by the system. If built-in info type is not specified in 'info_types' list then the name is
     * treated as a custom info type.
     */
    infoType: outputs.DataLossPreventionInspectTemplateInspectConfigCustomInfoTypeInfoType;
    /**
     * Likelihood to return for this CustomInfoType. This base value can be altered by a detection rule if the finding meets the criteria
     * specified by the rule. Default value: "VERY_LIKELY" Possible values: ["VERY_UNLIKELY", "UNLIKELY", "POSSIBLE", "LIKELY", "VERY_LIKELY"]
     */
    likelihood?: string;
    /**
     * Regular expression which defines the rule.
     */
    regex?: outputs.DataLossPreventionInspectTemplateInspectConfigCustomInfoTypeRegex;
    /**
     * Optional custom sensitivity for this InfoType. This only applies to data profiling.
     */
    sensitivityScore?: outputs.DataLossPreventionInspectTemplateInspectConfigCustomInfoTypeSensitivityScore;
    /**
     * A reference to a StoredInfoType to use with scanning.
     */
    storedType?: outputs.DataLossPreventionInspectTemplateInspectConfigCustomInfoTypeStoredType;
    /**
     * Message for detecting output from deidentification transformations that support reversing.
     */
    surrogateType?: outputs.DataLossPreventionInspectTemplateInspectConfigCustomInfoTypeSurrogateType;
}

export interface DataLossPreventionInspectTemplateInspectConfigCustomInfoTypeDictionary {
    /**
     * Newline-delimited file of words in Cloud Storage. Only a single file is accepted.
     */
    cloudStoragePath?: outputs.DataLossPreventionInspectTemplateInspectConfigCustomInfoTypeDictionaryCloudStoragePath;
    /**
     * List of words or phrases to search for.
     */
    wordList?: outputs.DataLossPreventionInspectTemplateInspectConfigCustomInfoTypeDictionaryWordList;
}

export interface DataLossPreventionInspectTemplateInspectConfigCustomInfoTypeDictionaryCloudStoragePath {
    /**
     * A url representing a file or path (no wildcards) in Cloud Storage. Example: 'gs://[BUCKET_NAME]/dictionary.txt'
     */
    path: string;
}

export interface DataLossPreventionInspectTemplateInspectConfigCustomInfoTypeDictionaryWordList {
    /**
     * Words or phrases defining the dictionary. The dictionary must contain at least one
     * phrase and every phrase must contain at least 2 characters that are letters or digits.
     */
    words: string[];
}

export interface DataLossPreventionInspectTemplateInspectConfigCustomInfoTypeInfoType {
    /**
     * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names
     * listed at https://cloud.google.com/dlp/docs/infotypes-reference when specifying a built-in type.
     */
    name: string;
    /**
     * Optional custom sensitivity for this InfoType. This only applies to data profiling.
     */
    sensitivityScore?: outputs.DataLossPreventionInspectTemplateInspectConfigCustomInfoTypeInfoTypeSensitivityScore;
    /**
     * Version name for this InfoType.
     */
    version?: string;
}

export interface DataLossPreventionInspectTemplateInspectConfigCustomInfoTypeInfoTypeSensitivityScore {
    /**
     * The sensitivity score applied to the resource. Possible values: ["SENSITIVITY_LOW", "SENSITIVITY_MODERATE", "SENSITIVITY_HIGH"]
     */
    score: string;
}

export interface DataLossPreventionInspectTemplateInspectConfigCustomInfoTypeRegex {
    /**
     * The index of the submatch to extract as findings. When not specified, the entire match is returned. No more than 3 may be included.
     */
    groupIndexes?: number[];
    /**
     * Pattern defining the regular expression.
     * Its syntax (https://github.com/google/re2/wiki/Syntax) can be found under the google/re2 repository on GitHub.
     */
    pattern: string;
}

export interface DataLossPreventionInspectTemplateInspectConfigCustomInfoTypeSensitivityScore {
    /**
     * The sensitivity score applied to the resource. Possible values: ["SENSITIVITY_LOW", "SENSITIVITY_MODERATE", "SENSITIVITY_HIGH"]
     */
    score: string;
}

export interface DataLossPreventionInspectTemplateInspectConfigCustomInfoTypeStoredType {
    /**
     * Resource name of the requested StoredInfoType, for example 'organizations/433245324/storedInfoTypes/432452342'
     * or 'projects/project-id/storedInfoTypes/432452342'.
     */
    name: string;
}

export interface DataLossPreventionInspectTemplateInspectConfigCustomInfoTypeSurrogateType {
}

export interface DataLossPreventionInspectTemplateInspectConfigInfoType {
    /**
     * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names listed
     * at https://cloud.google.com/dlp/docs/infotypes-reference when specifying a built-in type.
     */
    name: string;
    /**
     * Optional custom sensitivity for this InfoType. This only applies to data profiling.
     */
    sensitivityScore?: outputs.DataLossPreventionInspectTemplateInspectConfigInfoTypeSensitivityScore;
    /**
     * Version of the information type to use. By default, the version is set to stable
     */
    version?: string;
}

export interface DataLossPreventionInspectTemplateInspectConfigInfoTypeSensitivityScore {
    /**
     * The sensitivity score applied to the resource. Possible values: ["SENSITIVITY_LOW", "SENSITIVITY_MODERATE", "SENSITIVITY_HIGH"]
     */
    score: string;
}

export interface DataLossPreventionInspectTemplateInspectConfigLimits {
    /**
     * Configuration of findings limit given for specified infoTypes.
     */
    maxFindingsPerInfoTypes?: outputs.DataLossPreventionInspectTemplateInspectConfigLimitsMaxFindingsPerInfoType[];
    /**
     * Max number of findings that will be returned for each item scanned. The maximum returned is 2000.
     */
    maxFindingsPerItem: number;
    /**
     * Max number of findings that will be returned per request/job. The maximum returned is 2000.
     */
    maxFindingsPerRequest: number;
}

export interface DataLossPreventionInspectTemplateInspectConfigLimitsMaxFindingsPerInfoType {
    /**
     * Type of information the findings limit applies to. Only one limit per infoType should be provided. If InfoTypeLimit does
     * not have an infoType, the DLP API applies the limit against all infoTypes that are found but not
     * specified in another InfoTypeLimit.
     */
    infoType?: outputs.DataLossPreventionInspectTemplateInspectConfigLimitsMaxFindingsPerInfoTypeInfoType;
    /**
     * Max findings limit for the given infoType.
     */
    maxFindings: number;
}

export interface DataLossPreventionInspectTemplateInspectConfigLimitsMaxFindingsPerInfoTypeInfoType {
    /**
     * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names listed
     * at https://cloud.google.com/dlp/docs/infotypes-reference when specifying a built-in type.
     */
    name: string;
    /**
     * Optional custom sensitivity for this InfoType. This only applies to data profiling.
     */
    sensitivityScore?: outputs.DataLossPreventionInspectTemplateInspectConfigLimitsMaxFindingsPerInfoTypeInfoTypeSensitivityScore;
    /**
     * Version name for this InfoType.
     */
    version?: string;
}

export interface DataLossPreventionInspectTemplateInspectConfigLimitsMaxFindingsPerInfoTypeInfoTypeSensitivityScore {
    /**
     * The sensitivity score applied to the resource. Possible values: ["SENSITIVITY_LOW", "SENSITIVITY_MODERATE", "SENSITIVITY_HIGH"]
     */
    score: string;
}

export interface DataLossPreventionInspectTemplateInspectConfigRuleSet {
    /**
     * List of infoTypes this rule set is applied to.
     */
    infoTypes: outputs.DataLossPreventionInspectTemplateInspectConfigRuleSetInfoType[];
    /**
     * Set of rules to be applied to infoTypes. The rules are applied in order.
     */
    rules: outputs.DataLossPreventionInspectTemplateInspectConfigRuleSetRule[];
}

export interface DataLossPreventionInspectTemplateInspectConfigRuleSetInfoType {
    /**
     * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names listed
     * at https://cloud.google.com/dlp/docs/infotypes-reference when specifying a built-in type.
     */
    name: string;
    /**
     * Optional custom sensitivity for this InfoType. This only applies to data profiling.
     */
    sensitivityScore?: outputs.DataLossPreventionInspectTemplateInspectConfigRuleSetInfoTypeSensitivityScore;
    /**
     * Version name for this InfoType.
     */
    version?: string;
}

export interface DataLossPreventionInspectTemplateInspectConfigRuleSetInfoTypeSensitivityScore {
    /**
     * The sensitivity score applied to the resource. Possible values: ["SENSITIVITY_LOW", "SENSITIVITY_MODERATE", "SENSITIVITY_HIGH"]
     */
    score: string;
}

export interface DataLossPreventionInspectTemplateInspectConfigRuleSetRule {
    /**
     * The rule that specifies conditions when findings of infoTypes specified in InspectionRuleSet are removed from results.
     */
    exclusionRule?: outputs.DataLossPreventionInspectTemplateInspectConfigRuleSetRuleExclusionRule;
    /**
     * Hotword-based detection rule.
     */
    hotwordRule?: outputs.DataLossPreventionInspectTemplateInspectConfigRuleSetRuleHotwordRule;
}

export interface DataLossPreventionInspectTemplateInspectConfigRuleSetRuleExclusionRule {
    /**
     * Dictionary which defines the rule.
     */
    dictionary?: outputs.DataLossPreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleDictionary;
    /**
     * Drop if the hotword rule is contained in the proximate context.
     * For tabular data, the context includes the column name.
     */
    excludeByHotword?: outputs.DataLossPreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleExcludeByHotword;
    /**
     * Set of infoTypes for which findings would affect this rule.
     */
    excludeInfoTypes?: outputs.DataLossPreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleExcludeInfoTypes;
    /**
     * How the rule is applied. See the documentation for more information: https://cloud.google.com/dlp/docs/reference/rest/v2/InspectConfig#MatchingType Possible values: ["MATCHING_TYPE_FULL_MATCH", "MATCHING_TYPE_PARTIAL_MATCH", "MATCHING_TYPE_INVERSE_MATCH"]
     */
    matchingType: string;
    /**
     * Regular expression which defines the rule.
     */
    regex?: outputs.DataLossPreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleRegex;
}

export interface DataLossPreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleDictionary {
    /**
     * Newline-delimited file of words in Cloud Storage. Only a single file is accepted.
     */
    cloudStoragePath?: outputs.DataLossPreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleDictionaryCloudStoragePath;
    /**
     * List of words or phrases to search for.
     */
    wordList?: outputs.DataLossPreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleDictionaryWordList;
}

export interface DataLossPreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleDictionaryCloudStoragePath {
    /**
     * A url representing a file or path (no wildcards) in Cloud Storage. Example: 'gs://[BUCKET_NAME]/dictionary.txt'
     */
    path: string;
}

export interface DataLossPreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleDictionaryWordList {
    /**
     * Words or phrases defining the dictionary. The dictionary must contain at least one
     * phrase and every phrase must contain at least 2 characters that are letters or digits.
     */
    words: string[];
}

export interface DataLossPreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleExcludeByHotword {
    /**
     * Regular expression pattern defining what qualifies as a hotword.
     */
    hotwordRegex: outputs.DataLossPreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleExcludeByHotwordHotwordRegex;
    /**
     * Proximity of the finding within which the entire hotword must reside. The total length of the window cannot
     * exceed 1000 characters. Note that the finding itself will be included in the window, so that hotwords may be
     * used to match substrings of the finding itself. For example, the certainty of a phone number regex
     * '(\d{3}) \d{3}-\d{4}' could be adjusted upwards if the area code is known to be the local area code of a company
     * office using the hotword regex '(xxx)', where 'xxx' is the area code in question.
     */
    proximity: outputs.DataLossPreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleExcludeByHotwordProximity;
}

export interface DataLossPreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleExcludeByHotwordHotwordRegex {
    /**
     * The index of the submatch to extract as findings. When not specified,
     * the entire match is returned. No more than 3 may be included.
     */
    groupIndexes?: number[];
    /**
     * Pattern defining the regular expression. Its syntax
     * (https://github.com/google/re2/wiki/Syntax) can be found under the google/re2 repository on GitHub.
     */
    pattern: string;
}

export interface DataLossPreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleExcludeByHotwordProximity {
    /**
     * Number of characters after the finding to consider.
     */
    windowAfter?: number;
    /**
     * Number of characters before the finding to consider.
     */
    windowBefore?: number;
}

export interface DataLossPreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleExcludeInfoTypes {
    /**
     * If a finding is matched by any of the infoType detectors listed here, the finding will be excluded from the scan results.
     */
    infoTypes: outputs.DataLossPreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleExcludeInfoTypesInfoType[];
}

export interface DataLossPreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleExcludeInfoTypesInfoType {
    /**
     * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names listed
     * at https://cloud.google.com/dlp/docs/infotypes-reference when specifying a built-in type.
     */
    name: string;
    /**
     * Optional custom sensitivity for this InfoType. This only applies to data profiling.
     */
    sensitivityScore?: outputs.DataLossPreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleExcludeInfoTypesInfoTypeSensitivityScore;
    /**
     * Version name for this InfoType.
     */
    version?: string;
}

export interface DataLossPreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleExcludeInfoTypesInfoTypeSensitivityScore {
    /**
     * The sensitivity score applied to the resource. Possible values: ["SENSITIVITY_LOW", "SENSITIVITY_MODERATE", "SENSITIVITY_HIGH"]
     */
    score: string;
}

export interface DataLossPreventionInspectTemplateInspectConfigRuleSetRuleExclusionRuleRegex {
    /**
     * The index of the submatch to extract as findings. When not specified, the entire match is returned. No more than 3 may be included.
     */
    groupIndexes?: number[];
    /**
     * Pattern defining the regular expression.
     * Its syntax (https://github.com/google/re2/wiki/Syntax) can be found under the google/re2 repository on GitHub.
     */
    pattern: string;
}

export interface DataLossPreventionInspectTemplateInspectConfigRuleSetRuleHotwordRule {
    /**
     * Regular expression pattern defining what qualifies as a hotword.
     */
    hotwordRegex: outputs.DataLossPreventionInspectTemplateInspectConfigRuleSetRuleHotwordRuleHotwordRegex;
    /**
     * Likelihood adjustment to apply to all matching findings.
     */
    likelihoodAdjustment: outputs.DataLossPreventionInspectTemplateInspectConfigRuleSetRuleHotwordRuleLikelihoodAdjustment;
    /**
     * Proximity of the finding within which the entire hotword must reside. The total length of the window cannot
     * exceed 1000 characters. Note that the finding itself will be included in the window, so that hotwords may be
     * used to match substrings of the finding itself. For example, the certainty of a phone number regex
     * '(\d{3}) \d{3}-\d{4}' could be adjusted upwards if the area code is known to be the local area code of a company
     * office using the hotword regex '(xxx)', where 'xxx' is the area code in question.
     */
    proximity: outputs.DataLossPreventionInspectTemplateInspectConfigRuleSetRuleHotwordRuleProximity;
}

export interface DataLossPreventionInspectTemplateInspectConfigRuleSetRuleHotwordRuleHotwordRegex {
    /**
     * The index of the submatch to extract as findings. When not specified,
     * the entire match is returned. No more than 3 may be included.
     */
    groupIndexes?: number[];
    /**
     * Pattern defining the regular expression. Its syntax
     * (https://github.com/google/re2/wiki/Syntax) can be found under the google/re2 repository on GitHub.
     */
    pattern: string;
}

export interface DataLossPreventionInspectTemplateInspectConfigRuleSetRuleHotwordRuleLikelihoodAdjustment {
    /**
     * Set the likelihood of a finding to a fixed value. Either this or relative_likelihood can be set. Possible values: ["VERY_UNLIKELY", "UNLIKELY", "POSSIBLE", "LIKELY", "VERY_LIKELY"]
     */
    fixedLikelihood?: string;
    /**
     * Increase or decrease the likelihood by the specified number of levels. For example,
     * if a finding would be POSSIBLE without the detection rule and relativeLikelihood is 1,
     * then it is upgraded to LIKELY, while a value of -1 would downgrade it to UNLIKELY.
     * Likelihood may never drop below VERY_UNLIKELY or exceed VERY_LIKELY, so applying an
     * adjustment of 1 followed by an adjustment of -1 when base likelihood is VERY_LIKELY
     * will result in a final likelihood of LIKELY. Either this or fixed_likelihood can be set.
     */
    relativeLikelihood?: number;
}

export interface DataLossPreventionInspectTemplateInspectConfigRuleSetRuleHotwordRuleProximity {
    /**
     * Number of characters after the finding to consider. Either this or window_before must be specified
     */
    windowAfter?: number;
    /**
     * Number of characters before the finding to consider. Either this or window_after must be specified
     */
    windowBefore?: number;
}

export interface DataLossPreventionInspectTemplateTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DataLossPreventionJobTriggerInspectJob {
    actions?: outputs.DataLossPreventionJobTriggerInspectJobAction[];
    /**
     * The core content of the template.
     */
    inspectConfig?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfig;
    /**
     * The name of the template to run when this job is triggered.
     */
    inspectTemplateName?: string;
    /**
     * Information on where to inspect
     */
    storageConfig: outputs.DataLossPreventionJobTriggerInspectJobStorageConfig;
}

export interface DataLossPreventionJobTriggerInspectJobAction {
    /**
     * Create a de-identified copy of the requested table or files.
     */
    deidentify?: outputs.DataLossPreventionJobTriggerInspectJobActionDeidentify;
    /**
     * Sends an email when the job completes. The email goes to IAM project owners and technical Essential Contacts.
     */
    jobNotificationEmails?: outputs.DataLossPreventionJobTriggerInspectJobActionJobNotificationEmails;
    /**
     * Publish a message into a given Pub/Sub topic when the job completes.
     */
    pubSub?: outputs.DataLossPreventionJobTriggerInspectJobActionPubSub;
    /**
     * Publish findings of a DlpJob to Data Catalog.
     */
    publishFindingsToCloudDataCatalog?: outputs.DataLossPreventionJobTriggerInspectJobActionPublishFindingsToCloudDataCatalog;
    /**
     * Publish the result summary of a DlpJob to the Cloud Security Command Center.
     */
    publishSummaryToCscc?: outputs.DataLossPreventionJobTriggerInspectJobActionPublishSummaryToCscc;
    /**
     * Enable Stackdriver metric dlp.googleapis.com/findingCount.
     */
    publishToStackdriver?: outputs.DataLossPreventionJobTriggerInspectJobActionPublishToStackdriver;
    /**
     * If set, the detailed findings will be persisted to the specified OutputStorageConfig. Only a single instance of this action can be specified. Compatible with: Inspect, Risk
     */
    saveFindings?: outputs.DataLossPreventionJobTriggerInspectJobActionSaveFindings;
}

export interface DataLossPreventionJobTriggerInspectJobActionDeidentify {
    /**
     * User settable Cloud Storage bucket and folders to store de-identified files.
     *
     * This field must be set for cloud storage deidentification.
     *
     * The output Cloud Storage bucket must be different from the input bucket.
     *
     * De-identified files will overwrite files in the output path.
     *
     * Form of: gs://bucket/folder/ or gs://bucket
     */
    cloudStorageOutput: string;
    /**
     * List of user-specified file type groups to transform. If specified, only the files with these filetypes will be transformed.
     *
     * If empty, all supported files will be transformed. Supported types may be automatically added over time.
     *
     * If a file type is set in this field that isn't supported by the Deidentify action then the job will fail and will not be successfully created/started. Possible values: ["IMAGE", "TEXT_FILE", "CSV", "TSV"]
     */
    fileTypesToTransforms?: string[];
    /**
     * User specified deidentify templates and configs for structured, unstructured, and image files.
     */
    transformationConfig?: outputs.DataLossPreventionJobTriggerInspectJobActionDeidentifyTransformationConfig;
    /**
     * Config for storing transformation details.
     */
    transformationDetailsStorageConfig?: outputs.DataLossPreventionJobTriggerInspectJobActionDeidentifyTransformationDetailsStorageConfig;
}

export interface DataLossPreventionJobTriggerInspectJobActionDeidentifyTransformationConfig {
    /**
     * If this template is specified, it will serve as the default de-identify template.
     */
    deidentifyTemplate?: string;
    /**
     * If this template is specified, it will serve as the de-identify template for images.
     */
    imageRedactTemplate?: string;
    /**
     * If this template is specified, it will serve as the de-identify template for structured content such as delimited files and tables.
     */
    structuredDeidentifyTemplate?: string;
}

export interface DataLossPreventionJobTriggerInspectJobActionDeidentifyTransformationDetailsStorageConfig {
    /**
     * The BigQuery table in which to store the output.
     */
    table: outputs.DataLossPreventionJobTriggerInspectJobActionDeidentifyTransformationDetailsStorageConfigTable;
}

export interface DataLossPreventionJobTriggerInspectJobActionDeidentifyTransformationDetailsStorageConfigTable {
    /**
     * The ID of the dataset containing this table.
     */
    datasetId: string;
    /**
     * The ID of the project containing this table.
     */
    projectId: string;
    /**
     * The ID of the table. The ID must contain only letters (a-z,
     * A-Z), numbers (0-9), or underscores (_). The maximum length
     * is 1,024 characters.
     */
    tableId?: string;
}

export interface DataLossPreventionJobTriggerInspectJobActionJobNotificationEmails {
}

export interface DataLossPreventionJobTriggerInspectJobActionPubSub {
    /**
     * Cloud Pub/Sub topic to send notifications to.
     */
    topic: string;
}

export interface DataLossPreventionJobTriggerInspectJobActionPublishFindingsToCloudDataCatalog {
}

export interface DataLossPreventionJobTriggerInspectJobActionPublishSummaryToCscc {
}

export interface DataLossPreventionJobTriggerInspectJobActionPublishToStackdriver {
}

export interface DataLossPreventionJobTriggerInspectJobActionSaveFindings {
    /**
     * Information on where to store output
     */
    outputConfig: outputs.DataLossPreventionJobTriggerInspectJobActionSaveFindingsOutputConfig;
}

export interface DataLossPreventionJobTriggerInspectJobActionSaveFindingsOutputConfig {
    /**
     * Schema used for writing the findings for Inspect jobs. This field is only used for
     * Inspect and must be unspecified for Risk jobs. Columns are derived from the Finding
     * object. If appending to an existing table, any columns from the predefined schema
     * that are missing will be added. No columns in the existing table will be deleted.
     *
     * If unspecified, then all available columns will be used for a new table or an (existing)
     * table with no schema, and no changes will be made to an existing table that has a schema.
     * Only for use with external storage. Possible values: ["BASIC_COLUMNS", "GCS_COLUMNS", "DATASTORE_COLUMNS", "BIG_QUERY_COLUMNS", "ALL_COLUMNS"]
     */
    outputSchema?: string;
    /**
     * Information on the location of the target BigQuery Table.
     */
    table: outputs.DataLossPreventionJobTriggerInspectJobActionSaveFindingsOutputConfigTable;
}

export interface DataLossPreventionJobTriggerInspectJobActionSaveFindingsOutputConfigTable {
    /**
     * Dataset ID of the table.
     */
    datasetId: string;
    /**
     * The Google Cloud Platform project ID of the project containing the table.
     */
    projectId: string;
    /**
     * Name of the table. If is not set a new one will be generated for you with the following format:
     * 'dlp_googleapis_yyyy_mm_dd_[dlp_job_id]'. Pacific timezone will be used for generating the date details.
     */
    tableId?: string;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfig {
    /**
     * Custom info types to be used. See https://cloud.google.com/dlp/docs/creating-custom-infotypes to learn more.
     */
    customInfoTypes?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigCustomInfoType[];
    /**
     * When true, excludes type information of the findings.
     */
    excludeInfoTypes?: boolean;
    /**
     * When true, a contextual quote from the data that triggered a finding is included in the response.
     */
    includeQuote?: boolean;
    /**
     * Restricts what infoTypes to look for. The values must correspond to InfoType values returned by infoTypes.list
     * or listed at https://cloud.google.com/dlp/docs/infotypes-reference.
     *
     * When no InfoTypes or CustomInfoTypes are specified in a request, the system may automatically choose what detectors to run.
     * By default this may be all types, but may change over time as detectors are updated.
     */
    infoTypes?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigInfoType[];
    /**
     * Configuration to control the number of findings returned.
     */
    limits?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigLimits;
    /**
     * Only returns findings equal or above this threshold. See https://cloud.google.com/dlp/docs/likelihood for more info Default value: "POSSIBLE" Possible values: ["VERY_UNLIKELY", "UNLIKELY", "POSSIBLE", "LIKELY", "VERY_LIKELY"]
     */
    minLikelihood?: string;
    /**
     * Set of rules to apply to the findings for this InspectConfig. Exclusion rules, contained in the set are executed in the end,
     * other rules are executed in the order they are specified for each info type.
     */
    ruleSets?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigRuleSet[];
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigCustomInfoType {
    /**
     * Dictionary which defines the rule.
     */
    dictionary?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigCustomInfoTypeDictionary;
    /**
     * If set to EXCLUSION_TYPE_EXCLUDE this infoType will not cause a finding to be returned. It still can be used for rules matching. Possible values: ["EXCLUSION_TYPE_EXCLUDE"]
     */
    exclusionType?: string;
    /**
     * CustomInfoType can either be a new infoType, or an extension of built-in infoType, when the name matches one of existing
     * infoTypes and that infoType is specified in 'info_types' field. Specifying the latter adds findings to the
     * one detected by the system. If built-in info type is not specified in 'info_types' list then the name is
     * treated as a custom info type.
     */
    infoType: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigCustomInfoTypeInfoType;
    /**
     * Likelihood to return for this CustomInfoType. This base value can be altered by a detection rule if the finding meets the criteria
     * specified by the rule. Default value: "VERY_LIKELY" Possible values: ["VERY_UNLIKELY", "UNLIKELY", "POSSIBLE", "LIKELY", "VERY_LIKELY"]
     */
    likelihood?: string;
    /**
     * Regular expression which defines the rule.
     */
    regex?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigCustomInfoTypeRegex;
    /**
     * Optional custom sensitivity for this InfoType. This only applies to data profiling.
     */
    sensitivityScore?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigCustomInfoTypeSensitivityScore;
    /**
     * A reference to a StoredInfoType to use with scanning.
     */
    storedType?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigCustomInfoTypeStoredType;
    /**
     * Message for detecting output from deidentification transformations that support reversing.
     */
    surrogateType?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigCustomInfoTypeSurrogateType;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigCustomInfoTypeDictionary {
    /**
     * Newline-delimited file of words in Cloud Storage. Only a single file is accepted.
     */
    cloudStoragePath?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigCustomInfoTypeDictionaryCloudStoragePath;
    /**
     * List of words or phrases to search for.
     */
    wordList?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigCustomInfoTypeDictionaryWordList;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigCustomInfoTypeDictionaryCloudStoragePath {
    /**
     * A url representing a file or path (no wildcards) in Cloud Storage. Example: 'gs://[BUCKET_NAME]/dictionary.txt'
     */
    path: string;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigCustomInfoTypeDictionaryWordList {
    /**
     * Words or phrases defining the dictionary. The dictionary must contain at least one
     * phrase and every phrase must contain at least 2 characters that are letters or digits.
     */
    words: string[];
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigCustomInfoTypeInfoType {
    /**
     * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names
     * listed at https://cloud.google.com/dlp/docs/infotypes-reference when specifying a built-in type.
     */
    name: string;
    /**
     * Optional custom sensitivity for this InfoType. This only applies to data profiling.
     */
    sensitivityScore?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigCustomInfoTypeInfoTypeSensitivityScore;
    /**
     * Version of the information type to use. By default, the version is set to stable.
     */
    version?: string;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigCustomInfoTypeInfoTypeSensitivityScore {
    /**
     * The sensitivity score applied to the resource. Possible values: ["SENSITIVITY_LOW", "SENSITIVITY_MODERATE", "SENSITIVITY_HIGH"]
     */
    score: string;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigCustomInfoTypeRegex {
    /**
     * The index of the submatch to extract as findings. When not specified, the entire match is returned. No more than 3 may be included.
     */
    groupIndexes?: number[];
    /**
     * Pattern defining the regular expression.
     * Its syntax (https://github.com/google/re2/wiki/Syntax) can be found under the google/re2 repository on GitHub.
     */
    pattern: string;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigCustomInfoTypeSensitivityScore {
    /**
     * The sensitivity score applied to the resource. Possible values: ["SENSITIVITY_LOW", "SENSITIVITY_MODERATE", "SENSITIVITY_HIGH"]
     */
    score: string;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigCustomInfoTypeStoredType {
    /**
     * The creation timestamp of an inspectTemplate. Set by the server.
     */
    createTime: string;
    /**
     * Resource name of the requested StoredInfoType, for example 'organizations/433245324/storedInfoTypes/432452342'
     * or 'projects/project-id/storedInfoTypes/432452342'.
     */
    name: string;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigCustomInfoTypeSurrogateType {
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigInfoType {
    /**
     * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names listed
     * at https://cloud.google.com/dlp/docs/infotypes-reference when specifying a built-in type.
     */
    name: string;
    /**
     * Optional custom sensitivity for this InfoType. This only applies to data profiling.
     */
    sensitivityScore?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigInfoTypeSensitivityScore;
    /**
     * Version of the information type to use. By default, the version is set to stable
     */
    version?: string;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigInfoTypeSensitivityScore {
    /**
     * The sensitivity score applied to the resource. Possible values: ["SENSITIVITY_LOW", "SENSITIVITY_MODERATE", "SENSITIVITY_HIGH"]
     */
    score: string;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigLimits {
    /**
     * Configuration of findings limit given for specified infoTypes.
     */
    maxFindingsPerInfoTypes?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigLimitsMaxFindingsPerInfoType[];
    /**
     * Max number of findings that will be returned for each item scanned. The maximum returned is 2000.
     */
    maxFindingsPerItem?: number;
    /**
     * Max number of findings that will be returned per request/job. The maximum returned is 2000.
     */
    maxFindingsPerRequest?: number;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigLimitsMaxFindingsPerInfoType {
    /**
     * Type of information the findings limit applies to. Only one limit per infoType should be provided. If InfoTypeLimit does
     * not have an infoType, the DLP API applies the limit against all infoTypes that are found but not
     * specified in another InfoTypeLimit.
     */
    infoType?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigLimitsMaxFindingsPerInfoTypeInfoType;
    /**
     * Max findings limit for the given infoType.
     */
    maxFindings?: number;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigLimitsMaxFindingsPerInfoTypeInfoType {
    /**
     * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names listed
     * at https://cloud.google.com/dlp/docs/infotypes-reference when specifying a built-in type.
     */
    name: string;
    /**
     * Optional custom sensitivity for this InfoType. This only applies to data profiling.
     */
    sensitivityScore?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigLimitsMaxFindingsPerInfoTypeInfoTypeSensitivityScore;
    /**
     * Version of the information type to use. By default, the version is set to stable
     */
    version?: string;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigLimitsMaxFindingsPerInfoTypeInfoTypeSensitivityScore {
    /**
     * The sensitivity score applied to the resource. Possible values: ["SENSITIVITY_LOW", "SENSITIVITY_MODERATE", "SENSITIVITY_HIGH"]
     */
    score: string;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigRuleSet {
    /**
     * List of infoTypes this rule set is applied to.
     */
    infoTypes?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetInfoType[];
    /**
     * Set of rules to be applied to infoTypes. The rules are applied in order.
     */
    rules: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRule[];
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetInfoType {
    /**
     * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names listed
     * at https://cloud.google.com/dlp/docs/infotypes-reference when specifying a built-in type.
     */
    name: string;
    /**
     * Optional custom sensitivity for this InfoType. This only applies to data profiling.
     */
    sensitivityScore?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetInfoTypeSensitivityScore;
    /**
     * Version of the information type to use. By default, the version is set to stable.
     */
    version?: string;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetInfoTypeSensitivityScore {
    /**
     * The sensitivity score applied to the resource. Possible values: ["SENSITIVITY_LOW", "SENSITIVITY_MODERATE", "SENSITIVITY_HIGH"]
     */
    score: string;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRule {
    /**
     * The rule that specifies conditions when findings of infoTypes specified in InspectionRuleSet are removed from results.
     */
    exclusionRule?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRule;
    /**
     * Hotword-based detection rule.
     */
    hotwordRule?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleHotwordRule;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRule {
    /**
     * Dictionary which defines the rule.
     */
    dictionary?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleDictionary;
    /**
     * Drop if the hotword rule is contained in the proximate context.
     */
    excludeByHotword?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleExcludeByHotword;
    /**
     * Set of infoTypes for which findings would affect this rule.
     */
    excludeInfoTypes?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleExcludeInfoTypes;
    /**
     * How the rule is applied. See the documentation for more information: https://cloud.google.com/dlp/docs/reference/rest/v2/InspectConfig#MatchingType Possible values: ["MATCHING_TYPE_FULL_MATCH", "MATCHING_TYPE_PARTIAL_MATCH", "MATCHING_TYPE_INVERSE_MATCH"]
     */
    matchingType: string;
    /**
     * Regular expression which defines the rule.
     */
    regex?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleRegex;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleDictionary {
    /**
     * Newline-delimited file of words in Cloud Storage. Only a single file is accepted.
     */
    cloudStoragePath?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleDictionaryCloudStoragePath;
    /**
     * List of words or phrases to search for.
     */
    wordList?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleDictionaryWordList;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleDictionaryCloudStoragePath {
    /**
     * A url representing a file or path (no wildcards) in Cloud Storage. Example: 'gs://[BUCKET_NAME]/dictionary.txt'
     */
    path: string;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleDictionaryWordList {
    /**
     * Words or phrases defining the dictionary. The dictionary must contain at least one
     * phrase and every phrase must contain at least 2 characters that are letters or digits.
     */
    words: string[];
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleExcludeByHotword {
    /**
     * Regular expression pattern defining what qualifies as a hotword.
     */
    hotwordRegex?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleExcludeByHotwordHotwordRegex;
    /**
     * Proximity of the finding within which the entire hotword must reside. The total length of the window cannot
     * exceed 1000 characters. Note that the finding itself will be included in the window, so that hotwords may be
     * used to match substrings of the finding itself. For example, the certainty of a phone number regex
     * '(\d{3}) \d{3}-\d{4}' could be adjusted upwards if the area code is known to be the local area code of a company
     * office using the hotword regex '(xxx)', where 'xxx' is the area code in question.
     */
    proximity?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleExcludeByHotwordProximity;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleExcludeByHotwordHotwordRegex {
    /**
     * The index of the submatch to extract as findings. When not specified,
     * the entire match is returned. No more than 3 may be included.
     */
    groupIndexes?: number[];
    /**
     * Pattern defining the regular expression. Its syntax
     * (https://github.com/google/re2/wiki/Syntax) can be found under the google/re2 repository on GitHub.
     */
    pattern?: string;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleExcludeByHotwordProximity {
    /**
     * Number of characters after the finding to consider. Either this or window_before must be specified
     */
    windowAfter?: number;
    /**
     * Number of characters before the finding to consider. Either this or window_after must be specified
     */
    windowBefore?: number;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleExcludeInfoTypes {
    /**
     * If a finding is matched by any of the infoType detectors listed here, the finding will be excluded from the scan results.
     */
    infoTypes: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleExcludeInfoTypesInfoType[];
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleExcludeInfoTypesInfoType {
    /**
     * Name of the information type. Either a name of your choosing when creating a CustomInfoType, or one of the names listed
     * at https://cloud.google.com/dlp/docs/infotypes-reference when specifying a built-in type.
     */
    name: string;
    /**
     * Optional custom sensitivity for this InfoType. This only applies to data profiling.
     */
    sensitivityScore?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleExcludeInfoTypesInfoTypeSensitivityScore;
    /**
     * Version of the information type to use. By default, the version is set to stable.
     */
    version?: string;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleExcludeInfoTypesInfoTypeSensitivityScore {
    /**
     * The sensitivity score applied to the resource. Possible values: ["SENSITIVITY_LOW", "SENSITIVITY_MODERATE", "SENSITIVITY_HIGH"]
     */
    score: string;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleExclusionRuleRegex {
    /**
     * The index of the submatch to extract as findings. When not specified, the entire match is returned. No more than 3 may be included.
     */
    groupIndexes?: number[];
    /**
     * Pattern defining the regular expression.
     * Its syntax (https://github.com/google/re2/wiki/Syntax) can be found under the google/re2 repository on GitHub.
     */
    pattern: string;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleHotwordRule {
    /**
     * Regular expression pattern defining what qualifies as a hotword.
     */
    hotwordRegex?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleHotwordRuleHotwordRegex;
    /**
     * Likelihood adjustment to apply to all matching findings.
     */
    likelihoodAdjustment?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleHotwordRuleLikelihoodAdjustment;
    /**
     * Proximity of the finding within which the entire hotword must reside. The total length of the window cannot
     * exceed 1000 characters. Note that the finding itself will be included in the window, so that hotwords may be
     * used to match substrings of the finding itself. For example, the certainty of a phone number regex
     * '(\d{3}) \d{3}-\d{4}' could be adjusted upwards if the area code is known to be the local area code of a company
     * office using the hotword regex '(xxx)', where 'xxx' is the area code in question.
     */
    proximity?: outputs.DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleHotwordRuleProximity;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleHotwordRuleHotwordRegex {
    /**
     * The index of the submatch to extract as findings. When not specified,
     * the entire match is returned. No more than 3 may be included.
     */
    groupIndexes?: number[];
    /**
     * Pattern defining the regular expression. Its syntax
     * (https://github.com/google/re2/wiki/Syntax) can be found under the google/re2 repository on GitHub.
     */
    pattern?: string;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleHotwordRuleLikelihoodAdjustment {
    /**
     * Set the likelihood of a finding to a fixed value. Either this or relative_likelihood can be set. Possible values: ["VERY_UNLIKELY", "UNLIKELY", "POSSIBLE", "LIKELY", "VERY_LIKELY"]
     */
    fixedLikelihood?: string;
    /**
     * Increase or decrease the likelihood by the specified number of levels. For example,
     * if a finding would be POSSIBLE without the detection rule and relativeLikelihood is 1,
     * then it is upgraded to LIKELY, while a value of -1 would downgrade it to UNLIKELY.
     * Likelihood may never drop below VERY_UNLIKELY or exceed VERY_LIKELY, so applying an
     * adjustment of 1 followed by an adjustment of -1 when base likelihood is VERY_LIKELY
     * will result in a final likelihood of LIKELY. Either this or fixed_likelihood can be set.
     */
    relativeLikelihood?: number;
}

export interface DataLossPreventionJobTriggerInspectJobInspectConfigRuleSetRuleHotwordRuleProximity {
    /**
     * Number of characters after the finding to consider. Either this or window_before must be specified
     */
    windowAfter?: number;
    /**
     * Number of characters before the finding to consider. Either this or window_after must be specified
     */
    windowBefore?: number;
}

export interface DataLossPreventionJobTriggerInspectJobStorageConfig {
    /**
     * Options defining BigQuery table and row identifiers.
     */
    bigQueryOptions?: outputs.DataLossPreventionJobTriggerInspectJobStorageConfigBigQueryOptions;
    /**
     * Options defining a file or a set of files within a Google Cloud Storage bucket.
     */
    cloudStorageOptions?: outputs.DataLossPreventionJobTriggerInspectJobStorageConfigCloudStorageOptions;
    /**
     * Options defining a data set within Google Cloud Datastore.
     */
    datastoreOptions?: outputs.DataLossPreventionJobTriggerInspectJobStorageConfigDatastoreOptions;
    /**
     * Configuration to control jobs where the content being inspected is outside of Google Cloud Platform.
     */
    hybridOptions?: outputs.DataLossPreventionJobTriggerInspectJobStorageConfigHybridOptions;
    /**
     * Configuration of the timespan of the items to include in scanning
     */
    timespanConfig?: outputs.DataLossPreventionJobTriggerInspectJobStorageConfigTimespanConfig;
}

export interface DataLossPreventionJobTriggerInspectJobStorageConfigBigQueryOptions {
    /**
     * References to fields excluded from scanning.
     * This allows you to skip inspection of entire columns which you know have no findings.
     */
    excludedFields?: outputs.DataLossPreventionJobTriggerInspectJobStorageConfigBigQueryOptionsExcludedField[];
    /**
     * Specifies the BigQuery fields that will be returned with findings.
     * If not specified, no identifying fields will be returned for findings.
     */
    identifyingFields?: outputs.DataLossPreventionJobTriggerInspectJobStorageConfigBigQueryOptionsIdentifyingField[];
    /**
     * Limit scanning only to these fields.
     */
    includedFields?: outputs.DataLossPreventionJobTriggerInspectJobStorageConfigBigQueryOptionsIncludedField[];
    /**
     * Max number of rows to scan. If the table has more rows than this value, the rest of the rows are omitted.
     * If not set, or if set to 0, all rows will be scanned. Only one of rowsLimit and rowsLimitPercent can be
     * specified. Cannot be used in conjunction with TimespanConfig.
     */
    rowsLimit?: number;
    /**
     * Max percentage of rows to scan. The rest are omitted. The number of rows scanned is rounded down.
     * Must be between 0 and 100, inclusively. Both 0 and 100 means no limit. Defaults to 0. Only one of
     * rowsLimit and rowsLimitPercent can be specified. Cannot be used in conjunction with TimespanConfig.
     */
    rowsLimitPercent?: number;
    /**
     * How to sample rows if not all rows are scanned. Meaningful only when used in conjunction with either
     * rowsLimit or rowsLimitPercent. If not specified, rows are scanned in the order BigQuery reads them.
     * If TimespanConfig is set, set this to an empty string to avoid using the default value. Default value: "TOP" Possible values: ["TOP", "RANDOM_START"]
     */
    sampleMethod?: string;
    /**
     * Set of files to scan.
     */
    tableReference: outputs.DataLossPreventionJobTriggerInspectJobStorageConfigBigQueryOptionsTableReference;
}

export interface DataLossPreventionJobTriggerInspectJobStorageConfigBigQueryOptionsExcludedField {
    /**
     * Name describing the field excluded from scanning.
     */
    name: string;
}

export interface DataLossPreventionJobTriggerInspectJobStorageConfigBigQueryOptionsIdentifyingField {
    /**
     * Name of a BigQuery field to be returned with the findings.
     */
    name: string;
}

export interface DataLossPreventionJobTriggerInspectJobStorageConfigBigQueryOptionsIncludedField {
    /**
     * Name describing the field to which scanning is limited.
     */
    name: string;
}

export interface DataLossPreventionJobTriggerInspectJobStorageConfigBigQueryOptionsTableReference {
    /**
     * The dataset ID of the table.
     */
    datasetId: string;
    /**
     * The Google Cloud Platform project ID of the project containing the table.
     */
    projectId: string;
    /**
     * The name of the table.
     */
    tableId: string;
}

export interface DataLossPreventionJobTriggerInspectJobStorageConfigCloudStorageOptions {
    /**
     * Max number of bytes to scan from a file. If a scanned file's size is bigger than this value
     * then the rest of the bytes are omitted.
     */
    bytesLimitPerFile?: number;
    /**
     * Max percentage of bytes to scan from a file. The rest are omitted. The number of bytes scanned is rounded down.
     * Must be between 0 and 100, inclusively. Both 0 and 100 means no limit.
     */
    bytesLimitPerFilePercent?: number;
    /**
     * Set of files to scan.
     */
    fileSet: outputs.DataLossPreventionJobTriggerInspectJobStorageConfigCloudStorageOptionsFileSet;
    /**
     * List of file type groups to include in the scan. If empty, all files are scanned and available data
     * format processors are applied. In addition, the binary content of the selected files is always scanned as well.
     * Images are scanned only as binary if the specified region does not support image inspection and no fileTypes were specified. Possible values: ["BINARY_FILE", "TEXT_FILE", "IMAGE", "WORD", "PDF", "AVRO", "CSV", "TSV", "POWERPOINT", "EXCEL"]
     */
    fileTypes?: string[];
    /**
     * Limits the number of files to scan to this percentage of the input FileSet. Number of files scanned is rounded down.
     * Must be between 0 and 100, inclusively. Both 0 and 100 means no limit.
     */
    filesLimitPercent?: number;
    /**
     * How to sample bytes if not all bytes are scanned. Meaningful only when used in conjunction with bytesLimitPerFile.
     * If not specified, scanning would start from the top. Possible values: ["TOP", "RANDOM_START"]
     */
    sampleMethod?: string;
}

export interface DataLossPreventionJobTriggerInspectJobStorageConfigCloudStorageOptionsFileSet {
    /**
     * The regex-filtered set of files to scan.
     */
    regexFileSet?: outputs.DataLossPreventionJobTriggerInspectJobStorageConfigCloudStorageOptionsFileSetRegexFileSet;
    /**
     * The Cloud Storage url of the file(s) to scan, in the format 'gs://<bucket>/<path>'. Trailing wildcard
     * in the path is allowed.
     *
     * If the url ends in a trailing slash, the bucket or directory represented by the url will be scanned
     * non-recursively (content in sub-directories will not be scanned). This means that 'gs://mybucket/' is
     * equivalent to 'gs://mybucket/*', and 'gs://mybucket/directory/' is equivalent to 'gs://mybucket/directory/*'.
     */
    url?: string;
}

export interface DataLossPreventionJobTriggerInspectJobStorageConfigCloudStorageOptionsFileSetRegexFileSet {
    /**
     * The name of a Cloud Storage bucket.
     */
    bucketName: string;
    /**
     * A list of regular expressions matching file paths to exclude. All files in the bucket that match at
     * least one of these regular expressions will be excluded from the scan.
     */
    excludeRegexes?: string[];
    /**
     * A list of regular expressions matching file paths to include. All files in the bucket
     * that match at least one of these regular expressions will be included in the set of files,
     * except for those that also match an item in excludeRegex. Leaving this field empty will
     * match all files by default (this is equivalent to including .* in the list)
     */
    includeRegexes?: string[];
}

export interface DataLossPreventionJobTriggerInspectJobStorageConfigDatastoreOptions {
    /**
     * A representation of a Datastore kind.
     */
    kind: outputs.DataLossPreventionJobTriggerInspectJobStorageConfigDatastoreOptionsKind;
    /**
     * Datastore partition ID. A partition ID identifies a grouping of entities. The grouping
     * is always by project and namespace, however the namespace ID may be empty.
     */
    partitionId: outputs.DataLossPreventionJobTriggerInspectJobStorageConfigDatastoreOptionsPartitionId;
}

export interface DataLossPreventionJobTriggerInspectJobStorageConfigDatastoreOptionsKind {
    /**
     * The name of the Datastore kind.
     */
    name: string;
}

export interface DataLossPreventionJobTriggerInspectJobStorageConfigDatastoreOptionsPartitionId {
    /**
     * If not empty, the ID of the namespace to which the entities belong.
     */
    namespaceId?: string;
    /**
     * The ID of the project to which the entities belong.
     */
    projectId: string;
}

export interface DataLossPreventionJobTriggerInspectJobStorageConfigHybridOptions {
    /**
     * A short description of where the data is coming from. Will be stored once in the job. 256 max length.
     */
    description?: string;
    /**
     * To organize findings, these labels will be added to each finding.
     *
     * Label keys must be between 1 and 63 characters long and must conform to the following regular expression: 'a-z?'.
     *
     * Label values must be between 0 and 63 characters long and must conform to the regular expression '(a-z?)?'.
     *
     * No more than 10 labels can be associated with a given finding.
     *
     * Examples:
     * * '"environment" : "production"'
     * * '"pipeline" : "etl"'
     */
    labels?: {[key: string]: string};
    /**
     * These are labels that each inspection request must include within their 'finding_labels' map. Request
     * may contain others, but any missing one of these will be rejected.
     *
     * Label keys must be between 1 and 63 characters long and must conform to the following regular expression: 'a-z?'.
     *
     * No more than 10 keys can be required.
     */
    requiredFindingLabelKeys?: string[];
    /**
     * If the container is a table, additional information to make findings meaningful such as the columns that are primary keys.
     */
    tableOptions?: outputs.DataLossPreventionJobTriggerInspectJobStorageConfigHybridOptionsTableOptions;
}

export interface DataLossPreventionJobTriggerInspectJobStorageConfigHybridOptionsTableOptions {
    /**
     * The columns that are the primary keys for table objects included in ContentItem. A copy of this
     * cell's value will stored alongside alongside each finding so that the finding can be traced to
     * the specific row it came from. No more than 3 may be provided.
     */
    identifyingFields?: outputs.DataLossPreventionJobTriggerInspectJobStorageConfigHybridOptionsTableOptionsIdentifyingField[];
}

export interface DataLossPreventionJobTriggerInspectJobStorageConfigHybridOptionsTableOptionsIdentifyingField {
    /**
     * Name describing the field.
     */
    name: string;
}

export interface DataLossPreventionJobTriggerInspectJobStorageConfigTimespanConfig {
    /**
     * When the job is started by a JobTrigger we will automatically figure out a valid startTime to avoid
     * scanning files that have not been modified since the last time the JobTrigger executed. This will
     * be based on the time of the execution of the last run of the JobTrigger or the timespan endTime
     * used in the last run of the JobTrigger.
     */
    enableAutoPopulationOfTimespanConfig?: boolean;
    /**
     * Exclude files, tables, or rows newer than this value. If not set, no upper time limit is applied.
     */
    endTime?: string;
    /**
     * Exclude files, tables, or rows older than this value. If not set, no lower time limit is applied.
     */
    startTime?: string;
    /**
     * Specification of the field containing the timestamp of scanned items.
     */
    timestampField?: outputs.DataLossPreventionJobTriggerInspectJobStorageConfigTimespanConfigTimestampField;
}

export interface DataLossPreventionJobTriggerInspectJobStorageConfigTimespanConfigTimestampField {
    /**
     * Specification of the field containing the timestamp of scanned items. Used for data sources like Datastore and BigQuery.
     *
     * For BigQuery: Required to filter out rows based on the given start and end times. If not specified and the table was
     * modified between the given start and end times, the entire table will be scanned. The valid data types of the timestamp
     * field are: INTEGER, DATE, TIMESTAMP, or DATETIME BigQuery column.
     *
     * For Datastore. Valid data types of the timestamp field are: TIMESTAMP. Datastore entity will be scanned if the
     * timestamp property does not exist or its value is empty or invalid.
     */
    name: string;
}

export interface DataLossPreventionJobTriggerTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DataLossPreventionJobTriggerTrigger {
    /**
     * For use with hybrid jobs. Jobs must be manually created and finished.
     */
    manual?: outputs.DataLossPreventionJobTriggerTriggerManual;
    /**
     * Schedule for triggered jobs
     */
    schedule?: outputs.DataLossPreventionJobTriggerTriggerSchedule;
}

export interface DataLossPreventionJobTriggerTriggerManual {
}

export interface DataLossPreventionJobTriggerTriggerSchedule {
    /**
     * With this option a job is started a regular periodic basis. For example: every day (86400 seconds).
     *
     * A scheduled start time will be skipped if the previous execution has not ended when its scheduled time occurs.
     *
     * This value must be set to a time duration greater than or equal to 1 day and can be no longer than 60 days.
     *
     * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
     */
    recurrencePeriodDuration?: string;
}

export interface DataLossPreventionStoredInfoTypeDictionary {
    /**
     * Newline-delimited file of words in Cloud Storage. Only a single file is accepted.
     */
    cloudStoragePath?: outputs.DataLossPreventionStoredInfoTypeDictionaryCloudStoragePath;
    /**
     * List of words or phrases to search for.
     */
    wordList?: outputs.DataLossPreventionStoredInfoTypeDictionaryWordList;
}

export interface DataLossPreventionStoredInfoTypeDictionaryCloudStoragePath {
    /**
     * A url representing a file or path (no wildcards) in Cloud Storage. Example: 'gs://[BUCKET_NAME]/dictionary.txt'
     */
    path: string;
}

export interface DataLossPreventionStoredInfoTypeDictionaryWordList {
    /**
     * Words or phrases defining the dictionary. The dictionary must contain at least one
     * phrase and every phrase must contain at least 2 characters that are letters or digits.
     */
    words: string[];
}

export interface DataLossPreventionStoredInfoTypeLargeCustomDictionary {
    /**
     * Field in a BigQuery table where each cell represents a dictionary phrase.
     */
    bigQueryField?: outputs.DataLossPreventionStoredInfoTypeLargeCustomDictionaryBigQueryField;
    /**
     * Set of files containing newline-delimited lists of dictionary phrases.
     */
    cloudStorageFileSet?: outputs.DataLossPreventionStoredInfoTypeLargeCustomDictionaryCloudStorageFileSet;
    /**
     * Location to store dictionary artifacts in Google Cloud Storage. These files will only be accessible by project owners and the DLP API.
     * If any of these artifacts are modified, the dictionary is considered invalid and can no longer be used.
     */
    outputPath: outputs.DataLossPreventionStoredInfoTypeLargeCustomDictionaryOutputPath;
}

export interface DataLossPreventionStoredInfoTypeLargeCustomDictionaryBigQueryField {
    /**
     * Designated field in the BigQuery table.
     */
    field: outputs.DataLossPreventionStoredInfoTypeLargeCustomDictionaryBigQueryFieldField;
    /**
     * Field in a BigQuery table where each cell represents a dictionary phrase.
     */
    table: outputs.DataLossPreventionStoredInfoTypeLargeCustomDictionaryBigQueryFieldTable;
}

export interface DataLossPreventionStoredInfoTypeLargeCustomDictionaryBigQueryFieldField {
    /**
     * Name describing the field.
     */
    name: string;
}

export interface DataLossPreventionStoredInfoTypeLargeCustomDictionaryBigQueryFieldTable {
    /**
     * The dataset ID of the table.
     */
    datasetId: string;
    /**
     * The Google Cloud Platform project ID of the project containing the table.
     */
    projectId: string;
    /**
     * The name of the table.
     */
    tableId: string;
}

export interface DataLossPreventionStoredInfoTypeLargeCustomDictionaryCloudStorageFileSet {
    /**
     * The url, in the format 'gs://<bucket>/<path>'. Trailing wildcard in the path is allowed.
     */
    url: string;
}

export interface DataLossPreventionStoredInfoTypeLargeCustomDictionaryOutputPath {
    /**
     * A url representing a file or path (no wildcards) in Cloud Storage. Example: 'gs://[BUCKET_NAME]/dictionary.txt'
     */
    path: string;
}

export interface DataLossPreventionStoredInfoTypeRegex {
    /**
     * The index of the submatch to extract as findings. When not specified, the entire match is returned. No more than 3 may be included.
     */
    groupIndexes?: number[];
    /**
     * Pattern defining the regular expression.
     * Its syntax (https://github.com/google/re2/wiki/Syntax) can be found under the google/re2 repository on GitHub.
     */
    pattern: string;
}

export interface DataLossPreventionStoredInfoTypeTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DataPipelinePipelineScheduleInfo {
    /**
     * When the next Scheduler job is going to run.
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
     */
    nextJobTime: string;
    /**
     * Unix-cron format of the schedule. This information is retrieved from the linked Cloud Scheduler.
     */
    schedule?: string;
    /**
     * Timezone ID. This matches the timezone IDs used by the Cloud Scheduler API. If empty, UTC time is assumed.
     */
    timeZone?: string;
}

export interface DataPipelinePipelineTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DataPipelinePipelineWorkload {
    /**
     * Template information and additional parameters needed to launch a Dataflow job using the flex launch API.
     * https://cloud.google.com/dataflow/docs/reference/data-pipelines/rest/v1/projects.locations.pipelines#launchflextemplaterequest
     */
    dataflowFlexTemplateRequest?: outputs.DataPipelinePipelineWorkloadDataflowFlexTemplateRequest;
    /**
     * Template information and additional parameters needed to launch a Dataflow job using the standard launch API.
     * https://cloud.google.com/dataflow/docs/reference/data-pipelines/rest/v1/projects.locations.pipelines#launchtemplaterequest
     */
    dataflowLaunchTemplateRequest?: outputs.DataPipelinePipelineWorkloadDataflowLaunchTemplateRequest;
}

export interface DataPipelinePipelineWorkloadDataflowFlexTemplateRequest {
    /**
     * Parameter to launch a job from a Flex Template.
     * https://cloud.google.com/dataflow/docs/reference/data-pipelines/rest/v1/projects.locations.pipelines#launchflextemplateparameter
     */
    launchParameter: outputs.DataPipelinePipelineWorkloadDataflowFlexTemplateRequestLaunchParameter;
    /**
     * The regional endpoint to which to direct the request. For example, us-central1, us-west1.
     */
    location: string;
    /**
     * The ID of the Cloud Platform project that the job belongs to.
     */
    projectId: string;
    /**
     * If true, the request is validated but not actually executed. Defaults to false.
     */
    validateOnly?: boolean;
}

export interface DataPipelinePipelineWorkloadDataflowFlexTemplateRequestLaunchParameter {
    /**
     * Cloud Storage path to a file with a JSON-serialized ContainerSpec as content.
     */
    containerSpecGcsPath?: string;
    /**
     * The runtime environment for the Flex Template job.
     * https://cloud.google.com/dataflow/docs/reference/data-pipelines/rest/v1/projects.locations.pipelines#FlexTemplateRuntimeEnvironment
     */
    environment?: outputs.DataPipelinePipelineWorkloadDataflowFlexTemplateRequestLaunchParameterEnvironment;
    /**
     * The job name to use for the created job. For an update job request, the job name should be the same as the existing running job.
     */
    jobName: string;
    /**
     * Launch options for this Flex Template job. This is a common set of options across languages and templates. This should not be used to pass job parameters.
     * 'An object containing a list of "key": value pairs. Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.'
     */
    launchOptions?: {[key: string]: string};
    /**
     * 'The parameters for the Flex Template. Example: {"numWorkers":"5"}'
     * 'An object containing a list of "key": value pairs. Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.'
     */
    parameters?: {[key: string]: string};
    /**
     * 'Use this to pass transform name mappings for streaming update jobs. Example: {"oldTransformName":"newTransformName",...}'
     * 'An object containing a list of "key": value pairs. Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.'
     */
    transformNameMappings?: {[key: string]: string};
    /**
     * Set this to true if you are sending a request to update a running streaming job. When set, the job name should be the same as the running job.
     */
    update?: boolean;
}

export interface DataPipelinePipelineWorkloadDataflowFlexTemplateRequestLaunchParameterEnvironment {
    /**
     * Additional experiment flags for the job.
     */
    additionalExperiments?: string[];
    /**
     * Additional user labels to be specified for the job. Keys and values should follow the restrictions specified in the labeling restrictions page. An object containing a list of key/value pairs.
     * 'Example: { "name": "wrench", "mass": "1kg", "count": "3" }.'
     * 'An object containing a list of "key": value pairs. Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.'
     */
    additionalUserLabels?: {[key: string]: string};
    /**
     * Whether to enable Streaming Engine for the job.
     */
    enableStreamingEngine?: boolean;
    /**
     * Set FlexRS goal for the job. https://cloud.google.com/dataflow/docs/guides/flexrs
     * https://cloud.google.com/dataflow/docs/reference/data-pipelines/rest/v1/projects.locations.pipelines#FlexResourceSchedulingGoal Possible values: ["FLEXRS_UNSPECIFIED", "FLEXRS_SPEED_OPTIMIZED", "FLEXRS_COST_OPTIMIZED"]
     */
    flexrsGoal?: string;
    /**
     * Configuration for VM IPs.
     * https://cloud.google.com/dataflow/docs/reference/data-pipelines/rest/v1/projects.locations.pipelines#WorkerIPAddressConfiguration Possible values: ["WORKER_IP_UNSPECIFIED", "WORKER_IP_PUBLIC", "WORKER_IP_PRIVATE"]
     */
    ipConfiguration?: string;
    /**
     * 'Name for the Cloud KMS key for the job. The key format is: projects//locations//keyRings//cryptoKeys/'
     */
    kmsKeyName?: string;
    /**
     * The machine type to use for the job. Defaults to the value from the template if not specified.
     */
    machineType?: string;
    /**
     * The maximum number of Compute Engine instances to be made available to your pipeline during execution, from 1 to 1000.
     */
    maxWorkers?: number;
    /**
     * Network to which VMs will be assigned. If empty or unspecified, the service will use the network "default".
     */
    network?: string;
    /**
     * The initial number of Compute Engine instances for the job.
     */
    numWorkers?: number;
    /**
     * The email address of the service account to run the job as.
     */
    serviceAccountEmail?: string;
    /**
     * Subnetwork to which VMs will be assigned, if desired. You can specify a subnetwork using either a complete URL or an abbreviated path. Expected to be of the form "https://www.googleapis.com/compute/v1/projects/HOST_PROJECT_ID/regions/REGION/subnetworks/SUBNETWORK" or "regions/REGION/subnetworks/SUBNETWORK". If the subnetwork is located in a Shared VPC network, you must use the complete URL.
     */
    subnetwork?: string;
    /**
     * The Cloud Storage path to use for temporary files. Must be a valid Cloud Storage URL, beginning with gs://.
     */
    tempLocation?: string;
    /**
     * The Compute Engine region (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. "us-west1". Mutually exclusive with workerZone. If neither workerRegion nor workerZone is specified, default to the control plane's region.
     */
    workerRegion?: string;
    /**
     * The Compute Engine zone (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. "us-west1-a". Mutually exclusive with workerRegion. If neither workerRegion nor workerZone is specified, a zone in the control plane's region is chosen based on available capacity. If both workerZone and zone are set, workerZone takes precedence.
     */
    workerZone?: string;
    /**
     * The Compute Engine availability zone for launching worker instances to run your pipeline. In the future, workerZone will take precedence.
     */
    zone?: string;
}

export interface DataPipelinePipelineWorkloadDataflowLaunchTemplateRequest {
    /**
     * A Cloud Storage path to the template from which to create the job. Must be a valid Cloud Storage URL, beginning with 'gs://'.
     */
    gcsPath?: string;
    /**
     * The parameters of the template to launch. This should be part of the body of the POST request.
     * https://cloud.google.com/dataflow/docs/reference/data-pipelines/rest/v1/projects.locations.pipelines#launchtemplateparameters
     */
    launchParameters?: outputs.DataPipelinePipelineWorkloadDataflowLaunchTemplateRequestLaunchParameters;
    /**
     * The regional endpoint to which to direct the request.
     */
    location?: string;
    /**
     * The ID of the Cloud Platform project that the job belongs to.
     */
    projectId: string;
    validateOnly?: boolean;
}

export interface DataPipelinePipelineWorkloadDataflowLaunchTemplateRequestLaunchParameters {
    /**
     * The runtime environment for the job.
     * https://cloud.google.com/dataflow/docs/reference/data-pipelines/rest/v1/projects.locations.pipelines#RuntimeEnvironment
     */
    environment?: outputs.DataPipelinePipelineWorkloadDataflowLaunchTemplateRequestLaunchParametersEnvironment;
    /**
     * The job name to use for the created job.
     */
    jobName: string;
    /**
     * The runtime parameters to pass to the job.
     * 'An object containing a list of "key": value pairs. Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.'
     */
    parameters?: {[key: string]: string};
    /**
     * Map of transform name prefixes of the job to be replaced to the corresponding name prefixes of the new job. Only applicable when updating a pipeline.
     * 'An object containing a list of "key": value pairs. Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.'
     */
    transformNameMapping?: {[key: string]: string};
    /**
     * If set, replace the existing pipeline with the name specified by jobName with this pipeline, preserving state.
     */
    update?: boolean;
}

export interface DataPipelinePipelineWorkloadDataflowLaunchTemplateRequestLaunchParametersEnvironment {
    /**
     * Additional experiment flags for the job.
     */
    additionalExperiments?: string[];
    /**
     * Additional user labels to be specified for the job. Keys and values should follow the restrictions specified in the labeling restrictions page. An object containing a list of key/value pairs.
     * 'Example: { "name": "wrench", "mass": "1kg", "count": "3" }.'
     * 'An object containing a list of "key": value pairs. Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.'
     */
    additionalUserLabels?: {[key: string]: string};
    /**
     * Whether to bypass the safety checks for the job's temporary directory. Use with caution.
     */
    bypassTempDirValidation?: boolean;
    /**
     * Whether to enable Streaming Engine for the job.
     */
    enableStreamingEngine?: boolean;
    /**
     * Configuration for VM IPs.
     * https://cloud.google.com/dataflow/docs/reference/data-pipelines/rest/v1/projects.locations.pipelines#WorkerIPAddressConfiguration Possible values: ["WORKER_IP_UNSPECIFIED", "WORKER_IP_PUBLIC", "WORKER_IP_PRIVATE"]
     */
    ipConfiguration?: string;
    /**
     * 'Name for the Cloud KMS key for the job. The key format is: projects//locations//keyRings//cryptoKeys/'
     */
    kmsKeyName?: string;
    /**
     * The machine type to use for the job. Defaults to the value from the template if not specified.
     */
    machineType?: string;
    /**
     * The maximum number of Compute Engine instances to be made available to your pipeline during execution, from 1 to 1000.
     */
    maxWorkers?: number;
    /**
     * Network to which VMs will be assigned. If empty or unspecified, the service will use the network "default".
     */
    network: string;
    /**
     * The initial number of Compute Engine instances for the job.
     */
    numWorkers?: number;
    /**
     * The email address of the service account to run the job as.
     */
    serviceAccountEmail?: string;
    /**
     * Subnetwork to which VMs will be assigned, if desired. You can specify a subnetwork using either a complete URL or an abbreviated path. Expected to be of the form "https://www.googleapis.com/compute/v1/projects/HOST_PROJECT_ID/regions/REGION/subnetworks/SUBNETWORK" or "regions/REGION/subnetworks/SUBNETWORK". If the subnetwork is located in a Shared VPC network, you must use the complete URL.
     */
    subnetwork?: string;
    /**
     * The Cloud Storage path to use for temporary files. Must be a valid Cloud Storage URL, beginning with gs://.
     */
    tempLocation?: string;
    /**
     * The Compute Engine region (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. "us-west1". Mutually exclusive with workerZone. If neither workerRegion nor workerZone is specified, default to the control plane's region.
     */
    workerRegion?: string;
    /**
     * The Compute Engine zone (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. "us-west1-a". Mutually exclusive with workerRegion. If neither workerRegion nor workerZone is specified, a zone in the control plane's region is chosen based on available capacity. If both workerZone and zone are set, workerZone takes precedence.
     */
    workerZone?: string;
    /**
     * The Compute Engine availability zone for launching worker instances to run your pipeline. In the future, workerZone will take precedence.
     */
    zone?: string;
}

export interface DatabaseMigrationServiceConnectionProfileAlloydb {
    /**
     * Required. The AlloyDB cluster ID that this connection profile is associated with.
     */
    clusterId: string;
    /**
     * Immutable. Metadata used to create the destination AlloyDB cluster.
     */
    settings?: outputs.DatabaseMigrationServiceConnectionProfileAlloydbSettings;
}

export interface DatabaseMigrationServiceConnectionProfileAlloydbSettings {
    /**
     * Required. Input only. Initial user to setup during cluster creation.
     */
    initialUser: outputs.DatabaseMigrationServiceConnectionProfileAlloydbSettingsInitialUser;
    /**
     * Labels for the AlloyDB cluster created by DMS.
     */
    labels?: {[key: string]: string};
    /**
     * Settings for the cluster's primary instance
     */
    primaryInstanceSettings?: outputs.DatabaseMigrationServiceConnectionProfileAlloydbSettingsPrimaryInstanceSettings;
    /**
     * Required. The resource link for the VPC network in which cluster resources are created and from which they are accessible via Private IP. The network must belong to the same project as the cluster.
     * It is specified in the form: 'projects/{project_number}/global/networks/{network_id}'. This is required to create a cluster.
     */
    vpcNetwork: string;
}

export interface DatabaseMigrationServiceConnectionProfileAlloydbSettingsInitialUser {
    /**
     * The initial password for the user.
     */
    password: string;
    /**
     * Output only. Indicates if the initialUser.password field has been set.
     */
    passwordSet: boolean;
    /**
     * The database username.
     */
    user: string;
}

export interface DatabaseMigrationServiceConnectionProfileAlloydbSettingsPrimaryInstanceSettings {
    /**
     * Database flags to pass to AlloyDB when DMS is creating the AlloyDB cluster and instances. See the AlloyDB documentation for how these can be used.
     */
    databaseFlags?: {[key: string]: string};
    /**
     * The database username.
     */
    id: string;
    /**
     * Labels for the AlloyDB primary instance created by DMS.
     */
    labels?: {[key: string]: string};
    /**
     * Configuration for the machines that host the underlying database engine.
     */
    machineConfig: outputs.DatabaseMigrationServiceConnectionProfileAlloydbSettingsPrimaryInstanceSettingsMachineConfig;
    /**
     * Output only. The private IP address for the Instance. This is the connection endpoint for an end-user application.
     */
    privateIp: string;
}

export interface DatabaseMigrationServiceConnectionProfileAlloydbSettingsPrimaryInstanceSettingsMachineConfig {
    /**
     * The number of CPU's in the VM instance.
     */
    cpuCount: number;
}

export interface DatabaseMigrationServiceConnectionProfileCloudsql {
    /**
     * Output only. The Cloud SQL instance ID that this connection profile is associated with.
     */
    cloudSqlId: string;
    /**
     * Output only. The Cloud SQL database instance's private IP.
     */
    privateIp: string;
    /**
     * Output only. The Cloud SQL database instance's public IP.
     */
    publicIp: string;
    /**
     * Immutable. Metadata used to create the destination Cloud SQL database.
     */
    settings?: outputs.DatabaseMigrationServiceConnectionProfileCloudsqlSettings;
}

export interface DatabaseMigrationServiceConnectionProfileCloudsqlSettings {
    /**
     * The activation policy specifies when the instance is activated; it is applicable only when the instance state is 'RUNNABLE'. Possible values: ["ALWAYS", "NEVER"]
     */
    activationPolicy?: string;
    /**
     * If you enable this setting, Cloud SQL checks your available storage every 30 seconds. If the available storage falls below a threshold size, Cloud SQL automatically adds additional storage capacity.
     * If the available storage repeatedly falls below the threshold size, Cloud SQL continues to add storage until it reaches the maximum of 30 TB.
     */
    autoStorageIncrease?: boolean;
    /**
     * The KMS key name used for the csql instance.
     */
    cmekKeyName?: string;
    /**
     * The Cloud SQL default instance level collation.
     */
    collation?: string;
    /**
     * The storage capacity available to the database, in GB. The minimum (and default) size is 10GB.
     */
    dataDiskSizeGb?: string;
    /**
     * The type of storage. Possible values: ["PD_SSD", "PD_HDD"]
     */
    dataDiskType?: string;
    /**
     * The database flags passed to the Cloud SQL instance at startup.
     */
    databaseFlags?: {[key: string]: string};
    /**
     * The database engine type and version.
     * Currently supported values located at https://cloud.google.com/database-migration/docs/reference/rest/v1/projects.locations.connectionProfiles#sqldatabaseversion
     */
    databaseVersion?: string;
    /**
     * The edition of the given Cloud SQL instance. Possible values: ["ENTERPRISE", "ENTERPRISE_PLUS"]
     */
    edition?: string;
    /**
     * The settings for IP Management. This allows to enable or disable the instance IP and manage which external networks can connect to the instance. The IPv4 address cannot be disabled.
     */
    ipConfig?: outputs.DatabaseMigrationServiceConnectionProfileCloudsqlSettingsIpConfig;
    /**
     * Input only. Initial root password.
     */
    rootPassword?: string;
    /**
     * Output only. Indicates If this connection profile root password is stored.
     */
    rootPasswordSet: boolean;
    /**
     * The Database Migration Service source connection profile ID, in the format: projects/my_project_name/locations/us-central1/connectionProfiles/connection_profile_ID
     */
    sourceId: string;
    /**
     * The maximum size to which storage capacity can be automatically increased. The default value is 0, which specifies that there is no limit.
     */
    storageAutoResizeLimit?: string;
    /**
     * The tier (or machine type) for this instance, for example: db-n1-standard-1 (MySQL instances) or db-custom-1-3840 (PostgreSQL instances).
     * For more information, see https://cloud.google.com/sql/docs/mysql/instance-settings
     */
    tier?: string;
    /**
     * The resource labels for a Cloud SQL instance to use to annotate any related underlying resources such as Compute Engine VMs.
     */
    userLabels?: {[key: string]: string};
    /**
     * The Google Cloud Platform zone where your Cloud SQL datdabse instance is located.
     */
    zone?: string;
}

export interface DatabaseMigrationServiceConnectionProfileCloudsqlSettingsIpConfig {
    /**
     * The list of external networks that are allowed to connect to the instance using the IP.
     */
    authorizedNetworks?: outputs.DatabaseMigrationServiceConnectionProfileCloudsqlSettingsIpConfigAuthorizedNetwork[];
    /**
     * Whether the instance should be assigned an IPv4 address or not.
     */
    enableIpv4?: boolean;
    /**
     * The resource link for the VPC network from which the Cloud SQL instance is accessible for private IP. For example, projects/myProject/global/networks/default.
     * This setting can be updated, but it cannot be removed after it is set.
     */
    privateNetwork?: string;
    /**
     * Whether SSL connections over IP should be enforced or not.
     */
    requireSsl?: boolean;
}

export interface DatabaseMigrationServiceConnectionProfileCloudsqlSettingsIpConfigAuthorizedNetwork {
    /**
     * The time when this access control entry expires in RFC 3339 format.
     */
    expireTime?: string;
    /**
     * A label to identify this entry.
     */
    label?: string;
    /**
     * Input only. The time-to-leave of this access control entry.
     */
    ttl?: string;
    /**
     * The allowlisted value for the access control list.
     */
    value: string;
}

export interface DatabaseMigrationServiceConnectionProfileError {
    code: number;
    details: {[key: string]: string}[];
    message: string;
}

export interface DatabaseMigrationServiceConnectionProfileMysql {
    /**
     * If the source is a Cloud SQL database, use this field to provide the Cloud SQL instance ID of the source.
     */
    cloudSqlId?: string;
    /**
     * The IP or hostname of the source MySQL database.
     */
    host?: string;
    /**
     * Input only. The password for the user that Database Migration Service will be using to connect to the database.
     * This field is not returned on request, and the value is encrypted when stored in Database Migration Service.
     */
    password?: string;
    /**
     * Output only. Indicates If this connection profile password is stored.
     */
    passwordSet: boolean;
    /**
     * The network port of the source MySQL database.
     */
    port?: number;
    /**
     * SSL configuration for the destination to connect to the source database.
     */
    ssl?: outputs.DatabaseMigrationServiceConnectionProfileMysqlSsl;
    /**
     * The username that Database Migration Service will use to connect to the database. The value is encrypted when stored in Database Migration Service.
     */
    username?: string;
}

export interface DatabaseMigrationServiceConnectionProfileMysqlSsl {
    /**
     * Required. Input only. The x509 PEM-encoded certificate of the CA that signed the source database server's certificate.
     * The replica will use this certificate to verify it's connecting to the right host.
     */
    caCertificate: string;
    /**
     * Input only. The x509 PEM-encoded certificate that will be used by the replica to authenticate against the source database server.
     * If this field is used then the 'clientKey' field is mandatory
     */
    clientCertificate?: string;
    /**
     * Input only. The unencrypted PKCS#1 or PKCS#8 PEM-encoded private key associated with the Client Certificate.
     * If this field is used then the 'clientCertificate' field is mandatory.
     */
    clientKey?: string;
    /**
     * The current connection profile state.
     */
    type: string;
}

export interface DatabaseMigrationServiceConnectionProfileOracle {
    /**
     * Required. Database service for the Oracle connection.
     */
    databaseService: string;
    /**
     * SSL configuration for the destination to connect to the source database.
     */
    forwardSshConnectivity?: outputs.DatabaseMigrationServiceConnectionProfileOracleForwardSshConnectivity;
    /**
     * Required. The IP or hostname of the source Oracle database.
     */
    host: string;
    /**
     * Required. Input only. The password for the user that Database Migration Service will be using to connect to the database.
     * This field is not returned on request, and the value is encrypted when stored in Database Migration Service.
     */
    password: string;
    /**
     * Output only. Indicates If this connection profile password is stored.
     */
    passwordSet: boolean;
    /**
     * Required. The network port of the source Oracle database.
     */
    port: number;
    /**
     * Configuration for using a private network to communicate with the source database
     */
    privateConnectivity?: outputs.DatabaseMigrationServiceConnectionProfileOraclePrivateConnectivity;
    /**
     * SSL configuration for the destination to connect to the source database.
     */
    ssl?: outputs.DatabaseMigrationServiceConnectionProfileOracleSsl;
    /**
     * This object has no nested fields.
     *
     * Static IP address connectivity configured on service project.
     */
    staticServiceIpConnectivity?: outputs.DatabaseMigrationServiceConnectionProfileOracleStaticServiceIpConnectivity;
    /**
     * Required. The username that Database Migration Service will use to connect to the database. The value is encrypted when stored in Database Migration Service.
     */
    username: string;
}

export interface DatabaseMigrationServiceConnectionProfileOracleForwardSshConnectivity {
    /**
     * Required. Hostname for the SSH tunnel.
     */
    hostname: string;
    /**
     * Input only. SSH password. Only one of 'password' and 'private_key' can be configured.
     */
    password?: string;
    /**
     * Port for the SSH tunnel, default value is 22.
     */
    port: number;
    /**
     * Input only. SSH private key. Only one of 'password' and 'private_key' can be configured.
     */
    privateKey?: string;
    /**
     * Required. Username for the SSH tunnel.
     */
    username: string;
}

export interface DatabaseMigrationServiceConnectionProfileOraclePrivateConnectivity {
    /**
     * Required. The resource name (URI) of the private connection.
     */
    privateConnection: string;
}

export interface DatabaseMigrationServiceConnectionProfileOracleSsl {
    /**
     * Required. Input only. The x509 PEM-encoded certificate of the CA that signed the source database server's certificate.
     * The replica will use this certificate to verify it's connecting to the right host.
     */
    caCertificate: string;
    /**
     * Input only. The x509 PEM-encoded certificate that will be used by the replica to authenticate against the source database server.
     * If this field is used then the 'clientKey' field is mandatory
     */
    clientCertificate?: string;
    /**
     * Input only. The unencrypted PKCS#1 or PKCS#8 PEM-encoded private key associated with the Client Certificate.
     * If this field is used then the 'clientCertificate' field is mandatory.
     */
    clientKey?: string;
    /**
     * The current connection profile state.
     */
    type: string;
}

export interface DatabaseMigrationServiceConnectionProfileOracleStaticServiceIpConnectivity {
}

export interface DatabaseMigrationServiceConnectionProfilePostgresql {
    /**
     * If the connected database is an AlloyDB instance, use this field to provide the AlloyDB cluster ID.
     */
    alloydbClusterId?: string;
    /**
     * If the source is a Cloud SQL database, use this field to provide the Cloud SQL instance ID of the source.
     */
    cloudSqlId?: string;
    /**
     * The IP or hostname of the source MySQL database.
     */
    host?: string;
    /**
     * Output only. If the source is a Cloud SQL database, this field indicates the network architecture it's associated with.
     */
    networkArchitecture: string;
    /**
     * Input only. The password for the user that Database Migration Service will be using to connect to the database.
     * This field is not returned on request, and the value is encrypted when stored in Database Migration Service.
     */
    password?: string;
    /**
     * Output only. Indicates If this connection profile password is stored.
     */
    passwordSet: boolean;
    /**
     * The network port of the source MySQL database.
     */
    port?: number;
    /**
     * SSL configuration for the destination to connect to the source database.
     */
    ssl?: outputs.DatabaseMigrationServiceConnectionProfilePostgresqlSsl;
    /**
     * The username that Database Migration Service will use to connect to the database. The value is encrypted when stored in Database Migration Service.
     */
    username?: string;
}

export interface DatabaseMigrationServiceConnectionProfilePostgresqlSsl {
    /**
     * Required. Input only. The x509 PEM-encoded certificate of the CA that signed the source database server's certificate.
     * The replica will use this certificate to verify it's connecting to the right host.
     */
    caCertificate: string;
    /**
     * Input only. The x509 PEM-encoded certificate that will be used by the replica to authenticate against the source database server.
     * If this field is used then the 'clientKey' field is mandatory
     */
    clientCertificate?: string;
    /**
     * Input only. The unencrypted PKCS#1 or PKCS#8 PEM-encoded private key associated with the Client Certificate.
     * If this field is used then the 'clientCertificate' field is mandatory.
     */
    clientKey?: string;
    /**
     * The current connection profile state.
     */
    type: string;
}

export interface DatabaseMigrationServiceConnectionProfileTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DatabaseMigrationServicePrivateConnectionError {
    details: {[key: string]: string};
    message: string;
}

export interface DatabaseMigrationServicePrivateConnectionTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DatabaseMigrationServicePrivateConnectionVpcPeeringConfig {
    /**
     * A free subnet for peering. (CIDR of /29)
     */
    subnet: string;
    /**
     * Fully qualified name of the VPC that Database Migration Service will peer to.
     * Format: projects/{project}/global/{networks}/{name}
     */
    vpcName: string;
}

export interface DataflowJobTimeouts {
    update?: string;
}

export interface DataplexAspectTypeIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataplexAspectTypeIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataplexAspectTypeTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DataplexAssetDiscoverySpec {
    /**
     * Optional. Configuration for CSV data.
     */
    csvOptions?: outputs.DataplexAssetDiscoverySpecCsvOptions;
    /**
     * Required. Whether discovery is enabled.
     */
    enabled: boolean;
    /**
     * Optional. The list of patterns to apply for selecting data to exclude during discovery. For Cloud Storage bucket assets, these are interpreted as glob patterns used to match object names. For BigQuery dataset assets, these are interpreted as patterns to match table names.
     */
    excludePatterns?: string[];
    /**
     * Optional. The list of patterns to apply for selecting data to include during discovery if only a subset of the data should considered. For Cloud Storage bucket assets, these are interpreted as glob patterns used to match object names. For BigQuery dataset assets, these are interpreted as patterns to match table names.
     */
    includePatterns?: string[];
    /**
     * Optional. Configuration for Json data.
     */
    jsonOptions?: outputs.DataplexAssetDiscoverySpecJsonOptions;
    /**
     * Optional. Cron schedule (https://en.wikipedia.org/wiki/Cron) for running discovery periodically. Successive discovery runs must be scheduled at least 60 minutes apart. The default value is to run discovery every 60 minutes. To explicitly set a timezone to the cron tab, apply a prefix in the cron tab: "CRON_TZ=${IANA_TIME_ZONE}" or TZ=${IANA_TIME_ZONE}". The ${IANA_TIME_ZONE} may only be a valid string from IANA time zone database. For example, "CRON_TZ=America/New_York 1 * * * *", or "TZ=America/New_York 1 * * * *".
     */
    schedule?: string;
}

export interface DataplexAssetDiscoverySpecCsvOptions {
    /**
     * Optional. The delimiter being used to separate values. This defaults to ','.
     */
    delimiter?: string;
    /**
     * Optional. Whether to disable the inference of data type for CSV data. If true, all columns will be registered as strings.
     */
    disableTypeInference?: boolean;
    /**
     * Optional. The character encoding of the data. The default is UTF-8.
     */
    encoding?: string;
    /**
     * Optional. The number of rows to interpret as header rows that should be skipped when reading data rows.
     */
    headerRows?: number;
}

export interface DataplexAssetDiscoverySpecJsonOptions {
    /**
     * Optional. Whether to disable the inference of data type for Json data. If true, all columns will be registered as their primitive types (strings, number or boolean).
     */
    disableTypeInference?: boolean;
    /**
     * Optional. The character encoding of the data. The default is UTF-8.
     */
    encoding?: string;
}

export interface DataplexAssetDiscoveryStatus {
    lastRunDuration: string;
    lastRunTime: string;
    message: string;
    state: string;
    stats: outputs.DataplexAssetDiscoveryStatusStat[];
    updateTime: string;
}

export interface DataplexAssetDiscoveryStatusStat {
    dataItems: number;
    dataSize: number;
    filesets: number;
    tables: number;
}

export interface DataplexAssetIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataplexAssetIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataplexAssetResourceSpec {
    /**
     * Immutable. Relative name of the cloud resource that contains the data that is being managed within a lake. For example: `projects/{project_number}/buckets/{bucket_id}` `projects/{project_number}/datasets/{dataset_id}`
     */
    name?: string;
    /**
     * Optional. Determines how read permissions are handled for each asset and their associated tables. Only available to storage buckets assets. Possible values: DIRECT, MANAGED
     */
    readAccessMode: string;
    /**
     * Required. Immutable. Type of resource. Possible values: STORAGE_BUCKET, BIGQUERY_DATASET
     */
    type: string;
}

export interface DataplexAssetResourceStatus {
    message: string;
    state: string;
    updateTime: string;
}

export interface DataplexAssetSecurityStatus {
    message: string;
    state: string;
    updateTime: string;
}

export interface DataplexAssetTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DataplexDatascanData {
    /**
     * The Dataplex entity that represents the data source(e.g. BigQuery table) for Datascan.
     */
    entity?: string;
    /**
     * The service-qualified full resource name of the cloud resource for a DataScan job to scan against. The field could be:
     * (Cloud Storage bucket for DataDiscoveryScan)BigQuery table of type "TABLE" for DataProfileScan/DataQualityScan.
     */
    resource?: string;
}

export interface DataplexDatascanDataProfileSpec {
    /**
     * The fields to exclude from data profile.
     * If specified, the fields will be excluded from data profile, regardless of 'include_fields' value.
     */
    excludeFields?: outputs.DataplexDatascanDataProfileSpecExcludeFields;
    /**
     * The fields to include in data profile.
     * If not specified, all fields at the time of profile scan job execution are included, except for ones listed in 'exclude_fields'.
     */
    includeFields?: outputs.DataplexDatascanDataProfileSpecIncludeFields;
    /**
     * Actions to take upon job completion.
     */
    postScanActions?: outputs.DataplexDatascanDataProfileSpecPostScanActions;
    /**
     * A filter applied to all rows in a single DataScan job. The filter needs to be a valid SQL expression for a WHERE clause in BigQuery standard SQL syntax. Example: col1 >= 0 AND col2 < 10
     */
    rowFilter?: string;
    /**
     * The percentage of the records to be selected from the dataset for DataScan.
     * Value can range between 0.0 and 100.0 with up to 3 significant decimal digits.
     * Sampling is not applied if 'sampling_percent' is not specified, 0 or 100.
     */
    samplingPercent?: number;
}

export interface DataplexDatascanDataProfileSpecExcludeFields {
    /**
     * Expected input is a list of fully qualified names of fields as in the schema.
     * Only top-level field names for nested fields are supported.
     * For instance, if 'x' is of nested field type, listing 'x' is supported but 'x.y.z' is not supported. Here 'y' and 'y.z' are nested fields of 'x'.
     */
    fieldNames?: string[];
}

export interface DataplexDatascanDataProfileSpecIncludeFields {
    /**
     * Expected input is a list of fully qualified names of fields as in the schema.
     * Only top-level field names for nested fields are supported.
     * For instance, if 'x' is of nested field type, listing 'x' is supported but 'x.y.z' is not supported. Here 'y' and 'y.z' are nested fields of 'x'.
     */
    fieldNames?: string[];
}

export interface DataplexDatascanDataProfileSpecPostScanActions {
    /**
     * If set, results will be exported to the provided BigQuery table.
     */
    bigqueryExport?: outputs.DataplexDatascanDataProfileSpecPostScanActionsBigqueryExport;
}

export interface DataplexDatascanDataProfileSpecPostScanActionsBigqueryExport {
    /**
     * The BigQuery table to export DataProfileScan results to.
     * Format://bigquery.googleapis.com/projects/PROJECT_ID/datasets/DATASET_ID/tables/TABLE_ID
     */
    resultsTable?: string;
}

export interface DataplexDatascanDataQualitySpec {
    /**
     * Actions to take upon job completion.
     */
    postScanActions?: outputs.DataplexDatascanDataQualitySpecPostScanActions;
    /**
     * A filter applied to all rows in a single DataScan job. The filter needs to be a valid SQL expression for a WHERE clause in BigQuery standard SQL syntax. Example: col1 >= 0 AND col2 < 10
     */
    rowFilter?: string;
    /**
     * The list of rules to evaluate against a data source. At least one rule is required.
     */
    rules?: outputs.DataplexDatascanDataQualitySpecRule[];
    /**
     * The percentage of the records to be selected from the dataset for DataScan.
     * Value can range between 0.0 and 100.0 with up to 3 significant decimal digits.
     * Sampling is not applied if 'sampling_percent' is not specified, 0 or 100.
     */
    samplingPercent?: number;
}

export interface DataplexDatascanDataQualitySpecPostScanActions {
    /**
     * If set, results will be exported to the provided BigQuery table.
     */
    bigqueryExport?: outputs.DataplexDatascanDataQualitySpecPostScanActionsBigqueryExport;
}

export interface DataplexDatascanDataQualitySpecPostScanActionsBigqueryExport {
    /**
     * The BigQuery table to export DataQualityScan results to.
     * Format://bigquery.googleapis.com/projects/PROJECT_ID/datasets/DATASET_ID/tables/TABLE_ID
     */
    resultsTable?: string;
}

export interface DataplexDatascanDataQualitySpecRule {
    /**
     * The unnested column which this rule is evaluated against.
     */
    column?: string;
    /**
     * Description of the rule.
     * The maximum length is 1,024 characters.
     */
    description?: string;
    /**
     * The dimension a rule belongs to. Results are also aggregated at the dimension level. Supported dimensions are ["COMPLETENESS", "ACCURACY", "CONSISTENCY", "VALIDITY", "UNIQUENESS", "INTEGRITY"]
     */
    dimension: string;
    /**
     * Rows with null values will automatically fail a rule, unless ignoreNull is true. In that case, such null rows are trivially considered passing. Only applicable to ColumnMap rules.
     */
    ignoreNull?: boolean;
    /**
     * A mutable name for the rule.
     * The name must contain only letters (a-z, A-Z), numbers (0-9), or hyphens (-).
     * The maximum length is 63 characters.
     * Must start with a letter.
     * Must end with a number or a letter.
     */
    name?: string;
    /**
     * ColumnMap rule which evaluates whether each column value is null.
     */
    nonNullExpectation?: outputs.DataplexDatascanDataQualitySpecRuleNonNullExpectation;
    /**
     * ColumnMap rule which evaluates whether each column value lies between a specified range.
     */
    rangeExpectation?: outputs.DataplexDatascanDataQualitySpecRuleRangeExpectation;
    /**
     * ColumnMap rule which evaluates whether each column value matches a specified regex.
     */
    regexExpectation?: outputs.DataplexDatascanDataQualitySpecRuleRegexExpectation;
    /**
     * Table rule which evaluates whether each row passes the specified condition.
     */
    rowConditionExpectation?: outputs.DataplexDatascanDataQualitySpecRuleRowConditionExpectation;
    /**
     * ColumnMap rule which evaluates whether each column value is contained by a specified set.
     */
    setExpectation?: outputs.DataplexDatascanDataQualitySpecRuleSetExpectation;
    /**
     * Table rule which evaluates whether any row matches invalid state.
     */
    sqlAssertion?: outputs.DataplexDatascanDataQualitySpecRuleSqlAssertion;
    /**
     * ColumnAggregate rule which evaluates whether the column aggregate statistic lies between a specified range.
     */
    statisticRangeExpectation?: outputs.DataplexDatascanDataQualitySpecRuleStatisticRangeExpectation;
    /**
     * Table rule which evaluates whether the provided expression is true.
     */
    tableConditionExpectation?: outputs.DataplexDatascanDataQualitySpecRuleTableConditionExpectation;
    /**
     * The minimum ratio of passing_rows / total_rows required to pass this rule, with a range of [0.0, 1.0]. 0 indicates default value (i.e. 1.0).
     */
    threshold?: number;
    /**
     * Row-level rule which evaluates whether each column value is unique.
     */
    uniquenessExpectation?: outputs.DataplexDatascanDataQualitySpecRuleUniquenessExpectation;
}

export interface DataplexDatascanDataQualitySpecRuleNonNullExpectation {
}

export interface DataplexDatascanDataQualitySpecRuleRangeExpectation {
    /**
     * The maximum column value allowed for a row to pass this validation. At least one of minValue and maxValue need to be provided.
     */
    maxValue?: string;
    /**
     * The minimum column value allowed for a row to pass this validation. At least one of minValue and maxValue need to be provided.
     */
    minValue?: string;
    /**
     * Whether each value needs to be strictly lesser than ('<') the maximum, or if equality is allowed.
     * Only relevant if a maxValue has been defined. Default = false.
     */
    strictMaxEnabled?: boolean;
    /**
     * Whether each value needs to be strictly greater than ('>') the minimum, or if equality is allowed.
     * Only relevant if a minValue has been defined. Default = false.
     */
    strictMinEnabled?: boolean;
}

export interface DataplexDatascanDataQualitySpecRuleRegexExpectation {
    /**
     * A regular expression the column value is expected to match.
     */
    regex: string;
}

export interface DataplexDatascanDataQualitySpecRuleRowConditionExpectation {
    /**
     * The SQL expression.
     */
    sqlExpression: string;
}

export interface DataplexDatascanDataQualitySpecRuleSetExpectation {
    /**
     * Expected values for the column value.
     */
    values: string[];
}

export interface DataplexDatascanDataQualitySpecRuleSqlAssertion {
    /**
     * The SQL statement.
     */
    sqlStatement: string;
}

export interface DataplexDatascanDataQualitySpecRuleStatisticRangeExpectation {
    /**
     * The maximum column statistic value allowed for a row to pass this validation.
     * At least one of minValue and maxValue need to be provided.
     */
    maxValue?: string;
    /**
     * The minimum column statistic value allowed for a row to pass this validation.
     * At least one of minValue and maxValue need to be provided.
     */
    minValue?: string;
    /**
     * column statistics. Possible values: ["STATISTIC_UNDEFINED", "MEAN", "MIN", "MAX"]
     */
    statistic: string;
    /**
     * Whether column statistic needs to be strictly lesser than ('<') the maximum, or if equality is allowed.
     * Only relevant if a maxValue has been defined. Default = false.
     */
    strictMaxEnabled?: boolean;
    /**
     * Whether column statistic needs to be strictly greater than ('>') the minimum, or if equality is allowed.
     * Only relevant if a minValue has been defined. Default = false.
     */
    strictMinEnabled?: boolean;
}

export interface DataplexDatascanDataQualitySpecRuleTableConditionExpectation {
    /**
     * The SQL expression.
     */
    sqlExpression: string;
}

export interface DataplexDatascanDataQualitySpecRuleUniquenessExpectation {
}

export interface DataplexDatascanExecutionSpec {
    /**
     * The unnested field (of type Date or Timestamp) that contains values which monotonically increase over time. If not specified, a data scan will run for all data in the table.
     */
    field?: string;
    /**
     * Spec related to how often and when a scan should be triggered.
     */
    trigger: outputs.DataplexDatascanExecutionSpecTrigger;
}

export interface DataplexDatascanExecutionSpecTrigger {
    /**
     * The scan runs once via dataScans.run API.
     */
    onDemand?: outputs.DataplexDatascanExecutionSpecTriggerOnDemand;
    /**
     * The scan is scheduled to run periodically.
     */
    schedule?: outputs.DataplexDatascanExecutionSpecTriggerSchedule;
}

export interface DataplexDatascanExecutionSpecTriggerOnDemand {
}

export interface DataplexDatascanExecutionSpecTriggerSchedule {
    /**
     * Cron schedule for running scans periodically. This field is required for Schedule scans.
     */
    cron: string;
}

export interface DataplexDatascanExecutionStatus {
    latestJobEndTime: string;
    latestJobStartTime: string;
}

export interface DataplexDatascanIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataplexDatascanIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataplexDatascanTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DataplexEntryGroupIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataplexEntryGroupIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataplexEntryGroupTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DataplexEntryTypeIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataplexEntryTypeIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataplexEntryTypeRequiredAspect {
    /**
     * Required aspect type for the entry type.
     */
    type?: string;
}

export interface DataplexEntryTypeTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DataplexLakeAssetStatus {
    activeAssets: number;
    securityPolicyApplyingAssets: number;
    updateTime: string;
}

export interface DataplexLakeIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataplexLakeIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataplexLakeMetastore {
    /**
     * Optional. A relative reference to the Dataproc Metastore (https://cloud.google.com/dataproc-metastore/docs) service associated with the lake: `projects/{project_id}/locations/{location_id}/services/{service_id}`
     */
    service?: string;
}

export interface DataplexLakeMetastoreStatus {
    endpoint: string;
    message: string;
    state: string;
    updateTime: string;
}

export interface DataplexLakeTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DataplexTaskExecutionSpec {
    /**
     * The arguments to pass to the task. The args can use placeholders of the format ${placeholder} as part of key/value string. These will be interpolated before passing the args to the driver. Currently supported placeholders: - ${taskId} - ${job_time} To pass positional args, set the key as TASK_ARGS. The value should be a comma-separated string of all the positional arguments. To use a delimiter other than comma, refer to https://cloud.google.com/sdk/gcloud/reference/topic/escaping. In case of other keys being present in the args, then TASK_ARGS will be passed as the last argument. An object containing a list of 'key': value pairs. Example: { 'name': 'wrench', 'mass': '1.3kg', 'count': '3' }.
     */
    args?: {[key: string]: string};
    /**
     * The Cloud KMS key to use for encryption, of the form: projects/{project_number}/locations/{locationId}/keyRings/{key-ring-name}/cryptoKeys/{key-name}.
     */
    kmsKey?: string;
    /**
     * The maximum duration after which the job execution is expired. A duration in seconds with up to nine fractional digits, ending with 's'. Example: '3.5s'.
     */
    maxJobExecutionLifetime?: string;
    /**
     * The project in which jobs are run. By default, the project containing the Lake is used. If a project is provided, the ExecutionSpec.service_account must belong to this project.
     */
    project?: string;
    /**
     * Service account to use to execute a task. If not provided, the default Compute service account for the project is used.
     */
    serviceAccount: string;
}

export interface DataplexTaskExecutionStatus {
    latestJobs: outputs.DataplexTaskExecutionStatusLatestJob[];
    updateTime: string;
}

export interface DataplexTaskExecutionStatusLatestJob {
    endTime: string;
    message: string;
    name: string;
    retryCount: number;
    service: string;
    serviceJob: string;
    startTime: string;
    state: string;
    uid: string;
}

export interface DataplexTaskIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataplexTaskIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataplexTaskNotebook {
    /**
     * Cloud Storage URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
     */
    archiveUris?: string[];
    /**
     * Cloud Storage URIs of files to be placed in the working directory of each executor.
     */
    fileUris?: string[];
    /**
     * Infrastructure specification for the execution.
     */
    infrastructureSpec?: outputs.DataplexTaskNotebookInfrastructureSpec;
    /**
     * Path to input notebook. This can be the Cloud Storage URI of the notebook file or the path to a Notebook Content. The execution args are accessible as environment variables (TASK_key=value).
     */
    notebook: string;
}

export interface DataplexTaskNotebookInfrastructureSpec {
    /**
     * Compute resources needed for a Task when using Dataproc Serverless.
     */
    batch?: outputs.DataplexTaskNotebookInfrastructureSpecBatch;
    /**
     * Container Image Runtime Configuration.
     */
    containerImage?: outputs.DataplexTaskNotebookInfrastructureSpecContainerImage;
    /**
     * Vpc network.
     */
    vpcNetwork?: outputs.DataplexTaskNotebookInfrastructureSpecVpcNetwork;
}

export interface DataplexTaskNotebookInfrastructureSpecBatch {
    /**
     * Total number of job executors. Executor Count should be between 2 and 100. [Default=2]
     */
    executorsCount?: number;
    /**
     * Max configurable executors. If maxExecutorsCount > executorsCount, then auto-scaling is enabled. Max Executor Count should be between 2 and 1000. [Default=1000]
     */
    maxExecutorsCount?: number;
}

export interface DataplexTaskNotebookInfrastructureSpecContainerImage {
    /**
     * Container image to use.
     */
    image?: string;
    /**
     * A list of Java JARS to add to the classpath. Valid input includes Cloud Storage URIs to Jar binaries. For example, gs://bucket-name/my/path/to/file.jar
     */
    javaJars?: string[];
    /**
     * Override to common configuration of open source components installed on the Dataproc cluster. The properties to set on daemon config files. Property keys are specified in prefix:property format, for example core:hadoop.tmp.dir. For more information, see Cluster properties.
     */
    properties?: {[key: string]: string};
    /**
     * A list of python packages to be installed. Valid formats include Cloud Storage URI to a PIP installable library. For example, gs://bucket-name/my/path/to/lib.tar.gz
     */
    pythonPackages?: string[];
}

export interface DataplexTaskNotebookInfrastructureSpecVpcNetwork {
    /**
     * The Cloud VPC network in which the job is run. By default, the Cloud VPC network named Default within the project is used.
     */
    network?: string;
    /**
     * List of network tags to apply to the job.
     */
    networkTags?: string[];
    /**
     * The Cloud VPC sub-network in which the job is run.
     */
    subNetwork?: string;
}

export interface DataplexTaskSpark {
    /**
     * Cloud Storage URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
     */
    archiveUris?: string[];
    /**
     * Cloud Storage URIs of files to be placed in the working directory of each executor.
     */
    fileUris?: string[];
    /**
     * Infrastructure specification for the execution.
     */
    infrastructureSpec?: outputs.DataplexTaskSparkInfrastructureSpec;
    /**
     * The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris. The execution args are passed in as a sequence of named process arguments (--key=value).
     */
    mainClass?: string;
    /**
     * The Cloud Storage URI of the jar file that contains the main class. The execution args are passed in as a sequence of named process arguments (--key=value).
     */
    mainJarFileUri?: string;
    /**
     * The Gcloud Storage URI of the main Python file to use as the driver. Must be a .py file. The execution args are passed in as a sequence of named process arguments (--key=value).
     */
    pythonScriptFile?: string;
    /**
     * The query text. The execution args are used to declare a set of script variables (set key='value';).
     */
    sqlScript?: string;
    /**
     * A reference to a query file. This can be the Cloud Storage URI of the query file or it can the path to a SqlScript Content. The execution args are used to declare a set of script variables (set key='value';).
     */
    sqlScriptFile?: string;
}

export interface DataplexTaskSparkInfrastructureSpec {
    /**
     * Compute resources needed for a Task when using Dataproc Serverless.
     */
    batch?: outputs.DataplexTaskSparkInfrastructureSpecBatch;
    /**
     * Container Image Runtime Configuration.
     */
    containerImage?: outputs.DataplexTaskSparkInfrastructureSpecContainerImage;
    /**
     * Vpc network.
     */
    vpcNetwork?: outputs.DataplexTaskSparkInfrastructureSpecVpcNetwork;
}

export interface DataplexTaskSparkInfrastructureSpecBatch {
    /**
     * Total number of job executors. Executor Count should be between 2 and 100. [Default=2]
     */
    executorsCount?: number;
    /**
     * Max configurable executors. If maxExecutorsCount > executorsCount, then auto-scaling is enabled. Max Executor Count should be between 2 and 1000. [Default=1000]
     */
    maxExecutorsCount?: number;
}

export interface DataplexTaskSparkInfrastructureSpecContainerImage {
    /**
     * Container image to use.
     */
    image?: string;
    /**
     * A list of Java JARS to add to the classpath. Valid input includes Cloud Storage URIs to Jar binaries. For example, gs://bucket-name/my/path/to/file.jar
     */
    javaJars?: string[];
    /**
     * Override to common configuration of open source components installed on the Dataproc cluster. The properties to set on daemon config files. Property keys are specified in prefix:property format, for example core:hadoop.tmp.dir. For more information, see Cluster properties.
     */
    properties?: {[key: string]: string};
    /**
     * A list of python packages to be installed. Valid formats include Cloud Storage URI to a PIP installable library. For example, gs://bucket-name/my/path/to/lib.tar.gz
     */
    pythonPackages?: string[];
}

export interface DataplexTaskSparkInfrastructureSpecVpcNetwork {
    /**
     * The Cloud VPC network in which the job is run. By default, the Cloud VPC network named Default within the project is used.
     */
    network?: string;
    /**
     * List of network tags to apply to the job.
     */
    networkTags?: string[];
    /**
     * The Cloud VPC sub-network in which the job is run.
     */
    subNetwork?: string;
}

export interface DataplexTaskTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DataplexTaskTriggerSpec {
    /**
     * Prevent the task from executing. This does not cancel already running tasks. It is intended to temporarily disable RECURRING tasks.
     */
    disabled?: boolean;
    /**
     * Number of retry attempts before aborting. Set to zero to never attempt to retry a failed task.
     */
    maxRetries?: number;
    /**
     * Cron schedule (https://en.wikipedia.org/wiki/Cron) for running tasks periodically. To explicitly set a timezone to the cron tab, apply a prefix in the cron tab: 'CRON_TZ=${IANA_TIME_ZONE}' or 'TZ=${IANA_TIME_ZONE}'. The ${IANA_TIME_ZONE} may only be a valid string from IANA time zone database. For example, CRON_TZ=America/New_York 1 * * * *, or TZ=America/New_York 1 * * * *. This field is required for RECURRING tasks.
     */
    schedule?: string;
    /**
     * The first run of the task will be after this time. If not specified, the task will run shortly after being submitted if ON_DEMAND and based on the schedule if RECURRING.
     */
    startTime?: string;
    /**
     * Trigger type of the user-specified Task Possible values: ["ON_DEMAND", "RECURRING"]
     */
    type: string;
}

export interface DataplexZoneAssetStatus {
    activeAssets: number;
    securityPolicyApplyingAssets: number;
    updateTime: string;
}

export interface DataplexZoneDiscoverySpec {
    /**
     * Optional. Configuration for CSV data.
     */
    csvOptions?: outputs.DataplexZoneDiscoverySpecCsvOptions;
    /**
     * Required. Whether discovery is enabled.
     */
    enabled: boolean;
    /**
     * Optional. The list of patterns to apply for selecting data to exclude during discovery. For Cloud Storage bucket assets, these are interpreted as glob patterns used to match object names. For BigQuery dataset assets, these are interpreted as patterns to match table names.
     */
    excludePatterns?: string[];
    /**
     * Optional. The list of patterns to apply for selecting data to include during discovery if only a subset of the data should considered. For Cloud Storage bucket assets, these are interpreted as glob patterns used to match object names. For BigQuery dataset assets, these are interpreted as patterns to match table names.
     */
    includePatterns?: string[];
    /**
     * Optional. Configuration for Json data.
     */
    jsonOptions?: outputs.DataplexZoneDiscoverySpecJsonOptions;
    /**
     * Optional. Cron schedule (https://en.wikipedia.org/wiki/Cron) for running discovery periodically. Successive discovery runs must be scheduled at least 60 minutes apart. The default value is to run discovery every 60 minutes. To explicitly set a timezone to the cron tab, apply a prefix in the cron tab: "CRON_TZ=${IANA_TIME_ZONE}" or TZ=${IANA_TIME_ZONE}". The ${IANA_TIME_ZONE} may only be a valid string from IANA time zone database. For example, "CRON_TZ=America/New_York 1 * * * *", or "TZ=America/New_York 1 * * * *".
     */
    schedule: string;
}

export interface DataplexZoneDiscoverySpecCsvOptions {
    /**
     * Optional. The delimiter being used to separate values. This defaults to ','.
     */
    delimiter?: string;
    /**
     * Optional. Whether to disable the inference of data type for CSV data. If true, all columns will be registered as strings.
     */
    disableTypeInference?: boolean;
    /**
     * Optional. The character encoding of the data. The default is UTF-8.
     */
    encoding?: string;
    /**
     * Optional. The number of rows to interpret as header rows that should be skipped when reading data rows.
     */
    headerRows?: number;
}

export interface DataplexZoneDiscoverySpecJsonOptions {
    /**
     * Optional. Whether to disable the inference of data type for Json data. If true, all columns will be registered as their primitive types (strings, number or boolean).
     */
    disableTypeInference?: boolean;
    /**
     * Optional. The character encoding of the data. The default is UTF-8.
     */
    encoding?: string;
}

export interface DataplexZoneIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataplexZoneIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataplexZoneResourceSpec {
    /**
     * Required. Immutable. The location type of the resources that are allowed to be attached to the assets within this zone. Possible values: LOCATION_TYPE_UNSPECIFIED, SINGLE_REGION, MULTI_REGION
     */
    locationType: string;
}

export interface DataplexZoneTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DataprocAutoscalingPolicyBasicAlgorithm {
    /**
     * Duration between scaling events. A scaling period starts after the
     * update operation from the previous event has completed.
     *
     * Bounds: [2m, 1d]. Default: 2m.
     */
    cooldownPeriod?: string;
    /**
     * YARN autoscaling configuration.
     */
    yarnConfig: outputs.DataprocAutoscalingPolicyBasicAlgorithmYarnConfig;
}

export interface DataprocAutoscalingPolicyBasicAlgorithmYarnConfig {
    /**
     * Timeout for YARN graceful decommissioning of Node Managers. Specifies the
     * duration to wait for jobs to complete before forcefully removing workers
     * (and potentially interrupting jobs). Only applicable to downscaling operations.
     *
     * Bounds: [0s, 1d].
     */
    gracefulDecommissionTimeout: string;
    /**
     * Fraction of average pending memory in the last cooldown period for which to
     * remove workers. A scale-down factor of 1 will result in scaling down so that there
     * is no available memory remaining after the update (more aggressive scaling).
     * A scale-down factor of 0 disables removing workers, which can be beneficial for
     * autoscaling a single job.
     *
     * Bounds: [0.0, 1.0].
     */
    scaleDownFactor: number;
    /**
     * Minimum scale-down threshold as a fraction of total cluster size before scaling occurs.
     * For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must
     * recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0
     * means the autoscaler will scale down on any recommended change.
     *
     * Bounds: [0.0, 1.0]. Default: 0.0.
     */
    scaleDownMinWorkerFraction?: number;
    /**
     * Fraction of average pending memory in the last cooldown period for which to
     * add workers. A scale-up factor of 1.0 will result in scaling up so that there
     * is no pending memory remaining after the update (more aggressive scaling).
     * A scale-up factor closer to 0 will result in a smaller magnitude of scaling up
     * (less aggressive scaling).
     *
     * Bounds: [0.0, 1.0].
     */
    scaleUpFactor: number;
    /**
     * Minimum scale-up threshold as a fraction of total cluster size before scaling
     * occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler
     * must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of
     * 0 means the autoscaler will scale up on any recommended change.
     *
     * Bounds: [0.0, 1.0]. Default: 0.0.
     */
    scaleUpMinWorkerFraction?: number;
}

export interface DataprocAutoscalingPolicyIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataprocAutoscalingPolicyIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataprocAutoscalingPolicySecondaryWorkerConfig {
    /**
     * Maximum number of instances for this group. Note that by default, clusters will not use
     * secondary workers. Required for secondary workers if the minimum secondary instances is set.
     * Bounds: [minInstances, ). Defaults to 0.
     */
    maxInstances?: number;
    /**
     * Minimum number of instances for this group. Bounds: [0, maxInstances]. Defaults to 0.
     */
    minInstances?: number;
    /**
     * Weight for the instance group, which is used to determine the fraction of total workers
     * in the cluster from this instance group. For example, if primary workers have weight 2,
     * and secondary workers have weight 1, the cluster will have approximately 2 primary workers
     * for each secondary worker.
     *
     * The cluster may not reach the specified balance if constrained by min/max bounds or other
     * autoscaling settings. For example, if maxInstances for secondary workers is 0, then only
     * primary workers will be added. The cluster can also be out of balance when created.
     *
     * If weight is not set on any instance group, the cluster will default to equal weight for
     * all groups: the cluster will attempt to maintain an equal number of workers in each group
     * within the configured size bounds for each group. If weight is set for one group only,
     * the cluster will default to zero weight on the unset group. For example if weight is set
     * only on primary workers, the cluster will use primary workers only and no secondary workers.
     */
    weight?: number;
}

export interface DataprocAutoscalingPolicyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DataprocAutoscalingPolicyWorkerConfig {
    /**
     * Maximum number of instances for this group.
     */
    maxInstances: number;
    /**
     * Minimum number of instances for this group. Bounds: [2, maxInstances]. Defaults to 2.
     */
    minInstances?: number;
    /**
     * Weight for the instance group, which is used to determine the fraction of total workers
     * in the cluster from this instance group. For example, if primary workers have weight 2,
     * and secondary workers have weight 1, the cluster will have approximately 2 primary workers
     * for each secondary worker.
     *
     * The cluster may not reach the specified balance if constrained by min/max bounds or other
     * autoscaling settings. For example, if maxInstances for secondary workers is 0, then only
     * primary workers will be added. The cluster can also be out of balance when created.
     *
     * If weight is not set on any instance group, the cluster will default to equal weight for
     * all groups: the cluster will attempt to maintain an equal number of workers in each group
     * within the configured size bounds for each group. If weight is set for one group only,
     * the cluster will default to zero weight on the unset group. For example if weight is set
     * only on primary workers, the cluster will use primary workers only and no secondary workers.
     */
    weight?: number;
}

export interface DataprocClusterClusterConfig {
    /**
     * The autoscaling policy config associated with the cluster.
     */
    autoscalingConfig?: outputs.DataprocClusterClusterConfigAutoscalingConfig;
    /**
     * The node group settings.
     */
    auxiliaryNodeGroups?: outputs.DataprocClusterClusterConfigAuxiliaryNodeGroup[];
    /**
     * The name of the cloud storage bucket ultimately used to house the staging data for the cluster. If staging_bucket is specified, it will contain this value, otherwise it will be the auto generated name.
     */
    bucket: string;
    /**
     * The config for Dataproc metrics.
     */
    dataprocMetricConfig?: outputs.DataprocClusterClusterConfigDataprocMetricConfig;
    /**
     * The Customer managed encryption keys settings for the cluster.
     */
    encryptionConfig?: outputs.DataprocClusterClusterConfigEncryptionConfig;
    /**
     * The config settings for port access on the cluster. Structure defined below.
     */
    endpointConfig?: outputs.DataprocClusterClusterConfigEndpointConfig;
    /**
     * Common config settings for resources of Google Compute Engine cluster instances, applicable to all instances in the cluster.
     */
    gceClusterConfig?: outputs.DataprocClusterClusterConfigGceClusterConfig;
    /**
     * Commands to execute on each node after config is completed. You can specify multiple versions of these.
     */
    initializationActions?: outputs.DataprocClusterClusterConfigInitializationAction[];
    /**
     * The settings for auto deletion cluster schedule.
     */
    lifecycleConfig?: outputs.DataprocClusterClusterConfigLifecycleConfig;
    /**
     * The Compute Engine config settings for the cluster's master instance.
     */
    masterConfig?: outputs.DataprocClusterClusterConfigMasterConfig;
    /**
     * Specifies a Metastore configuration.
     */
    metastoreConfig?: outputs.DataprocClusterClusterConfigMetastoreConfig;
    /**
     * The Google Compute Engine config settings for the additional (aka preemptible) instances in a cluster.
     */
    preemptibleWorkerConfig?: outputs.DataprocClusterClusterConfigPreemptibleWorkerConfig;
    /**
     * Security related configuration.
     */
    securityConfig?: outputs.DataprocClusterClusterConfigSecurityConfig;
    /**
     * The config settings for software inside the cluster.
     */
    softwareConfig?: outputs.DataprocClusterClusterConfigSoftwareConfig;
    /**
     * The Cloud Storage staging bucket used to stage files, such as Hadoop jars, between client machines and the cluster. Note: If you don't explicitly specify a staging_bucket then GCP will auto create / assign one for you. However, you are not guaranteed an auto generated bucket which is solely dedicated to your cluster; it may be shared with other clusters in the same region/zone also choosing to use the auto generation option.
     */
    stagingBucket?: string;
    /**
     * The Cloud Storage temp bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. Note: If you don't explicitly specify a temp_bucket then GCP will auto create / assign one for you.
     */
    tempBucket: string;
    /**
     * The Compute Engine config settings for the cluster's worker instances.
     */
    workerConfig?: outputs.DataprocClusterClusterConfigWorkerConfig;
}

export interface DataprocClusterClusterConfigAutoscalingConfig {
    /**
     * The autoscaling policy used by the cluster.
     */
    policyUri: string;
}

export interface DataprocClusterClusterConfigAuxiliaryNodeGroup {
    /**
     * A node group ID. Generated if not specified. The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of from 3 to 33 characters.
     */
    nodeGroupId: string;
    /**
     * Node group configuration.
     */
    nodeGroups: outputs.DataprocClusterClusterConfigAuxiliaryNodeGroupNodeGroup[];
}

export interface DataprocClusterClusterConfigAuxiliaryNodeGroupNodeGroup {
    /**
     * The Node group resource name.
     */
    name: string;
    /**
     * The node group instance group configuration.
     */
    nodeGroupConfig?: outputs.DataprocClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfig;
    /**
     * Node group roles.
     */
    roles: string[];
}

export interface DataprocClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfig {
    /**
     * The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
     */
    accelerators?: outputs.DataprocClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigAccelerator[];
    /**
     * Disk Config
     */
    diskConfig?: outputs.DataprocClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigDiskConfig;
    /**
     * List of auxiliary node group instance names which have been assigned to the cluster.
     */
    instanceNames: string[];
    /**
     * The name of a Google Compute Engine machine type to create for the master
     */
    machineType: string;
    /**
     * The name of a minimum generation of CPU family for the auxiliary node group. If not specified, GCP will default to a predetermined computed value for each zone.
     */
    minCpuPlatform: string;
    /**
     * Specifies the number of auxiliary nodes to create. If not specified, GCP will default to a predetermined computed value.
     */
    numInstances: number;
}

export interface DataprocClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigAccelerator {
    /**
     * The number of the accelerator cards of this type exposed to this instance. Often restricted to one of 1, 2, 4, or 8.
     */
    acceleratorCount: number;
    /**
     * The short name of the accelerator type to expose to this instance. For example, nvidia-tesla-k80.
     */
    acceleratorType: string;
}

export interface DataprocClusterClusterConfigAuxiliaryNodeGroupNodeGroupNodeGroupConfigDiskConfig {
    /**
     * Size of the primary disk attached to each node, specified in GB. The primary disk contains the boot volume and system libraries, and the smallest allowed disk size is 10GB. GCP will default to a predetermined computed value if not set (currently 500GB). Note: If SSDs are not attached, it also contains the HDFS data blocks and Hadoop working directories.
     */
    bootDiskSizeGb: number;
    /**
     * The disk type of the primary disk attached to each node. Such as "pd-ssd" or "pd-standard". Defaults to "pd-standard".
     */
    bootDiskType?: string;
    /**
     * Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express).
     */
    localSsdInterface?: string;
    /**
     * The amount of local SSD disks that will be attached to each master cluster node. Defaults to 0.
     */
    numLocalSsds: number;
}

export interface DataprocClusterClusterConfigDataprocMetricConfig {
    /**
     * Metrics sources to enable.
     */
    metrics: outputs.DataprocClusterClusterConfigDataprocMetricConfigMetric[];
}

export interface DataprocClusterClusterConfigDataprocMetricConfigMetric {
    /**
     * Specify one or more [available OSS metrics] (https://cloud.google.com/dataproc/docs/guides/monitoring#available_oss_metrics) to collect.
     */
    metricOverrides?: string[];
    /**
     * A source for the collection of Dataproc OSS metrics (see [available OSS metrics] (https://cloud.google.com//dataproc/docs/guides/monitoring#available_oss_metrics)).
     */
    metricSource: string;
}

export interface DataprocClusterClusterConfigEncryptionConfig {
    /**
     * The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
     */
    kmsKeyName: string;
}

export interface DataprocClusterClusterConfigEndpointConfig {
    /**
     * The flag to enable http access to specific ports on the cluster from external sources (aka Component Gateway). Defaults to false.
     */
    enableHttpPortAccess: boolean;
    /**
     * The map of port descriptions to URLs. Will only be populated if enable_http_port_access is true.
     */
    httpPorts: {[key: string]: string};
}

export interface DataprocClusterClusterConfigGceClusterConfig {
    /**
     * By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. If set to true, all instances in the cluster will only have internal IP addresses. Note: Private Google Access (also known as privateIpGoogleAccess) must be enabled on the subnetwork that the cluster will be launched in.
     */
    internalIpOnly?: boolean;
    /**
     * A map of the Compute Engine metadata entries to add to all instances
     */
    metadata?: {[key: string]: string};
    /**
     * The name or self_link of the Google Compute Engine network to the cluster will be part of. Conflicts with subnetwork. If neither is specified, this defaults to the "default" network.
     */
    network: string;
    /**
     * Node Group Affinity for sole-tenant clusters.
     */
    nodeGroupAffinity?: outputs.DataprocClusterClusterConfigGceClusterConfigNodeGroupAffinity;
    /**
     * Reservation Affinity for consuming Zonal reservation.
     */
    reservationAffinity?: outputs.DataprocClusterClusterConfigGceClusterConfigReservationAffinity;
    /**
     * The service account to be used by the Node VMs. If not specified, the "default" service account is used.
     */
    serviceAccount?: string;
    /**
     * The set of Google API scopes to be made available on all of the node VMs under the service_account specified. These can be either FQDNs, or scope aliases.
     */
    serviceAccountScopes: string[];
    /**
     * Shielded Instance Config for clusters using Compute Engine Shielded VMs.
     */
    shieldedInstanceConfig?: outputs.DataprocClusterClusterConfigGceClusterConfigShieldedInstanceConfig;
    /**
     * The name or self_link of the Google Compute Engine subnetwork the cluster will be part of. Conflicts with network.
     */
    subnetwork?: string;
    /**
     * The list of instance tags applied to instances in the cluster. Tags are used to identify valid sources or targets for network firewalls.
     */
    tags?: string[];
    /**
     * The GCP zone where your data is stored and used (i.e. where the master and the worker nodes will be created in). If region is set to 'global' (default) then zone is mandatory, otherwise GCP is able to make use of Auto Zone Placement to determine this automatically for you. Note: This setting additionally determines and restricts which computing resources are available for use with other configs such as cluster_config.master_config.machine_type and cluster_config.worker_config.machine_type.
     */
    zone: string;
}

export interface DataprocClusterClusterConfigGceClusterConfigNodeGroupAffinity {
    /**
     * The URI of a sole-tenant that the cluster will be created on.
     */
    nodeGroupUri: string;
}

export interface DataprocClusterClusterConfigGceClusterConfigReservationAffinity {
    /**
     * Type of reservation to consume.
     */
    consumeReservationType?: string;
    /**
     * Corresponds to the label key of reservation resource.
     */
    key?: string;
    /**
     * Corresponds to the label values of reservation resource.
     */
    values?: string[];
}

export interface DataprocClusterClusterConfigGceClusterConfigShieldedInstanceConfig {
    /**
     * Defines whether instances have integrity monitoring enabled.
     */
    enableIntegrityMonitoring?: boolean;
    /**
     * Defines whether instances have Secure Boot enabled.
     */
    enableSecureBoot?: boolean;
    /**
     * Defines whether instances have the vTPM enabled.
     */
    enableVtpm?: boolean;
}

export interface DataprocClusterClusterConfigInitializationAction {
    /**
     * The script to be executed during initialization of the cluster. The script must be a GCS file with a gs:// prefix.
     */
    script: string;
    /**
     * The maximum duration (in seconds) which script is allowed to take to execute its action. GCP will default to a predetermined computed value if not set (currently 300).
     */
    timeoutSec?: number;
}

export interface DataprocClusterClusterConfigLifecycleConfig {
    /**
     * The time when cluster will be auto-deleted. A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds. Example: "2014-10-02T15:01:23.045123456Z".
     */
    autoDeleteTime?: string;
    /**
     * The duration to keep the cluster alive while idling (no jobs running). After this TTL, the cluster will be deleted. Valid range: [10m, 14d].
     */
    idleDeleteTtl?: string;
    /**
     * Time when the cluster became idle (most recent job finished) and became eligible for deletion due to idleness.
     */
    idleStartTime: string;
}

export interface DataprocClusterClusterConfigMasterConfig {
    /**
     * The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
     */
    accelerators?: outputs.DataprocClusterClusterConfigMasterConfigAccelerator[];
    /**
     * Disk Config
     */
    diskConfig?: outputs.DataprocClusterClusterConfigMasterConfigDiskConfig;
    /**
     * The URI for the image to use for this master
     */
    imageUri: string;
    /**
     * List of master instance names which have been assigned to the cluster.
     */
    instanceNames: string[];
    /**
     * The name of a Google Compute Engine machine type to create for the master
     */
    machineType: string;
    /**
     * The name of a minimum generation of CPU family for the master. If not specified, GCP will default to a predetermined computed value for each zone.
     */
    minCpuPlatform: string;
    /**
     * Specifies the number of master nodes to create. If not specified, GCP will default to a predetermined computed value.
     */
    numInstances: number;
}

export interface DataprocClusterClusterConfigMasterConfigAccelerator {
    /**
     * The number of the accelerator cards of this type exposed to this instance. Often restricted to one of 1, 2, 4, or 8.
     */
    acceleratorCount: number;
    /**
     * The short name of the accelerator type to expose to this instance. For example, nvidia-tesla-k80.
     */
    acceleratorType: string;
}

export interface DataprocClusterClusterConfigMasterConfigDiskConfig {
    /**
     * Size of the primary disk attached to each node, specified in GB. The primary disk contains the boot volume and system libraries, and the smallest allowed disk size is 10GB. GCP will default to a predetermined computed value if not set (currently 500GB). Note: If SSDs are not attached, it also contains the HDFS data blocks and Hadoop working directories.
     */
    bootDiskSizeGb: number;
    /**
     * The disk type of the primary disk attached to each node. Such as "pd-ssd" or "pd-standard". Defaults to "pd-standard".
     */
    bootDiskType?: string;
    /**
     * Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express).
     */
    localSsdInterface?: string;
    /**
     * The amount of local SSD disks that will be attached to each master cluster node. Defaults to 0.
     */
    numLocalSsds: number;
}

export interface DataprocClusterClusterConfigMetastoreConfig {
    /**
     * Resource name of an existing Dataproc Metastore service.
     */
    dataprocMetastoreService: string;
}

export interface DataprocClusterClusterConfigPreemptibleWorkerConfig {
    /**
     * Disk Config
     */
    diskConfig?: outputs.DataprocClusterClusterConfigPreemptibleWorkerConfigDiskConfig;
    /**
     * Instance flexibility Policy allowing a mixture of VM shapes and provisioning models.
     */
    instanceFlexibilityPolicy?: outputs.DataprocClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicy;
    /**
     * List of preemptible instance names which have been assigned to the cluster.
     */
    instanceNames: string[];
    /**
     * Specifies the number of preemptible nodes to create. Defaults to 0.
     */
    numInstances: number;
    /**
     * Specifies the preemptibility of the secondary nodes. Defaults to PREEMPTIBLE.
     */
    preemptibility?: string;
}

export interface DataprocClusterClusterConfigPreemptibleWorkerConfigDiskConfig {
    /**
     * Size of the primary disk attached to each preemptible worker node, specified in GB. The smallest allowed disk size is 10GB. GCP will default to a predetermined computed value if not set (currently 500GB). Note: If SSDs are not attached, it also contains the HDFS data blocks and Hadoop working directories.
     */
    bootDiskSizeGb: number;
    /**
     * The disk type of the primary disk attached to each preemptible worker node. Such as "pd-ssd" or "pd-standard". Defaults to "pd-standard".
     */
    bootDiskType?: string;
    /**
     * Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express).
     */
    localSsdInterface?: string;
    /**
     * The amount of local SSD disks that will be attached to each preemptible worker node. Defaults to 0.
     */
    numLocalSsds: number;
}

export interface DataprocClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicy {
    /**
     * List of instance selection options that the group will use when creating new VMs.
     */
    instanceSelectionLists?: outputs.DataprocClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionList[];
    /**
     * A list of instance selection results in the group.
     */
    instanceSelectionResults: outputs.DataprocClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionResult[];
}

export interface DataprocClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionList {
    /**
     * Full machine-type names, e.g. "n1-standard-16".
     */
    machineTypes: string[];
    /**
     * Preference of this instance selection. Lower number means higher preference. Dataproc will first try to create a VM based on the machine-type with priority rank and fallback to next rank based on availability. Machine types and instance selections with the same priority have the same preference.
     */
    rank: number;
}

export interface DataprocClusterClusterConfigPreemptibleWorkerConfigInstanceFlexibilityPolicyInstanceSelectionResult {
    machineType: string;
    vmCount: number;
}

export interface DataprocClusterClusterConfigSecurityConfig {
    /**
     * Kerberos related configuration
     */
    kerberosConfig: outputs.DataprocClusterClusterConfigSecurityConfigKerberosConfig;
}

export interface DataprocClusterClusterConfigSecurityConfigKerberosConfig {
    /**
     * The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
     */
    crossRealmTrustAdminServer?: string;
    /**
     * The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
     */
    crossRealmTrustKdc?: string;
    /**
     * The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
     */
    crossRealmTrustRealm?: string;
    /**
     * The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster
     * Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
     */
    crossRealmTrustSharedPasswordUri?: string;
    /**
     * Flag to indicate whether to Kerberize the cluster.
     */
    enableKerberos?: boolean;
    /**
     * The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
     */
    kdcDbKeyUri?: string;
    /**
     * The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
     */
    keyPasswordUri?: string;
    /**
     * The Cloud Storage URI of a KMS encrypted file containing
     * the password to the user provided keystore. For the self-signed certificate, this password is generated
     * by Dataproc
     */
    keystorePasswordUri?: string;
    /**
     * The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
     */
    keystoreUri?: string;
    /**
     * The uri of the KMS key used to encrypt various sensitive files.
     */
    kmsKeyUri: string;
    /**
     * The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
     */
    realm?: string;
    /**
     * The cloud Storage URI of a KMS encrypted file containing the root principal password.
     */
    rootPrincipalPasswordUri: string;
    /**
     * The lifetime of the ticket granting ticket, in hours.
     */
    tgtLifetimeHours?: number;
    /**
     * The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
     */
    truststorePasswordUri?: string;
    /**
     * The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
     */
    truststoreUri?: string;
}

export interface DataprocClusterClusterConfigSoftwareConfig {
    /**
     * The Cloud Dataproc image version to use for the cluster - this controls the sets of software versions installed onto the nodes when you create clusters. If not specified, defaults to the latest version.
     */
    imageVersion: string;
    /**
     * The set of optional components to activate on the cluster.
     */
    optionalComponents?: string[];
    /**
     * A list of override and additional properties (key/value pairs) used to modify various aspects of the common configuration files used when creating a cluster.
     */
    overrideProperties?: {[key: string]: string};
    /**
     * A list of the properties used to set the daemon config files. This will include any values supplied by the user via cluster_config.software_config.override_properties
     */
    properties: {[key: string]: string};
}

export interface DataprocClusterClusterConfigWorkerConfig {
    /**
     * The Compute Engine accelerator (GPU) configuration for these instances. Can be specified multiple times.
     */
    accelerators?: outputs.DataprocClusterClusterConfigWorkerConfigAccelerator[];
    /**
     * Disk Config
     */
    diskConfig?: outputs.DataprocClusterClusterConfigWorkerConfigDiskConfig;
    /**
     * The URI for the image to use for this master/worker
     */
    imageUri: string;
    /**
     * List of master/worker instance names which have been assigned to the cluster.
     */
    instanceNames: string[];
    /**
     * The name of a Google Compute Engine machine type to create for the master/worker
     */
    machineType: string;
    /**
     * The name of a minimum generation of CPU family for the master/worker. If not specified, GCP will default to a predetermined computed value for each zone.
     */
    minCpuPlatform: string;
    /**
     * The minimum number of primary worker instances to create.
     */
    minNumInstances: number;
    /**
     * Specifies the number of worker nodes to create. If not specified, GCP will default to a predetermined computed value.
     */
    numInstances: number;
}

export interface DataprocClusterClusterConfigWorkerConfigAccelerator {
    /**
     * The number of the accelerator cards of this type exposed to this instance. Often restricted to one of 1, 2, 4, or 8.
     */
    acceleratorCount: number;
    /**
     * The short name of the accelerator type to expose to this instance. For example, nvidia-tesla-k80.
     */
    acceleratorType: string;
}

export interface DataprocClusterClusterConfigWorkerConfigDiskConfig {
    /**
     * Size of the primary disk attached to each node, specified in GB. The primary disk contains the boot volume and system libraries, and the smallest allowed disk size is 10GB. GCP will default to a predetermined computed value if not set (currently 500GB). Note: If SSDs are not attached, it also contains the HDFS data blocks and Hadoop working directories.
     */
    bootDiskSizeGb: number;
    /**
     * The disk type of the primary disk attached to each node. Such as "pd-ssd" or "pd-standard". Defaults to "pd-standard".
     */
    bootDiskType?: string;
    /**
     * Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express).
     */
    localSsdInterface?: string;
    /**
     * The amount of local SSD disks that will be attached to each master cluster node. Defaults to 0.
     */
    numLocalSsds: number;
}

export interface DataprocClusterIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataprocClusterIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataprocClusterTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DataprocClusterVirtualClusterConfig {
    /**
     * Auxiliary services configuration for a Cluster.
     */
    auxiliaryServicesConfig?: outputs.DataprocClusterVirtualClusterConfigAuxiliaryServicesConfig;
    /**
     * The configuration for running the Dataproc cluster on Kubernetes.
     */
    kubernetesClusterConfig?: outputs.DataprocClusterVirtualClusterConfigKubernetesClusterConfig;
    /**
     * A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket.
     */
    stagingBucket?: string;
}

export interface DataprocClusterVirtualClusterConfigAuxiliaryServicesConfig {
    /**
     * The Hive Metastore configuration for this workload.
     */
    metastoreConfig?: outputs.DataprocClusterVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig;
    /**
     * The Spark History Server configuration for the workload.
     */
    sparkHistoryServerConfig?: outputs.DataprocClusterVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig;
}

export interface DataprocClusterVirtualClusterConfigAuxiliaryServicesConfigMetastoreConfig {
    /**
     * The Hive Metastore configuration for this workload.
     */
    dataprocMetastoreService?: string;
}

export interface DataprocClusterVirtualClusterConfigAuxiliaryServicesConfigSparkHistoryServerConfig {
    /**
     * Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.
     */
    dataprocCluster?: string;
}

export interface DataprocClusterVirtualClusterConfigKubernetesClusterConfig {
    /**
     * The configuration for running the Dataproc cluster on GKE.
     */
    gkeClusterConfig: outputs.DataprocClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig;
    /**
     * A namespace within the Kubernetes cluster to deploy into. If this namespace does not exist, it is created. If it exists, Dataproc verifies that another Dataproc VirtualCluster is not installed into it. If not specified, the name of the Dataproc Cluster is used.
     */
    kubernetesNamespace?: string;
    /**
     * The software configuration for this Dataproc cluster running on Kubernetes.
     */
    kubernetesSoftwareConfig: outputs.DataprocClusterVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig;
}

export interface DataprocClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfig {
    /**
     * A target GKE cluster to deploy to. It must be in the same project and region as the Dataproc cluster (the GKE cluster can be zonal or regional). Format: 'projects/{project}/locations/{location}/clusters/{cluster_id}'
     */
    gkeClusterTarget?: string;
    /**
     * GKE node pools where workloads will be scheduled. At least one node pool must be assigned the DEFAULT GkeNodePoolTarget.Role. If a GkeNodePoolTarget is not specified, Dataproc constructs a DEFAULT GkeNodePoolTarget.
     */
    nodePoolTargets?: outputs.DataprocClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget[];
}

export interface DataprocClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTarget {
    /**
     * The target GKE node pool. Format: 'projects/{project}/locations/{location}/clusters/{cluster}/nodePools/{nodePool}'
     */
    nodePool: string;
    /**
     * Input only. The configuration for the GKE node pool.
     */
    nodePoolConfig?: outputs.DataprocClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig;
    /**
     * The roles associated with the GKE node pool.
     */
    roles: string[];
}

export interface DataprocClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfig {
    /**
     * The autoscaler configuration for this node pool. The autoscaler is enabled only when a valid configuration is present.
     */
    autoscaling?: outputs.DataprocClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling;
    /**
     * The node pool configuration.
     */
    config?: outputs.DataprocClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig;
    /**
     * The list of Compute Engine zones where node pool nodes associated with a Dataproc on GKE virtual cluster will be located.
     */
    locations: string[];
}

export interface DataprocClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigAutoscaling {
    /**
     * The maximum number of nodes in the node pool. Must be >= minNodeCount, and must be > 0.
     */
    maxNodeCount?: number;
    /**
     * The minimum number of nodes in the node pool. Must be >= 0 and <= maxNodeCount.
     */
    minNodeCount?: number;
}

export interface DataprocClusterVirtualClusterConfigKubernetesClusterConfigGkeClusterConfigNodePoolTargetNodePoolConfigConfig {
    /**
     * The minimum number of nodes in the node pool. Must be >= 0 and <= maxNodeCount.
     */
    localSsdCount?: number;
    /**
     * The name of a Compute Engine machine type.
     */
    machineType?: string;
    /**
     * Minimum CPU platform to be used by this instance. The instance may be scheduled on the specified or a newer CPU platform. Specify the friendly names of CPU platforms, such as "Intel Haswell" or "Intel Sandy Bridge".
     */
    minCpuPlatform?: string;
    /**
     * Whether the nodes are created as preemptible VM instances. Preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).
     */
    preemptible?: boolean;
    /**
     * Spot flag for enabling Spot VM, which is a rebrand of the existing preemptible flag.
     */
    spot?: boolean;
}

export interface DataprocClusterVirtualClusterConfigKubernetesClusterConfigKubernetesSoftwareConfig {
    /**
     * The components that should be installed in this Dataproc cluster. The key must be a string from the KubernetesComponent enumeration. The value is the version of the software to be installed.
     */
    componentVersion: {[key: string]: string};
    /**
     * The properties to set on daemon config files. Property keys are specified in prefix:property format, for example spark:spark.kubernetes.container.image.
     */
    properties: {[key: string]: string};
}

export interface DataprocJobHadoopConfig {
    /**
     * HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
     */
    archiveUris?: string[];
    /**
     * The arguments to pass to the driver.
     */
    args?: string[];
    /**
     * HCFS URIs of files to be copied to the working directory of Spark drivers and distributed tasks. Useful for naively parallel tasks.
     */
    fileUris?: string[];
    /**
     * HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
     */
    jarFileUris?: string[];
    /**
     * The runtime logging config of the job
     */
    loggingConfig?: outputs.DataprocJobHadoopConfigLoggingConfig;
    /**
     * The class containing the main method of the driver. Must be in a provided jar or jar that is already on the classpath. Conflicts with main_jar_file_uri
     */
    mainClass?: string;
    /**
     * The HCFS URI of jar file containing the driver jar. Conflicts with main_class
     */
    mainJarFileUri?: string;
    /**
     * A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
     */
    properties?: {[key: string]: string};
}

export interface DataprocJobHadoopConfigLoggingConfig {
    /**
     * Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
     */
    driverLogLevels: {[key: string]: string};
}

export interface DataprocJobHiveConfig {
    /**
     * Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
     */
    continueOnFailure?: boolean;
    /**
     * HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
     */
    jarFileUris?: string[];
    /**
     * A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code.
     */
    properties?: {[key: string]: string};
    /**
     * HCFS URI of file containing Hive script to execute as the job. Conflicts with query_list
     */
    queryFileUri?: string;
    /**
     * The list of Hive queries or statements to execute as part of the job. Conflicts with query_file_uri
     */
    queryLists?: string[];
    /**
     * Mapping of query variable names to values (equivalent to the Hive command: SET name="value";).
     */
    scriptVariables?: {[key: string]: string};
}

export interface DataprocJobIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataprocJobIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataprocJobPigConfig {
    /**
     * Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries. Defaults to false.
     */
    continueOnFailure?: boolean;
    /**
     * HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
     */
    jarFileUris?: string[];
    /**
     * The runtime logging config of the job
     */
    loggingConfig?: outputs.DataprocJobPigConfigLoggingConfig;
    /**
     * A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
     */
    properties?: {[key: string]: string};
    /**
     * HCFS URI of file containing Hive script to execute as the job. Conflicts with query_list
     */
    queryFileUri?: string;
    /**
     * The list of Hive queries or statements to execute as part of the job. Conflicts with query_file_uri
     */
    queryLists?: string[];
    /**
     * Mapping of query variable names to values (equivalent to the Pig command: name=[value]).
     */
    scriptVariables?: {[key: string]: string};
}

export interface DataprocJobPigConfigLoggingConfig {
    /**
     * Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
     */
    driverLogLevels: {[key: string]: string};
}

export interface DataprocJobPlacement {
    /**
     * The name of the cluster where the job will be submitted
     */
    clusterName: string;
    /**
     * Output-only. A cluster UUID generated by the Cloud Dataproc service when the job is submitted
     */
    clusterUuid: string;
}

export interface DataprocJobPrestoConfig {
    /**
     * Presto client tags to attach to this query.
     */
    clientTags?: string[];
    /**
     * Whether to continue executing queries if a query fails. Setting to true can be useful when executing independent parallel queries. Defaults to false.
     */
    continueOnFailure?: boolean;
    /**
     * The runtime logging config of the job
     */
    loggingConfig?: outputs.DataprocJobPrestoConfigLoggingConfig;
    /**
     * The format in which query output will be displayed. See the Presto documentation for supported output formats.
     */
    outputFormat?: string;
    /**
     * A mapping of property names to values. Used to set Presto session properties Equivalent to using the --session flag in the Presto CLI.
     */
    properties?: {[key: string]: string};
    /**
     * The HCFS URI of the script that contains SQL queries. Conflicts with query_list
     */
    queryFileUri?: string;
    /**
     * The list of SQL queries or statements to execute as part of the job. Conflicts with query_file_uri
     */
    queryLists?: string[];
}

export interface DataprocJobPrestoConfigLoggingConfig {
    /**
     * Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
     */
    driverLogLevels: {[key: string]: string};
}

export interface DataprocJobPysparkConfig {
    /**
     * Optional. HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip
     */
    archiveUris?: string[];
    /**
     * Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission
     */
    args?: string[];
    /**
     * Optional. HCFS URIs of files to be copied to the working directory of Python drivers and distributed tasks. Useful for naively parallel tasks
     */
    fileUris?: string[];
    /**
     * Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks
     */
    jarFileUris?: string[];
    /**
     * The runtime logging config of the job
     */
    loggingConfig?: outputs.DataprocJobPysparkConfigLoggingConfig;
    /**
     * Required. The HCFS URI of the main Python file to use as the driver. Must be a .py file
     */
    mainPythonFileUri: string;
    /**
     * Optional. A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code
     */
    properties?: {[key: string]: string};
    /**
     * Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip
     */
    pythonFileUris?: string[];
}

export interface DataprocJobPysparkConfigLoggingConfig {
    /**
     * Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
     */
    driverLogLevels: {[key: string]: string};
}

export interface DataprocJobReference {
    /**
     * The job ID, which must be unique within the project. The job ID is generated by the server upon job submission or provided by the user as a means to perform retries without creating duplicate jobs
     */
    jobId: string;
}

export interface DataprocJobScheduling {
    /**
     * Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
     */
    maxFailuresPerHour: number;
    /**
     * Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.
     */
    maxFailuresTotal: number;
}

export interface DataprocJobSparkConfig {
    /**
     * HCFS URIs of archives to be extracted in the working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
     */
    archiveUris?: string[];
    /**
     * The arguments to pass to the driver.
     */
    args?: string[];
    /**
     * HCFS URIs of files to be copied to the working directory of Spark drivers and distributed tasks. Useful for naively parallel tasks.
     */
    fileUris?: string[];
    /**
     * HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
     */
    jarFileUris?: string[];
    /**
     * The runtime logging config of the job
     */
    loggingConfig?: outputs.DataprocJobSparkConfigLoggingConfig;
    /**
     * The class containing the main method of the driver. Must be in a provided jar or jar that is already on the classpath. Conflicts with main_jar_file_uri
     */
    mainClass?: string;
    /**
     * The HCFS URI of jar file containing the driver jar. Conflicts with main_class
     */
    mainJarFileUri?: string;
    /**
     * A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Cloud Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
     */
    properties?: {[key: string]: string};
}

export interface DataprocJobSparkConfigLoggingConfig {
    /**
     * Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
     */
    driverLogLevels: {[key: string]: string};
}

export interface DataprocJobSparksqlConfig {
    /**
     * HCFS URIs of jar files to be added to the Spark CLASSPATH.
     */
    jarFileUris?: string[];
    /**
     * The runtime logging config of the job
     */
    loggingConfig?: outputs.DataprocJobSparksqlConfigLoggingConfig;
    /**
     * A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Cloud Dataproc API may be overwritten.
     */
    properties?: {[key: string]: string};
    /**
     * The HCFS URI of the script that contains SQL queries. Conflicts with query_list
     */
    queryFileUri?: string;
    /**
     * The list of SQL queries or statements to execute as part of the job. Conflicts with query_file_uri
     */
    queryLists?: string[];
    /**
     * Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
     */
    scriptVariables?: {[key: string]: string};
}

export interface DataprocJobSparksqlConfigLoggingConfig {
    /**
     * Optional. The per-package log levels for the driver. This may include 'root' package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'.
     */
    driverLogLevels: {[key: string]: string};
}

export interface DataprocJobStatus {
    details: string;
    state: string;
    stateStartTime: string;
    substate: string;
}

export interface DataprocJobTimeouts {
    create?: string;
    delete?: string;
}

export interface DataprocMetastoreFederationBackendMetastore {
    /**
     * The type of the backend metastore. Possible values: ["METASTORE_TYPE_UNSPECIFIED", "DATAPROC_METASTORE", "BIGQUERY"]
     */
    metastoreType: string;
    /**
     * The relative resource name of the metastore that is being federated. The formats of the relative resource names for the currently supported metastores are listed below: Dataplex: projects/{projectId}/locations/{location}/lakes/{lake_id} BigQuery: projects/{projectId} Dataproc Metastore: projects/{projectId}/locations/{location}/services/{serviceId}
     */
    name: string;
    rank: string;
}

export interface DataprocMetastoreFederationIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataprocMetastoreFederationIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataprocMetastoreFederationTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DataprocMetastoreServiceEncryptionConfig {
    /**
     * The fully qualified customer provided Cloud KMS key name to use for customer data encryption.
     * Use the following format: 'projects/([^/]+)/locations/([^/]+)/keyRings/([^/]+)/cryptoKeys/([^/]+)'
     */
    kmsKey: string;
}

export interface DataprocMetastoreServiceHiveMetastoreConfig {
    /**
     * A mapping of Hive metastore version to the auxiliary version configuration.
     * When specified, a secondary Hive metastore service is created along with the primary service.
     * All auxiliary versions must be less than the service's primary version.
     * The key is the auxiliary service name and it must match the regular expression a-z?.
     * This means that the first character must be a lowercase letter, and all the following characters must be hyphens, lowercase letters, or digits, except the last character, which cannot be a hyphen.
     */
    auxiliaryVersions?: outputs.DataprocMetastoreServiceHiveMetastoreConfigAuxiliaryVersion[];
    /**
     * A mapping of Hive metastore configuration key-value pairs to apply to the Hive metastore (configured in hive-site.xml).
     * The mappings override system defaults (some keys cannot be overridden)
     */
    configOverrides: {[key: string]: string};
    /**
     * The protocol to use for the metastore service endpoint. If unspecified, defaults to 'THRIFT'. Default value: "THRIFT" Possible values: ["THRIFT", "GRPC"]
     */
    endpointProtocol?: string;
    /**
     * Information used to configure the Hive metastore service as a service principal in a Kerberos realm.
     */
    kerberosConfig?: outputs.DataprocMetastoreServiceHiveMetastoreConfigKerberosConfig;
    /**
     * The Hive metastore schema version.
     */
    version: string;
}

export interface DataprocMetastoreServiceHiveMetastoreConfigAuxiliaryVersion {
    /**
     * A mapping of Hive metastore configuration key-value pairs to apply to the auxiliary Hive metastore (configured in hive-site.xml) in addition to the primary version's overrides.
     * If keys are present in both the auxiliary version's overrides and the primary version's overrides, the value from the auxiliary version's overrides takes precedence.
     */
    configOverrides?: {[key: string]: string};
    key: string;
    /**
     * The Hive metastore version of the auxiliary service. It must be less than the primary Hive metastore service's version.
     */
    version: string;
}

export interface DataprocMetastoreServiceHiveMetastoreConfigKerberosConfig {
    /**
     * A Kerberos keytab file that can be used to authenticate a service principal with a Kerberos Key Distribution Center (KDC).
     */
    keytab: outputs.DataprocMetastoreServiceHiveMetastoreConfigKerberosConfigKeytab;
    /**
     * A Cloud Storage URI that specifies the path to a krb5.conf file. It is of the form gs://{bucket_name}/path/to/krb5.conf, although the file does not need to be named krb5.conf explicitly.
     */
    krb5ConfigGcsUri: string;
    /**
     * A Kerberos principal that exists in the both the keytab the KDC to authenticate as. A typical principal is of the form "primary/instance@REALM", but there is no exact format.
     */
    principal: string;
}

export interface DataprocMetastoreServiceHiveMetastoreConfigKerberosConfigKeytab {
    /**
     * The relative resource name of a Secret Manager secret version, in the following form:
     *
     * "projects/{projectNumber}/secrets/{secret_id}/versions/{version_id}".
     */
    cloudSecret: string;
}

export interface DataprocMetastoreServiceIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataprocMetastoreServiceIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DataprocMetastoreServiceMaintenanceWindow {
    /**
     * The day of week, when the window starts. Possible values: ["MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    dayOfWeek: string;
    /**
     * The hour of day (0-23) when the window starts.
     */
    hourOfDay: number;
}

export interface DataprocMetastoreServiceMetadataIntegration {
    /**
     * The integration config for the Data Catalog service.
     */
    dataCatalogConfig: outputs.DataprocMetastoreServiceMetadataIntegrationDataCatalogConfig;
}

export interface DataprocMetastoreServiceMetadataIntegrationDataCatalogConfig {
    /**
     * Defines whether the metastore metadata should be synced to Data Catalog. The default value is to disable syncing metastore metadata to Data Catalog.
     */
    enabled: boolean;
}

export interface DataprocMetastoreServiceNetworkConfig {
    /**
     * The consumer-side network configuration for the Dataproc Metastore instance.
     */
    consumers: outputs.DataprocMetastoreServiceNetworkConfigConsumer[];
}

export interface DataprocMetastoreServiceNetworkConfigConsumer {
    /**
     * The URI of the endpoint used to access the metastore service.
     */
    endpointUri: string;
    /**
     * The subnetwork of the customer project from which an IP address is reserved and used as the Dataproc Metastore service's endpoint.
     * It is accessible to hosts in the subnet and to all hosts in a subnet in the same region and same network.
     * There must be at least one IP address available in the subnet's primary range. The subnet is specified in the following form:
     * 'projects/{projectNumber}/regions/{region_id}/subnetworks/{subnetwork_id}
     */
    subnetwork: string;
}

export interface DataprocMetastoreServiceScalingConfig {
    /**
     * Metastore instance sizes. Possible values: ["EXTRA_SMALL", "SMALL", "MEDIUM", "LARGE", "EXTRA_LARGE"]
     */
    instanceSize?: string;
    /**
     * Scaling factor, in increments of 0.1 for values less than 1.0, and increments of 1.0 for values greater than 1.0.
     */
    scalingFactor?: number;
}

export interface DataprocMetastoreServiceScheduledBackup {
    /**
     * A Cloud Storage URI of a folder, in the format gs://<bucket_name>/<path_inside_bucket>. A sub-folder <backup_folder> containing backup files will be stored below it.
     */
    backupLocation: string;
    /**
     * The scheduled interval in Cron format, see https://en.wikipedia.org/wiki/Cron The default is empty: scheduled backup is not enabled. Must be specified to enable scheduled backups.
     */
    cronSchedule?: string;
    /**
     * Defines whether the scheduled backup is enabled. The default value is false.
     */
    enabled: boolean;
    /**
     * Specifies the time zone to be used when interpreting cronSchedule. Must be a time zone name from the time zone database (https://en.wikipedia.org/wiki/List_of_tz_database_time_zones), e.g. America/Los_Angeles or Africa/Abidjan. If left unspecified, the default is UTC.
     */
    timeZone: string;
}

export interface DataprocMetastoreServiceTelemetryConfig {
    /**
     * The output format of the Dataproc Metastore service's logs. Default value: "JSON" Possible values: ["LEGACY", "JSON"]
     */
    logFormat?: string;
}

export interface DataprocMetastoreServiceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DataprocWorkflowTemplateJob {
    /**
     * Optional. Job is a Hadoop job.
     */
    hadoopJob?: outputs.DataprocWorkflowTemplateJobHadoopJob;
    /**
     * Optional. Job is a Hive job.
     */
    hiveJob?: outputs.DataprocWorkflowTemplateJobHiveJob;
    /**
     * Optional. The labels to associate with this job. Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: p{Ll}p{Lo}{0,62} Label values must be between 1 and 63 characters long, and must conform to the following regular expression: [p{Ll}p{Lo}p{N}_-]{0,63} No more than 32 labels can be associated with a given job.
     */
    labels?: {[key: string]: string};
    /**
     * Optional. Job is a Pig job.
     */
    pigJob?: outputs.DataprocWorkflowTemplateJobPigJob;
    /**
     * Optional. The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
     */
    prerequisiteStepIds?: string[];
    /**
     * Optional. Job is a Presto job.
     */
    prestoJob?: outputs.DataprocWorkflowTemplateJobPrestoJob;
    /**
     * Optional. Job is a PySpark job.
     */
    pysparkJob?: outputs.DataprocWorkflowTemplateJobPysparkJob;
    /**
     * Optional. Job scheduling configuration.
     */
    scheduling?: outputs.DataprocWorkflowTemplateJobScheduling;
    /**
     * Optional. Job is a Spark job.
     */
    sparkJob?: outputs.DataprocWorkflowTemplateJobSparkJob;
    /**
     * Optional. Job is a SparkR job.
     */
    sparkRJob?: outputs.DataprocWorkflowTemplateJobSparkRJob;
    /**
     * Optional. Job is a SparkSql job.
     */
    sparkSqlJob?: outputs.DataprocWorkflowTemplateJobSparkSqlJob;
    /**
     * Required. The step id. The id must be unique among all jobs within the template. The step id is used as prefix for job id, as job `goog-dataproc-workflow-step-id` label, and in prerequisiteStepIds field from other steps. The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
     */
    stepId: string;
}

export interface DataprocWorkflowTemplateJobHadoopJob {
    /**
     * Optional. HCFS URIs of archives to be extracted in the working directory of Hadoop drivers and tasks. Supported file types: .jar, .tar, .tar.gz, .tgz, or .zip.
     */
    archiveUris?: string[];
    /**
     * Optional. The arguments to pass to the driver. Do not include arguments, such as `-libjars` or `-Dfoo=bar`, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
     */
    args?: string[];
    /**
     * Optional. HCFS (Hadoop Compatible Filesystem) URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
     */
    fileUris?: string[];
    /**
     * Optional. Jar file URIs to add to the CLASSPATHs of the Hadoop driver and tasks.
     */
    jarFileUris?: string[];
    /**
     * Optional. The runtime log config for job execution.
     */
    loggingConfig?: outputs.DataprocWorkflowTemplateJobHadoopJobLoggingConfig;
    /**
     * The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in `jar_file_uris`.
     */
    mainClass?: string;
    /**
     * The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
     */
    mainJarFileUri?: string;
    /**
     * Optional. A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code.
     */
    properties?: {[key: string]: string};
}

export interface DataprocWorkflowTemplateJobHadoopJobLoggingConfig {
    /**
     * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
     */
    driverLogLevels?: {[key: string]: string};
}

export interface DataprocWorkflowTemplateJobHiveJob {
    /**
     * Optional. Whether to continue executing queries if a query fails. The default value is `false`. Setting to `true` can be useful when executing independent parallel queries.
     */
    continueOnFailure?: boolean;
    /**
     * Optional. HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
     */
    jarFileUris?: string[];
    /**
     * Optional. A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code.
     */
    properties?: {[key: string]: string};
    /**
     * The HCFS URI of the script that contains Hive queries.
     */
    queryFileUri?: string;
    /**
     * A list of queries.
     */
    queryList?: outputs.DataprocWorkflowTemplateJobHiveJobQueryList;
    /**
     * Optional. Mapping of query variable names to values (equivalent to the Hive command: `SET name="value";`).
     */
    scriptVariables?: {[key: string]: string};
}

export interface DataprocWorkflowTemplateJobHiveJobQueryList {
    /**
     * Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": [ "query1", "query2", "query3;query4", ] } }
     */
    queries: string[];
}

export interface DataprocWorkflowTemplateJobPigJob {
    /**
     * Optional. Whether to continue executing queries if a query fails. The default value is `false`. Setting to `true` can be useful when executing independent parallel queries.
     */
    continueOnFailure?: boolean;
    /**
     * Optional. HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
     */
    jarFileUris?: string[];
    /**
     * Optional. The runtime log config for job execution.
     */
    loggingConfig?: outputs.DataprocWorkflowTemplateJobPigJobLoggingConfig;
    /**
     * Optional. A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
     */
    properties?: {[key: string]: string};
    /**
     * The HCFS URI of the script that contains the Pig queries.
     */
    queryFileUri?: string;
    /**
     * A list of queries.
     */
    queryList?: outputs.DataprocWorkflowTemplateJobPigJobQueryList;
    /**
     * Optional. Mapping of query variable names to values (equivalent to the Pig command: `name=[value]`).
     */
    scriptVariables?: {[key: string]: string};
}

export interface DataprocWorkflowTemplateJobPigJobLoggingConfig {
    /**
     * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
     */
    driverLogLevels?: {[key: string]: string};
}

export interface DataprocWorkflowTemplateJobPigJobQueryList {
    /**
     * Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": [ "query1", "query2", "query3;query4", ] } }
     */
    queries: string[];
}

export interface DataprocWorkflowTemplateJobPrestoJob {
    /**
     * Optional. Presto client tags to attach to this query
     */
    clientTags?: string[];
    /**
     * Optional. Whether to continue executing queries if a query fails. The default value is `false`. Setting to `true` can be useful when executing independent parallel queries.
     */
    continueOnFailure?: boolean;
    /**
     * Optional. The runtime log config for job execution.
     */
    loggingConfig?: outputs.DataprocWorkflowTemplateJobPrestoJobLoggingConfig;
    /**
     * Optional. The format in which query output will be displayed. See the Presto documentation for supported output formats
     */
    outputFormat?: string;
    /**
     * Optional. A mapping of property names to values. Used to set Presto [session properties](https://prestodb.io/docs/current/sql/set-session.html) Equivalent to using the --session flag in the Presto CLI
     */
    properties?: {[key: string]: string};
    /**
     * The HCFS URI of the script that contains SQL queries.
     */
    queryFileUri?: string;
    /**
     * A list of queries.
     */
    queryList?: outputs.DataprocWorkflowTemplateJobPrestoJobQueryList;
}

export interface DataprocWorkflowTemplateJobPrestoJobLoggingConfig {
    /**
     * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
     */
    driverLogLevels?: {[key: string]: string};
}

export interface DataprocWorkflowTemplateJobPrestoJobQueryList {
    /**
     * Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": [ "query1", "query2", "query3;query4", ] } }
     */
    queries: string[];
}

export interface DataprocWorkflowTemplateJobPysparkJob {
    /**
     * Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
     */
    archiveUris?: string[];
    /**
     * Optional. The arguments to pass to the driver. Do not include arguments, such as `--conf`, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
     */
    args?: string[];
    /**
     * Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
     */
    fileUris?: string[];
    /**
     * Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
     */
    jarFileUris?: string[];
    /**
     * Optional. The runtime log config for job execution.
     */
    loggingConfig?: outputs.DataprocWorkflowTemplateJobPysparkJobLoggingConfig;
    /**
     * Required. The HCFS URI of the main Python file to use as the driver. Must be a .py file.
     */
    mainPythonFileUri: string;
    /**
     * Optional. A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
     */
    properties?: {[key: string]: string};
    /**
     * Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
     */
    pythonFileUris?: string[];
}

export interface DataprocWorkflowTemplateJobPysparkJobLoggingConfig {
    /**
     * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
     */
    driverLogLevels?: {[key: string]: string};
}

export interface DataprocWorkflowTemplateJobScheduling {
    /**
     * Optional. Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. A job may be reported as thrashing if driver exits with non-zero code 4 times within 10 minute window. Maximum value is 10.
     */
    maxFailuresPerHour?: number;
    /**
     * Optional. Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. Maximum value is 240.
     */
    maxFailuresTotal?: number;
}

export interface DataprocWorkflowTemplateJobSparkJob {
    /**
     * Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
     */
    archiveUris?: string[];
    /**
     * Optional. The arguments to pass to the driver. Do not include arguments, such as `--conf`, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
     */
    args?: string[];
    /**
     * Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
     */
    fileUris?: string[];
    /**
     * Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
     */
    jarFileUris?: string[];
    /**
     * Optional. The runtime log config for job execution.
     */
    loggingConfig?: outputs.DataprocWorkflowTemplateJobSparkJobLoggingConfig;
    /**
     * The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in `jar_file_uris`.
     */
    mainClass?: string;
    /**
     * The HCFS URI of the jar file that contains the main class.
     */
    mainJarFileUri?: string;
    /**
     * Optional. A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
     */
    properties?: {[key: string]: string};
}

export interface DataprocWorkflowTemplateJobSparkJobLoggingConfig {
    /**
     * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
     */
    driverLogLevels?: {[key: string]: string};
}

export interface DataprocWorkflowTemplateJobSparkRJob {
    /**
     * Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
     */
    archiveUris?: string[];
    /**
     * Optional. The arguments to pass to the driver. Do not include arguments, such as `--conf`, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
     */
    args?: string[];
    /**
     * Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
     */
    fileUris?: string[];
    /**
     * Optional. The runtime log config for job execution.
     */
    loggingConfig?: outputs.DataprocWorkflowTemplateJobSparkRJobLoggingConfig;
    /**
     * Required. The HCFS URI of the main R file to use as the driver. Must be a .R file.
     */
    mainRFileUri: string;
    /**
     * Optional. A mapping of property names to values, used to configure SparkR. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
     */
    properties?: {[key: string]: string};
}

export interface DataprocWorkflowTemplateJobSparkRJobLoggingConfig {
    /**
     * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
     */
    driverLogLevels?: {[key: string]: string};
}

export interface DataprocWorkflowTemplateJobSparkSqlJob {
    /**
     * Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
     */
    jarFileUris?: string[];
    /**
     * Optional. The runtime log config for job execution.
     */
    loggingConfig?: outputs.DataprocWorkflowTemplateJobSparkSqlJobLoggingConfig;
    /**
     * Optional. A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
     */
    properties?: {[key: string]: string};
    /**
     * The HCFS URI of the script that contains SQL queries.
     */
    queryFileUri?: string;
    /**
     * A list of queries.
     */
    queryList?: outputs.DataprocWorkflowTemplateJobSparkSqlJobQueryList;
    /**
     * Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET `name="value";`).
     */
    scriptVariables?: {[key: string]: string};
}

export interface DataprocWorkflowTemplateJobSparkSqlJobLoggingConfig {
    /**
     * The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
     */
    driverLogLevels?: {[key: string]: string};
}

export interface DataprocWorkflowTemplateJobSparkSqlJobQueryList {
    /**
     * Required. The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": [ "query1", "query2", "query3;query4", ] } }
     */
    queries: string[];
}

export interface DataprocWorkflowTemplateParameter {
    /**
     * Optional. Brief description of the parameter. Must not exceed 1024 characters.
     */
    description?: string;
    /**
     * Required. Paths to all fields that the parameter replaces. A field is allowed to appear in at most one parameter's list of field paths. A field path is similar in syntax to a google.protobuf.FieldMask. For example, a field path that references the zone field of a workflow template's cluster selector would be specified as `placement.clusterSelector.zone`. Also, field paths can reference fields using the following syntax: * Values in maps can be referenced by key: * labels['key'] * placement.clusterSelector.clusterLabels['key'] * placement.managedCluster.labels['key'] * placement.clusterSelector.clusterLabels['key'] * jobs['step-id'].labels['key'] * Jobs in the jobs list can be referenced by step-id: * jobs['step-id'].hadoopJob.mainJarFileUri * jobs['step-id'].hiveJob.queryFileUri * jobs['step-id'].pySparkJob.mainPythonFileUri * jobs['step-id'].hadoopJob.jarFileUris[0] * jobs['step-id'].hadoopJob.archiveUris[0] * jobs['step-id'].hadoopJob.fileUris[0] * jobs['step-id'].pySparkJob.pythonFileUris[0] * Items in repeated fields can be referenced by a zero-based index: * jobs['step-id'].sparkJob.args[0] * Other examples: * jobs['step-id'].hadoopJob.properties['key'] * jobs['step-id'].hadoopJob.args[0] * jobs['step-id'].hiveJob.scriptVariables['key'] * jobs['step-id'].hadoopJob.mainJarFileUri * placement.clusterSelector.zone It may not be possible to parameterize maps and repeated fields in their entirety since only individual map values and individual items in repeated fields can be referenced. For example, the following field paths are invalid: - placement.clusterSelector.clusterLabels - jobs['step-id'].sparkJob.args
     */
    fields: string[];
    /**
     * Required. Parameter name. The parameter name is used as the key, and paired with the parameter value, which are passed to the template when the template is instantiated. The name must contain only capital letters (A-Z), numbers (0-9), and underscores (_), and must not start with a number. The maximum length is 40 characters.
     */
    name: string;
    /**
     * Optional. Validation rules to be applied to this parameter's value.
     */
    validation?: outputs.DataprocWorkflowTemplateParameterValidation;
}

export interface DataprocWorkflowTemplateParameterValidation {
    /**
     * Validation based on regular expressions.
     */
    regex?: outputs.DataprocWorkflowTemplateParameterValidationRegex;
    /**
     * Validation based on a list of allowed values.
     */
    values?: outputs.DataprocWorkflowTemplateParameterValidationValues;
}

export interface DataprocWorkflowTemplateParameterValidationRegex {
    /**
     * Required. RE2 regular expressions used to validate the parameter's value. The value must match the regex in its entirety (substring matches are not sufficient).
     */
    regexes: string[];
}

export interface DataprocWorkflowTemplateParameterValidationValues {
    /**
     * Required. List of allowed values for the parameter.
     */
    values: string[];
}

export interface DataprocWorkflowTemplatePlacement {
    /**
     * Optional. A selector that chooses target cluster for jobs based on metadata. The selector is evaluated at the time each job is submitted.
     */
    clusterSelector?: outputs.DataprocWorkflowTemplatePlacementClusterSelector;
    /**
     * A cluster that is managed by the workflow.
     */
    managedCluster?: outputs.DataprocWorkflowTemplatePlacementManagedCluster;
}

export interface DataprocWorkflowTemplatePlacementClusterSelector {
    /**
     * Required. The cluster labels. Cluster must have all labels to match.
     */
    clusterLabels: {[key: string]: string};
    /**
     * Optional. The zone where workflow process executes. This parameter does not affect the selection of the cluster. If unspecified, the zone of the first cluster matching the selector is used.
     */
    zone: string;
}

export interface DataprocWorkflowTemplatePlacementManagedCluster {
    /**
     * Required. The cluster name prefix. A unique cluster name will be formed by appending a random suffix. The name must contain only lower-case letters (a-z), numbers (0-9), and hyphens (-). Must begin with a letter. Cannot begin or end with hyphen. Must consist of between 2 and 35 characters.
     */
    clusterName: string;
    /**
     * Required. The cluster configuration.
     */
    config: outputs.DataprocWorkflowTemplatePlacementManagedClusterConfig;
    /**
     * Optional. The labels to associate with this cluster. Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: p{Ll}p{Lo}{0,62} Label values must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: [p{Ll}p{Lo}p{N}_-]{0,63} No more than 32 labels can be associated with a given cluster.
     */
    labels?: {[key: string]: string};
}

export interface DataprocWorkflowTemplatePlacementManagedClusterConfig {
    /**
     * Optional. Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
     */
    autoscalingConfig?: outputs.DataprocWorkflowTemplatePlacementManagedClusterConfigAutoscalingConfig;
    /**
     * Optional. Encryption settings for the cluster.
     */
    encryptionConfig?: outputs.DataprocWorkflowTemplatePlacementManagedClusterConfigEncryptionConfig;
    /**
     * Optional. Port/endpoint configuration for this cluster
     */
    endpointConfig?: outputs.DataprocWorkflowTemplatePlacementManagedClusterConfigEndpointConfig;
    /**
     * Optional. The shared Compute Engine config settings for all instances in a cluster.
     */
    gceClusterConfig?: outputs.DataprocWorkflowTemplatePlacementManagedClusterConfigGceClusterConfig;
    /**
     * Optional. Commands to execute on each node after config is completed. By default, executables are run on master and all worker nodes. You can test a node's `role` metadata to run an executable on a master or worker node, as shown below using `curl` (you can also use `wget`): ROLE=$(curl -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if [[ "${ROLE}" == 'Master' ]]; then ... master specific actions ... else ... worker specific actions ... fi
     */
    initializationActions?: outputs.DataprocWorkflowTemplatePlacementManagedClusterConfigInitializationAction[];
    /**
     * Optional. Lifecycle setting for the cluster.
     */
    lifecycleConfig?: outputs.DataprocWorkflowTemplatePlacementManagedClusterConfigLifecycleConfig;
    /**
     * Optional. The Compute Engine config settings for the master instance in a cluster.
     */
    masterConfig?: outputs.DataprocWorkflowTemplatePlacementManagedClusterConfigMasterConfig;
    /**
     * Optional. The Compute Engine config settings for additional worker instances in a cluster.
     */
    secondaryWorkerConfig?: outputs.DataprocWorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfig;
    /**
     * Optional. Security settings for the cluster.
     */
    securityConfig?: outputs.DataprocWorkflowTemplatePlacementManagedClusterConfigSecurityConfig;
    /**
     * Optional. The config settings for software inside the cluster.
     */
    softwareConfig?: outputs.DataprocWorkflowTemplatePlacementManagedClusterConfigSoftwareConfig;
    /**
     * Optional. A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see [Dataproc staging bucket](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). **This field requires a Cloud Storage bucket name, not a URI to a Cloud Storage bucket.**
     */
    stagingBucket?: string;
    /**
     * Optional. A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket. **This field requires a Cloud Storage bucket name, not a URI to a Cloud Storage bucket.**
     */
    tempBucket?: string;
    /**
     * Optional. The Compute Engine config settings for worker instances in a cluster.
     */
    workerConfig?: outputs.DataprocWorkflowTemplatePlacementManagedClusterConfigWorkerConfig;
}

export interface DataprocWorkflowTemplatePlacementManagedClusterConfigAutoscalingConfig {
    /**
     * Optional. The autoscaling policy used by the cluster. Only resource names including projectid and location (region) are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]` * `projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]` Note that the policy must be in the same project and Dataproc region.
     */
    policy?: string;
}

export interface DataprocWorkflowTemplatePlacementManagedClusterConfigEncryptionConfig {
    /**
     * Optional. The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
     */
    gcePdKmsKeyName?: string;
}

export interface DataprocWorkflowTemplatePlacementManagedClusterConfigEndpointConfig {
    /**
     * Optional. If true, enable http access to specific ports on the cluster from external sources. Defaults to false.
     */
    enableHttpPortAccess?: boolean;
    /**
     * Output only. The map of port descriptions to URLs. Will only be populated if enable_http_port_access is true.
     */
    httpPorts: {[key: string]: string};
}

export interface DataprocWorkflowTemplatePlacementManagedClusterConfigGceClusterConfig {
    /**
     * Optional. If true, all instances in the cluster will only have internal IP addresses. By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. This `internal_ip_only` restriction can only be enabled for subnetwork enabled networks, and all off-cluster dependencies must be configured to be accessible without external IP addresses.
     */
    internalIpOnly: boolean;
    /**
     * The Compute Engine metadata entries to add to all instances (see [Project and instance metadata](https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
     */
    metadata?: {[key: string]: string};
    /**
     * Optional. The Compute Engine network to be used for machine communications. Cannot be specified with subnetwork_uri. If neither `network_uri` nor `subnetwork_uri` is specified, the "default" network of the project is used, if it exists. Cannot be a "Custom Subnet Network" (see [Using Subnetworks](https://cloud.google.com/compute/docs/subnetworks) for more information). A full URL, partial URI, or short name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/[project_id]/regions/global/default` * `projects/[project_id]/regions/global/default` * `default`
     */
    network?: string;
    /**
     * Optional. Node Group Affinity for sole-tenant clusters.
     */
    nodeGroupAffinity?: outputs.DataprocWorkflowTemplatePlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity;
    /**
     * Optional. The type of IPv6 access for a cluster. Possible values: PRIVATE_IPV6_GOOGLE_ACCESS_UNSPECIFIED, INHERIT_FROM_SUBNETWORK, OUTBOUND, BIDIRECTIONAL
     */
    privateIpv6GoogleAccess?: string;
    /**
     * Optional. Reservation Affinity for consuming Zonal reservation.
     */
    reservationAffinity?: outputs.DataprocWorkflowTemplatePlacementManagedClusterConfigGceClusterConfigReservationAffinity;
    /**
     * Optional. The [Dataproc service account](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#service_accounts_in_dataproc) (also see [VM Data Plane identity](https://cloud.google.com/dataproc/docs/concepts/iam/dataproc-principals#vm_service_account_data_plane_identity)) used by Dataproc cluster VM instances to access Google Cloud Platform services. If not specified, the [Compute Engine default service account](https://cloud.google.com/compute/docs/access/service-accounts#default_service_account) is used.
     */
    serviceAccount?: string;
    /**
     * Optional. The URIs of service account scopes to be included in Compute Engine instances. The following base set of scopes is always included: * https://www.googleapis.com/auth/cloud.useraccounts.readonly * https://www.googleapis.com/auth/devstorage.read_write * https://www.googleapis.com/auth/logging.write If no scopes are specified, the following defaults are also provided: * https://www.googleapis.com/auth/bigquery * https://www.googleapis.com/auth/bigtable.admin.table * https://www.googleapis.com/auth/bigtable.data * https://www.googleapis.com/auth/devstorage.full_control
     */
    serviceAccountScopes?: string[];
    /**
     * Optional. Shielded Instance Config for clusters using Compute Engine Shielded VMs.
     */
    shieldedInstanceConfig?: outputs.DataprocWorkflowTemplatePlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig;
    /**
     * Optional. The Compute Engine subnetwork to be used for machine communications. Cannot be specified with network_uri. A full URL, partial URI, or short name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/[project_id]/regions/us-east1/subnetworks/sub0` * `projects/[project_id]/regions/us-east1/subnetworks/sub0` * `sub0`
     */
    subnetwork?: string;
    /**
     * The Compute Engine tags to add to all instances (see [Tagging instances](https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
     */
    tags?: string[];
    /**
     * Optional. The zone where the Compute Engine cluster will be located. On a create request, it is required in the "global" region. If omitted in a non-global Dataproc region, the service will pick a zone in the corresponding Compute Engine region. On a get request, zone will always be present. A full URL, partial URI, or short name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]` * `projects/[project_id]/zones/[zone]` * `us-central1-f`
     */
    zone: string;
}

export interface DataprocWorkflowTemplatePlacementManagedClusterConfigGceClusterConfigNodeGroupAffinity {
    /**
     * Required. The URI of a sole-tenant [node group resource](https://cloud.google.com/compute/docs/reference/rest/v1/nodeGroups) that the cluster will be created on. A full URL, partial URI, or node group name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-central1-a/nodeGroups/node-group-1` * `projects/[project_id]/zones/us-central1-a/nodeGroups/node-group-1` * `node-group-1`
     */
    nodeGroup: string;
}

export interface DataprocWorkflowTemplatePlacementManagedClusterConfigGceClusterConfigReservationAffinity {
    /**
     * Optional. Type of reservation to consume Possible values: TYPE_UNSPECIFIED, NO_RESERVATION, ANY_RESERVATION, SPECIFIC_RESERVATION
     */
    consumeReservationType?: string;
    /**
     * Optional. Corresponds to the label key of reservation resource.
     */
    key?: string;
    /**
     * Optional. Corresponds to the label values of reservation resource.
     */
    values?: string[];
}

export interface DataprocWorkflowTemplatePlacementManagedClusterConfigGceClusterConfigShieldedInstanceConfig {
    /**
     * Optional. Defines whether instances have integrity monitoring enabled. Integrity monitoring compares the most recent boot measurements to the integrity policy baseline and returns a pair of pass/fail results depending on whether they match or not.
     */
    enableIntegrityMonitoring?: boolean;
    /**
     * Optional. Defines whether the instances have Secure Boot enabled. Secure Boot helps ensure that the system only runs authentic software by verifying the digital signature of all boot components, and halting the boot process if signature verification fails.
     */
    enableSecureBoot?: boolean;
    /**
     * Optional. Defines whether the instance have the vTPM enabled. Virtual Trusted Platform Module protects objects like keys, certificates and enables Measured Boot by performing the measurements needed to create a known good boot baseline, called the integrity policy baseline.
     */
    enableVtpm?: boolean;
}

export interface DataprocWorkflowTemplatePlacementManagedClusterConfigInitializationAction {
    /**
     * Required. Cloud Storage URI of executable file.
     */
    executableFile?: string;
    /**
     * Optional. Amount of time executable has to complete. Default is 10 minutes (see JSON representation of [Duration](https://developers.google.com/protocol-buffers/docs/proto3#json)). Cluster creation fails with an explanatory error message (the name of the executable that caused the error and the exceeded timeout period) if the executable is not completed at end of the timeout period.
     */
    executionTimeout?: string;
}

export interface DataprocWorkflowTemplatePlacementManagedClusterConfigLifecycleConfig {
    /**
     * Optional. The time when cluster will be auto-deleted (see JSON representation of [Timestamp](https://developers.google.com/protocol-buffers/docs/proto3#json)).
     */
    autoDeleteTime?: string;
    /**
     * Optional. The lifetime duration of cluster. The cluster will be auto-deleted at the end of this period. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of [Duration](https://developers.google.com/protocol-buffers/docs/proto3#json)).
     */
    autoDeleteTtl?: string;
    /**
     * Optional. The duration to keep the cluster alive while idling (when no jobs are running). Passing this threshold will cause the cluster to be deleted. Minimum value is 5 minutes; maximum value is 14 days (see JSON representation of [Duration](https://developers.google.com/protocol-buffers/docs/proto3#json)).
     */
    idleDeleteTtl?: string;
    /**
     * Output only. The time when cluster became idle (most recent job finished) and became eligible for deletion due to idleness (see JSON representation of [Timestamp](https://developers.google.com/protocol-buffers/docs/proto3#json)).
     */
    idleStartTime: string;
}

export interface DataprocWorkflowTemplatePlacementManagedClusterConfigMasterConfig {
    /**
     * Optional. The Compute Engine accelerator configuration for these instances.
     */
    accelerators?: outputs.DataprocWorkflowTemplatePlacementManagedClusterConfigMasterConfigAccelerator[];
    /**
     * Optional. Disk option config settings.
     */
    diskConfig?: outputs.DataprocWorkflowTemplatePlacementManagedClusterConfigMasterConfigDiskConfig;
    /**
     * Optional. The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/[image-id]` * `projects/[project_id]/global/images/[image-id]` * `image-id` Image family examples. Dataproc will use the most recent image from the family: * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/family/[custom-image-family-name]` * `projects/[project_id]/global/images/family/[custom-image-family-name]` If the URI is unspecified, it will be inferred from `SoftwareConfig.image_version` or the system default.
     */
    image?: string;
    /**
     * Output only. The list of instance names. Dataproc derives the names from `cluster_name`, `num_instances`, and the instance group.
     */
    instanceNames: string[];
    /**
     * Output only. Specifies that this instance group contains preemptible instances.
     */
    isPreemptible: boolean;
    /**
     * Optional. The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2` * `projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2` * `n1-standard-2` **Auto Zone Exception**: If you are using the Dataproc [Auto Zone Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, `n1-standard-2`.
     */
    machineType?: string;
    /**
     * Output only. The config for Compute Engine Instance Group Manager that manages this group. This is only used for preemptible instance groups.
     */
    managedGroupConfigs: outputs.DataprocWorkflowTemplatePlacementManagedClusterConfigMasterConfigManagedGroupConfig[];
    /**
     * Optional. Specifies the minimum cpu platform for the Instance Group. See [Dataproc > Minimum CPU Platform](https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
     */
    minCpuPlatform: string;
    /**
     * Optional. The number of VM instances in the instance group. For [HA cluster](https://www.terraform.io/dataproc/docs/concepts/configuring-clusters/high-availability) master_config groups, **must be set to 3**. For standard cluster master_config groups, **must be set to 1**.
     */
    numInstances?: number;
    /**
     * Optional. Specifies the preemptibility of the instance group. The default value for master and worker groups is `NON_PREEMPTIBLE`. This default cannot be changed. The default value for secondary instances is `PREEMPTIBLE`. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
     */
    preemptibility?: string;
}

export interface DataprocWorkflowTemplatePlacementManagedClusterConfigMasterConfigAccelerator {
    /**
     * The number of the accelerator cards of this type exposed to this instance.
     */
    acceleratorCount?: number;
    /**
     * Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See [Compute Engine AcceleratorTypes](https://cloud.google.com/compute/docs/reference/beta/acceleratorTypes). Examples: * `https://www.googleapis.com/compute/beta/projects/[project_id]/zones/us-east1-a/acceleratorTypes/nvidia-tesla-k80` * `projects/[project_id]/zones/us-east1-a/acceleratorTypes/nvidia-tesla-k80` * `nvidia-tesla-k80` **Auto Zone Exception**: If you are using the Dataproc [Auto Zone Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, `nvidia-tesla-k80`.
     */
    acceleratorType?: string;
}

export interface DataprocWorkflowTemplatePlacementManagedClusterConfigMasterConfigDiskConfig {
    /**
     * Optional. Size in GB of the boot disk (default is 500GB).
     */
    bootDiskSizeGb?: number;
    /**
     * Optional. Type of the boot disk (default is "pd-standard"). Valid values: "pd-balanced" (Persistent Disk Balanced Solid State Drive), "pd-ssd" (Persistent Disk Solid State Drive), or "pd-standard" (Persistent Disk Hard Disk Drive). See [Disk types](https://cloud.google.com/compute/docs/disks#disk-types).
     */
    bootDiskType?: string;
    /**
     * Optional. Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and [HDFS](https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
     */
    numLocalSsds: number;
}

export interface DataprocWorkflowTemplatePlacementManagedClusterConfigMasterConfigManagedGroupConfig {
    instanceGroupManagerName: string;
    instanceTemplateName: string;
}

export interface DataprocWorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfig {
    /**
     * Optional. The Compute Engine accelerator configuration for these instances.
     */
    accelerators?: outputs.DataprocWorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigAccelerator[];
    /**
     * Optional. Disk option config settings.
     */
    diskConfig?: outputs.DataprocWorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig;
    /**
     * Optional. The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/[image-id]` * `projects/[project_id]/global/images/[image-id]` * `image-id` Image family examples. Dataproc will use the most recent image from the family: * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/family/[custom-image-family-name]` * `projects/[project_id]/global/images/family/[custom-image-family-name]` If the URI is unspecified, it will be inferred from `SoftwareConfig.image_version` or the system default.
     */
    image?: string;
    /**
     * Output only. The list of instance names. Dataproc derives the names from `cluster_name`, `num_instances`, and the instance group.
     */
    instanceNames: string[];
    /**
     * Output only. Specifies that this instance group contains preemptible instances.
     */
    isPreemptible: boolean;
    /**
     * Optional. The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2` * `projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2` * `n1-standard-2` **Auto Zone Exception**: If you are using the Dataproc [Auto Zone Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, `n1-standard-2`.
     */
    machineType?: string;
    /**
     * Output only. The config for Compute Engine Instance Group Manager that manages this group. This is only used for preemptible instance groups.
     */
    managedGroupConfigs: outputs.DataprocWorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigManagedGroupConfig[];
    /**
     * Optional. Specifies the minimum cpu platform for the Instance Group. See [Dataproc > Minimum CPU Platform](https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
     */
    minCpuPlatform: string;
    /**
     * Optional. The number of VM instances in the instance group. For [HA cluster](https://www.terraform.io/dataproc/docs/concepts/configuring-clusters/high-availability) master_config groups, **must be set to 3**. For standard cluster master_config groups, **must be set to 1**.
     */
    numInstances?: number;
    /**
     * Optional. Specifies the preemptibility of the instance group. The default value for master and worker groups is `NON_PREEMPTIBLE`. This default cannot be changed. The default value for secondary instances is `PREEMPTIBLE`. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
     */
    preemptibility?: string;
}

export interface DataprocWorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigAccelerator {
    /**
     * The number of the accelerator cards of this type exposed to this instance.
     */
    acceleratorCount?: number;
    /**
     * Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See [Compute Engine AcceleratorTypes](https://cloud.google.com/compute/docs/reference/beta/acceleratorTypes). Examples: * `https://www.googleapis.com/compute/beta/projects/[project_id]/zones/us-east1-a/acceleratorTypes/nvidia-tesla-k80` * `projects/[project_id]/zones/us-east1-a/acceleratorTypes/nvidia-tesla-k80` * `nvidia-tesla-k80` **Auto Zone Exception**: If you are using the Dataproc [Auto Zone Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, `nvidia-tesla-k80`.
     */
    acceleratorType?: string;
}

export interface DataprocWorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigDiskConfig {
    /**
     * Optional. Size in GB of the boot disk (default is 500GB).
     */
    bootDiskSizeGb?: number;
    /**
     * Optional. Type of the boot disk (default is "pd-standard"). Valid values: "pd-balanced" (Persistent Disk Balanced Solid State Drive), "pd-ssd" (Persistent Disk Solid State Drive), or "pd-standard" (Persistent Disk Hard Disk Drive). See [Disk types](https://cloud.google.com/compute/docs/disks#disk-types).
     */
    bootDiskType?: string;
    /**
     * Optional. Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and [HDFS](https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
     */
    numLocalSsds: number;
}

export interface DataprocWorkflowTemplatePlacementManagedClusterConfigSecondaryWorkerConfigManagedGroupConfig {
    instanceGroupManagerName: string;
    instanceTemplateName: string;
}

export interface DataprocWorkflowTemplatePlacementManagedClusterConfigSecurityConfig {
    /**
     * Optional. Kerberos related configuration.
     */
    kerberosConfig?: outputs.DataprocWorkflowTemplatePlacementManagedClusterConfigSecurityConfigKerberosConfig;
}

export interface DataprocWorkflowTemplatePlacementManagedClusterConfigSecurityConfigKerberosConfig {
    /**
     * Optional. The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
     */
    crossRealmTrustAdminServer?: string;
    /**
     * Optional. The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
     */
    crossRealmTrustKdc?: string;
    /**
     * Optional. The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
     */
    crossRealmTrustRealm?: string;
    /**
     * Optional. The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
     */
    crossRealmTrustSharedPassword?: string;
    /**
     * Optional. Flag to indicate whether to Kerberize the cluster (default: false). Set this field to true to enable Kerberos on a cluster.
     */
    enableKerberos?: boolean;
    /**
     * Optional. The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
     */
    kdcDbKey?: string;
    /**
     * Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
     */
    keyPassword?: string;
    /**
     * Optional. The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
     */
    keystore?: string;
    /**
     * Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificate, this password is generated by Dataproc.
     */
    keystorePassword?: string;
    /**
     * Optional. The uri of the KMS key used to encrypt various sensitive files.
     */
    kmsKey?: string;
    /**
     * Optional. The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
     */
    realm?: string;
    /**
     * Optional. The Cloud Storage URI of a KMS encrypted file containing the root principal password.
     */
    rootPrincipalPassword?: string;
    /**
     * Optional. The lifetime of the ticket granting ticket, in hours. If not specified, or user specifies 0, then default value 10 will be used.
     */
    tgtLifetimeHours?: number;
    /**
     * Optional. The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
     */
    truststore?: string;
    /**
     * Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
     */
    truststorePassword?: string;
}

export interface DataprocWorkflowTemplatePlacementManagedClusterConfigSoftwareConfig {
    /**
     * Optional. The version of software inside the cluster. It must be one of the supported [Dataproc Versions](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#supported_dataproc_versions), such as "1.2" (including a subminor version, such as "1.2.29"), or the ["preview" version](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#other_versions). If unspecified, it defaults to the latest Debian version.
     */
    imageVersion?: string;
    /**
     * Optional. The set of components to activate on the cluster.
     */
    optionalComponents?: string[];
    /**
     * Optional. The properties to set on daemon config files. Property keys are specified in `prefix:property` format, for example `core:hadoop.tmp.dir`. The following are supported prefixes and their mappings: * capacity-scheduler: `capacity-scheduler.xml` * core: `core-site.xml` * distcp: `distcp-default.xml` * hdfs: `hdfs-site.xml` * hive: `hive-site.xml` * mapred: `mapred-site.xml` * pig: `pig.properties` * spark: `spark-defaults.conf` * yarn: `yarn-site.xml` For more information, see [Cluster properties](https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
     */
    properties?: {[key: string]: string};
}

export interface DataprocWorkflowTemplatePlacementManagedClusterConfigWorkerConfig {
    /**
     * Optional. The Compute Engine accelerator configuration for these instances.
     */
    accelerators?: outputs.DataprocWorkflowTemplatePlacementManagedClusterConfigWorkerConfigAccelerator[];
    /**
     * Optional. Disk option config settings.
     */
    diskConfig?: outputs.DataprocWorkflowTemplatePlacementManagedClusterConfigWorkerConfigDiskConfig;
    /**
     * Optional. The Compute Engine image resource used for cluster instances. The URI can represent an image or image family. Image examples: * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/[image-id]` * `projects/[project_id]/global/images/[image-id]` * `image-id` Image family examples. Dataproc will use the most recent image from the family: * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/family/[custom-image-family-name]` * `projects/[project_id]/global/images/family/[custom-image-family-name]` If the URI is unspecified, it will be inferred from `SoftwareConfig.image_version` or the system default.
     */
    image?: string;
    /**
     * Output only. The list of instance names. Dataproc derives the names from `cluster_name`, `num_instances`, and the instance group.
     */
    instanceNames: string[];
    /**
     * Output only. Specifies that this instance group contains preemptible instances.
     */
    isPreemptible: boolean;
    /**
     * Optional. The Compute Engine machine type used for cluster instances. A full URL, partial URI, or short name are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2` * `projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2` * `n1-standard-2` **Auto Zone Exception**: If you are using the Dataproc [Auto Zone Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, `n1-standard-2`.
     */
    machineType?: string;
    /**
     * Output only. The config for Compute Engine Instance Group Manager that manages this group. This is only used for preemptible instance groups.
     */
    managedGroupConfigs: outputs.DataprocWorkflowTemplatePlacementManagedClusterConfigWorkerConfigManagedGroupConfig[];
    /**
     * Optional. Specifies the minimum cpu platform for the Instance Group. See [Dataproc > Minimum CPU Platform](https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
     */
    minCpuPlatform: string;
    /**
     * Optional. The number of VM instances in the instance group. For [HA cluster](https://www.terraform.io/dataproc/docs/concepts/configuring-clusters/high-availability) master_config groups, **must be set to 3**. For standard cluster master_config groups, **must be set to 1**.
     */
    numInstances?: number;
    /**
     * Optional. Specifies the preemptibility of the instance group. The default value for master and worker groups is `NON_PREEMPTIBLE`. This default cannot be changed. The default value for secondary instances is `PREEMPTIBLE`. Possible values: PREEMPTIBILITY_UNSPECIFIED, NON_PREEMPTIBLE, PREEMPTIBLE
     */
    preemptibility?: string;
}

export interface DataprocWorkflowTemplatePlacementManagedClusterConfigWorkerConfigAccelerator {
    /**
     * The number of the accelerator cards of this type exposed to this instance.
     */
    acceleratorCount?: number;
    /**
     * Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See [Compute Engine AcceleratorTypes](https://cloud.google.com/compute/docs/reference/beta/acceleratorTypes). Examples: * `https://www.googleapis.com/compute/beta/projects/[project_id]/zones/us-east1-a/acceleratorTypes/nvidia-tesla-k80` * `projects/[project_id]/zones/us-east1-a/acceleratorTypes/nvidia-tesla-k80` * `nvidia-tesla-k80` **Auto Zone Exception**: If you are using the Dataproc [Auto Zone Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, `nvidia-tesla-k80`.
     */
    acceleratorType?: string;
}

export interface DataprocWorkflowTemplatePlacementManagedClusterConfigWorkerConfigDiskConfig {
    /**
     * Optional. Size in GB of the boot disk (default is 500GB).
     */
    bootDiskSizeGb?: number;
    /**
     * Optional. Type of the boot disk (default is "pd-standard"). Valid values: "pd-balanced" (Persistent Disk Balanced Solid State Drive), "pd-ssd" (Persistent Disk Solid State Drive), or "pd-standard" (Persistent Disk Hard Disk Drive). See [Disk types](https://cloud.google.com/compute/docs/disks#disk-types).
     */
    bootDiskType?: string;
    /**
     * Optional. Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and [HDFS](https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
     */
    numLocalSsds: number;
}

export interface DataprocWorkflowTemplatePlacementManagedClusterConfigWorkerConfigManagedGroupConfig {
    instanceGroupManagerName: string;
    instanceTemplateName: string;
}

export interface DataprocWorkflowTemplateTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DatastreamConnectionProfileBigqueryProfile {
}

export interface DatastreamConnectionProfileForwardSshConnectivity {
    /**
     * Hostname for the SSH tunnel.
     */
    hostname: string;
    /**
     * SSH password.
     */
    password?: string;
    /**
     * Port for the SSH tunnel.
     */
    port?: number;
    /**
     * SSH private key.
     */
    privateKey?: string;
    /**
     * Username for the SSH tunnel.
     */
    username: string;
}

export interface DatastreamConnectionProfileGcsProfile {
    /**
     * The Cloud Storage bucket name.
     */
    bucket: string;
    /**
     * The root path inside the Cloud Storage bucket.
     */
    rootPath?: string;
}

export interface DatastreamConnectionProfileMysqlProfile {
    /**
     * Hostname for the MySQL connection.
     */
    hostname: string;
    /**
     * Password for the MySQL connection.
     */
    password: string;
    /**
     * Port for the MySQL connection.
     */
    port?: number;
    /**
     * SSL configuration for the MySQL connection.
     */
    sslConfig?: outputs.DatastreamConnectionProfileMysqlProfileSslConfig;
    /**
     * Username for the MySQL connection.
     */
    username: string;
}

export interface DatastreamConnectionProfileMysqlProfileSslConfig {
    /**
     * PEM-encoded certificate of the CA that signed the source database
     * server's certificate.
     */
    caCertificate?: string;
    /**
     * Indicates whether the clientKey field is set.
     */
    caCertificateSet: boolean;
    /**
     * PEM-encoded certificate that will be used by the replica to
     * authenticate against the source database server. If this field
     * is used then the 'clientKey' and the 'caCertificate' fields are
     * mandatory.
     */
    clientCertificate?: string;
    /**
     * Indicates whether the clientCertificate field is set.
     */
    clientCertificateSet: boolean;
    /**
     * PEM-encoded private key associated with the Client Certificate.
     * If this field is used then the 'client_certificate' and the
     * 'ca_certificate' fields are mandatory.
     */
    clientKey?: string;
    /**
     * Indicates whether the clientKey field is set.
     */
    clientKeySet: boolean;
}

export interface DatastreamConnectionProfileOracleProfile {
    /**
     * Connection string attributes
     */
    connectionAttributes?: {[key: string]: string};
    /**
     * Database for the Oracle connection.
     */
    databaseService: string;
    /**
     * Hostname for the Oracle connection.
     */
    hostname: string;
    /**
     * Password for the Oracle connection.
     */
    password: string;
    /**
     * Port for the Oracle connection.
     */
    port?: number;
    /**
     * Username for the Oracle connection.
     */
    username: string;
}

export interface DatastreamConnectionProfilePostgresqlProfile {
    /**
     * Database for the PostgreSQL connection.
     */
    database: string;
    /**
     * Hostname for the PostgreSQL connection.
     */
    hostname: string;
    /**
     * Password for the PostgreSQL connection.
     */
    password: string;
    /**
     * Port for the PostgreSQL connection.
     */
    port?: number;
    /**
     * Username for the PostgreSQL connection.
     */
    username: string;
}

export interface DatastreamConnectionProfilePrivateConnectivity {
    /**
     * A reference to a private connection resource. Format: 'projects/{project}/locations/{location}/privateConnections/{name}'
     */
    privateConnection: string;
}

export interface DatastreamConnectionProfileSqlServerProfile {
    /**
     * Database for the SQL Server connection.
     */
    database: string;
    /**
     * Hostname for the SQL Server connection.
     */
    hostname: string;
    /**
     * Password for the SQL Server connection.
     */
    password: string;
    /**
     * Port for the SQL Server connection.
     */
    port?: number;
    /**
     * Username for the SQL Server connection.
     */
    username: string;
}

export interface DatastreamConnectionProfileTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DatastreamPrivateConnectionError {
    details: {[key: string]: string};
    message: string;
}

export interface DatastreamPrivateConnectionTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DatastreamPrivateConnectionVpcPeeringConfig {
    /**
     * A free subnet for peering. (CIDR of /29)
     */
    subnet: string;
    /**
     * Fully qualified name of the VPC that Datastream will peer to.
     * Format: projects/{project}/global/{networks}/{name}
     */
    vpc: string;
}

export interface DatastreamStreamBackfillAll {
    /**
     * MySQL data source objects to avoid backfilling.
     */
    mysqlExcludedObjects?: outputs.DatastreamStreamBackfillAllMysqlExcludedObjects;
    /**
     * PostgreSQL data source objects to avoid backfilling.
     */
    oracleExcludedObjects?: outputs.DatastreamStreamBackfillAllOracleExcludedObjects;
    /**
     * PostgreSQL data source objects to avoid backfilling.
     */
    postgresqlExcludedObjects?: outputs.DatastreamStreamBackfillAllPostgresqlExcludedObjects;
    /**
     * SQL Server data source objects to avoid backfilling.
     */
    sqlServerExcludedObjects?: outputs.DatastreamStreamBackfillAllSqlServerExcludedObjects;
}

export interface DatastreamStreamBackfillAllMysqlExcludedObjects {
    /**
     * MySQL databases on the server
     */
    mysqlDatabases: outputs.DatastreamStreamBackfillAllMysqlExcludedObjectsMysqlDatabase[];
}

export interface DatastreamStreamBackfillAllMysqlExcludedObjectsMysqlDatabase {
    /**
     * Database name.
     */
    database: string;
    /**
     * Tables in the database.
     */
    mysqlTables?: outputs.DatastreamStreamBackfillAllMysqlExcludedObjectsMysqlDatabaseMysqlTable[];
}

export interface DatastreamStreamBackfillAllMysqlExcludedObjectsMysqlDatabaseMysqlTable {
    /**
     * MySQL columns in the schema. When unspecified as part of include/exclude objects, includes/excludes everything.
     */
    mysqlColumns?: outputs.DatastreamStreamBackfillAllMysqlExcludedObjectsMysqlDatabaseMysqlTableMysqlColumn[];
    /**
     * Table name.
     */
    table: string;
}

export interface DatastreamStreamBackfillAllMysqlExcludedObjectsMysqlDatabaseMysqlTableMysqlColumn {
    /**
     * Column collation.
     */
    collation?: string;
    /**
     * Column name.
     */
    column?: string;
    /**
     * The MySQL data type. Full data types list can be found here:
     * https://dev.mysql.com/doc/refman/8.0/en/data-types.html
     */
    dataType?: string;
    /**
     * Column length.
     */
    length: number;
    /**
     * Whether or not the column can accept a null value.
     */
    nullable?: boolean;
    /**
     * The ordinal position of the column in the table.
     */
    ordinalPosition?: number;
    /**
     * Whether or not the column represents a primary key.
     */
    primaryKey?: boolean;
}

export interface DatastreamStreamBackfillAllOracleExcludedObjects {
    /**
     * Oracle schemas/databases in the database server
     */
    oracleSchemas: outputs.DatastreamStreamBackfillAllOracleExcludedObjectsOracleSchema[];
}

export interface DatastreamStreamBackfillAllOracleExcludedObjectsOracleSchema {
    /**
     * Tables in the database.
     */
    oracleTables?: outputs.DatastreamStreamBackfillAllOracleExcludedObjectsOracleSchemaOracleTable[];
    /**
     * Schema name.
     */
    schema: string;
}

export interface DatastreamStreamBackfillAllOracleExcludedObjectsOracleSchemaOracleTable {
    /**
     * Oracle columns in the schema. When unspecified as part of include/exclude objects, includes/excludes everything.
     */
    oracleColumns?: outputs.DatastreamStreamBackfillAllOracleExcludedObjectsOracleSchemaOracleTableOracleColumn[];
    /**
     * Table name.
     */
    table: string;
}

export interface DatastreamStreamBackfillAllOracleExcludedObjectsOracleSchemaOracleTableOracleColumn {
    /**
     * Column name.
     */
    column?: string;
    /**
     * The Oracle data type. Full data types list can be found here:
     * https://docs.oracle.com/en/database/oracle/oracle-database/21/sqlrf/Data-Types.html
     */
    dataType?: string;
    /**
     * Column encoding.
     */
    encoding: string;
    /**
     * Column length.
     */
    length: number;
    /**
     * Whether or not the column can accept a null value.
     */
    nullable: boolean;
    /**
     * The ordinal position of the column in the table.
     */
    ordinalPosition: number;
    /**
     * Column precision.
     */
    precision: number;
    /**
     * Whether or not the column represents a primary key.
     */
    primaryKey: boolean;
    /**
     * Column scale.
     */
    scale: number;
}

export interface DatastreamStreamBackfillAllPostgresqlExcludedObjects {
    /**
     * PostgreSQL schemas on the server
     */
    postgresqlSchemas: outputs.DatastreamStreamBackfillAllPostgresqlExcludedObjectsPostgresqlSchema[];
}

export interface DatastreamStreamBackfillAllPostgresqlExcludedObjectsPostgresqlSchema {
    /**
     * Tables in the schema.
     */
    postgresqlTables?: outputs.DatastreamStreamBackfillAllPostgresqlExcludedObjectsPostgresqlSchemaPostgresqlTable[];
    /**
     * Database name.
     */
    schema: string;
}

export interface DatastreamStreamBackfillAllPostgresqlExcludedObjectsPostgresqlSchemaPostgresqlTable {
    /**
     * PostgreSQL columns in the schema. When unspecified as part of include/exclude objects, includes/excludes everything.
     */
    postgresqlColumns?: outputs.DatastreamStreamBackfillAllPostgresqlExcludedObjectsPostgresqlSchemaPostgresqlTablePostgresqlColumn[];
    /**
     * Table name.
     */
    table: string;
}

export interface DatastreamStreamBackfillAllPostgresqlExcludedObjectsPostgresqlSchemaPostgresqlTablePostgresqlColumn {
    /**
     * Column name.
     */
    column?: string;
    /**
     * The PostgreSQL data type. Full data types list can be found here:
     * https://www.postgresql.org/docs/current/datatype.html
     */
    dataType?: string;
    /**
     * Column length.
     */
    length: number;
    /**
     * Whether or not the column can accept a null value.
     */
    nullable?: boolean;
    /**
     * The ordinal position of the column in the table.
     */
    ordinalPosition?: number;
    /**
     * Column precision.
     */
    precision: number;
    /**
     * Whether or not the column represents a primary key.
     */
    primaryKey?: boolean;
    /**
     * Column scale.
     */
    scale: number;
}

export interface DatastreamStreamBackfillAllSqlServerExcludedObjects {
    /**
     * SQL Server schemas/databases in the database server
     */
    schemas: outputs.DatastreamStreamBackfillAllSqlServerExcludedObjectsSchema[];
}

export interface DatastreamStreamBackfillAllSqlServerExcludedObjectsSchema {
    /**
     * Schema name.
     */
    schema: string;
    /**
     * Tables in the database.
     */
    tables?: outputs.DatastreamStreamBackfillAllSqlServerExcludedObjectsSchemaTable[];
}

export interface DatastreamStreamBackfillAllSqlServerExcludedObjectsSchemaTable {
    /**
     * SQL Server columns in the schema. When unspecified as part of include/exclude objects, includes/excludes everything.
     */
    columns?: outputs.DatastreamStreamBackfillAllSqlServerExcludedObjectsSchemaTableColumn[];
    /**
     * Table name.
     */
    table: string;
}

export interface DatastreamStreamBackfillAllSqlServerExcludedObjectsSchemaTableColumn {
    /**
     * Column name.
     */
    column?: string;
    /**
     * The SQL Server data type. Full data types list can be found here:
     * https://learn.microsoft.com/en-us/sql/t-sql/data-types/data-types-transact-sql?view=sql-server-ver16
     */
    dataType?: string;
    /**
     * Column length.
     */
    length: number;
    /**
     * Whether or not the column can accept a null value.
     */
    nullable: boolean;
    /**
     * The ordinal position of the column in the table.
     */
    ordinalPosition: number;
    /**
     * Column precision.
     */
    precision: number;
    /**
     * Whether or not the column represents a primary key.
     */
    primaryKey: boolean;
    /**
     * Column scale.
     */
    scale: number;
}

export interface DatastreamStreamBackfillNone {
}

export interface DatastreamStreamDestinationConfig {
    /**
     * A configuration for how data should be loaded to Google BigQuery.
     */
    bigqueryDestinationConfig?: outputs.DatastreamStreamDestinationConfigBigqueryDestinationConfig;
    /**
     * Destination connection profile resource. Format: projects/{project}/locations/{location}/connectionProfiles/{name}
     */
    destinationConnectionProfile: string;
    /**
     * A configuration for how data should be loaded to Cloud Storage.
     */
    gcsDestinationConfig?: outputs.DatastreamStreamDestinationConfigGcsDestinationConfig;
}

export interface DatastreamStreamDestinationConfigBigqueryDestinationConfig {
    /**
     * AppendOnly mode defines that the stream of changes (INSERT, UPDATE-INSERT, UPDATE-DELETE and DELETE
     * events) to a source table will be written to the destination Google BigQuery table, retaining the
     * historical state of the data.
     */
    appendOnly?: outputs.DatastreamStreamDestinationConfigBigqueryDestinationConfigAppendOnly;
    /**
     * The guaranteed data freshness (in seconds) when querying tables created by the stream.
     * Editing this field will only affect new tables created in the future, but existing tables
     * will not be impacted. Lower values mean that queries will return fresher data, but may result in higher cost.
     * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s". Defaults to 900s.
     */
    dataFreshness?: string;
    /**
     * Merge mode defines that all changes to a table will be merged at the destination Google BigQuery
     * table. This is the default write mode. When selected, BigQuery reflects the way the data is stored
     * in the source database. With Merge mode, no historical record of the change events is kept.
     */
    merge?: outputs.DatastreamStreamDestinationConfigBigqueryDestinationConfigMerge;
    /**
     * A single target dataset to which all data will be streamed.
     */
    singleTargetDataset?: outputs.DatastreamStreamDestinationConfigBigqueryDestinationConfigSingleTargetDataset;
    /**
     * Destination datasets are created so that hierarchy of the destination data objects matches the source hierarchy.
     */
    sourceHierarchyDatasets?: outputs.DatastreamStreamDestinationConfigBigqueryDestinationConfigSourceHierarchyDatasets;
}

export interface DatastreamStreamDestinationConfigBigqueryDestinationConfigAppendOnly {
}

export interface DatastreamStreamDestinationConfigBigqueryDestinationConfigMerge {
}

export interface DatastreamStreamDestinationConfigBigqueryDestinationConfigSingleTargetDataset {
    /**
     * Dataset ID in the format projects/{project}/datasets/{dataset_id} or
     * {project}:{dataset_id}
     */
    datasetId: string;
}

export interface DatastreamStreamDestinationConfigBigqueryDestinationConfigSourceHierarchyDatasets {
    /**
     * Dataset template used for dynamic dataset creation.
     */
    datasetTemplate: outputs.DatastreamStreamDestinationConfigBigqueryDestinationConfigSourceHierarchyDatasetsDatasetTemplate;
}

export interface DatastreamStreamDestinationConfigBigqueryDestinationConfigSourceHierarchyDatasetsDatasetTemplate {
    /**
     * If supplied, every created dataset will have its name prefixed by the provided value.
     * The prefix and name will be separated by an underscore. i.e. _.
     */
    datasetIdPrefix?: string;
    /**
     * Describes the Cloud KMS encryption key that will be used to protect destination BigQuery
     * table. The BigQuery Service Account associated with your project requires access to this
     * encryption key. i.e. projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{cryptoKey}.
     * See https://cloud.google.com/bigquery/docs/customer-managed-encryption for more information.
     */
    kmsKeyName?: string;
    /**
     * The geographic location where the dataset should reside.
     * See https://cloud.google.com/bigquery/docs/locations for supported locations.
     */
    location: string;
}

export interface DatastreamStreamDestinationConfigGcsDestinationConfig {
    /**
     * AVRO file format configuration.
     */
    avroFileFormat?: outputs.DatastreamStreamDestinationConfigGcsDestinationConfigAvroFileFormat;
    /**
     * The maximum duration for which new events are added before a file is closed and a new file is created.
     * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s". Defaults to 900s.
     */
    fileRotationInterval: string;
    /**
     * The maximum file size to be saved in the bucket.
     */
    fileRotationMb: number;
    /**
     * JSON file format configuration.
     */
    jsonFileFormat?: outputs.DatastreamStreamDestinationConfigGcsDestinationConfigJsonFileFormat;
    /**
     * Path inside the Cloud Storage bucket to write data to.
     */
    path?: string;
}

export interface DatastreamStreamDestinationConfigGcsDestinationConfigAvroFileFormat {
}

export interface DatastreamStreamDestinationConfigGcsDestinationConfigJsonFileFormat {
    /**
     * Compression of the loaded JSON file. Possible values: ["NO_COMPRESSION", "GZIP"]
     */
    compression?: string;
    /**
     * The schema file format along JSON data files. Possible values: ["NO_SCHEMA_FILE", "AVRO_SCHEMA_FILE"]
     */
    schemaFileFormat?: string;
}

export interface DatastreamStreamSourceConfig {
    /**
     * MySQL data source configuration.
     */
    mysqlSourceConfig?: outputs.DatastreamStreamSourceConfigMysqlSourceConfig;
    /**
     * MySQL data source configuration.
     */
    oracleSourceConfig?: outputs.DatastreamStreamSourceConfigOracleSourceConfig;
    /**
     * PostgreSQL data source configuration.
     */
    postgresqlSourceConfig?: outputs.DatastreamStreamSourceConfigPostgresqlSourceConfig;
    /**
     * Source connection profile resource. Format: projects/{project}/locations/{location}/connectionProfiles/{name}
     */
    sourceConnectionProfile: string;
    /**
     * SQL Server data source configuration.
     */
    sqlServerSourceConfig?: outputs.DatastreamStreamSourceConfigSqlServerSourceConfig;
}

export interface DatastreamStreamSourceConfigMysqlSourceConfig {
    /**
     * MySQL objects to exclude from the stream.
     */
    excludeObjects?: outputs.DatastreamStreamSourceConfigMysqlSourceConfigExcludeObjects;
    /**
     * MySQL objects to retrieve from the source.
     */
    includeObjects?: outputs.DatastreamStreamSourceConfigMysqlSourceConfigIncludeObjects;
    /**
     * Maximum number of concurrent backfill tasks. The number should be non negative.
     * If not set (or set to 0), the system's default value will be used.
     */
    maxConcurrentBackfillTasks: number;
    /**
     * Maximum number of concurrent CDC tasks. The number should be non negative.
     * If not set (or set to 0), the system's default value will be used.
     */
    maxConcurrentCdcTasks: number;
}

export interface DatastreamStreamSourceConfigMysqlSourceConfigExcludeObjects {
    /**
     * MySQL databases on the server
     */
    mysqlDatabases: outputs.DatastreamStreamSourceConfigMysqlSourceConfigExcludeObjectsMysqlDatabase[];
}

export interface DatastreamStreamSourceConfigMysqlSourceConfigExcludeObjectsMysqlDatabase {
    /**
     * Database name.
     */
    database: string;
    /**
     * Tables in the database.
     */
    mysqlTables?: outputs.DatastreamStreamSourceConfigMysqlSourceConfigExcludeObjectsMysqlDatabaseMysqlTable[];
}

export interface DatastreamStreamSourceConfigMysqlSourceConfigExcludeObjectsMysqlDatabaseMysqlTable {
    /**
     * MySQL columns in the schema. When unspecified as part of include/exclude objects, includes/excludes everything.
     */
    mysqlColumns?: outputs.DatastreamStreamSourceConfigMysqlSourceConfigExcludeObjectsMysqlDatabaseMysqlTableMysqlColumn[];
    /**
     * Table name.
     */
    table: string;
}

export interface DatastreamStreamSourceConfigMysqlSourceConfigExcludeObjectsMysqlDatabaseMysqlTableMysqlColumn {
    /**
     * Column collation.
     */
    collation?: string;
    /**
     * Column name.
     */
    column?: string;
    /**
     * The MySQL data type. Full data types list can be found here:
     * https://dev.mysql.com/doc/refman/8.0/en/data-types.html
     */
    dataType?: string;
    /**
     * Column length.
     */
    length: number;
    /**
     * Whether or not the column can accept a null value.
     */
    nullable?: boolean;
    /**
     * The ordinal position of the column in the table.
     */
    ordinalPosition?: number;
    /**
     * Whether or not the column represents a primary key.
     */
    primaryKey?: boolean;
}

export interface DatastreamStreamSourceConfigMysqlSourceConfigIncludeObjects {
    /**
     * MySQL databases on the server
     */
    mysqlDatabases: outputs.DatastreamStreamSourceConfigMysqlSourceConfigIncludeObjectsMysqlDatabase[];
}

export interface DatastreamStreamSourceConfigMysqlSourceConfigIncludeObjectsMysqlDatabase {
    /**
     * Database name.
     */
    database: string;
    /**
     * Tables in the database.
     */
    mysqlTables?: outputs.DatastreamStreamSourceConfigMysqlSourceConfigIncludeObjectsMysqlDatabaseMysqlTable[];
}

export interface DatastreamStreamSourceConfigMysqlSourceConfigIncludeObjectsMysqlDatabaseMysqlTable {
    /**
     * MySQL columns in the schema. When unspecified as part of include/exclude objects, includes/excludes everything.
     */
    mysqlColumns?: outputs.DatastreamStreamSourceConfigMysqlSourceConfigIncludeObjectsMysqlDatabaseMysqlTableMysqlColumn[];
    /**
     * Table name.
     */
    table: string;
}

export interface DatastreamStreamSourceConfigMysqlSourceConfigIncludeObjectsMysqlDatabaseMysqlTableMysqlColumn {
    /**
     * Column collation.
     */
    collation?: string;
    /**
     * Column name.
     */
    column?: string;
    /**
     * The MySQL data type. Full data types list can be found here:
     * https://dev.mysql.com/doc/refman/8.0/en/data-types.html
     */
    dataType?: string;
    /**
     * Column length.
     */
    length: number;
    /**
     * Whether or not the column can accept a null value.
     */
    nullable?: boolean;
    /**
     * The ordinal position of the column in the table.
     */
    ordinalPosition?: number;
    /**
     * Whether or not the column represents a primary key.
     */
    primaryKey?: boolean;
}

export interface DatastreamStreamSourceConfigOracleSourceConfig {
    /**
     * Configuration to drop large object values.
     */
    dropLargeObjects?: outputs.DatastreamStreamSourceConfigOracleSourceConfigDropLargeObjects;
    /**
     * Oracle objects to exclude from the stream.
     */
    excludeObjects?: outputs.DatastreamStreamSourceConfigOracleSourceConfigExcludeObjects;
    /**
     * Oracle objects to retrieve from the source.
     */
    includeObjects?: outputs.DatastreamStreamSourceConfigOracleSourceConfigIncludeObjects;
    /**
     * Maximum number of concurrent backfill tasks. The number should be non negative.
     * If not set (or set to 0), the system's default value will be used.
     */
    maxConcurrentBackfillTasks: number;
    /**
     * Maximum number of concurrent CDC tasks. The number should be non negative.
     * If not set (or set to 0), the system's default value will be used.
     */
    maxConcurrentCdcTasks: number;
    /**
     * Configuration to drop large object values.
     */
    streamLargeObjects?: outputs.DatastreamStreamSourceConfigOracleSourceConfigStreamLargeObjects;
}

export interface DatastreamStreamSourceConfigOracleSourceConfigDropLargeObjects {
}

export interface DatastreamStreamSourceConfigOracleSourceConfigExcludeObjects {
    /**
     * Oracle schemas/databases in the database server
     */
    oracleSchemas: outputs.DatastreamStreamSourceConfigOracleSourceConfigExcludeObjectsOracleSchema[];
}

export interface DatastreamStreamSourceConfigOracleSourceConfigExcludeObjectsOracleSchema {
    /**
     * Tables in the database.
     */
    oracleTables?: outputs.DatastreamStreamSourceConfigOracleSourceConfigExcludeObjectsOracleSchemaOracleTable[];
    /**
     * Schema name.
     */
    schema: string;
}

export interface DatastreamStreamSourceConfigOracleSourceConfigExcludeObjectsOracleSchemaOracleTable {
    /**
     * Oracle columns in the schema. When unspecified as part of include/exclude objects, includes/excludes everything.
     */
    oracleColumns?: outputs.DatastreamStreamSourceConfigOracleSourceConfigExcludeObjectsOracleSchemaOracleTableOracleColumn[];
    /**
     * Table name.
     */
    table: string;
}

export interface DatastreamStreamSourceConfigOracleSourceConfigExcludeObjectsOracleSchemaOracleTableOracleColumn {
    /**
     * Column name.
     */
    column?: string;
    /**
     * The Oracle data type. Full data types list can be found here:
     * https://docs.oracle.com/en/database/oracle/oracle-database/21/sqlrf/Data-Types.html
     */
    dataType?: string;
    /**
     * Column encoding.
     */
    encoding: string;
    /**
     * Column length.
     */
    length: number;
    /**
     * Whether or not the column can accept a null value.
     */
    nullable: boolean;
    /**
     * The ordinal position of the column in the table.
     */
    ordinalPosition: number;
    /**
     * Column precision.
     */
    precision: number;
    /**
     * Whether or not the column represents a primary key.
     */
    primaryKey: boolean;
    /**
     * Column scale.
     */
    scale: number;
}

export interface DatastreamStreamSourceConfigOracleSourceConfigIncludeObjects {
    /**
     * Oracle schemas/databases in the database server
     */
    oracleSchemas: outputs.DatastreamStreamSourceConfigOracleSourceConfigIncludeObjectsOracleSchema[];
}

export interface DatastreamStreamSourceConfigOracleSourceConfigIncludeObjectsOracleSchema {
    /**
     * Tables in the database.
     */
    oracleTables?: outputs.DatastreamStreamSourceConfigOracleSourceConfigIncludeObjectsOracleSchemaOracleTable[];
    /**
     * Schema name.
     */
    schema: string;
}

export interface DatastreamStreamSourceConfigOracleSourceConfigIncludeObjectsOracleSchemaOracleTable {
    /**
     * Oracle columns in the schema. When unspecified as part of include/exclude objects, includes/excludes everything.
     */
    oracleColumns?: outputs.DatastreamStreamSourceConfigOracleSourceConfigIncludeObjectsOracleSchemaOracleTableOracleColumn[];
    /**
     * Table name.
     */
    table: string;
}

export interface DatastreamStreamSourceConfigOracleSourceConfigIncludeObjectsOracleSchemaOracleTableOracleColumn {
    /**
     * Column name.
     */
    column?: string;
    /**
     * The Oracle data type. Full data types list can be found here:
     * https://docs.oracle.com/en/database/oracle/oracle-database/21/sqlrf/Data-Types.html
     */
    dataType?: string;
    /**
     * Column encoding.
     */
    encoding: string;
    /**
     * Column length.
     */
    length: number;
    /**
     * Whether or not the column can accept a null value.
     */
    nullable: boolean;
    /**
     * The ordinal position of the column in the table.
     */
    ordinalPosition: number;
    /**
     * Column precision.
     */
    precision: number;
    /**
     * Whether or not the column represents a primary key.
     */
    primaryKey: boolean;
    /**
     * Column scale.
     */
    scale: number;
}

export interface DatastreamStreamSourceConfigOracleSourceConfigStreamLargeObjects {
}

export interface DatastreamStreamSourceConfigPostgresqlSourceConfig {
    /**
     * PostgreSQL objects to exclude from the stream.
     */
    excludeObjects?: outputs.DatastreamStreamSourceConfigPostgresqlSourceConfigExcludeObjects;
    /**
     * PostgreSQL objects to retrieve from the source.
     */
    includeObjects?: outputs.DatastreamStreamSourceConfigPostgresqlSourceConfigIncludeObjects;
    /**
     * Maximum number of concurrent backfill tasks. The number should be non
     * negative. If not set (or set to 0), the system's default value will be used.
     */
    maxConcurrentBackfillTasks: number;
    /**
     * The name of the publication that includes the set of all tables
     * that are defined in the stream's include_objects.
     */
    publication: string;
    /**
     * The name of the logical replication slot that's configured with
     * the pgoutput plugin.
     */
    replicationSlot: string;
}

export interface DatastreamStreamSourceConfigPostgresqlSourceConfigExcludeObjects {
    /**
     * PostgreSQL schemas on the server
     */
    postgresqlSchemas: outputs.DatastreamStreamSourceConfigPostgresqlSourceConfigExcludeObjectsPostgresqlSchema[];
}

export interface DatastreamStreamSourceConfigPostgresqlSourceConfigExcludeObjectsPostgresqlSchema {
    /**
     * Tables in the schema.
     */
    postgresqlTables?: outputs.DatastreamStreamSourceConfigPostgresqlSourceConfigExcludeObjectsPostgresqlSchemaPostgresqlTable[];
    /**
     * Database name.
     */
    schema: string;
}

export interface DatastreamStreamSourceConfigPostgresqlSourceConfigExcludeObjectsPostgresqlSchemaPostgresqlTable {
    /**
     * PostgreSQL columns in the schema. When unspecified as part of include/exclude objects, includes/excludes everything.
     */
    postgresqlColumns?: outputs.DatastreamStreamSourceConfigPostgresqlSourceConfigExcludeObjectsPostgresqlSchemaPostgresqlTablePostgresqlColumn[];
    /**
     * Table name.
     */
    table: string;
}

export interface DatastreamStreamSourceConfigPostgresqlSourceConfigExcludeObjectsPostgresqlSchemaPostgresqlTablePostgresqlColumn {
    /**
     * Column name.
     */
    column?: string;
    /**
     * The PostgreSQL data type. Full data types list can be found here:
     * https://www.postgresql.org/docs/current/datatype.html
     */
    dataType?: string;
    /**
     * Column length.
     */
    length: number;
    /**
     * Whether or not the column can accept a null value.
     */
    nullable?: boolean;
    /**
     * The ordinal position of the column in the table.
     */
    ordinalPosition?: number;
    /**
     * Column precision.
     */
    precision: number;
    /**
     * Whether or not the column represents a primary key.
     */
    primaryKey?: boolean;
    /**
     * Column scale.
     */
    scale: number;
}

export interface DatastreamStreamSourceConfigPostgresqlSourceConfigIncludeObjects {
    /**
     * PostgreSQL schemas on the server
     */
    postgresqlSchemas: outputs.DatastreamStreamSourceConfigPostgresqlSourceConfigIncludeObjectsPostgresqlSchema[];
}

export interface DatastreamStreamSourceConfigPostgresqlSourceConfigIncludeObjectsPostgresqlSchema {
    /**
     * Tables in the schema.
     */
    postgresqlTables?: outputs.DatastreamStreamSourceConfigPostgresqlSourceConfigIncludeObjectsPostgresqlSchemaPostgresqlTable[];
    /**
     * Database name.
     */
    schema: string;
}

export interface DatastreamStreamSourceConfigPostgresqlSourceConfigIncludeObjectsPostgresqlSchemaPostgresqlTable {
    /**
     * PostgreSQL columns in the schema. When unspecified as part of include/exclude objects, includes/excludes everything.
     */
    postgresqlColumns?: outputs.DatastreamStreamSourceConfigPostgresqlSourceConfigIncludeObjectsPostgresqlSchemaPostgresqlTablePostgresqlColumn[];
    /**
     * Table name.
     */
    table: string;
}

export interface DatastreamStreamSourceConfigPostgresqlSourceConfigIncludeObjectsPostgresqlSchemaPostgresqlTablePostgresqlColumn {
    /**
     * Column name.
     */
    column?: string;
    /**
     * The PostgreSQL data type. Full data types list can be found here:
     * https://www.postgresql.org/docs/current/datatype.html
     */
    dataType?: string;
    /**
     * Column length.
     */
    length: number;
    /**
     * Whether or not the column can accept a null value.
     */
    nullable?: boolean;
    /**
     * The ordinal position of the column in the table.
     */
    ordinalPosition?: number;
    /**
     * Column precision.
     */
    precision: number;
    /**
     * Whether or not the column represents a primary key.
     */
    primaryKey?: boolean;
    /**
     * Column scale.
     */
    scale: number;
}

export interface DatastreamStreamSourceConfigSqlServerSourceConfig {
    /**
     * CDC reader reads from change tables.
     */
    changeTables?: outputs.DatastreamStreamSourceConfigSqlServerSourceConfigChangeTables;
    /**
     * SQL Server objects to exclude from the stream.
     */
    excludeObjects?: outputs.DatastreamStreamSourceConfigSqlServerSourceConfigExcludeObjects;
    /**
     * SQL Server objects to retrieve from the source.
     */
    includeObjects?: outputs.DatastreamStreamSourceConfigSqlServerSourceConfigIncludeObjects;
    /**
     * Max concurrent backfill tasks.
     */
    maxConcurrentBackfillTasks: number;
    /**
     * Max concurrent CDC tasks.
     */
    maxConcurrentCdcTasks: number;
    /**
     * CDC reader reads from transaction logs.
     */
    transactionLogs?: outputs.DatastreamStreamSourceConfigSqlServerSourceConfigTransactionLogs;
}

export interface DatastreamStreamSourceConfigSqlServerSourceConfigChangeTables {
}

export interface DatastreamStreamSourceConfigSqlServerSourceConfigExcludeObjects {
    /**
     * SQL Server schemas/databases in the database server
     */
    schemas: outputs.DatastreamStreamSourceConfigSqlServerSourceConfigExcludeObjectsSchema[];
}

export interface DatastreamStreamSourceConfigSqlServerSourceConfigExcludeObjectsSchema {
    /**
     * Schema name.
     */
    schema: string;
    /**
     * Tables in the database.
     */
    tables?: outputs.DatastreamStreamSourceConfigSqlServerSourceConfigExcludeObjectsSchemaTable[];
}

export interface DatastreamStreamSourceConfigSqlServerSourceConfigExcludeObjectsSchemaTable {
    /**
     * SQL Server columns in the schema. When unspecified as part of include/exclude objects, includes/excludes everything.
     */
    columns?: outputs.DatastreamStreamSourceConfigSqlServerSourceConfigExcludeObjectsSchemaTableColumn[];
    /**
     * Table name.
     */
    table: string;
}

export interface DatastreamStreamSourceConfigSqlServerSourceConfigExcludeObjectsSchemaTableColumn {
    /**
     * Column name.
     */
    column?: string;
    /**
     * The SQL Server data type. Full data types list can be found here:
     * https://learn.microsoft.com/en-us/sql/t-sql/data-types/data-types-transact-sql?view=sql-server-ver16
     */
    dataType?: string;
    /**
     * Column length.
     */
    length: number;
    /**
     * Whether or not the column can accept a null value.
     */
    nullable: boolean;
    /**
     * The ordinal position of the column in the table.
     */
    ordinalPosition: number;
    /**
     * Column precision.
     */
    precision: number;
    /**
     * Whether or not the column represents a primary key.
     */
    primaryKey: boolean;
    /**
     * Column scale.
     */
    scale: number;
}

export interface DatastreamStreamSourceConfigSqlServerSourceConfigIncludeObjects {
    /**
     * SQL Server schemas/databases in the database server
     */
    schemas: outputs.DatastreamStreamSourceConfigSqlServerSourceConfigIncludeObjectsSchema[];
}

export interface DatastreamStreamSourceConfigSqlServerSourceConfigIncludeObjectsSchema {
    /**
     * Schema name.
     */
    schema: string;
    /**
     * Tables in the database.
     */
    tables?: outputs.DatastreamStreamSourceConfigSqlServerSourceConfigIncludeObjectsSchemaTable[];
}

export interface DatastreamStreamSourceConfigSqlServerSourceConfigIncludeObjectsSchemaTable {
    /**
     * SQL Server columns in the schema. When unspecified as part of include/exclude objects, includes/excludes everything.
     */
    columns?: outputs.DatastreamStreamSourceConfigSqlServerSourceConfigIncludeObjectsSchemaTableColumn[];
    /**
     * Table name.
     */
    table: string;
}

export interface DatastreamStreamSourceConfigSqlServerSourceConfigIncludeObjectsSchemaTableColumn {
    /**
     * Column name.
     */
    column?: string;
    /**
     * The SQL Server data type. Full data types list can be found here:
     * https://learn.microsoft.com/en-us/sql/t-sql/data-types/data-types-transact-sql?view=sql-server-ver16
     */
    dataType?: string;
    /**
     * Column length.
     */
    length: number;
    /**
     * Whether or not the column can accept a null value.
     */
    nullable: boolean;
    /**
     * The ordinal position of the column in the table.
     */
    ordinalPosition: number;
    /**
     * Column precision.
     */
    precision: number;
    /**
     * Whether or not the column represents a primary key.
     */
    primaryKey: boolean;
    /**
     * Column scale.
     */
    scale: number;
}

export interface DatastreamStreamSourceConfigSqlServerSourceConfigTransactionLogs {
}

export interface DatastreamStreamTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DeploymentManagerDeploymentLabel {
    /**
     * Key for label.
     */
    key?: string;
    /**
     * Value of label.
     */
    value?: string;
}

export interface DeploymentManagerDeploymentTarget {
    /**
     * The root configuration file to use for this deployment.
     */
    config: outputs.DeploymentManagerDeploymentTargetConfig;
    /**
     * Specifies import files for this configuration. This can be
     * used to import templates or other files. For example, you might
     * import a text file in order to use the file in a template.
     */
    imports?: outputs.DeploymentManagerDeploymentTargetImport[];
}

export interface DeploymentManagerDeploymentTargetConfig {
    /**
     * The full YAML contents of your configuration file.
     */
    content: string;
}

export interface DeploymentManagerDeploymentTargetImport {
    /**
     * The full contents of the template that you want to import.
     */
    content?: string;
    /**
     * The name of the template to import, as declared in the YAML
     * configuration.
     */
    name?: string;
}

export interface DeploymentManagerDeploymentTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DialogflowAgentTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DialogflowCxAgentAdvancedSettings {
    /**
     * If present, incoming audio is exported by Dialogflow to the configured Google Cloud Storage destination. Exposed at the following levels:
     * * Agent level
     * * Flow level
     */
    audioExportGcsDestination?: outputs.DialogflowCxAgentAdvancedSettingsAudioExportGcsDestination;
    /**
     * Define behaviors for DTMF (dual tone multi frequency). DTMF settings does not override each other. DTMF settings set at different levels define DTMF detections running in parallel. Exposed at the following levels:
     * * Agent level
     * * Flow level
     * * Page level
     * * Parameter level
     */
    dtmfSettings?: outputs.DialogflowCxAgentAdvancedSettingsDtmfSettings;
}

export interface DialogflowCxAgentAdvancedSettingsAudioExportGcsDestination {
    /**
     * The Google Cloud Storage URI for the exported objects. Whether a full object name, or just a prefix, its usage depends on the Dialogflow operation.
     * Format: gs://bucket/object-name-or-prefix
     */
    uri?: string;
}

export interface DialogflowCxAgentAdvancedSettingsDtmfSettings {
    /**
     * If true, incoming audio is processed for DTMF (dual tone multi frequency) events. For example, if the caller presses a button on their telephone keypad and DTMF processing is enabled, Dialogflow will detect the event (e.g. a "3" was pressed) in the incoming audio and pass the event to the bot to drive business logic (e.g. when 3 is pressed, return the account balance).
     */
    enabled?: boolean;
    /**
     * The digit that terminates a DTMF digit sequence.
     */
    finishDigit?: string;
    /**
     * Max length of DTMF digits.
     */
    maxDigits?: number;
}

export interface DialogflowCxAgentGitIntegrationSettings {
    /**
     * Settings of integration with GitHub.
     */
    githubSettings?: outputs.DialogflowCxAgentGitIntegrationSettingsGithubSettings;
}

export interface DialogflowCxAgentGitIntegrationSettingsGithubSettings {
    /**
     * The access token used to authenticate the access to the GitHub repository.
     */
    accessToken?: string;
    /**
     * A list of branches configured to be used from Dialogflow.
     */
    branches?: string[];
    /**
     * The unique repository display name for the GitHub repository.
     */
    displayName?: string;
    /**
     * The GitHub repository URI related to the agent.
     */
    repositoryUri?: string;
    /**
     * The branch of the GitHub repository tracked for this agent.
     */
    trackingBranch?: string;
}

export interface DialogflowCxAgentSpeechToTextSettings {
    /**
     * Whether to use speech adaptation for speech recognition.
     */
    enableSpeechAdaptation?: boolean;
}

export interface DialogflowCxAgentTextToSpeechSettings {
    /**
     * Configuration of how speech should be synthesized, mapping from [language](https://cloud.google.com/dialogflow/cx/docs/reference/language) to [SynthesizeSpeechConfig](https://cloud.google.com/dialogflow/cx/docs/reference/rest/v3/projects.locations.agents#synthesizespeechconfig).
     * These settings affect:
     * * The phone gateway synthesize configuration set via Agent.text_to_speech_settings.
     * * How speech is synthesized when invoking session APIs. 'Agent.text_to_speech_settings' only applies if 'OutputAudioConfig.synthesize_speech_config' is not specified.
     */
    synthesizeSpeechConfigs?: string;
}

export interface DialogflowCxAgentTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DialogflowCxEntityTypeEntity {
    /**
     * A collection of value synonyms. For example, if the entity type is vegetable, and value is scallions, a synonym could be green onions.
     * For KIND_LIST entity types: This collection must contain exactly one synonym equal to value.
     */
    synonyms?: string[];
    /**
     * The primary value associated with this entity entry. For example, if the entity type is vegetable, the value could be scallions.
     * For KIND_MAP entity types: A canonical value to be used in place of synonyms.
     * For KIND_LIST entity types: A string that can contain references to other entity types (with or without aliases).
     */
    value?: string;
}

export interface DialogflowCxEntityTypeExcludedPhrase {
    /**
     * The word or phrase to be excluded.
     */
    value?: string;
}

export interface DialogflowCxEntityTypeTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DialogflowCxEnvironmentTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DialogflowCxEnvironmentVersionConfig {
    /**
     * Format: projects/{{project}}/locations/{{location}}/agents/{{agent}}/flows/{{flow}}/versions/{{version}}.
     */
    version: string;
}

export interface DialogflowCxFlowAdvancedSettings {
    /**
     * If present, incoming audio is exported by Dialogflow to the configured Google Cloud Storage destination. Exposed at the following levels:
     * * Agent level
     * * Flow level
     */
    audioExportGcsDestination?: outputs.DialogflowCxFlowAdvancedSettingsAudioExportGcsDestination;
    /**
     * Define behaviors for DTMF (dual tone multi frequency). DTMF settings does not override each other. DTMF settings set at different levels define DTMF detections running in parallel. Exposed at the following levels:
     * * Agent level
     * * Flow level
     * * Page level
     * * Parameter level
     */
    dtmfSettings?: outputs.DialogflowCxFlowAdvancedSettingsDtmfSettings;
}

export interface DialogflowCxFlowAdvancedSettingsAudioExportGcsDestination {
    /**
     * The Google Cloud Storage URI for the exported objects. Whether a full object name, or just a prefix, its usage depends on the Dialogflow operation.
     * Format: gs://bucket/object-name-or-prefix
     */
    uri?: string;
}

export interface DialogflowCxFlowAdvancedSettingsDtmfSettings {
    /**
     * If true, incoming audio is processed for DTMF (dual tone multi frequency) events. For example, if the caller presses a button on their telephone keypad and DTMF processing is enabled, Dialogflow will detect the event (e.g. a "3" was pressed) in the incoming audio and pass the event to the bot to drive business logic (e.g. when 3 is pressed, return the account balance).
     */
    enabled?: boolean;
    /**
     * The digit that terminates a DTMF digit sequence.
     */
    finishDigit?: string;
    /**
     * Max length of DTMF digits.
     */
    maxDigits?: number;
}

export interface DialogflowCxFlowEventHandler {
    /**
     * The name of the event to handle.
     */
    event?: string;
    /**
     * The unique identifier of this event handler.
     */
    name: string;
    /**
     * The target flow to transition to.
     * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>.
     */
    targetFlow?: string;
    /**
     * The target page to transition to.
     * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>/pages/<Page ID>.
     */
    targetPage?: string;
    /**
     * The fulfillment to call when the event occurs. Handling webhook errors with a fulfillment enabled with webhook could cause infinite loop. It is invalid to specify such fulfillment for a handler handling webhooks.
     */
    triggerFulfillment?: outputs.DialogflowCxFlowEventHandlerTriggerFulfillment;
}

export interface DialogflowCxFlowEventHandlerTriggerFulfillment {
    /**
     * Conditional cases for this fulfillment.
     */
    conditionalCases?: outputs.DialogflowCxFlowEventHandlerTriggerFulfillmentConditionalCase[];
    /**
     * The list of rich message responses to present to the user.
     */
    messages?: outputs.DialogflowCxFlowEventHandlerTriggerFulfillmentMessage[];
    /**
     * Whether Dialogflow should return currently queued fulfillment response messages in streaming APIs. If a webhook is specified, it happens before Dialogflow invokes webhook. Warning: 1) This flag only affects streaming API. Responses are still queued and returned once in non-streaming API. 2) The flag can be enabled in any fulfillment but only the first 3 partial responses will be returned. You may only want to apply it to fulfillments that have slow webhooks.
     */
    returnPartialResponses?: boolean;
    /**
     * Set parameter values before executing the webhook.
     */
    setParameterActions?: outputs.DialogflowCxFlowEventHandlerTriggerFulfillmentSetParameterAction[];
    /**
     * The tag used by the webhook to identify which fulfillment is being called. This field is required if webhook is specified.
     */
    tag?: string;
    /**
     * The webhook to call. Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/webhooks/<Webhook ID>.
     */
    webhook?: string;
}

export interface DialogflowCxFlowEventHandlerTriggerFulfillmentConditionalCase {
    /**
     * A JSON encoded list of cascading if-else conditions. Cases are mutually exclusive. The first one with a matching condition is selected, all the rest ignored.
     * See [Case](https://cloud.google.com/dialogflow/cx/docs/reference/rest/v3/Fulfillment#case) for the schema.
     */
    cases?: string;
}

export interface DialogflowCxFlowEventHandlerTriggerFulfillmentMessage {
    /**
     * The channel which the response is associated with. Clients can specify the channel via QueryParameters.channel, and only associated channel response will be returned.
     */
    channel?: string;
    /**
     * Indicates that the conversation succeeded, i.e., the bot handled the issue that the customer talked to it about.
     * Dialogflow only uses this to determine which conversations should be counted as successful and doesn't process the metadata in this message in any way. Note that Dialogflow also considers conversations that get to the conversation end page as successful even if they don't return ConversationSuccess.
     * You may set this, for example:
     * * In the entryFulfillment of a Page if entering the page indicates that the conversation succeeded.
     * * In a webhook response when you determine that you handled the customer issue.
     */
    conversationSuccess?: outputs.DialogflowCxFlowEventHandlerTriggerFulfillmentMessageConversationSuccess;
    /**
     * Indicates that the conversation should be handed off to a live agent.
     * Dialogflow only uses this to determine which conversations were handed off to a human agent for measurement purposes. What else to do with this signal is up to you and your handoff procedures.
     * You may set this, for example:
     * * In the entryFulfillment of a Page if entering the page indicates something went extremely wrong in the conversation.
     * * In a webhook response when you determine that the customer issue can only be handled by a human.
     */
    liveAgentHandoff?: outputs.DialogflowCxFlowEventHandlerTriggerFulfillmentMessageLiveAgentHandoff;
    /**
     * A text or ssml response that is preferentially used for TTS output audio synthesis, as described in the comment on the ResponseMessage message.
     */
    outputAudioText?: outputs.DialogflowCxFlowEventHandlerTriggerFulfillmentMessageOutputAudioText;
    /**
     * A custom, platform-specific payload.
     */
    payload?: string;
    /**
     * Specifies an audio clip to be played by the client as part of the response.
     */
    playAudio?: outputs.DialogflowCxFlowEventHandlerTriggerFulfillmentMessagePlayAudio;
    /**
     * Represents the signal that telles the client to transfer the phone call connected to the agent to a third-party endpoint.
     */
    telephonyTransferCall?: outputs.DialogflowCxFlowEventHandlerTriggerFulfillmentMessageTelephonyTransferCall;
    /**
     * The text response message.
     */
    text?: outputs.DialogflowCxFlowEventHandlerTriggerFulfillmentMessageText;
}

export interface DialogflowCxFlowEventHandlerTriggerFulfillmentMessageConversationSuccess {
    /**
     * Custom metadata. Dialogflow doesn't impose any structure on this.
     */
    metadata?: string;
}

export interface DialogflowCxFlowEventHandlerTriggerFulfillmentMessageLiveAgentHandoff {
    /**
     * Custom metadata. Dialogflow doesn't impose any structure on this.
     */
    metadata?: string;
}

export interface DialogflowCxFlowEventHandlerTriggerFulfillmentMessageOutputAudioText {
    /**
     * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
     */
    allowPlaybackInterruption: boolean;
    /**
     * The SSML text to be synthesized. For more information, see SSML.
     */
    ssml?: string;
    /**
     * The raw text to be synthesized.
     */
    text?: string;
}

export interface DialogflowCxFlowEventHandlerTriggerFulfillmentMessagePlayAudio {
    /**
     * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
     */
    allowPlaybackInterruption: boolean;
    /**
     * URI of the audio clip. Dialogflow does not impose any validation on this value. It is specific to the client that reads it.
     */
    audioUri: string;
}

export interface DialogflowCxFlowEventHandlerTriggerFulfillmentMessageTelephonyTransferCall {
    /**
     * Transfer the call to a phone number in E.164 format.
     */
    phoneNumber: string;
}

export interface DialogflowCxFlowEventHandlerTriggerFulfillmentMessageText {
    /**
     * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
     */
    allowPlaybackInterruption: boolean;
    /**
     * A collection of text responses.
     */
    texts?: string[];
}

export interface DialogflowCxFlowEventHandlerTriggerFulfillmentSetParameterAction {
    /**
     * Display name of the parameter.
     */
    parameter?: string;
    /**
     * The new JSON-encoded value of the parameter. A null value clears the parameter.
     */
    value?: string;
}

export interface DialogflowCxFlowNluSettings {
    /**
     * To filter out false positive results and still get variety in matched natural language inputs for your agent, you can tune the machine learning classification threshold.
     * If the returned score value is less than the threshold value, then a no-match event will be triggered. The score values range from 0.0 (completely uncertain) to 1.0 (completely certain). If set to 0.0, the default of 0.3 is used.
     */
    classificationThreshold?: number;
    /**
     * Indicates NLU model training mode.
     * * MODEL_TRAINING_MODE_AUTOMATIC: NLU model training is automatically triggered when a flow gets modified. User can also manually trigger model training in this mode.
     * * MODEL_TRAINING_MODE_MANUAL: User needs to manually trigger NLU model training. Best for large flows whose models take long time to train. Possible values: ["MODEL_TRAINING_MODE_AUTOMATIC", "MODEL_TRAINING_MODE_MANUAL"]
     */
    modelTrainingMode?: string;
    /**
     * Indicates the type of NLU model.
     * * MODEL_TYPE_STANDARD: Use standard NLU model.
     * * MODEL_TYPE_ADVANCED: Use advanced NLU model. Possible values: ["MODEL_TYPE_STANDARD", "MODEL_TYPE_ADVANCED"]
     */
    modelType?: string;
}

export interface DialogflowCxFlowTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DialogflowCxFlowTransitionRoute {
    /**
     * The condition to evaluate against form parameters or session parameters.
     * At least one of intent or condition must be specified. When both intent and condition are specified, the transition can only happen when both are fulfilled.
     */
    condition?: string;
    /**
     * The unique identifier of an Intent.
     * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/intents/<Intent ID>. Indicates that the transition can only happen when the given intent is matched. At least one of intent or condition must be specified. When both intent and condition are specified, the transition can only happen when both are fulfilled.
     */
    intent?: string;
    /**
     * The unique identifier of this transition route.
     */
    name: string;
    /**
     * The target flow to transition to.
     * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>.
     */
    targetFlow?: string;
    /**
     * The target page to transition to.
     * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>/pages/<Page ID>.
     */
    targetPage?: string;
    /**
     * The fulfillment to call when the condition is satisfied. At least one of triggerFulfillment and target must be specified. When both are defined, triggerFulfillment is executed first.
     */
    triggerFulfillment?: outputs.DialogflowCxFlowTransitionRouteTriggerFulfillment;
}

export interface DialogflowCxFlowTransitionRouteTriggerFulfillment {
    /**
     * Conditional cases for this fulfillment.
     */
    conditionalCases?: outputs.DialogflowCxFlowTransitionRouteTriggerFulfillmentConditionalCase[];
    /**
     * The list of rich message responses to present to the user.
     */
    messages?: outputs.DialogflowCxFlowTransitionRouteTriggerFulfillmentMessage[];
    /**
     * Whether Dialogflow should return currently queued fulfillment response messages in streaming APIs. If a webhook is specified, it happens before Dialogflow invokes webhook. Warning: 1) This flag only affects streaming API. Responses are still queued and returned once in non-streaming API. 2) The flag can be enabled in any fulfillment but only the first 3 partial responses will be returned. You may only want to apply it to fulfillments that have slow webhooks.
     */
    returnPartialResponses?: boolean;
    /**
     * Set parameter values before executing the webhook.
     */
    setParameterActions?: outputs.DialogflowCxFlowTransitionRouteTriggerFulfillmentSetParameterAction[];
    /**
     * The tag used by the webhook to identify which fulfillment is being called. This field is required if webhook is specified.
     */
    tag?: string;
    /**
     * The webhook to call. Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/webhooks/<Webhook ID>.
     */
    webhook?: string;
}

export interface DialogflowCxFlowTransitionRouteTriggerFulfillmentConditionalCase {
    /**
     * A JSON encoded list of cascading if-else conditions. Cases are mutually exclusive. The first one with a matching condition is selected, all the rest ignored.
     * See [Case](https://cloud.google.com/dialogflow/cx/docs/reference/rest/v3/Fulfillment#case) for the schema.
     */
    cases?: string;
}

export interface DialogflowCxFlowTransitionRouteTriggerFulfillmentMessage {
    /**
     * The channel which the response is associated with. Clients can specify the channel via QueryParameters.channel, and only associated channel response will be returned.
     */
    channel?: string;
    /**
     * Indicates that the conversation succeeded, i.e., the bot handled the issue that the customer talked to it about.
     * Dialogflow only uses this to determine which conversations should be counted as successful and doesn't process the metadata in this message in any way. Note that Dialogflow also considers conversations that get to the conversation end page as successful even if they don't return ConversationSuccess.
     * You may set this, for example:
     * * In the entryFulfillment of a Page if entering the page indicates that the conversation succeeded.
     * * In a webhook response when you determine that you handled the customer issue.
     */
    conversationSuccess?: outputs.DialogflowCxFlowTransitionRouteTriggerFulfillmentMessageConversationSuccess;
    /**
     * Indicates that the conversation should be handed off to a live agent.
     * Dialogflow only uses this to determine which conversations were handed off to a human agent for measurement purposes. What else to do with this signal is up to you and your handoff procedures.
     * You may set this, for example:
     * * In the entryFulfillment of a Page if entering the page indicates something went extremely wrong in the conversation.
     * * In a webhook response when you determine that the customer issue can only be handled by a human.
     */
    liveAgentHandoff?: outputs.DialogflowCxFlowTransitionRouteTriggerFulfillmentMessageLiveAgentHandoff;
    /**
     * A text or ssml response that is preferentially used for TTS output audio synthesis, as described in the comment on the ResponseMessage message.
     */
    outputAudioText?: outputs.DialogflowCxFlowTransitionRouteTriggerFulfillmentMessageOutputAudioText;
    /**
     * A custom, platform-specific payload.
     */
    payload?: string;
    /**
     * Specifies an audio clip to be played by the client as part of the response.
     */
    playAudio?: outputs.DialogflowCxFlowTransitionRouteTriggerFulfillmentMessagePlayAudio;
    /**
     * Represents the signal that telles the client to transfer the phone call connected to the agent to a third-party endpoint.
     */
    telephonyTransferCall?: outputs.DialogflowCxFlowTransitionRouteTriggerFulfillmentMessageTelephonyTransferCall;
    /**
     * The text response message.
     */
    text?: outputs.DialogflowCxFlowTransitionRouteTriggerFulfillmentMessageText;
}

export interface DialogflowCxFlowTransitionRouteTriggerFulfillmentMessageConversationSuccess {
    /**
     * Custom metadata. Dialogflow doesn't impose any structure on this.
     */
    metadata?: string;
}

export interface DialogflowCxFlowTransitionRouteTriggerFulfillmentMessageLiveAgentHandoff {
    /**
     * Custom metadata. Dialogflow doesn't impose any structure on this.
     */
    metadata?: string;
}

export interface DialogflowCxFlowTransitionRouteTriggerFulfillmentMessageOutputAudioText {
    /**
     * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
     */
    allowPlaybackInterruption: boolean;
    /**
     * The SSML text to be synthesized. For more information, see SSML.
     */
    ssml?: string;
    /**
     * The raw text to be synthesized.
     */
    text?: string;
}

export interface DialogflowCxFlowTransitionRouteTriggerFulfillmentMessagePlayAudio {
    /**
     * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
     */
    allowPlaybackInterruption: boolean;
    /**
     * URI of the audio clip. Dialogflow does not impose any validation on this value. It is specific to the client that reads it.
     */
    audioUri: string;
}

export interface DialogflowCxFlowTransitionRouteTriggerFulfillmentMessageTelephonyTransferCall {
    /**
     * Transfer the call to a phone number in E.164 format.
     */
    phoneNumber: string;
}

export interface DialogflowCxFlowTransitionRouteTriggerFulfillmentMessageText {
    /**
     * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
     */
    allowPlaybackInterruption: boolean;
    /**
     * A collection of text responses.
     */
    texts?: string[];
}

export interface DialogflowCxFlowTransitionRouteTriggerFulfillmentSetParameterAction {
    /**
     * Display name of the parameter.
     */
    parameter?: string;
    /**
     * The new JSON-encoded value of the parameter. A null value clears the parameter.
     */
    value?: string;
}

export interface DialogflowCxIntentParameter {
    /**
     * The entity type of the parameter.
     * Format: projects/-/locations/-/agents/-/entityTypes/<System Entity Type ID> for system entity types (for example, projects/-/locations/-/agents/-/entityTypes/sys.date), or projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/entityTypes/<Entity Type ID> for developer entity types.
     */
    entityType: string;
    /**
     * The unique identifier of the parameter. This field is used by training phrases to annotate their parts.
     */
    id: string;
    /**
     * Indicates whether the parameter represents a list of values.
     */
    isList?: boolean;
    /**
     * Indicates whether the parameter content should be redacted in log. If redaction is enabled, the parameter content will be replaced by parameter name during logging.
     * Note: the parameter content is subject to redaction if either parameter level redaction or entity type level redaction is enabled.
     */
    redact?: boolean;
}

export interface DialogflowCxIntentTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DialogflowCxIntentTrainingPhrase {
    /**
     * The unique identifier of the training phrase.
     */
    id: string;
    /**
     * The ordered list of training phrase parts. The parts are concatenated in order to form the training phrase.
     * Note: The API does not automatically annotate training phrases like the Dialogflow Console does.
     * Note: Do not forget to include whitespace at part boundaries, so the training phrase is well formatted when the parts are concatenated.
     * If the training phrase does not need to be annotated with parameters, you just need a single part with only the Part.text field set.
     * If you want to annotate the training phrase, you must create multiple parts, where the fields of each part are populated in one of two ways:
     * Part.text is set to a part of the phrase that has no parameters.
     * Part.text is set to a part of the phrase that you want to annotate, and the parameterId field is set.
     */
    parts: outputs.DialogflowCxIntentTrainingPhrasePart[];
    /**
     * Indicates how many times this example was added to the intent.
     */
    repeatCount?: number;
}

export interface DialogflowCxIntentTrainingPhrasePart {
    /**
     * The parameter used to annotate this part of the training phrase. This field is required for annotated parts of the training phrase.
     */
    parameterId?: string;
    /**
     * The text for this part.
     */
    text: string;
}

export interface DialogflowCxPageAdvancedSettings {
    /**
     * Define behaviors for DTMF (dual tone multi frequency). DTMF settings does not override each other. DTMF settings set at different levels define DTMF detections running in parallel. Exposed at the following levels:
     * * Agent level
     * * Flow level
     * * Page level
     * * Parameter level
     */
    dtmfSettings?: outputs.DialogflowCxPageAdvancedSettingsDtmfSettings;
}

export interface DialogflowCxPageAdvancedSettingsDtmfSettings {
    /**
     * If true, incoming audio is processed for DTMF (dual tone multi frequency) events. For example, if the caller presses a button on their telephone keypad and DTMF processing is enabled, Dialogflow will detect the event (e.g. a "3" was pressed) in the incoming audio and pass the event to the bot to drive business logic (e.g. when 3 is pressed, return the account balance).
     */
    enabled?: boolean;
    /**
     * The digit that terminates a DTMF digit sequence.
     */
    finishDigit?: string;
    /**
     * Max length of DTMF digits.
     */
    maxDigits?: number;
}

export interface DialogflowCxPageEntryFulfillment {
    /**
     * Conditional cases for this fulfillment.
     */
    conditionalCases?: outputs.DialogflowCxPageEntryFulfillmentConditionalCase[];
    /**
     * The list of rich message responses to present to the user.
     */
    messages?: outputs.DialogflowCxPageEntryFulfillmentMessage[];
    /**
     * Whether Dialogflow should return currently queued fulfillment response messages in streaming APIs. If a webhook is specified, it happens before Dialogflow invokes webhook. Warning: 1) This flag only affects streaming API. Responses are still queued and returned once in non-streaming API. 2) The flag can be enabled in any fulfillment but only the first 3 partial responses will be returned. You may only want to apply it to fulfillments that have slow webhooks.
     */
    returnPartialResponses?: boolean;
    /**
     * Set parameter values before executing the webhook.
     */
    setParameterActions?: outputs.DialogflowCxPageEntryFulfillmentSetParameterAction[];
    /**
     * The tag used by the webhook to identify which fulfillment is being called. This field is required if webhook is specified.
     */
    tag?: string;
    /**
     * The webhook to call. Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/webhooks/<Webhook ID>.
     */
    webhook?: string;
}

export interface DialogflowCxPageEntryFulfillmentConditionalCase {
    /**
     * A JSON encoded list of cascading if-else conditions. Cases are mutually exclusive. The first one with a matching condition is selected, all the rest ignored.
     * See [Case](https://cloud.google.com/dialogflow/cx/docs/reference/rest/v3/Fulfillment#case) for the schema.
     */
    cases?: string;
}

export interface DialogflowCxPageEntryFulfillmentMessage {
    /**
     * The channel which the response is associated with. Clients can specify the channel via QueryParameters.channel, and only associated channel response will be returned.
     */
    channel?: string;
    /**
     * Indicates that the conversation succeeded, i.e., the bot handled the issue that the customer talked to it about.
     * Dialogflow only uses this to determine which conversations should be counted as successful and doesn't process the metadata in this message in any way. Note that Dialogflow also considers conversations that get to the conversation end page as successful even if they don't return ConversationSuccess.
     * You may set this, for example:
     * * In the entryFulfillment of a Page if entering the page indicates that the conversation succeeded.
     * * In a webhook response when you determine that you handled the customer issue.
     */
    conversationSuccess?: outputs.DialogflowCxPageEntryFulfillmentMessageConversationSuccess;
    /**
     * Indicates that the conversation should be handed off to a live agent.
     * Dialogflow only uses this to determine which conversations were handed off to a human agent for measurement purposes. What else to do with this signal is up to you and your handoff procedures.
     * You may set this, for example:
     * * In the entryFulfillment of a Page if entering the page indicates something went extremely wrong in the conversation.
     * * In a webhook response when you determine that the customer issue can only be handled by a human.
     */
    liveAgentHandoff?: outputs.DialogflowCxPageEntryFulfillmentMessageLiveAgentHandoff;
    /**
     * A text or ssml response that is preferentially used for TTS output audio synthesis, as described in the comment on the ResponseMessage message.
     */
    outputAudioText?: outputs.DialogflowCxPageEntryFulfillmentMessageOutputAudioText;
    /**
     * A custom, platform-specific payload.
     */
    payload?: string;
    /**
     * Specifies an audio clip to be played by the client as part of the response.
     */
    playAudio?: outputs.DialogflowCxPageEntryFulfillmentMessagePlayAudio;
    /**
     * Represents the signal that telles the client to transfer the phone call connected to the agent to a third-party endpoint.
     */
    telephonyTransferCall?: outputs.DialogflowCxPageEntryFulfillmentMessageTelephonyTransferCall;
    /**
     * The text response message.
     */
    text?: outputs.DialogflowCxPageEntryFulfillmentMessageText;
}

export interface DialogflowCxPageEntryFulfillmentMessageConversationSuccess {
    /**
     * Custom metadata. Dialogflow doesn't impose any structure on this.
     */
    metadata?: string;
}

export interface DialogflowCxPageEntryFulfillmentMessageLiveAgentHandoff {
    /**
     * Custom metadata. Dialogflow doesn't impose any structure on this.
     */
    metadata?: string;
}

export interface DialogflowCxPageEntryFulfillmentMessageOutputAudioText {
    /**
     * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
     */
    allowPlaybackInterruption: boolean;
    /**
     * The SSML text to be synthesized. For more information, see SSML.
     */
    ssml?: string;
    /**
     * The raw text to be synthesized.
     */
    text?: string;
}

export interface DialogflowCxPageEntryFulfillmentMessagePlayAudio {
    /**
     * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
     */
    allowPlaybackInterruption: boolean;
    /**
     * URI of the audio clip. Dialogflow does not impose any validation on this value. It is specific to the client that reads it.
     */
    audioUri: string;
}

export interface DialogflowCxPageEntryFulfillmentMessageTelephonyTransferCall {
    /**
     * Transfer the call to a phone number in E.164 format.
     */
    phoneNumber: string;
}

export interface DialogflowCxPageEntryFulfillmentMessageText {
    /**
     * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
     */
    allowPlaybackInterruption: boolean;
    /**
     * A collection of text responses.
     */
    texts?: string[];
}

export interface DialogflowCxPageEntryFulfillmentSetParameterAction {
    /**
     * Display name of the parameter.
     */
    parameter?: string;
    /**
     * The new JSON-encoded value of the parameter. A null value clears the parameter.
     */
    value?: string;
}

export interface DialogflowCxPageEventHandler {
    /**
     * The name of the event to handle.
     */
    event?: string;
    /**
     * The unique identifier of this event handler.
     */
    name: string;
    /**
     * The target flow to transition to.
     * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>.
     */
    targetFlow?: string;
    /**
     * The target page to transition to.
     * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>/pages/<Page ID>.
     */
    targetPage?: string;
    /**
     * The fulfillment to call when the event occurs. Handling webhook errors with a fulfillment enabled with webhook could cause infinite loop. It is invalid to specify such fulfillment for a handler handling webhooks.
     */
    triggerFulfillment?: outputs.DialogflowCxPageEventHandlerTriggerFulfillment;
}

export interface DialogflowCxPageEventHandlerTriggerFulfillment {
    /**
     * Conditional cases for this fulfillment.
     */
    conditionalCases?: outputs.DialogflowCxPageEventHandlerTriggerFulfillmentConditionalCase[];
    /**
     * The list of rich message responses to present to the user.
     */
    messages?: outputs.DialogflowCxPageEventHandlerTriggerFulfillmentMessage[];
    /**
     * Whether Dialogflow should return currently queued fulfillment response messages in streaming APIs. If a webhook is specified, it happens before Dialogflow invokes webhook. Warning: 1) This flag only affects streaming API. Responses are still queued and returned once in non-streaming API. 2) The flag can be enabled in any fulfillment but only the first 3 partial responses will be returned. You may only want to apply it to fulfillments that have slow webhooks.
     */
    returnPartialResponses?: boolean;
    /**
     * Set parameter values before executing the webhook.
     */
    setParameterActions?: outputs.DialogflowCxPageEventHandlerTriggerFulfillmentSetParameterAction[];
    /**
     * The tag used by the webhook to identify which fulfillment is being called. This field is required if webhook is specified.
     */
    tag?: string;
    /**
     * The webhook to call. Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/webhooks/<Webhook ID>.
     */
    webhook?: string;
}

export interface DialogflowCxPageEventHandlerTriggerFulfillmentConditionalCase {
    /**
     * A JSON encoded list of cascading if-else conditions. Cases are mutually exclusive. The first one with a matching condition is selected, all the rest ignored.
     * See [Case](https://cloud.google.com/dialogflow/cx/docs/reference/rest/v3/Fulfillment#case) for the schema.
     */
    cases?: string;
}

export interface DialogflowCxPageEventHandlerTriggerFulfillmentMessage {
    /**
     * The channel which the response is associated with. Clients can specify the channel via QueryParameters.channel, and only associated channel response will be returned.
     */
    channel?: string;
    /**
     * Indicates that the conversation succeeded, i.e., the bot handled the issue that the customer talked to it about.
     * Dialogflow only uses this to determine which conversations should be counted as successful and doesn't process the metadata in this message in any way. Note that Dialogflow also considers conversations that get to the conversation end page as successful even if they don't return ConversationSuccess.
     * You may set this, for example:
     * * In the entryFulfillment of a Page if entering the page indicates that the conversation succeeded.
     * * In a webhook response when you determine that you handled the customer issue.
     */
    conversationSuccess?: outputs.DialogflowCxPageEventHandlerTriggerFulfillmentMessageConversationSuccess;
    /**
     * Indicates that the conversation should be handed off to a live agent.
     * Dialogflow only uses this to determine which conversations were handed off to a human agent for measurement purposes. What else to do with this signal is up to you and your handoff procedures.
     * You may set this, for example:
     * * In the entryFulfillment of a Page if entering the page indicates something went extremely wrong in the conversation.
     * * In a webhook response when you determine that the customer issue can only be handled by a human.
     */
    liveAgentHandoff?: outputs.DialogflowCxPageEventHandlerTriggerFulfillmentMessageLiveAgentHandoff;
    /**
     * A text or ssml response that is preferentially used for TTS output audio synthesis, as described in the comment on the ResponseMessage message.
     */
    outputAudioText?: outputs.DialogflowCxPageEventHandlerTriggerFulfillmentMessageOutputAudioText;
    /**
     * A custom, platform-specific payload.
     */
    payload?: string;
    /**
     * Specifies an audio clip to be played by the client as part of the response.
     */
    playAudio?: outputs.DialogflowCxPageEventHandlerTriggerFulfillmentMessagePlayAudio;
    /**
     * Represents the signal that telles the client to transfer the phone call connected to the agent to a third-party endpoint.
     */
    telephonyTransferCall?: outputs.DialogflowCxPageEventHandlerTriggerFulfillmentMessageTelephonyTransferCall;
    /**
     * The text response message.
     */
    text?: outputs.DialogflowCxPageEventHandlerTriggerFulfillmentMessageText;
}

export interface DialogflowCxPageEventHandlerTriggerFulfillmentMessageConversationSuccess {
    /**
     * Custom metadata. Dialogflow doesn't impose any structure on this.
     */
    metadata?: string;
}

export interface DialogflowCxPageEventHandlerTriggerFulfillmentMessageLiveAgentHandoff {
    /**
     * Custom metadata. Dialogflow doesn't impose any structure on this.
     */
    metadata?: string;
}

export interface DialogflowCxPageEventHandlerTriggerFulfillmentMessageOutputAudioText {
    /**
     * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
     */
    allowPlaybackInterruption: boolean;
    /**
     * The SSML text to be synthesized. For more information, see SSML.
     */
    ssml?: string;
    /**
     * The raw text to be synthesized.
     */
    text?: string;
}

export interface DialogflowCxPageEventHandlerTriggerFulfillmentMessagePlayAudio {
    /**
     * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
     */
    allowPlaybackInterruption: boolean;
    /**
     * URI of the audio clip. Dialogflow does not impose any validation on this value. It is specific to the client that reads it.
     */
    audioUri: string;
}

export interface DialogflowCxPageEventHandlerTriggerFulfillmentMessageTelephonyTransferCall {
    /**
     * Transfer the call to a phone number in E.164 format.
     */
    phoneNumber: string;
}

export interface DialogflowCxPageEventHandlerTriggerFulfillmentMessageText {
    /**
     * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
     */
    allowPlaybackInterruption: boolean;
    /**
     * A collection of text responses.
     */
    texts?: string[];
}

export interface DialogflowCxPageEventHandlerTriggerFulfillmentSetParameterAction {
    /**
     * Display name of the parameter.
     */
    parameter?: string;
    /**
     * The new JSON-encoded value of the parameter. A null value clears the parameter.
     */
    value?: string;
}

export interface DialogflowCxPageForm {
    /**
     * Parameters to collect from the user.
     */
    parameters?: outputs.DialogflowCxPageFormParameter[];
}

export interface DialogflowCxPageFormParameter {
    /**
     * Hierarchical advanced settings for this parameter. The settings exposed at the lower level overrides the settings exposed at the higher level.
     * Hierarchy: Agent->Flow->Page->Fulfillment/Parameter.
     */
    advancedSettings?: outputs.DialogflowCxPageFormParameterAdvancedSettings;
    /**
     * The default value of an optional parameter. If the parameter is required, the default value will be ignored.
     */
    defaultValue?: string;
    /**
     * The human-readable name of the parameter, unique within the form.
     */
    displayName?: string;
    /**
     * The entity type of the parameter.
     * Format: projects/-/locations/-/agents/-/entityTypes/<System Entity Type ID> for system entity types (for example, projects/-/locations/-/agents/-/entityTypes/sys.date), or projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/entityTypes/<Entity Type ID> for developer entity types.
     */
    entityType?: string;
    /**
     * Defines fill behavior for the parameter.
     */
    fillBehavior?: outputs.DialogflowCxPageFormParameterFillBehavior;
    /**
     * Indicates whether the parameter represents a list of values.
     */
    isList?: boolean;
    /**
     * Indicates whether the parameter content should be redacted in log.
     * If redaction is enabled, the parameter content will be replaced by parameter name during logging. Note: the parameter content is subject to redaction if either parameter level redaction or entity type level redaction is enabled.
     */
    redact?: boolean;
    /**
     * Indicates whether the parameter is required. Optional parameters will not trigger prompts; however, they are filled if the user specifies them.
     * Required parameters must be filled before form filling concludes.
     */
    required?: boolean;
}

export interface DialogflowCxPageFormParameterAdvancedSettings {
    /**
     * Define behaviors for DTMF (dual tone multi frequency). DTMF settings does not override each other. DTMF settings set at different levels define DTMF detections running in parallel. Exposed at the following levels:
     * * Agent level
     * * Flow level
     * * Page level
     * * Parameter level
     */
    dtmfSettings?: outputs.DialogflowCxPageFormParameterAdvancedSettingsDtmfSettings;
}

export interface DialogflowCxPageFormParameterAdvancedSettingsDtmfSettings {
    /**
     * If true, incoming audio is processed for DTMF (dual tone multi frequency) events. For example, if the caller presses a button on their telephone keypad and DTMF processing is enabled, Dialogflow will detect the event (e.g. a "3" was pressed) in the incoming audio and pass the event to the bot to drive business logic (e.g. when 3 is pressed, return the account balance).
     */
    enabled?: boolean;
    /**
     * The digit that terminates a DTMF digit sequence.
     */
    finishDigit?: string;
    /**
     * Max length of DTMF digits.
     */
    maxDigits?: number;
}

export interface DialogflowCxPageFormParameterFillBehavior {
    /**
     * The fulfillment to provide the initial prompt that the agent can present to the user in order to fill the parameter.
     */
    initialPromptFulfillment?: outputs.DialogflowCxPageFormParameterFillBehaviorInitialPromptFulfillment;
    /**
     * The handlers for parameter-level events, used to provide reprompt for the parameter or transition to a different page/flow. The supported events are:
     * * sys.no-match-<N>, where N can be from 1 to 6
     * * sys.no-match-default
     * * sys.no-input-<N>, where N can be from 1 to 6
     * * sys.no-input-default
     * * sys.invalid-parameter
     * [initialPromptFulfillment][initialPromptFulfillment] provides the first prompt for the parameter.
     * If the user's response does not fill the parameter, a no-match/no-input event will be triggered, and the fulfillment associated with the sys.no-match-1/sys.no-input-1 handler (if defined) will be called to provide a prompt. The sys.no-match-2/sys.no-input-2 handler (if defined) will respond to the next no-match/no-input event, and so on.
     * A sys.no-match-default or sys.no-input-default handler will be used to handle all following no-match/no-input events after all numbered no-match/no-input handlers for the parameter are consumed.
     * A sys.invalid-parameter handler can be defined to handle the case where the parameter values have been invalidated by webhook. For example, if the user's response fill the parameter, however the parameter was invalidated by webhook, the fulfillment associated with the sys.invalid-parameter handler (if defined) will be called to provide a prompt.
     * If the event handler for the corresponding event can't be found on the parameter, initialPromptFulfillment will be re-prompted.
     */
    repromptEventHandlers?: outputs.DialogflowCxPageFormParameterFillBehaviorRepromptEventHandler[];
}

export interface DialogflowCxPageFormParameterFillBehaviorInitialPromptFulfillment {
    /**
     * Conditional cases for this fulfillment.
     */
    conditionalCases?: outputs.DialogflowCxPageFormParameterFillBehaviorInitialPromptFulfillmentConditionalCase[];
    /**
     * The list of rich message responses to present to the user.
     */
    messages?: outputs.DialogflowCxPageFormParameterFillBehaviorInitialPromptFulfillmentMessage[];
    /**
     * Whether Dialogflow should return currently queued fulfillment response messages in streaming APIs. If a webhook is specified, it happens before Dialogflow invokes webhook. Warning: 1) This flag only affects streaming API. Responses are still queued and returned once in non-streaming API. 2) The flag can be enabled in any fulfillment but only the first 3 partial responses will be returned. You may only want to apply it to fulfillments that have slow webhooks.
     */
    returnPartialResponses?: boolean;
    /**
     * Set parameter values before executing the webhook.
     */
    setParameterActions?: outputs.DialogflowCxPageFormParameterFillBehaviorInitialPromptFulfillmentSetParameterAction[];
    /**
     * The tag used by the webhook to identify which fulfillment is being called. This field is required if webhook is specified.
     */
    tag?: string;
    /**
     * The webhook to call. Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/webhooks/<Webhook ID>.
     */
    webhook?: string;
}

export interface DialogflowCxPageFormParameterFillBehaviorInitialPromptFulfillmentConditionalCase {
    /**
     * A JSON encoded list of cascading if-else conditions. Cases are mutually exclusive. The first one with a matching condition is selected, all the rest ignored.
     * See [Case](https://cloud.google.com/dialogflow/cx/docs/reference/rest/v3/Fulfillment#case) for the schema.
     */
    cases?: string;
}

export interface DialogflowCxPageFormParameterFillBehaviorInitialPromptFulfillmentMessage {
    /**
     * The channel which the response is associated with. Clients can specify the channel via QueryParameters.channel, and only associated channel response will be returned.
     */
    channel?: string;
    /**
     * Indicates that the conversation succeeded, i.e., the bot handled the issue that the customer talked to it about.
     * Dialogflow only uses this to determine which conversations should be counted as successful and doesn't process the metadata in this message in any way. Note that Dialogflow also considers conversations that get to the conversation end page as successful even if they don't return ConversationSuccess.
     * You may set this, for example:
     * * In the entryFulfillment of a Page if entering the page indicates that the conversation succeeded.
     * * In a webhook response when you determine that you handled the customer issue.
     */
    conversationSuccess?: outputs.DialogflowCxPageFormParameterFillBehaviorInitialPromptFulfillmentMessageConversationSuccess;
    /**
     * Indicates that the conversation should be handed off to a live agent.
     * Dialogflow only uses this to determine which conversations were handed off to a human agent for measurement purposes. What else to do with this signal is up to you and your handoff procedures.
     * You may set this, for example:
     * * In the entryFulfillment of a Page if entering the page indicates something went extremely wrong in the conversation.
     * * In a webhook response when you determine that the customer issue can only be handled by a human.
     */
    liveAgentHandoff?: outputs.DialogflowCxPageFormParameterFillBehaviorInitialPromptFulfillmentMessageLiveAgentHandoff;
    /**
     * A text or ssml response that is preferentially used for TTS output audio synthesis, as described in the comment on the ResponseMessage message.
     */
    outputAudioText?: outputs.DialogflowCxPageFormParameterFillBehaviorInitialPromptFulfillmentMessageOutputAudioText;
    /**
     * A custom, platform-specific payload.
     */
    payload?: string;
    /**
     * Specifies an audio clip to be played by the client as part of the response.
     */
    playAudio?: outputs.DialogflowCxPageFormParameterFillBehaviorInitialPromptFulfillmentMessagePlayAudio;
    /**
     * Represents the signal that telles the client to transfer the phone call connected to the agent to a third-party endpoint.
     */
    telephonyTransferCall?: outputs.DialogflowCxPageFormParameterFillBehaviorInitialPromptFulfillmentMessageTelephonyTransferCall;
    /**
     * The text response message.
     */
    text?: outputs.DialogflowCxPageFormParameterFillBehaviorInitialPromptFulfillmentMessageText;
}

export interface DialogflowCxPageFormParameterFillBehaviorInitialPromptFulfillmentMessageConversationSuccess {
    /**
     * Custom metadata. Dialogflow doesn't impose any structure on this.
     */
    metadata?: string;
}

export interface DialogflowCxPageFormParameterFillBehaviorInitialPromptFulfillmentMessageLiveAgentHandoff {
    /**
     * Custom metadata. Dialogflow doesn't impose any structure on this.
     */
    metadata?: string;
}

export interface DialogflowCxPageFormParameterFillBehaviorInitialPromptFulfillmentMessageOutputAudioText {
    /**
     * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
     */
    allowPlaybackInterruption: boolean;
    /**
     * The SSML text to be synthesized. For more information, see SSML.
     */
    ssml?: string;
    /**
     * The raw text to be synthesized.
     */
    text?: string;
}

export interface DialogflowCxPageFormParameterFillBehaviorInitialPromptFulfillmentMessagePlayAudio {
    /**
     * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
     */
    allowPlaybackInterruption: boolean;
    /**
     * URI of the audio clip. Dialogflow does not impose any validation on this value. It is specific to the client that reads it.
     */
    audioUri: string;
}

export interface DialogflowCxPageFormParameterFillBehaviorInitialPromptFulfillmentMessageTelephonyTransferCall {
    /**
     * Transfer the call to a phone number in E.164 format.
     */
    phoneNumber: string;
}

export interface DialogflowCxPageFormParameterFillBehaviorInitialPromptFulfillmentMessageText {
    /**
     * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
     */
    allowPlaybackInterruption: boolean;
    /**
     * A collection of text responses.
     */
    texts?: string[];
}

export interface DialogflowCxPageFormParameterFillBehaviorInitialPromptFulfillmentSetParameterAction {
    /**
     * Display name of the parameter.
     */
    parameter?: string;
    /**
     * The new JSON-encoded value of the parameter. A null value clears the parameter.
     */
    value?: string;
}

export interface DialogflowCxPageFormParameterFillBehaviorRepromptEventHandler {
    /**
     * The name of the event to handle.
     */
    event?: string;
    /**
     * The unique identifier of this event handler.
     */
    name: string;
    /**
     * The target flow to transition to.
     * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>.
     */
    targetFlow?: string;
    /**
     * The target page to transition to.
     * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>/pages/<Page ID>.
     */
    targetPage?: string;
    /**
     * The fulfillment to call when the event occurs. Handling webhook errors with a fulfillment enabled with webhook could cause infinite loop. It is invalid to specify such fulfillment for a handler handling webhooks.
     */
    triggerFulfillment?: outputs.DialogflowCxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillment;
}

export interface DialogflowCxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillment {
    /**
     * Conditional cases for this fulfillment.
     */
    conditionalCases?: outputs.DialogflowCxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentConditionalCase[];
    /**
     * The list of rich message responses to present to the user.
     */
    messages?: outputs.DialogflowCxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessage[];
    /**
     * Whether Dialogflow should return currently queued fulfillment response messages in streaming APIs. If a webhook is specified, it happens before Dialogflow invokes webhook. Warning: 1) This flag only affects streaming API. Responses are still queued and returned once in non-streaming API. 2) The flag can be enabled in any fulfillment but only the first 3 partial responses will be returned. You may only want to apply it to fulfillments that have slow webhooks.
     */
    returnPartialResponses?: boolean;
    /**
     * Set parameter values before executing the webhook.
     */
    setParameterActions?: outputs.DialogflowCxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentSetParameterAction[];
    /**
     * The tag used by the webhook to identify which fulfillment is being called. This field is required if webhook is specified.
     */
    tag?: string;
    /**
     * The webhook to call. Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/webhooks/<Webhook ID>.
     */
    webhook?: string;
}

export interface DialogflowCxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentConditionalCase {
    /**
     * A JSON encoded list of cascading if-else conditions. Cases are mutually exclusive. The first one with a matching condition is selected, all the rest ignored.
     * See [Case](https://cloud.google.com/dialogflow/cx/docs/reference/rest/v3/Fulfillment#case) for the schema.
     */
    cases?: string;
}

export interface DialogflowCxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessage {
    /**
     * The channel which the response is associated with. Clients can specify the channel via QueryParameters.channel, and only associated channel response will be returned.
     */
    channel?: string;
    /**
     * Indicates that the conversation succeeded, i.e., the bot handled the issue that the customer talked to it about.
     * Dialogflow only uses this to determine which conversations should be counted as successful and doesn't process the metadata in this message in any way. Note that Dialogflow also considers conversations that get to the conversation end page as successful even if they don't return ConversationSuccess.
     * You may set this, for example:
     * * In the entryFulfillment of a Page if entering the page indicates that the conversation succeeded.
     * * In a webhook response when you determine that you handled the customer issue.
     */
    conversationSuccess?: outputs.DialogflowCxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessageConversationSuccess;
    /**
     * Indicates that the conversation should be handed off to a live agent.
     * Dialogflow only uses this to determine which conversations were handed off to a human agent for measurement purposes. What else to do with this signal is up to you and your handoff procedures.
     * You may set this, for example:
     * * In the entryFulfillment of a Page if entering the page indicates something went extremely wrong in the conversation.
     * * In a webhook response when you determine that the customer issue can only be handled by a human.
     */
    liveAgentHandoff?: outputs.DialogflowCxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessageLiveAgentHandoff;
    /**
     * A text or ssml response that is preferentially used for TTS output audio synthesis, as described in the comment on the ResponseMessage message.
     */
    outputAudioText?: outputs.DialogflowCxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessageOutputAudioText;
    /**
     * A custom, platform-specific payload.
     */
    payload?: string;
    /**
     * Specifies an audio clip to be played by the client as part of the response.
     */
    playAudio?: outputs.DialogflowCxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessagePlayAudio;
    /**
     * Represents the signal that telles the client to transfer the phone call connected to the agent to a third-party endpoint.
     */
    telephonyTransferCall?: outputs.DialogflowCxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessageTelephonyTransferCall;
    /**
     * The text response message.
     */
    text?: outputs.DialogflowCxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessageText;
}

export interface DialogflowCxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessageConversationSuccess {
    /**
     * Custom metadata. Dialogflow doesn't impose any structure on this.
     */
    metadata?: string;
}

export interface DialogflowCxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessageLiveAgentHandoff {
    /**
     * Custom metadata. Dialogflow doesn't impose any structure on this.
     */
    metadata?: string;
}

export interface DialogflowCxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessageOutputAudioText {
    /**
     * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
     */
    allowPlaybackInterruption: boolean;
    /**
     * The SSML text to be synthesized. For more information, see SSML.
     */
    ssml?: string;
    /**
     * The raw text to be synthesized.
     */
    text?: string;
}

export interface DialogflowCxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessagePlayAudio {
    /**
     * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
     */
    allowPlaybackInterruption: boolean;
    /**
     * URI of the audio clip. Dialogflow does not impose any validation on this value. It is specific to the client that reads it.
     */
    audioUri: string;
}

export interface DialogflowCxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessageTelephonyTransferCall {
    /**
     * Transfer the call to a phone number in E.164 format.
     */
    phoneNumber: string;
}

export interface DialogflowCxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentMessageText {
    /**
     * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
     */
    allowPlaybackInterruption: boolean;
    /**
     * A collection of text responses.
     */
    texts?: string[];
}

export interface DialogflowCxPageFormParameterFillBehaviorRepromptEventHandlerTriggerFulfillmentSetParameterAction {
    /**
     * Display name of the parameter.
     */
    parameter?: string;
    /**
     * The new JSON-encoded value of the parameter. A null value clears the parameter.
     */
    value?: string;
}

export interface DialogflowCxPageTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DialogflowCxPageTransitionRoute {
    /**
     * The condition to evaluate against form parameters or session parameters.
     * At least one of intent or condition must be specified. When both intent and condition are specified, the transition can only happen when both are fulfilled.
     */
    condition?: string;
    /**
     * The unique identifier of an Intent.
     * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/intents/<Intent ID>. Indicates that the transition can only happen when the given intent is matched. At least one of intent or condition must be specified. When both intent and condition are specified, the transition can only happen when both are fulfilled.
     */
    intent?: string;
    /**
     * The unique identifier of this transition route.
     */
    name: string;
    /**
     * The target flow to transition to.
     * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>.
     */
    targetFlow?: string;
    /**
     * The target page to transition to.
     * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>/pages/<Page ID>.
     */
    targetPage?: string;
    /**
     * The fulfillment to call when the condition is satisfied. At least one of triggerFulfillment and target must be specified. When both are defined, triggerFulfillment is executed first.
     */
    triggerFulfillment?: outputs.DialogflowCxPageTransitionRouteTriggerFulfillment;
}

export interface DialogflowCxPageTransitionRouteTriggerFulfillment {
    /**
     * Conditional cases for this fulfillment.
     */
    conditionalCases?: outputs.DialogflowCxPageTransitionRouteTriggerFulfillmentConditionalCase[];
    /**
     * The list of rich message responses to present to the user.
     */
    messages?: outputs.DialogflowCxPageTransitionRouteTriggerFulfillmentMessage[];
    /**
     * Whether Dialogflow should return currently queued fulfillment response messages in streaming APIs. If a webhook is specified, it happens before Dialogflow invokes webhook. Warning: 1) This flag only affects streaming API. Responses are still queued and returned once in non-streaming API. 2) The flag can be enabled in any fulfillment but only the first 3 partial responses will be returned. You may only want to apply it to fulfillments that have slow webhooks.
     */
    returnPartialResponses?: boolean;
    /**
     * Set parameter values before executing the webhook.
     */
    setParameterActions?: outputs.DialogflowCxPageTransitionRouteTriggerFulfillmentSetParameterAction[];
    /**
     * The tag used by the webhook to identify which fulfillment is being called. This field is required if webhook is specified.
     */
    tag?: string;
    /**
     * The webhook to call. Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/webhooks/<Webhook ID>.
     */
    webhook?: string;
}

export interface DialogflowCxPageTransitionRouteTriggerFulfillmentConditionalCase {
    /**
     * A JSON encoded list of cascading if-else conditions. Cases are mutually exclusive. The first one with a matching condition is selected, all the rest ignored.
     * See [Case](https://cloud.google.com/dialogflow/cx/docs/reference/rest/v3/Fulfillment#case) for the schema.
     */
    cases?: string;
}

export interface DialogflowCxPageTransitionRouteTriggerFulfillmentMessage {
    /**
     * The channel which the response is associated with. Clients can specify the channel via QueryParameters.channel, and only associated channel response will be returned.
     */
    channel?: string;
    /**
     * Indicates that the conversation succeeded, i.e., the bot handled the issue that the customer talked to it about.
     * Dialogflow only uses this to determine which conversations should be counted as successful and doesn't process the metadata in this message in any way. Note that Dialogflow also considers conversations that get to the conversation end page as successful even if they don't return ConversationSuccess.
     * You may set this, for example:
     * * In the entryFulfillment of a Page if entering the page indicates that the conversation succeeded.
     * * In a webhook response when you determine that you handled the customer issue.
     */
    conversationSuccess?: outputs.DialogflowCxPageTransitionRouteTriggerFulfillmentMessageConversationSuccess;
    /**
     * Indicates that the conversation should be handed off to a live agent.
     * Dialogflow only uses this to determine which conversations were handed off to a human agent for measurement purposes. What else to do with this signal is up to you and your handoff procedures.
     * You may set this, for example:
     * * In the entryFulfillment of a Page if entering the page indicates something went extremely wrong in the conversation.
     * * In a webhook response when you determine that the customer issue can only be handled by a human.
     */
    liveAgentHandoff?: outputs.DialogflowCxPageTransitionRouteTriggerFulfillmentMessageLiveAgentHandoff;
    /**
     * A text or ssml response that is preferentially used for TTS output audio synthesis, as described in the comment on the ResponseMessage message.
     */
    outputAudioText?: outputs.DialogflowCxPageTransitionRouteTriggerFulfillmentMessageOutputAudioText;
    /**
     * A custom, platform-specific payload.
     */
    payload?: string;
    /**
     * Specifies an audio clip to be played by the client as part of the response.
     */
    playAudio?: outputs.DialogflowCxPageTransitionRouteTriggerFulfillmentMessagePlayAudio;
    /**
     * Represents the signal that telles the client to transfer the phone call connected to the agent to a third-party endpoint.
     */
    telephonyTransferCall?: outputs.DialogflowCxPageTransitionRouteTriggerFulfillmentMessageTelephonyTransferCall;
    /**
     * The text response message.
     */
    text?: outputs.DialogflowCxPageTransitionRouteTriggerFulfillmentMessageText;
}

export interface DialogflowCxPageTransitionRouteTriggerFulfillmentMessageConversationSuccess {
    /**
     * Custom metadata. Dialogflow doesn't impose any structure on this.
     */
    metadata?: string;
}

export interface DialogflowCxPageTransitionRouteTriggerFulfillmentMessageLiveAgentHandoff {
    /**
     * Custom metadata. Dialogflow doesn't impose any structure on this.
     */
    metadata?: string;
}

export interface DialogflowCxPageTransitionRouteTriggerFulfillmentMessageOutputAudioText {
    /**
     * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
     */
    allowPlaybackInterruption: boolean;
    /**
     * The SSML text to be synthesized. For more information, see SSML.
     */
    ssml?: string;
    /**
     * The raw text to be synthesized.
     */
    text?: string;
}

export interface DialogflowCxPageTransitionRouteTriggerFulfillmentMessagePlayAudio {
    /**
     * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
     */
    allowPlaybackInterruption: boolean;
    /**
     * URI of the audio clip. Dialogflow does not impose any validation on this value. It is specific to the client that reads it.
     */
    audioUri: string;
}

export interface DialogflowCxPageTransitionRouteTriggerFulfillmentMessageTelephonyTransferCall {
    /**
     * Transfer the call to a phone number in E.164 format.
     */
    phoneNumber: string;
}

export interface DialogflowCxPageTransitionRouteTriggerFulfillmentMessageText {
    /**
     * Whether the playback of this message can be interrupted by the end user's speech and the client can then starts the next Dialogflow request.
     */
    allowPlaybackInterruption: boolean;
    /**
     * A collection of text responses.
     */
    texts?: string[];
}

export interface DialogflowCxPageTransitionRouteTriggerFulfillmentSetParameterAction {
    /**
     * Display name of the parameter.
     */
    parameter?: string;
    /**
     * The new JSON-encoded value of the parameter. A null value clears the parameter.
     */
    value?: string;
}

export interface DialogflowCxSecuritySettingsAudioExportSettings {
    /**
     * Filename pattern for exported audio.
     */
    audioExportPattern?: string;
    /**
     * File format for exported audio file. Currently only in telephony recordings.
     * * MULAW: G.711 mu-law PCM with 8kHz sample rate.
     * * MP3: MP3 file format.
     * * OGG: OGG Vorbis. Possible values: ["MULAW", "MP3", "OGG"]
     */
    audioFormat?: string;
    /**
     * Enable audio redaction if it is true.
     */
    enableAudioRedaction?: boolean;
    /**
     * Cloud Storage bucket to export audio record to. Setting this field would grant the Storage Object Creator role to the Dialogflow Service Agent. API caller that tries to modify this field should have the permission of storage.buckets.setIamPolicy.
     */
    gcsBucket?: string;
}

export interface DialogflowCxSecuritySettingsInsightsExportSettings {
    /**
     * If enabled, we will automatically exports conversations to Insights and Insights runs its analyzers.
     */
    enableInsightsExport: boolean;
}

export interface DialogflowCxSecuritySettingsTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DialogflowCxTestCaseLastTestResult {
    conversationTurns: outputs.DialogflowCxTestCaseLastTestResultConversationTurn[];
    environment: string;
    name: string;
    testResult: string;
    testTime: string;
}

export interface DialogflowCxTestCaseLastTestResultConversationTurn {
    userInputs: outputs.DialogflowCxTestCaseLastTestResultConversationTurnUserInput[];
    virtualAgentOutputs: outputs.DialogflowCxTestCaseLastTestResultConversationTurnVirtualAgentOutput[];
}

export interface DialogflowCxTestCaseLastTestResultConversationTurnUserInput {
    enableSentimentAnalysis: boolean;
    injectedParameters: string;
    inputs: outputs.DialogflowCxTestCaseLastTestResultConversationTurnUserInputInput[];
    isWebhookEnabled: boolean;
}

export interface DialogflowCxTestCaseLastTestResultConversationTurnUserInputInput {
    dtmfs: outputs.DialogflowCxTestCaseLastTestResultConversationTurnUserInputInputDtmf[];
    events: outputs.DialogflowCxTestCaseLastTestResultConversationTurnUserInputInputEvent[];
    languageCode: string;
    texts: outputs.DialogflowCxTestCaseLastTestResultConversationTurnUserInputInputText[];
}

export interface DialogflowCxTestCaseLastTestResultConversationTurnUserInputInputDtmf {
    digits: string;
    finishDigit: string;
}

export interface DialogflowCxTestCaseLastTestResultConversationTurnUserInputInputEvent {
    event: string;
}

export interface DialogflowCxTestCaseLastTestResultConversationTurnUserInputInputText {
    text: string;
}

export interface DialogflowCxTestCaseLastTestResultConversationTurnVirtualAgentOutput {
    currentPages: outputs.DialogflowCxTestCaseLastTestResultConversationTurnVirtualAgentOutputCurrentPage[];
    differences: outputs.DialogflowCxTestCaseLastTestResultConversationTurnVirtualAgentOutputDifference[];
    sessionParameters: string;
    statuses: outputs.DialogflowCxTestCaseLastTestResultConversationTurnVirtualAgentOutputStatus[];
    textResponses: outputs.DialogflowCxTestCaseLastTestResultConversationTurnVirtualAgentOutputTextResponse[];
    triggeredIntents: outputs.DialogflowCxTestCaseLastTestResultConversationTurnVirtualAgentOutputTriggeredIntent[];
}

export interface DialogflowCxTestCaseLastTestResultConversationTurnVirtualAgentOutputCurrentPage {
    displayName: string;
    name: string;
}

export interface DialogflowCxTestCaseLastTestResultConversationTurnVirtualAgentOutputDifference {
    description: string;
    type: string;
}

export interface DialogflowCxTestCaseLastTestResultConversationTurnVirtualAgentOutputStatus {
    code: number;
    details: string;
    message: string;
}

export interface DialogflowCxTestCaseLastTestResultConversationTurnVirtualAgentOutputTextResponse {
    texts: string[];
}

export interface DialogflowCxTestCaseLastTestResultConversationTurnVirtualAgentOutputTriggeredIntent {
    displayName: string;
    name: string;
}

export interface DialogflowCxTestCaseTestCaseConversationTurn {
    /**
     * The user input.
     */
    userInput?: outputs.DialogflowCxTestCaseTestCaseConversationTurnUserInput;
    /**
     * The virtual agent output.
     */
    virtualAgentOutput?: outputs.DialogflowCxTestCaseTestCaseConversationTurnVirtualAgentOutput;
}

export interface DialogflowCxTestCaseTestCaseConversationTurnUserInput {
    /**
     * Whether sentiment analysis is enabled.
     */
    enableSentimentAnalysis?: boolean;
    /**
     * Parameters that need to be injected into the conversation during intent detection.
     */
    injectedParameters?: string;
    /**
     * User input. Supports text input, event input, dtmf input in the test case.
     */
    input?: outputs.DialogflowCxTestCaseTestCaseConversationTurnUserInputInput;
    /**
     * If webhooks should be allowed to trigger in response to the user utterance. Often if parameters are injected, webhooks should not be enabled.
     */
    isWebhookEnabled?: boolean;
}

export interface DialogflowCxTestCaseTestCaseConversationTurnUserInputInput {
    /**
     * The DTMF event to be handled.
     */
    dtmf?: outputs.DialogflowCxTestCaseTestCaseConversationTurnUserInputInputDtmf;
    /**
     * The event to be triggered.
     */
    event?: outputs.DialogflowCxTestCaseTestCaseConversationTurnUserInputInputEvent;
    /**
     * The language of the input. See [Language Support](https://cloud.google.com/dialogflow/cx/docs/reference/language) for a list of the currently supported language codes.
     * Note that queries in the same session do not necessarily need to specify the same language.
     */
    languageCode?: string;
    /**
     * The natural language text to be processed.
     */
    text?: outputs.DialogflowCxTestCaseTestCaseConversationTurnUserInputInputText;
}

export interface DialogflowCxTestCaseTestCaseConversationTurnUserInputInputDtmf {
    /**
     * The dtmf digits.
     */
    digits?: string;
    /**
     * The finish digit (if any).
     */
    finishDigit?: string;
}

export interface DialogflowCxTestCaseTestCaseConversationTurnUserInputInputEvent {
    /**
     * Name of the event.
     */
    event: string;
}

export interface DialogflowCxTestCaseTestCaseConversationTurnUserInputInputText {
    /**
     * The natural language text to be processed. Text length must not exceed 256 characters.
     */
    text: string;
}

export interface DialogflowCxTestCaseTestCaseConversationTurnVirtualAgentOutput {
    /**
     * The [Page](https://cloud.google.com/dialogflow/cx/docs/reference/rest/v3/projects.locations.agents.flows.pages#Page) on which the utterance was spoken.
     */
    currentPage?: outputs.DialogflowCxTestCaseTestCaseConversationTurnVirtualAgentOutputCurrentPage;
    /**
     * The session parameters available to the bot at this point.
     */
    sessionParameters?: string;
    /**
     * The text responses from the agent for the turn.
     */
    textResponses?: outputs.DialogflowCxTestCaseTestCaseConversationTurnVirtualAgentOutputTextResponse[];
    /**
     * The [Intent](https://cloud.google.com/dialogflow/cx/docs/reference/rest/v3/projects.locations.agents.intents#Intent) that triggered the response.
     */
    triggeredIntent?: outputs.DialogflowCxTestCaseTestCaseConversationTurnVirtualAgentOutputTriggeredIntent;
}

export interface DialogflowCxTestCaseTestCaseConversationTurnVirtualAgentOutputCurrentPage {
    /**
     * The human-readable name of the page, unique within the flow.
     */
    displayName: string;
    /**
     * The unique identifier of the page.
     * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>/pages/<Page ID>.
     */
    name?: string;
}

export interface DialogflowCxTestCaseTestCaseConversationTurnVirtualAgentOutputTextResponse {
    /**
     * A collection of text responses.
     */
    texts?: string[];
}

export interface DialogflowCxTestCaseTestCaseConversationTurnVirtualAgentOutputTriggeredIntent {
    /**
     * The human-readable name of the intent, unique within the agent.
     */
    displayName: string;
    /**
     * The unique identifier of the intent.
     * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/intents/<Intent ID>.
     */
    name?: string;
}

export interface DialogflowCxTestCaseTestConfig {
    /**
     * Flow name to start the test case with.
     * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>.
     * Only one of flow and page should be set to indicate the starting point of the test case. If neither is set, the test case will start with start page on the default start flow.
     */
    flow?: string;
    /**
     * The page to start the test case with.
     * Format: projects/<Project ID>/locations/<Location ID>/agents/<Agent ID>/flows/<Flow ID>/pages/<Page ID>.
     * Only one of flow and page should be set to indicate the starting point of the test case. If neither is set, the test case will start with start page on the default start flow.
     */
    page?: string;
    /**
     * Session parameters to be compared when calculating differences.
     */
    trackingParameters?: string[];
}

export interface DialogflowCxTestCaseTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DialogflowCxVersionNluSetting {
    classificationThreshold: number;
    modelTrainingMode: string;
    modelType: string;
}

export interface DialogflowCxVersionTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DialogflowCxWebhookGenericWebService {
    /**
     * Specifies a list of allowed custom CA certificates (in DER format) for HTTPS verification.
     */
    allowedCaCerts?: string[];
    /**
     * The HTTP request headers to send together with webhook requests.
     */
    requestHeaders?: {[key: string]: string};
    /**
     * Whether to use speech adaptation for speech recognition.
     */
    uri: string;
}

export interface DialogflowCxWebhookServiceDirectory {
    /**
     * The name of Service Directory service.
     */
    genericWebService: outputs.DialogflowCxWebhookServiceDirectoryGenericWebService;
    /**
     * The name of Service Directory service.
     */
    service: string;
}

export interface DialogflowCxWebhookServiceDirectoryGenericWebService {
    /**
     * Specifies a list of allowed custom CA certificates (in DER format) for HTTPS verification.
     */
    allowedCaCerts?: string[];
    /**
     * The HTTP request headers to send together with webhook requests.
     */
    requestHeaders?: {[key: string]: string};
    /**
     * Whether to use speech adaptation for speech recognition.
     */
    uri: string;
}

export interface DialogflowCxWebhookTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DialogflowEntityTypeEntity {
    /**
     * A collection of value synonyms. For example, if the entity type is vegetable, and value is scallions, a synonym
     * could be green onions.
     * For KIND_LIST entity types:
     * * This collection must contain exactly one synonym equal to value.
     */
    synonyms: string[];
    /**
     * The primary value associated with this entity entry. For example, if the entity type is vegetable, the value
     * could be scallions.
     * For KIND_MAP entity types:
     * * A reference value to be used in place of synonyms.
     * For KIND_LIST entity types:
     * * A string that can contain references to other entity types (with or without aliases).
     */
    value: string;
}

export interface DialogflowEntityTypeTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DialogflowFulfillmentFeature {
    /**
     * The type of the feature that enabled for fulfillment.
     * * SMALLTALK: Fulfillment is enabled for SmallTalk. Possible values: ["SMALLTALK"]
     */
    type: string;
}

export interface DialogflowFulfillmentGenericWebService {
    /**
     * The password for HTTP Basic authentication.
     */
    password?: string;
    /**
     * The HTTP request headers to send together with fulfillment requests.
     */
    requestHeaders?: {[key: string]: string};
    /**
     * The fulfillment URI for receiving POST requests. It must use https protocol.
     */
    uri: string;
    /**
     * The user name for HTTP Basic authentication.
     */
    username?: string;
}

export interface DialogflowFulfillmentTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DialogflowIntentFollowupIntentInfo {
    followupIntentName: string;
    parentFollowupIntentName: string;
}

export interface DialogflowIntentTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DiscoveryEngineChatEngineChatEngineConfig {
    /**
     * The configuration to generate the Dialogflow agent that is associated to this Engine.
     */
    agentCreationConfig: outputs.DiscoveryEngineChatEngineChatEngineConfigAgentCreationConfig;
}

export interface DiscoveryEngineChatEngineChatEngineConfigAgentCreationConfig {
    /**
     * Name of the company, organization or other entity that the agent represents. Used for knowledge connector LLM prompt and for knowledge search.
     */
    business?: string;
    /**
     * The default language of the agent as a language tag. See [Language Support](https://cloud.google.com/dialogflow/docs/reference/language) for a list of the currently supported language codes.
     */
    defaultLanguageCode: string;
    /**
     * Agent location for Agent creation, currently supported values: global/us/eu, it needs to be the same region as the Chat Engine.
     */
    location?: string;
    /**
     * The time zone of the agent from the [time zone database](https://www.iana.org/time-zones), e.g., America/New_York, Europe/Paris.
     */
    timeZone: string;
}

export interface DiscoveryEngineChatEngineChatEngineMetadata {
    dialogflowAgent: string;
}

export interface DiscoveryEngineChatEngineCommonConfig {
    /**
     * The name of the company, business or entity that is associated with the engine. Setting this may help improve LLM related features.
     */
    companyName?: string;
}

export interface DiscoveryEngineChatEngineTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DiscoveryEngineDataStoreDocumentProcessingConfig {
    /**
     * Whether chunking mode is enabled.
     */
    chunkingConfig?: outputs.DiscoveryEngineDataStoreDocumentProcessingConfigChunkingConfig;
    /**
     * Configurations for default Document parser. If not specified, this resource
     * will be configured to use a default DigitalParsingConfig, and the default parsing
     * config will be applied to all file types for Document parsing.
     */
    defaultParsingConfig?: outputs.DiscoveryEngineDataStoreDocumentProcessingConfigDefaultParsingConfig;
    /**
     * The full resource name of the Document Processing Config. Format:
     * 'projects/{project}/locations/{location}/collections/{collection_id}/dataStores/{data_store_id}/documentProcessingConfig'.
     */
    name: string;
    /**
     * Map from file type to override the default parsing configuration based on the file type. Supported keys:
     *   * 'pdf': Override parsing config for PDF files, either digital parsing, ocr parsing or layout parsing is supported.
     *   * 'html': Override parsing config for HTML files, only digital parsing and or layout parsing are supported.
     *   * 'docx': Override parsing config for DOCX files, only digital parsing and or layout parsing are supported.
     */
    parsingConfigOverrides?: outputs.DiscoveryEngineDataStoreDocumentProcessingConfigParsingConfigOverride[];
}

export interface DiscoveryEngineDataStoreDocumentProcessingConfigChunkingConfig {
    /**
     * Configuration for the layout based chunking.
     */
    layoutBasedChunkingConfig?: outputs.DiscoveryEngineDataStoreDocumentProcessingConfigChunkingConfigLayoutBasedChunkingConfig;
}

export interface DiscoveryEngineDataStoreDocumentProcessingConfigChunkingConfigLayoutBasedChunkingConfig {
    /**
     * The token size limit for each chunk.
     * Supported values: 100-500 (inclusive). Default value: 500.
     */
    chunkSize?: number;
    /**
     * Whether to include appending different levels of headings to chunks from the middle of the document to prevent context loss.
     * Default value: False.
     */
    includeAncestorHeadings?: boolean;
}

export interface DiscoveryEngineDataStoreDocumentProcessingConfigDefaultParsingConfig {
    /**
     * Configurations applied to digital parser.
     */
    digitalParsingConfig?: outputs.DiscoveryEngineDataStoreDocumentProcessingConfigDefaultParsingConfigDigitalParsingConfig;
    /**
     * Configurations applied to layout parser.
     */
    layoutParsingConfig?: outputs.DiscoveryEngineDataStoreDocumentProcessingConfigDefaultParsingConfigLayoutParsingConfig;
    /**
     * Configurations applied to OCR parser. Currently it only applies to PDFs.
     */
    ocrParsingConfig?: outputs.DiscoveryEngineDataStoreDocumentProcessingConfigDefaultParsingConfigOcrParsingConfig;
}

export interface DiscoveryEngineDataStoreDocumentProcessingConfigDefaultParsingConfigDigitalParsingConfig {
}

export interface DiscoveryEngineDataStoreDocumentProcessingConfigDefaultParsingConfigLayoutParsingConfig {
}

export interface DiscoveryEngineDataStoreDocumentProcessingConfigDefaultParsingConfigOcrParsingConfig {
    /**
     * If true, will use native text instead of OCR text on pages containing native text.
     */
    useNativeText?: boolean;
}

export interface DiscoveryEngineDataStoreDocumentProcessingConfigParsingConfigOverride {
    /**
     * Configurations applied to digital parser.
     */
    digitalParsingConfig?: outputs.DiscoveryEngineDataStoreDocumentProcessingConfigParsingConfigOverrideDigitalParsingConfig;
    fileType: string;
    /**
     * Configurations applied to layout parser.
     */
    layoutParsingConfig?: outputs.DiscoveryEngineDataStoreDocumentProcessingConfigParsingConfigOverrideLayoutParsingConfig;
    /**
     * Configurations applied to OCR parser. Currently it only applies to PDFs.
     */
    ocrParsingConfig?: outputs.DiscoveryEngineDataStoreDocumentProcessingConfigParsingConfigOverrideOcrParsingConfig;
}

export interface DiscoveryEngineDataStoreDocumentProcessingConfigParsingConfigOverrideDigitalParsingConfig {
}

export interface DiscoveryEngineDataStoreDocumentProcessingConfigParsingConfigOverrideLayoutParsingConfig {
}

export interface DiscoveryEngineDataStoreDocumentProcessingConfigParsingConfigOverrideOcrParsingConfig {
    /**
     * If true, will use native text instead of OCR text on pages containing native text.
     */
    useNativeText?: boolean;
}

export interface DiscoveryEngineDataStoreTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DiscoveryEngineSchemaTimeouts {
    create?: string;
    delete?: string;
}

export interface DiscoveryEngineSearchEngineCommonConfig {
    /**
     * The name of the company, business or entity that is associated with the engine. Setting this may help improve LLM related features.cd
     */
    companyName?: string;
}

export interface DiscoveryEngineSearchEngineSearchEngineConfig {
    /**
     * The add-on that this search engine enables. Possible values: ["SEARCH_ADD_ON_LLM"]
     */
    searchAddOns?: string[];
    /**
     * The search feature tier of this engine. Defaults to SearchTier.SEARCH_TIER_STANDARD if not specified. Default value: "SEARCH_TIER_STANDARD" Possible values: ["SEARCH_TIER_STANDARD", "SEARCH_TIER_ENTERPRISE"]
     */
    searchTier?: string;
}

export interface DiscoveryEngineSearchEngineTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DnsManagedZoneCloudLoggingConfig {
    /**
     * If set, enable query logging for this ManagedZone. False by default, making logging opt-in.
     */
    enableLogging: boolean;
}

export interface DnsManagedZoneDnssecConfig {
    /**
     * Specifies parameters that will be used for generating initial DnsKeys
     * for this ManagedZone. If you provide a spec for keySigning or zoneSigning,
     * you must also provide one for the other.
     * default_key_specs can only be updated when the state is 'off'.
     */
    defaultKeySpecs?: outputs.DnsManagedZoneDnssecConfigDefaultKeySpec[];
    /**
     * Identifies what kind of resource this is
     */
    kind?: string;
    /**
     * Specifies the mechanism used to provide authenticated denial-of-existence responses.
     * non_existence can only be updated when the state is 'off'. Possible values: ["nsec", "nsec3"]
     */
    nonExistence: string;
    /**
     * Specifies whether DNSSEC is enabled, and what mode it is in Possible values: ["off", "on", "transfer"]
     */
    state?: string;
}

export interface DnsManagedZoneDnssecConfigDefaultKeySpec {
    /**
     * String mnemonic specifying the DNSSEC algorithm of this key Possible values: ["ecdsap256sha256", "ecdsap384sha384", "rsasha1", "rsasha256", "rsasha512"]
     */
    algorithm?: string;
    /**
     * Length of the keys in bits
     */
    keyLength?: number;
    /**
     * Specifies whether this is a key signing key (KSK) or a zone
     * signing key (ZSK). Key signing keys have the Secure Entry
     * Point flag set and, when active, will only be used to sign
     * resource record sets of type DNSKEY. Zone signing keys do
     * not have the Secure Entry Point flag set and will be used
     * to sign all other types of resource record sets. Possible values: ["keySigning", "zoneSigning"]
     */
    keyType?: string;
    /**
     * Identifies what kind of resource this is
     */
    kind?: string;
}

export interface DnsManagedZoneForwardingConfig {
    /**
     * List of target name servers to forward to. Cloud DNS will
     * select the best available name server if more than
     * one target is given.
     */
    targetNameServers: outputs.DnsManagedZoneForwardingConfigTargetNameServer[];
}

export interface DnsManagedZoneForwardingConfigTargetNameServer {
    /**
     * Forwarding path for this TargetNameServer. If unset or 'default' Cloud DNS will make forwarding
     * decision based on address ranges, i.e. RFC1918 addresses go to the VPC, Non-RFC1918 addresses go
     * to the Internet. When set to 'private', Cloud DNS will always send queries through VPC for this target Possible values: ["default", "private"]
     */
    forwardingPath?: string;
    /**
     * IPv4 address of a target name server.
     */
    ipv4Address: string;
}

export interface DnsManagedZoneIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DnsManagedZoneIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface DnsManagedZonePeeringConfig {
    /**
     * The network with which to peer.
     */
    targetNetwork: outputs.DnsManagedZonePeeringConfigTargetNetwork;
}

export interface DnsManagedZonePeeringConfigTargetNetwork {
    /**
     * The id or fully qualified URL of the VPC network to forward queries to.
     * This should be formatted like 'projects/{project}/global/networks/{network}' or
     * 'https://www.googleapis.com/compute/v1/projects/{project}/global/networks/{network}'
     */
    networkUrl: string;
}

export interface DnsManagedZonePrivateVisibilityConfig {
    /**
     * The list of Google Kubernetes Engine clusters that can see this zone.
     */
    gkeClusters?: outputs.DnsManagedZonePrivateVisibilityConfigGkeCluster[];
    networks?: outputs.DnsManagedZonePrivateVisibilityConfigNetwork[];
}

export interface DnsManagedZonePrivateVisibilityConfigGkeCluster {
    /**
     * The resource name of the cluster to bind this ManagedZone to.
     * This should be specified in the format like
     * 'projects/*&#47;locations/*&#47;clusters/*'
     */
    gkeClusterName: string;
}

export interface DnsManagedZonePrivateVisibilityConfigNetwork {
    /**
     * The id or fully qualified URL of the VPC network to bind to.
     * This should be formatted like 'projects/{project}/global/networks/{network}' or
     * 'https://www.googleapis.com/compute/v1/projects/{project}/global/networks/{network}'
     */
    networkUrl: string;
}

export interface DnsManagedZoneTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DnsPolicyAlternativeNameServerConfig {
    /**
     * Sets an alternative name server for the associated networks. When specified,
     * all DNS queries are forwarded to a name server that you choose. Names such as .internal
     * are not available when an alternative name server is specified.
     */
    targetNameServers: outputs.DnsPolicyAlternativeNameServerConfigTargetNameServer[];
}

export interface DnsPolicyAlternativeNameServerConfigTargetNameServer {
    /**
     * Forwarding path for this TargetNameServer. If unset or 'default' Cloud DNS will make forwarding
     * decision based on address ranges, i.e. RFC1918 addresses go to the VPC, Non-RFC1918 addresses go
     * to the Internet. When set to 'private', Cloud DNS will always send queries through VPC for this target Possible values: ["default", "private"]
     */
    forwardingPath?: string;
    /**
     * IPv4 address to forward to.
     */
    ipv4Address: string;
}

export interface DnsPolicyNetwork {
    /**
     * The id or fully qualified URL of the VPC network to forward queries to.
     * This should be formatted like 'projects/{project}/global/networks/{network}' or
     * 'https://www.googleapis.com/compute/v1/projects/{project}/global/networks/{network}'
     */
    networkUrl: string;
}

export interface DnsPolicyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DnsRecordSetRoutingPolicy {
    /**
     * Specifies whether to enable fencing for geo queries.
     */
    enableGeoFencing?: boolean;
    /**
     * The configuration for Geo location based routing policy.
     */
    geos?: outputs.DnsRecordSetRoutingPolicyGeo[];
    /**
     * The configuration for a failover policy with global to regional failover. Queries are responded to with the global primary targets, but if none of the primary targets are healthy, then we fallback to a regional failover policy.
     */
    primaryBackup?: outputs.DnsRecordSetRoutingPolicyPrimaryBackup;
    /**
     * The configuration for Weighted Round Robin based routing policy.
     */
    wrrs?: outputs.DnsRecordSetRoutingPolicyWrr[];
}

export interface DnsRecordSetRoutingPolicyGeo {
    /**
     * For A and AAAA types only. The list of targets to be health checked. These can be specified along with `rrdatas` within this item.
     */
    healthCheckedTargets?: outputs.DnsRecordSetRoutingPolicyGeoHealthCheckedTargets;
    /**
     * The location name defined in Google Cloud.
     */
    location: string;
    rrdatas?: string[];
}

export interface DnsRecordSetRoutingPolicyGeoHealthCheckedTargets {
    /**
     * The list of internal load balancers to health check.
     */
    internalLoadBalancers: outputs.DnsRecordSetRoutingPolicyGeoHealthCheckedTargetsInternalLoadBalancer[];
}

export interface DnsRecordSetRoutingPolicyGeoHealthCheckedTargetsInternalLoadBalancer {
    /**
     * The frontend IP address of the load balancer.
     */
    ipAddress: string;
    /**
     * The configured IP protocol of the load balancer. This value is case-sensitive. Possible values: ["tcp", "udp"]
     */
    ipProtocol: string;
    /**
     * The type of load balancer. This value is case-sensitive. Possible values: ["regionalL4ilb", "regionalL7ilb", "globalL7ilb"]
     */
    loadBalancerType?: string;
    /**
     * The fully qualified url of the network in which the load balancer belongs. This should be formatted like `https://www.googleapis.com/compute/v1/projects/{project}/global/networks/{network}`.
     */
    networkUrl: string;
    /**
     * The configured port of the load balancer.
     */
    port: string;
    /**
     * The ID of the project in which the load balancer belongs.
     */
    project: string;
    /**
     * The region of the load balancer. Only needed for regional load balancers.
     */
    region?: string;
}

export interface DnsRecordSetRoutingPolicyPrimaryBackup {
    /**
     * The backup geo targets, which provide a regional failover policy for the otherwise global primary targets.
     */
    backupGeos: outputs.DnsRecordSetRoutingPolicyPrimaryBackupBackupGeo[];
    /**
     * Specifies whether to enable fencing for backup geo queries.
     */
    enableGeoFencingForBackups?: boolean;
    /**
     * The list of global primary targets to be health checked.
     */
    primary: outputs.DnsRecordSetRoutingPolicyPrimaryBackupPrimary;
    /**
     * Specifies the percentage of traffic to send to the backup targets even when the primary targets are healthy.
     */
    trickleRatio?: number;
}

export interface DnsRecordSetRoutingPolicyPrimaryBackupBackupGeo {
    /**
     * For A and AAAA types only. The list of targets to be health checked. These can be specified along with `rrdatas` within this item.
     */
    healthCheckedTargets?: outputs.DnsRecordSetRoutingPolicyPrimaryBackupBackupGeoHealthCheckedTargets;
    /**
     * The location name defined in Google Cloud.
     */
    location: string;
    rrdatas?: string[];
}

export interface DnsRecordSetRoutingPolicyPrimaryBackupBackupGeoHealthCheckedTargets {
    /**
     * The list of internal load balancers to health check.
     */
    internalLoadBalancers: outputs.DnsRecordSetRoutingPolicyPrimaryBackupBackupGeoHealthCheckedTargetsInternalLoadBalancer[];
}

export interface DnsRecordSetRoutingPolicyPrimaryBackupBackupGeoHealthCheckedTargetsInternalLoadBalancer {
    /**
     * The frontend IP address of the load balancer.
     */
    ipAddress: string;
    /**
     * The configured IP protocol of the load balancer. This value is case-sensitive. Possible values: ["tcp", "udp"]
     */
    ipProtocol: string;
    /**
     * The type of load balancer. This value is case-sensitive. Possible values: ["regionalL4ilb", "regionalL7ilb", "globalL7ilb"]
     */
    loadBalancerType?: string;
    /**
     * The fully qualified url of the network in which the load balancer belongs. This should be formatted like `https://www.googleapis.com/compute/v1/projects/{project}/global/networks/{network}`.
     */
    networkUrl: string;
    /**
     * The configured port of the load balancer.
     */
    port: string;
    /**
     * The ID of the project in which the load balancer belongs.
     */
    project: string;
    /**
     * The region of the load balancer. Only needed for regional load balancers.
     */
    region?: string;
}

export interface DnsRecordSetRoutingPolicyPrimaryBackupPrimary {
    /**
     * The list of internal load balancers to health check.
     */
    internalLoadBalancers: outputs.DnsRecordSetRoutingPolicyPrimaryBackupPrimaryInternalLoadBalancer[];
}

export interface DnsRecordSetRoutingPolicyPrimaryBackupPrimaryInternalLoadBalancer {
    /**
     * The frontend IP address of the load balancer.
     */
    ipAddress: string;
    /**
     * The configured IP protocol of the load balancer. This value is case-sensitive. Possible values: ["tcp", "udp"]
     */
    ipProtocol: string;
    /**
     * The type of load balancer. This value is case-sensitive. Possible values: ["regionalL4ilb", "regionalL7ilb", "globalL7ilb"]
     */
    loadBalancerType?: string;
    /**
     * The fully qualified url of the network in which the load balancer belongs. This should be formatted like `https://www.googleapis.com/compute/v1/projects/{project}/global/networks/{network}`.
     */
    networkUrl: string;
    /**
     * The configured port of the load balancer.
     */
    port: string;
    /**
     * The ID of the project in which the load balancer belongs.
     */
    project: string;
    /**
     * The region of the load balancer. Only needed for regional load balancers.
     */
    region?: string;
}

export interface DnsRecordSetRoutingPolicyWrr {
    /**
     * The list of targets to be health checked. Note that if DNSSEC is enabled for this zone, only one of `rrdatas` or `health_checked_targets` can be set.
     */
    healthCheckedTargets?: outputs.DnsRecordSetRoutingPolicyWrrHealthCheckedTargets;
    rrdatas?: string[];
    /**
     * The ratio of traffic routed to the target.
     */
    weight: number;
}

export interface DnsRecordSetRoutingPolicyWrrHealthCheckedTargets {
    /**
     * The list of internal load balancers to health check.
     */
    internalLoadBalancers: outputs.DnsRecordSetRoutingPolicyWrrHealthCheckedTargetsInternalLoadBalancer[];
}

export interface DnsRecordSetRoutingPolicyWrrHealthCheckedTargetsInternalLoadBalancer {
    /**
     * The frontend IP address of the load balancer.
     */
    ipAddress: string;
    /**
     * The configured IP protocol of the load balancer. This value is case-sensitive. Possible values: ["tcp", "udp"]
     */
    ipProtocol: string;
    /**
     * The type of load balancer. This value is case-sensitive. Possible values: ["regionalL4ilb", "regionalL7ilb", "globalL7ilb"]
     */
    loadBalancerType?: string;
    /**
     * The fully qualified url of the network in which the load balancer belongs. This should be formatted like `https://www.googleapis.com/compute/v1/projects/{project}/global/networks/{network}`.
     */
    networkUrl: string;
    /**
     * The configured port of the load balancer.
     */
    port: string;
    /**
     * The ID of the project in which the load balancer belongs.
     */
    project: string;
    /**
     * The region of the load balancer. Only needed for regional load balancers.
     */
    region?: string;
}

export interface DnsResponsePolicyGkeCluster {
    /**
     * The resource name of the cluster to bind this ManagedZone to.
     * This should be specified in the format like
     * 'projects/*&#47;locations/*&#47;clusters/*'
     */
    gkeClusterName: string;
}

export interface DnsResponsePolicyNetwork {
    /**
     * The fully qualified URL of the VPC network to bind to.
     * This should be formatted like
     * 'https://www.googleapis.com/compute/v1/projects/{project}/global/networks/{network}'
     */
    networkUrl: string;
}

export interface DnsResponsePolicyRuleLocalData {
    /**
     * All resource record sets for this selector, one per resource record type. The name must match the dns_name.
     */
    localDatas: outputs.DnsResponsePolicyRuleLocalDataLocalData[];
}

export interface DnsResponsePolicyRuleLocalDataLocalData {
    /**
     * For example, www.example.com.
     */
    name: string;
    /**
     * As defined in RFC 1035 (section 5) and RFC 1034 (section 3.6.1)
     */
    rrdatas?: string[];
    /**
     * Number of seconds that this ResourceRecordSet can be cached by
     * resolvers.
     */
    ttl?: number;
    /**
     * One of valid DNS resource types. Possible values: ["A", "AAAA", "CAA", "CNAME", "DNSKEY", "DS", "HTTPS", "IPSECVPNKEY", "MX", "NAPTR", "NS", "PTR", "SOA", "SPF", "SRV", "SSHFP", "SVCB", "TLSA", "TXT"]
     */
    type: string;
}

export interface DnsResponsePolicyRuleTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DnsResponsePolicyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface DocumentAiProcessorDefaultVersionTimeouts {
    create?: string;
    delete?: string;
}

export interface DocumentAiProcessorTimeouts {
    create?: string;
    delete?: string;
}

export interface DocumentAiWarehouseDocumentSchemaPropertyDefinition {
    /**
     * Date time property. Not supported by CMEK compliant deployment.
     */
    dateTimeTypeOptions?: outputs.DocumentAiWarehouseDocumentSchemaPropertyDefinitionDateTimeTypeOptions;
    /**
     * The display-name for the property, used for front-end.
     */
    displayName?: string;
    /**
     * Enum/categorical property.
     */
    enumTypeOptions?: outputs.DocumentAiWarehouseDocumentSchemaPropertyDefinitionEnumTypeOptions;
    /**
     * Float property.
     */
    floatTypeOptions?: outputs.DocumentAiWarehouseDocumentSchemaPropertyDefinitionFloatTypeOptions;
    /**
     * Integer property.
     */
    integerTypeOptions?: outputs.DocumentAiWarehouseDocumentSchemaPropertyDefinitionIntegerTypeOptions;
    /**
     * Whether the property can be filtered. If this is a sub-property, all the parent properties must be marked filterable.
     */
    isFilterable?: boolean;
    /**
     * Whether the property is user supplied metadata.
     */
    isMetadata?: boolean;
    /**
     * Whether the property can have multiple values.
     */
    isRepeatable?: boolean;
    /**
     * Whether the property is mandatory.
     */
    isRequired?: boolean;
    /**
     * Indicates that the property should be included in a global search.
     */
    isSearchable?: boolean;
    /**
     * Map property.
     */
    mapTypeOptions?: outputs.DocumentAiWarehouseDocumentSchemaPropertyDefinitionMapTypeOptions;
    /**
     * The name of the metadata property.
     */
    name: string;
    /**
     * Nested structured data property.
     */
    propertyTypeOptions?: outputs.DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptions;
    /**
     * Stores the retrieval importance. Possible values: ["HIGHEST", "HIGHER", "HIGH", "MEDIUM", "LOW", "LOWEST"]
     */
    retrievalImportance?: string;
    /**
     * The schema source information.
     */
    schemaSources?: outputs.DocumentAiWarehouseDocumentSchemaPropertyDefinitionSchemaSource[];
    /**
     * Text/string property.
     */
    textTypeOptions?: outputs.DocumentAiWarehouseDocumentSchemaPropertyDefinitionTextTypeOptions;
    /**
     * Timestamp property. Not supported by CMEK compliant deployment.
     */
    timestampTypeOptions?: outputs.DocumentAiWarehouseDocumentSchemaPropertyDefinitionTimestampTypeOptions;
}

export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionDateTimeTypeOptions {
}

export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionEnumTypeOptions {
    /**
     * List of possible enum values.
     */
    possibleValues: string[];
    /**
     * Make sure the enum property value provided in the document is in the possile value list during document creation. The validation check runs by default.
     */
    validationCheckDisabled?: boolean;
}

export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionFloatTypeOptions {
}

export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionIntegerTypeOptions {
}

export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionMapTypeOptions {
}

export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptions {
    /**
     * Defines the metadata for a schema property.
     */
    propertyDefinitions: outputs.DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinition[];
}

export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinition {
    /**
     * Date time property. Not supported by CMEK compliant deployment.
     */
    dateTimeTypeOptions?: outputs.DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionDateTimeTypeOptions;
    /**
     * The display-name for the property, used for front-end.
     */
    displayName?: string;
    /**
     * Enum/categorical property.
     */
    enumTypeOptions?: outputs.DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionEnumTypeOptions;
    /**
     * Float property.
     */
    floatTypeOptions?: outputs.DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionFloatTypeOptions;
    /**
     * Integer property.
     */
    integerTypeOptions?: outputs.DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionIntegerTypeOptions;
    /**
     * Whether the property can be filtered. If this is a sub-property, all the parent properties must be marked filterable.
     */
    isFilterable?: boolean;
    /**
     * Whether the property is user supplied metadata.
     */
    isMetadata?: boolean;
    /**
     * Whether the property can have multiple values.
     */
    isRepeatable?: boolean;
    /**
     * Whether the property is mandatory.
     */
    isRequired?: boolean;
    /**
     * Indicates that the property should be included in a global search.
     */
    isSearchable?: boolean;
    /**
     * Map property.
     */
    mapTypeOptions?: outputs.DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionMapTypeOptions;
    /**
     * The name of the metadata property.
     */
    name: string;
    /**
     * Stores the retrieval importance. Possible values: ["HIGHEST", "HIGHER", "HIGH", "MEDIUM", "LOW", "LOWEST"]
     */
    retrievalImportance?: string;
    /**
     * The schema source information.
     */
    schemaSources?: outputs.DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionSchemaSource[];
    /**
     * Text property.
     */
    textTypeOptions?: outputs.DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionTextTypeOptions;
    /**
     * Timestamp property. Not supported by CMEK compliant deployment.
     */
    timestampTypeOptions?: outputs.DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionTimestampTypeOptions;
}

export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionDateTimeTypeOptions {
}

export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionEnumTypeOptions {
    /**
     * List of possible enum values.
     */
    possibleValues: string[];
    /**
     * Make sure the enum property value provided in the document is in the possile value list during document creation. The validation check runs by default.
     */
    validationCheckDisabled?: boolean;
}

export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionFloatTypeOptions {
}

export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionIntegerTypeOptions {
}

export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionMapTypeOptions {
}

export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionSchemaSource {
    /**
     * The schema name in the source.
     */
    name?: string;
    /**
     * The Doc AI processor type name.
     */
    processorType?: string;
}

export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionTextTypeOptions {
}

export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionPropertyTypeOptionsPropertyDefinitionTimestampTypeOptions {
}

export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionSchemaSource {
    /**
     * The schema name in the source.
     */
    name?: string;
    /**
     * The Doc AI processor type name.
     */
    processorType?: string;
}

export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionTextTypeOptions {
}

export interface DocumentAiWarehouseDocumentSchemaPropertyDefinitionTimestampTypeOptions {
}

export interface DocumentAiWarehouseDocumentSchemaTimeouts {
    create?: string;
    delete?: string;
}

export interface DocumentAiWarehouseLocationTimeouts {
    create?: string;
    delete?: string;
}

export interface EdgecontainerClusterAuthorization {
    /**
     * User that will be granted the cluster-admin role on the cluster, providing
     * full access to the cluster. Currently, this is a singular field, but will
     * be expanded to allow multiple admins in the future.
     */
    adminUsers: outputs.EdgecontainerClusterAuthorizationAdminUsers;
}

export interface EdgecontainerClusterAuthorizationAdminUsers {
    /**
     * An active Google username.
     */
    username: string;
}

export interface EdgecontainerClusterControlPlane {
    /**
     * Local control plane configuration.
     */
    local?: outputs.EdgecontainerClusterControlPlaneLocal;
    /**
     * Remote control plane configuration.
     */
    remote?: outputs.EdgecontainerClusterControlPlaneRemote;
}

export interface EdgecontainerClusterControlPlaneEncryption {
    /**
     * The Cloud KMS CryptoKey e.g.
     * projects/{project}/locations/{location}/keyRings/{keyRing}/cryptoKeys/{cryptoKey}
     * to use for protecting control plane disks. If not specified, a
     * Google-managed key will be used instead.
     */
    kmsKey: string;
    /**
     * The Cloud KMS CryptoKeyVersion currently in use for protecting control
     * plane disks. Only applicable if kms_key is set.
     */
    kmsKeyActiveVersion: string;
    /**
     * Availability of the Cloud KMS CryptoKey. If not 'KEY_AVAILABLE', then
     * nodes may go offline as they cannot access their local data. This can be
     * caused by a lack of permissions to use the key, or if the key is disabled
     * or deleted.
     */
    kmsKeyState: string;
    /**
     * Error status returned by Cloud KMS when using this key. This field may be
     * populated only if 'kms_key_state' is not 'KMS_KEY_STATE_KEY_AVAILABLE'.
     * If populated, this field contains the error status reported by Cloud KMS.
     */
    kmsStatuses: outputs.EdgecontainerClusterControlPlaneEncryptionKmsStatus[];
}

export interface EdgecontainerClusterControlPlaneEncryptionKmsStatus {
    code: number;
    message: string;
}

export interface EdgecontainerClusterControlPlaneLocal {
    /**
     * Only machines matching this filter will be allowed to host control
     * plane nodes. The filtering language accepts strings like "name=<name>",
     * and is documented here: [AIP-160](https://google.aip.dev/160).
     */
    machineFilter?: string;
    /**
     * The number of nodes to serve as replicas of the Control Plane.
     * Only 1 and 3 are supported.
     */
    nodeCount: number;
    /**
     * Name of the Google Distributed Cloud Edge zones where this node pool
     * will be created. For example: 'us-central1-edge-customer-a'.
     */
    nodeLocation: string;
    /**
     * Policy configuration about how user applications are deployed. Possible values: ["SHARED_DEPLOYMENT_POLICY_UNSPECIFIED", "ALLOWED", "DISALLOWED"]
     */
    sharedDeploymentPolicy: string;
}

export interface EdgecontainerClusterControlPlaneRemote {
    /**
     * Name of the Google Distributed Cloud Edge zones where this node pool
     * will be created. For example: 'us-central1-edge-customer-a'.
     */
    nodeLocation: string;
}

export interface EdgecontainerClusterFleet {
    /**
     * The name of the managed Hub Membership resource associated to this cluster.
     * Membership names are formatted as
     * 'projects/<project-number>/locations/global/membership/<cluster-id>'.
     */
    membership: string;
    /**
     * The name of the Fleet host project where this cluster will be registered.
     * Project names are formatted as
     * 'projects/<project-number>'.
     */
    project: string;
}

export interface EdgecontainerClusterMaintenanceEvent {
    createTime: string;
    endTime: string;
    operation: string;
    schedule: string;
    startTime: string;
    state: string;
    targetVersion: string;
    type: string;
    updateTime: string;
    uuid: string;
}

export interface EdgecontainerClusterMaintenancePolicy {
    /**
     * Exclusions to automatic maintenance. Non-emergency maintenance should not occur
     * in these windows. Each exclusion has a unique name and may be active or expired.
     * The max number of maintenance exclusions allowed at a given time is 3.
     */
    maintenanceExclusions?: outputs.EdgecontainerClusterMaintenancePolicyMaintenanceExclusion[];
    /**
     * Specifies the maintenance window in which maintenance may be performed.
     */
    window: outputs.EdgecontainerClusterMaintenancePolicyWindow;
}

export interface EdgecontainerClusterMaintenancePolicyMaintenanceExclusion {
    /**
     * A unique (per cluster) id for the window.
     */
    id: string;
    /**
     * Represents an arbitrary window of time.
     */
    window?: outputs.EdgecontainerClusterMaintenancePolicyMaintenanceExclusionWindow;
}

export interface EdgecontainerClusterMaintenancePolicyMaintenanceExclusionWindow {
    /**
     * The time that the window ends. The end time must take place after the
     * start time.
     */
    endTime: string;
    /**
     * The time that the window first starts.
     */
    startTime: string;
}

export interface EdgecontainerClusterMaintenancePolicyWindow {
    /**
     * Represents an arbitrary window of time that recurs.
     */
    recurringWindow: outputs.EdgecontainerClusterMaintenancePolicyWindowRecurringWindow;
}

export interface EdgecontainerClusterMaintenancePolicyWindowRecurringWindow {
    /**
     * An RRULE (https://tools.ietf.org/html/rfc5545#section-3.8.5.3) for how
     * this window recurs. They go on for the span of time between the start and
     * end time.
     */
    recurrence: string;
    /**
     * Represents an arbitrary window of time.
     */
    window?: outputs.EdgecontainerClusterMaintenancePolicyWindowRecurringWindowWindow;
}

export interface EdgecontainerClusterMaintenancePolicyWindowRecurringWindowWindow {
    /**
     * The time that the window ends. The end time must take place after the
     * start time.
     */
    endTime: string;
    /**
     * The time that the window first starts.
     */
    startTime: string;
}

export interface EdgecontainerClusterNetworking {
    /**
     * All pods in the cluster are assigned an RFC1918 IPv4 address from these
     * blocks. Only a single block is supported. This field cannot be changed
     * after creation.
     */
    clusterIpv4CidrBlocks: string[];
    /**
     * If specified, dual stack mode is enabled and all pods in the cluster are
     * assigned an IPv6 address from these blocks alongside from an IPv4
     * address. Only a single block is supported. This field cannot be changed
     * after creation.
     */
    clusterIpv6CidrBlocks?: string[];
    /**
     * IP addressing type of this cluster i.e. SINGLESTACK_V4 vs DUALSTACK_V4_V6.
     */
    networkType: string;
    /**
     * All services in the cluster are assigned an RFC1918 IPv4 address from these
     * blocks. Only a single block is supported. This field cannot be changed
     * after creation.
     */
    servicesIpv4CidrBlocks: string[];
    /**
     * If specified, dual stack mode is enabled and all services in the cluster are
     * assigned an IPv6 address from these blocks alongside from an IPv4
     * address. Only a single block is supported. This field cannot be changed
     * after creation.
     */
    servicesIpv6CidrBlocks?: string[];
}

export interface EdgecontainerClusterSystemAddonsConfig {
    /**
     * Config for the Ingress add-on which allows customers to create an Ingress
     * object to manage external access to the servers in a cluster. The add-on
     * consists of istiod and istio-ingress.
     */
    ingress?: outputs.EdgecontainerClusterSystemAddonsConfigIngress;
}

export interface EdgecontainerClusterSystemAddonsConfigIngress {
    /**
     * Whether Ingress is disabled.
     */
    disabled: boolean;
    /**
     * Ingress VIP.
     */
    ipv4Vip: string;
}

export interface EdgecontainerClusterTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface EdgecontainerNodePoolLocalDiskEncryption {
    /**
     * The Cloud KMS CryptoKey e.g. projects/{project}/locations/{location}/keyRings/{keyRing}/cryptoKeys/{cryptoKey} to use for protecting node local disks.
     * If not specified, a Google-managed key will be used instead.
     */
    kmsKey?: string;
    /**
     * The Cloud KMS CryptoKeyVersion currently in use for protecting node local disks. Only applicable if kmsKey is set.
     */
    kmsKeyActiveVersion: string;
    /**
     * Availability of the Cloud KMS CryptoKey. If not KEY_AVAILABLE, then nodes may go offline as they cannot access their local data.
     * This can be caused by a lack of permissions to use the key, or if the key is disabled or deleted.
     */
    kmsKeyState: string;
}

export interface EdgecontainerNodePoolNodeConfig {
    /**
     * "The Kubernetes node labels"
     */
    labels: {[key: string]: string};
}

export interface EdgecontainerNodePoolTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface EdgecontainerVpnConnectionDetail {
    cloudRouters: outputs.EdgecontainerVpnConnectionDetailCloudRouter[];
    cloudVpns: outputs.EdgecontainerVpnConnectionDetailCloudVpn[];
    error: string;
    state: string;
}

export interface EdgecontainerVpnConnectionDetailCloudRouter {
    name: string;
}

export interface EdgecontainerVpnConnectionDetailCloudVpn {
    gateway: string;
}

export interface EdgecontainerVpnConnectionTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface EdgecontainerVpnConnectionVpcProject {
    /**
     * The project of the VPC to connect to. If not specified, it is the same as the cluster project.
     */
    projectId?: string;
}

export interface EdgenetworkNetworkTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface EdgenetworkSubnetTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface EndpointsServiceApi {
    methods: outputs.EndpointsServiceApiMethod[];
    name: string;
    syntax: string;
    version: string;
}

export interface EndpointsServiceApiMethod {
    name: string;
    requestType: string;
    responseType: string;
    syntax: string;
}

export interface EndpointsServiceConsumersIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface EndpointsServiceConsumersIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface EndpointsServiceEndpoint {
    address: string;
    name: string;
}

export interface EndpointsServiceIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface EndpointsServiceIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface EndpointsServiceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface EssentialContactsContactTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface EventarcChannelTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface EventarcGoogleChannelConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface EventarcTriggerDestination {
    /**
     * The Cloud Function resource name. Only Cloud Functions V2 is supported. Format projects/{project}/locations/{location}/functions/{function} This is a read-only field. [WARNING] Creating Cloud Functions V2 triggers is only supported via the Cloud Functions product. An error will be returned if the user sets this value.
     */
    cloudFunction: string;
    /**
     * Cloud Run fully-managed service that receives the events. The service should be running in the same project of the trigger.
     */
    cloudRunService?: outputs.EventarcTriggerDestinationCloudRunService;
    /**
     * A GKE service capable of receiving events. The service should be running in the same project as the trigger.
     */
    gke?: outputs.EventarcTriggerDestinationGke;
    /**
     * An HTTP endpoint destination described by an URI.
     */
    httpEndpoint?: outputs.EventarcTriggerDestinationHttpEndpoint;
    /**
     * Optional. Network config is used to configure how Eventarc resolves and connect to a destination. This should only be used with HttpEndpoint destination type.
     */
    networkConfig?: outputs.EventarcTriggerDestinationNetworkConfig;
    /**
     * The resource name of the Workflow whose Executions are triggered by the events. The Workflow resource should be deployed in the same project as the trigger. Format: `projects/{project}/locations/{location}/workflows/{workflow}`
     */
    workflow?: string;
}

export interface EventarcTriggerDestinationCloudRunService {
    /**
     * Optional. The relative path on the Cloud Run service the events should be sent to. The value must conform to the definition of URI path segment (section 3.3 of RFC2396). Examples: "/route", "route", "route/subroute".
     */
    path?: string;
    /**
     * Required. The region the Cloud Run service is deployed in.
     */
    region: string;
    /**
     * Required. The name of the Cloud Run service being addressed. See https://cloud.google.com/run/docs/reference/rest/v1/namespaces.services. Only services located in the same project of the trigger object can be addressed.
     */
    service: string;
}

export interface EventarcTriggerDestinationGke {
    /**
     * Required. The name of the cluster the GKE service is running in. The cluster must be running in the same project as the trigger being created.
     */
    cluster: string;
    /**
     * Required. The name of the Google Compute Engine in which the cluster resides, which can either be compute zone (for example, us-central1-a) for the zonal clusters or region (for example, us-central1) for regional clusters.
     */
    location: string;
    /**
     * Required. The namespace the GKE service is running in.
     */
    namespace: string;
    /**
     * Optional. The relative path on the GKE service the events should be sent to. The value must conform to the definition of a URI path segment (section 3.3 of RFC2396). Examples: "/route", "route", "route/subroute".
     */
    path?: string;
    /**
     * Required. Name of the GKE service.
     */
    service: string;
}

export interface EventarcTriggerDestinationHttpEndpoint {
    /**
     * Required. The URI of the HTTP enpdoint. The value must be a RFC2396 URI string. Examples: `http://10.10.10.8:80/route`, `http://svc.us-central1.p.local:8080/`. Only HTTP and HTTPS protocols are supported. The host can be either a static IP addressable from the VPC specified by the network config, or an internal DNS hostname of the service resolvable via Cloud DNS.
     */
    uri: string;
}

export interface EventarcTriggerDestinationNetworkConfig {
    /**
     * Required. Name of the NetworkAttachment that allows access to the destination VPC. Format: `projects/{PROJECT_ID}/regions/{REGION}/networkAttachments/{NETWORK_ATTACHMENT_NAME}`
     */
    networkAttachment: string;
}

export interface EventarcTriggerMatchingCriteria {
    /**
     * Required. The name of a CloudEvents attribute. Currently, only a subset of attributes are supported for filtering. All triggers MUST provide a filter for the 'type' attribute.
     */
    attribute: string;
    /**
     * Optional. The operator used for matching the events with the value of the filter. If not specified, only events that have an exact key-value pair specified in the filter are matched. The only allowed value is `match-path-pattern`.
     */
    operator?: string;
    /**
     * Required. The value for the attribute. See https://cloud.google.com/eventarc/docs/creating-triggers#trigger-gcloud for available values.
     */
    value: string;
}

export interface EventarcTriggerTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface EventarcTriggerTransport {
    /**
     * The Pub/Sub topic and subscription used by Eventarc as delivery intermediary.
     */
    pubsub?: outputs.EventarcTriggerTransportPubsub;
}

export interface EventarcTriggerTransportPubsub {
    /**
     * Output only. The name of the Pub/Sub subscription created and managed by Eventarc system as a transport for the event delivery. Format: `projects/{PROJECT_ID}/subscriptions/{SUBSCRIPTION_NAME}`.
     */
    subscription: string;
    /**
     * Optional. The name of the Pub/Sub topic created and managed by Eventarc system as a transport for the event delivery. Format: `projects/{PROJECT_ID}/topics/{TOPIC_NAME}. You may set an existing topic for triggers of the type google.cloud.pubsub.topic.v1.messagePublished` only. The topic you provide here will not be deleted by Eventarc at trigger deletion.
     */
    topic?: string;
}

export interface FilestoreBackupTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface FilestoreInstanceFileShares {
    /**
     * File share capacity in GiB. This must be at least 1024 GiB
     * for the standard tier, or 2560 GiB for the premium tier.
     */
    capacityGb: number;
    /**
     * The name of the fileshare (16 characters or less)
     */
    name: string;
    /**
     * Nfs Export Options. There is a limit of 10 export options per file share.
     */
    nfsExportOptions?: outputs.FilestoreInstanceFileSharesNfsExportOption[];
    /**
     * The resource name of the backup, in the format
     * projects/{projectId}/locations/{locationId}/backups/{backupId},
     * that this file share has been restored from.
     */
    sourceBackup?: string;
}

export interface FilestoreInstanceFileSharesNfsExportOption {
    /**
     * Either READ_ONLY, for allowing only read requests on the exported directory,
     * or READ_WRITE, for allowing both read and write requests. The default is READ_WRITE. Default value: "READ_WRITE" Possible values: ["READ_ONLY", "READ_WRITE"]
     */
    accessMode?: string;
    /**
     * An integer representing the anonymous group id with a default value of 65534.
     * Anon_gid may only be set with squashMode of ROOT_SQUASH. An error will be returned
     * if this field is specified for other squashMode settings.
     */
    anonGid?: number;
    /**
     * An integer representing the anonymous user id with a default value of 65534.
     * Anon_uid may only be set with squashMode of ROOT_SQUASH. An error will be returned
     * if this field is specified for other squashMode settings.
     */
    anonUid?: number;
    /**
     * List of either IPv4 addresses, or ranges in CIDR notation which may mount the file share.
     * Overlapping IP ranges are not allowed, both within and across NfsExportOptions. An error will be returned.
     * The limit is 64 IP ranges/addresses for each FileShareConfig among all NfsExportOptions.
     */
    ipRanges?: string[];
    /**
     * Either NO_ROOT_SQUASH, for allowing root access on the exported directory, or ROOT_SQUASH,
     * for not allowing root access. The default is NO_ROOT_SQUASH. Default value: "NO_ROOT_SQUASH" Possible values: ["NO_ROOT_SQUASH", "ROOT_SQUASH"]
     */
    squashMode?: string;
}

export interface FilestoreInstanceNetwork {
    /**
     * The network connect mode of the Filestore instance.
     * If not provided, the connect mode defaults to
     * DIRECT_PEERING. Default value: "DIRECT_PEERING" Possible values: ["DIRECT_PEERING", "PRIVATE_SERVICE_ACCESS"]
     */
    connectMode?: string;
    /**
     * A list of IPv4 or IPv6 addresses.
     */
    ipAddresses: string[];
    /**
     * IP versions for which the instance has
     * IP addresses assigned. Possible values: ["ADDRESS_MODE_UNSPECIFIED", "MODE_IPV4", "MODE_IPV6"]
     */
    modes: string[];
    /**
     * The name of the GCE VPC network to which the
     * instance is connected.
     */
    network: string;
    /**
     * A /29 CIDR block that identifies the range of IP
     * addresses reserved for this instance.
     */
    reservedIpRange: string;
}

export interface FilestoreInstanceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface FilestoreSnapshotTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface FirebaseAppCheckAppAttestConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface FirebaseAppCheckDebugTokenTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface FirebaseAppCheckDeviceCheckConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface FirebaseAppCheckPlayIntegrityConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface FirebaseAppCheckRecaptchaEnterpriseConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface FirebaseAppCheckRecaptchaV3ConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface FirebaseAppCheckServiceConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface FirebaserulesReleaseTimeouts {
    create?: string;
    delete?: string;
}

export interface FirebaserulesRulesetMetadata {
    services: string[];
}

export interface FirebaserulesRulesetSource {
    /**
     * `File` set constituting the `Source` bundle.
     */
    files: outputs.FirebaserulesRulesetSourceFile[];
    /**
     * `Language` of the `Source` bundle. If unspecified, the language will default to `FIREBASE_RULES`. Possible values: LANGUAGE_UNSPECIFIED, FIREBASE_RULES, EVENT_FLOW_TRIGGERS
     */
    language?: string;
}

export interface FirebaserulesRulesetSourceFile {
    /**
     * Textual Content.
     */
    content: string;
    /**
     * Fingerprint (e.g. github sha) associated with the `File`.
     */
    fingerprint?: string;
    /**
     * File name.
     */
    name: string;
}

export interface FirebaserulesRulesetTimeouts {
    create?: string;
    delete?: string;
}

export interface FirestoreBackupScheduleDailyRecurrence {
}

export interface FirestoreBackupScheduleTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface FirestoreBackupScheduleWeeklyRecurrence {
    /**
     * The day of week to run. Possible values: ["DAY_OF_WEEK_UNSPECIFIED", "MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    day?: string;
}

export interface FirestoreDatabaseCmekConfig {
    /**
     * Currently in-use KMS key versions (https://cloud.google.com/kms/docs/resource-hierarchy#key_versions).
     * During key rotation (https://cloud.google.com/kms/docs/key-rotation), there can be
     * multiple in-use key versions.
     *
     * The expected format is
     * 'projects/{project_id}/locations/{kms_location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}/cryptoKeyVersions/{key_version}'.
     */
    activeKeyVersions: string[];
    /**
     * The resource ID of a Cloud KMS key. If set, the database created will
     * be a Customer-managed Encryption Key (CMEK) database encrypted with
     * this key. This feature is allowlist only in initial launch.
     *
     * Only keys in the same location as this database are allowed to be used
     * for encryption. For Firestore's nam5 multi-region, this corresponds to Cloud KMS
     * multi-region us. For Firestore's eur3 multi-region, this corresponds to
     * Cloud KMS multi-region europe. See https://cloud.google.com/kms/docs/locations.
     *
     * This value should be the KMS key resource ID in the format of
     * 'projects/{project_id}/locations/{kms_location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}'.
     * How to retrive this resource ID is listed at
     * https://cloud.google.com/kms/docs/getting-resource-ids#getting_the_id_for_a_key_and_version.
     */
    kmsKeyName: string;
}

export interface FirestoreDatabaseTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface FirestoreDocumentTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface FirestoreFieldIndexConfig {
    /**
     * The indexes to configure on the field. Order or array contains must be specified.
     */
    indexes?: outputs.FirestoreFieldIndexConfigIndex[];
}

export interface FirestoreFieldIndexConfigIndex {
    /**
     * Indicates that this field supports operations on arrayValues. Only one of 'order' and 'arrayConfig' can
     * be specified. Possible values: ["CONTAINS"]
     */
    arrayConfig?: string;
    /**
     * Indicates that this field supports ordering by the specified order or comparing using =, <, <=, >, >=, !=.
     * Only one of 'order' and 'arrayConfig' can be specified. Possible values: ["ASCENDING", "DESCENDING"]
     */
    order?: string;
    /**
     * The scope at which a query is run. Collection scoped queries require you specify
     * the collection at query time. Collection group scope allows queries across all
     * collections with the same id. Default value: "COLLECTION" Possible values: ["COLLECTION", "COLLECTION_GROUP"]
     */
    queryScope?: string;
}

export interface FirestoreFieldTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface FirestoreFieldTtlConfig {
    /**
     * The state of TTL (time-to-live) configuration for documents that have this Field set.
     */
    state: string;
}

export interface FirestoreIndexField {
    /**
     * Indicates that this field supports operations on arrayValues. Only one of 'order', 'arrayConfig', and
     * 'vectorConfig' can be specified. Possible values: ["CONTAINS"]
     */
    arrayConfig?: string;
    /**
     * Name of the field.
     */
    fieldPath?: string;
    /**
     * Indicates that this field supports ordering by the specified order or comparing using =, <, <=, >, >=.
     * Only one of 'order', 'arrayConfig', and 'vectorConfig' can be specified. Possible values: ["ASCENDING", "DESCENDING"]
     */
    order?: string;
    /**
     * Indicates that this field supports vector search operations. Only one of 'order', 'arrayConfig', and
     * 'vectorConfig' can be specified. Vector Fields should come after the field path '__name__'.
     */
    vectorConfig?: outputs.FirestoreIndexFieldVectorConfig;
}

export interface FirestoreIndexFieldVectorConfig {
    /**
     * The resulting index will only include vectors of this dimension, and can be used for vector search
     * with the same dimension.
     */
    dimension?: number;
    /**
     * Indicates the vector index is a flat index.
     */
    flat?: outputs.FirestoreIndexFieldVectorConfigFlat;
}

export interface FirestoreIndexFieldVectorConfigFlat {
}

export interface FirestoreIndexTimeouts {
    create?: string;
    delete?: string;
}

export interface FolderAccessApprovalSettingsEnrolledService {
    /**
     * The product for which Access Approval will be enrolled. Allowed values are listed (case-sensitive):
     *   * all
     *   * App Engine
     *   * BigQuery
     *   * Cloud Bigtable
     *   * Cloud Key Management Service
     *   * Compute Engine
     *   * Cloud Dataflow
     *   * Cloud Identity and Access Management
     *   * Cloud Pub/Sub
     *   * Cloud Storage
     *   * Persistent Disk
     *
     * Note: These values are supported as input, but considered a legacy format:
     *   * all
     *   * appengine.googleapis.com
     *   * bigquery.googleapis.com
     *   * bigtable.googleapis.com
     *   * cloudkms.googleapis.com
     *   * compute.googleapis.com
     *   * dataflow.googleapis.com
     *   * iam.googleapis.com
     *   * pubsub.googleapis.com
     *   * storage.googleapis.com
     */
    cloudProduct: string;
    /**
     * The enrollment level of the service. Default value: "BLOCK_ALL" Possible values: ["BLOCK_ALL"]
     */
    enrollmentLevel?: string;
}

export interface FolderAccessApprovalSettingsTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface FolderIamAuditConfigAuditLogConfig {
    /**
     * Identities that do not cause logging for this type of permission. Each entry can have one of the following values:user:{emailid}: An email address that represents a specific Google account. For example, alice@gmail.com or joe@example.com. serviceAccount:{emailid}: An email address that represents a service account. For example, my-other-app@appspot.gserviceaccount.com. group:{emailid}: An email address that represents a Google group. For example, admins@example.com. domain:{domain}: A G Suite domain (primary, instead of alias) name that represents all the users of that domain. For example, google.com or example.com.
     */
    exemptedMembers?: string[];
    /**
     * Permission type for which logging is to be configured. Must be one of DATA_READ, DATA_WRITE, or ADMIN_READ.
     */
    logType: string;
}

export interface FolderIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface FolderIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface FolderOrganizationPolicyBooleanPolicy {
    /**
     * If true, then the Policy is enforced. If false, then any configuration is acceptable.
     */
    enforced: boolean;
}

export interface FolderOrganizationPolicyListPolicy {
    /**
     * One or the other must be set.
     */
    allow?: outputs.FolderOrganizationPolicyListPolicyAllow;
    /**
     * One or the other must be set.
     */
    deny?: outputs.FolderOrganizationPolicyListPolicyDeny;
    /**
     * If set to true, the values from the effective Policy of the parent resource are inherited, meaning the values set in this Policy are added to the values inherited up the hierarchy.
     */
    inheritFromParent?: boolean;
    /**
     * The Google Cloud Console will try to default to a configuration that matches the value specified in this field.
     */
    suggestedValue: string;
}

export interface FolderOrganizationPolicyListPolicyAllow {
    /**
     * The policy allows or denies all values.
     */
    all?: boolean;
    /**
     * The policy can define specific values that are allowed or denied.
     */
    values?: string[];
}

export interface FolderOrganizationPolicyListPolicyDeny {
    /**
     * The policy allows or denies all values.
     */
    all?: boolean;
    /**
     * The policy can define specific values that are allowed or denied.
     */
    values?: string[];
}

export interface FolderOrganizationPolicyRestorePolicy {
    /**
     * May only be set to true. If set, then the default Policy is restored.
     */
    default: boolean;
}

export interface FolderOrganizationPolicyTimeouts {
    create?: string;
    delete?: string;
    read?: string;
    update?: string;
}

export interface FolderTimeouts {
    create?: string;
    delete?: string;
    read?: string;
    update?: string;
}

export interface GetAlloydbLocationsLocation {
    displayName: string;
    labels: {[key: string]: string};
    locationId: string;
    metadata: {[key: string]: string};
    name: string;
}

export interface GetAlloydbSupportedDatabaseFlagsSupportedDatabaseFlag {
    acceptsMultipleValues: boolean;
    flagName: string;
    integerRestrictions: outputs.GetAlloydbSupportedDatabaseFlagsSupportedDatabaseFlagIntegerRestriction[];
    name: string;
    requiresDbRestart: boolean;
    stringRestrictions: outputs.GetAlloydbSupportedDatabaseFlagsSupportedDatabaseFlagStringRestriction[];
    supportedDbVersions: string[];
    valueType: string;
}

export interface GetAlloydbSupportedDatabaseFlagsSupportedDatabaseFlagIntegerRestriction {
    maxValue: string;
    minValue: string;
}

export interface GetAlloydbSupportedDatabaseFlagsSupportedDatabaseFlagStringRestriction {
    allowedValues: string[];
}

export interface GetApphubApplicationAttribute {
    businessOwners: outputs.GetApphubApplicationAttributeBusinessOwner[];
    criticalities: outputs.GetApphubApplicationAttributeCriticality[];
    developerOwners: outputs.GetApphubApplicationAttributeDeveloperOwner[];
    environments: outputs.GetApphubApplicationAttributeEnvironment[];
    operatorOwners: outputs.GetApphubApplicationAttributeOperatorOwner[];
}

export interface GetApphubApplicationAttributeBusinessOwner {
    displayName: string;
    email: string;
}

export interface GetApphubApplicationAttributeCriticality {
    type: string;
}

export interface GetApphubApplicationAttributeDeveloperOwner {
    displayName: string;
    email: string;
}

export interface GetApphubApplicationAttributeEnvironment {
    type: string;
}

export interface GetApphubApplicationAttributeOperatorOwner {
    displayName: string;
    email: string;
}

export interface GetApphubApplicationScope {
    type: string;
}

export interface GetApphubDiscoveredServiceServiceProperty {
    gcpProject: string;
    location: string;
    zone: string;
}

export interface GetApphubDiscoveredServiceServiceReference {
    path: string;
    uri: string;
}

export interface GetApphubDiscoveredWorkloadWorkloadProperty {
    gcpProject: string;
    location: string;
    zone: string;
}

export interface GetApphubDiscoveredWorkloadWorkloadReference {
    uri: string;
}

export interface GetArtifactRegistryRepositoryCleanupPolicy {
    action: string;
    conditions: outputs.GetArtifactRegistryRepositoryCleanupPolicyCondition[];
    id: string;
    mostRecentVersions: outputs.GetArtifactRegistryRepositoryCleanupPolicyMostRecentVersion[];
}

export interface GetArtifactRegistryRepositoryCleanupPolicyCondition {
    newerThan: string;
    olderThan: string;
    packageNamePrefixes: string[];
    tagPrefixes: string[];
    tagState: string;
    versionNamePrefixes: string[];
}

export interface GetArtifactRegistryRepositoryCleanupPolicyMostRecentVersion {
    keepCount: number;
    packageNamePrefixes: string[];
}

export interface GetArtifactRegistryRepositoryDockerConfig {
    immutableTags: boolean;
}

export interface GetArtifactRegistryRepositoryMavenConfig {
    allowSnapshotOverwrites: boolean;
    versionPolicy: string;
}

export interface GetArtifactRegistryRepositoryRemoteRepositoryConfig {
    aptRepositories: outputs.GetArtifactRegistryRepositoryRemoteRepositoryConfigAptRepository[];
    description: string;
    disableUpstreamValidation: boolean;
    dockerRepositories: outputs.GetArtifactRegistryRepositoryRemoteRepositoryConfigDockerRepository[];
    mavenRepositories: outputs.GetArtifactRegistryRepositoryRemoteRepositoryConfigMavenRepository[];
    npmRepositories: outputs.GetArtifactRegistryRepositoryRemoteRepositoryConfigNpmRepository[];
    pythonRepositories: outputs.GetArtifactRegistryRepositoryRemoteRepositoryConfigPythonRepository[];
    upstreamCredentials: outputs.GetArtifactRegistryRepositoryRemoteRepositoryConfigUpstreamCredential[];
    yumRepositories: outputs.GetArtifactRegistryRepositoryRemoteRepositoryConfigYumRepository[];
}

export interface GetArtifactRegistryRepositoryRemoteRepositoryConfigAptRepository {
    publicRepositories: outputs.GetArtifactRegistryRepositoryRemoteRepositoryConfigAptRepositoryPublicRepository[];
}

export interface GetArtifactRegistryRepositoryRemoteRepositoryConfigAptRepositoryPublicRepository {
    repositoryBase: string;
    repositoryPath: string;
}

export interface GetArtifactRegistryRepositoryRemoteRepositoryConfigDockerRepository {
    customRepositories: outputs.GetArtifactRegistryRepositoryRemoteRepositoryConfigDockerRepositoryCustomRepository[];
    publicRepository: string;
}

export interface GetArtifactRegistryRepositoryRemoteRepositoryConfigDockerRepositoryCustomRepository {
    uri: string;
}

export interface GetArtifactRegistryRepositoryRemoteRepositoryConfigMavenRepository {
    customRepositories: outputs.GetArtifactRegistryRepositoryRemoteRepositoryConfigMavenRepositoryCustomRepository[];
    publicRepository: string;
}

export interface GetArtifactRegistryRepositoryRemoteRepositoryConfigMavenRepositoryCustomRepository {
    uri: string;
}

export interface GetArtifactRegistryRepositoryRemoteRepositoryConfigNpmRepository {
    customRepositories: outputs.GetArtifactRegistryRepositoryRemoteRepositoryConfigNpmRepositoryCustomRepository[];
    publicRepository: string;
}

export interface GetArtifactRegistryRepositoryRemoteRepositoryConfigNpmRepositoryCustomRepository {
    uri: string;
}

export interface GetArtifactRegistryRepositoryRemoteRepositoryConfigPythonRepository {
    customRepositories: outputs.GetArtifactRegistryRepositoryRemoteRepositoryConfigPythonRepositoryCustomRepository[];
    publicRepository: string;
}

export interface GetArtifactRegistryRepositoryRemoteRepositoryConfigPythonRepositoryCustomRepository {
    uri: string;
}

export interface GetArtifactRegistryRepositoryRemoteRepositoryConfigUpstreamCredential {
    usernamePasswordCredentials: outputs.GetArtifactRegistryRepositoryRemoteRepositoryConfigUpstreamCredentialUsernamePasswordCredential[];
}

export interface GetArtifactRegistryRepositoryRemoteRepositoryConfigUpstreamCredentialUsernamePasswordCredential {
    passwordSecretVersion: string;
    username: string;
}

export interface GetArtifactRegistryRepositoryRemoteRepositoryConfigYumRepository {
    publicRepositories: outputs.GetArtifactRegistryRepositoryRemoteRepositoryConfigYumRepositoryPublicRepository[];
}

export interface GetArtifactRegistryRepositoryRemoteRepositoryConfigYumRepositoryPublicRepository {
    repositoryBase: string;
    repositoryPath: string;
}

export interface GetArtifactRegistryRepositoryVirtualRepositoryConfig {
    upstreamPolicies: outputs.GetArtifactRegistryRepositoryVirtualRepositoryConfigUpstreamPolicy[];
}

export interface GetArtifactRegistryRepositoryVirtualRepositoryConfigUpstreamPolicy {
    id: string;
    priority: number;
    repository: string;
}

export interface GetBeyondcorpAppConnectionApplicationEndpoint {
    host: string;
    port: number;
}

export interface GetBeyondcorpAppConnectionGateway {
    appGateway: string;
    ingressPort: number;
    type: string;
    uri: string;
}

export interface GetBeyondcorpAppConnectorPrincipalInfo {
    serviceAccounts: outputs.GetBeyondcorpAppConnectorPrincipalInfoServiceAccount[];
}

export interface GetBeyondcorpAppConnectorPrincipalInfoServiceAccount {
    email: string;
}

export interface GetBeyondcorpAppGatewayAllocatedConnection {
    ingressPort: number;
    pscUri: string;
}

export interface GetBigqueryDatasetAccess {
    datasets: outputs.GetBigqueryDatasetAccessDataset[];
    domain: string;
    groupByEmail: string;
    iamMember: string;
    role: string;
    routines: outputs.GetBigqueryDatasetAccessRoutine[];
    specialGroup: string;
    userByEmail: string;
    views: outputs.GetBigqueryDatasetAccessView[];
}

export interface GetBigqueryDatasetAccessDataset {
    datasets: outputs.GetBigqueryDatasetAccessDatasetDataset[];
    targetTypes: string[];
}

export interface GetBigqueryDatasetAccessDatasetDataset {
    datasetId: string;
    projectId: string;
}

export interface GetBigqueryDatasetAccessRoutine {
    datasetId: string;
    projectId: string;
    routineId: string;
}

export interface GetBigqueryDatasetAccessView {
    datasetId: string;
    projectId: string;
    tableId: string;
}

export interface GetBigqueryDatasetDefaultEncryptionConfiguration {
    kmsKeyName: string;
}

export interface GetBigqueryDatasetExternalDatasetReference {
    connection: string;
    externalSource: string;
}

export interface GetCertificateManagerCertificateMapGclbTarget {
    ipConfigs: outputs.GetCertificateManagerCertificateMapGclbTargetIpConfig[];
    targetHttpsProxy: string;
    targetSslProxy: string;
}

export interface GetCertificateManagerCertificateMapGclbTargetIpConfig {
    ipAddress: string;
    ports: number[];
}

export interface GetCloudAssetSearchAllResourcesResult {
    assetType: string;
    createTime: string;
    description: string;
    displayName: string;
    folders: string[];
    kmsKeys: string[];
    labels: {[key: string]: string};
    location: string;
    name: string;
    networkTags: string[];
    organization: string;
    parentAssetType: string;
    parentFullResourceName: string;
    project: string;
    state: string;
    updateTime: string;
}

export interface GetCloudIdentityGroupLookupGroupKey {
    /**
     * The ID of the entity. For Google-managed entities, the id should be the email address of an existing group or user.
     * For external-identity-mapped entities, the id must be a string conforming to the Identity Source's requirements.
     * Must be unique within a namespace.
     */
    id: string;
    /**
     * The namespace in which the entity exists. If not specified, the EntityKey represents a Google-managed entity such as a Google user or a Google Group.
     * If specified, the EntityKey represents an external-identity-mapped group. The namespace must correspond to an identity source created in Admin Console and must be in the form of identitysources/{identity_source}.
     */
    namespace?: string;
}

export interface GetCloudIdentityGroupMembershipsMembership {
    createTime: string;
    group: string;
    name: string;
    preferredMemberKeys: outputs.GetCloudIdentityGroupMembershipsMembershipPreferredMemberKey[];
    roles: outputs.GetCloudIdentityGroupMembershipsMembershipRole[];
    type: string;
    updateTime: string;
}

export interface GetCloudIdentityGroupMembershipsMembershipPreferredMemberKey {
    id: string;
    namespace: string;
}

export interface GetCloudIdentityGroupMembershipsMembershipRole {
    expiryDetails: outputs.GetCloudIdentityGroupMembershipsMembershipRoleExpiryDetail[];
    name: string;
}

export interface GetCloudIdentityGroupMembershipsMembershipRoleExpiryDetail {
    expireTime: string;
}

export interface GetCloudIdentityGroupTransitiveMembershipsMembership {
    member: string;
    preferredMemberKeys: outputs.GetCloudIdentityGroupTransitiveMembershipsMembershipPreferredMemberKey[];
    relationType: string;
    roles: outputs.GetCloudIdentityGroupTransitiveMembershipsMembershipRole[];
}

export interface GetCloudIdentityGroupTransitiveMembershipsMembershipPreferredMemberKey {
    id: string;
    namespace: string;
}

export interface GetCloudIdentityGroupTransitiveMembershipsMembershipRole {
    role: string;
}

export interface GetCloudIdentityGroupsGroup {
    additionalGroupKeys: outputs.GetCloudIdentityGroupsGroupAdditionalGroupKey[];
    createTime: string;
    description: string;
    displayName: string;
    groupKeys: outputs.GetCloudIdentityGroupsGroupGroupKey[];
    initialGroupConfig: string;
    labels: {[key: string]: string};
    name: string;
    parent: string;
    updateTime: string;
}

export interface GetCloudIdentityGroupsGroupAdditionalGroupKey {
    id: string;
    namespace: string;
}

export interface GetCloudIdentityGroupsGroupGroupKey {
    id: string;
    namespace: string;
}

export interface GetCloudQuotasQuotaInfoDimensionsInfo {
    applicableLocations: string[];
    details: outputs.GetCloudQuotasQuotaInfoDimensionsInfoDetail[];
    dimensions: {[key: string]: string};
}

export interface GetCloudQuotasQuotaInfoDimensionsInfoDetail {
    value: string;
}

export interface GetCloudQuotasQuotaInfoQuotaIncreaseEligibility {
    ineligibilityReason: string;
    isEligible: boolean;
}

export interface GetCloudQuotasQuotaInfosQuotaInfo {
    containerType: string;
    dimensions: string[];
    dimensionsInfos: outputs.GetCloudQuotasQuotaInfosQuotaInfoDimensionsInfo[];
    isConcurrent: boolean;
    isFixed: boolean;
    isPrecise: boolean;
    metric: string;
    metricDisplayName: string;
    metricUnit: string;
    name: string;
    quotaDisplayName: string;
    quotaId: string;
    quotaIncreaseEligibilities: outputs.GetCloudQuotasQuotaInfosQuotaInfoQuotaIncreaseEligibility[];
    refreshInterval: string;
    service: string;
    serviceRequestQuotaUri: string;
}

export interface GetCloudQuotasQuotaInfosQuotaInfoDimensionsInfo {
    applicableLocations: string[];
    details: outputs.GetCloudQuotasQuotaInfosQuotaInfoDimensionsInfoDetail[];
    dimensions: {[key: string]: string};
}

export interface GetCloudQuotasQuotaInfosQuotaInfoDimensionsInfoDetail {
    value: string;
}

export interface GetCloudQuotasQuotaInfosQuotaInfoQuotaIncreaseEligibility {
    ineligibilityReason: string;
    isEligible: boolean;
}

export interface GetCloudRunServiceMetadata {
    annotations: {[key: string]: string};
    effectiveAnnotations: {[key: string]: string};
    effectiveLabels: {[key: string]: string};
    generation: number;
    labels: {[key: string]: string};
    namespace: string;
    resourceVersion: string;
    selfLink: string;
    terraformLabels: {[key: string]: string};
    uid: string;
}

export interface GetCloudRunServiceStatus {
    conditions: outputs.GetCloudRunServiceStatusCondition[];
    latestCreatedRevisionName: string;
    latestReadyRevisionName: string;
    observedGeneration: number;
    traffics: outputs.GetCloudRunServiceStatusTraffic[];
    url: string;
}

export interface GetCloudRunServiceStatusCondition {
    message: string;
    reason: string;
    status: string;
    type: string;
}

export interface GetCloudRunServiceStatusTraffic {
    latestRevision: boolean;
    percent: number;
    revisionName: string;
    tag: string;
    url: string;
}

export interface GetCloudRunServiceTemplate {
    metadatas: outputs.GetCloudRunServiceTemplateMetadata[];
    specs: outputs.GetCloudRunServiceTemplateSpec[];
}

export interface GetCloudRunServiceTemplateMetadata {
    annotations: {[key: string]: string};
    generation: number;
    labels: {[key: string]: string};
    name: string;
    namespace: string;
    resourceVersion: string;
    selfLink: string;
    uid: string;
}

export interface GetCloudRunServiceTemplateSpec {
    containerConcurrency: number;
    containers: outputs.GetCloudRunServiceTemplateSpecContainer[];
    serviceAccountName: string;
    servingState: string;
    timeoutSeconds: number;
    volumes: outputs.GetCloudRunServiceTemplateSpecVolume[];
}

export interface GetCloudRunServiceTemplateSpecContainer {
    args: string[];
    commands: string[];
    envFroms: outputs.GetCloudRunServiceTemplateSpecContainerEnvFrom[];
    envs: outputs.GetCloudRunServiceTemplateSpecContainerEnv[];
    image: string;
    livenessProbes: outputs.GetCloudRunServiceTemplateSpecContainerLivenessProbe[];
    name: string;
    ports: outputs.GetCloudRunServiceTemplateSpecContainerPort[];
    resources: outputs.GetCloudRunServiceTemplateSpecContainerResource[];
    startupProbes: outputs.GetCloudRunServiceTemplateSpecContainerStartupProbe[];
    volumeMounts: outputs.GetCloudRunServiceTemplateSpecContainerVolumeMount[];
    workingDir: string;
}

export interface GetCloudRunServiceTemplateSpecContainerEnv {
    name: string;
    value: string;
    valueFroms: outputs.GetCloudRunServiceTemplateSpecContainerEnvValueFrom[];
}

export interface GetCloudRunServiceTemplateSpecContainerEnvFrom {
    configMapReves: outputs.GetCloudRunServiceTemplateSpecContainerEnvFromConfigMapRef[];
    prefix: string;
    secretReves: outputs.GetCloudRunServiceTemplateSpecContainerEnvFromSecretRef[];
}

export interface GetCloudRunServiceTemplateSpecContainerEnvFromConfigMapRef {
    localObjectReferences: outputs.GetCloudRunServiceTemplateSpecContainerEnvFromConfigMapRefLocalObjectReference[];
    optional: boolean;
}

export interface GetCloudRunServiceTemplateSpecContainerEnvFromConfigMapRefLocalObjectReference {
    name: string;
}

export interface GetCloudRunServiceTemplateSpecContainerEnvFromSecretRef {
    localObjectReferences: outputs.GetCloudRunServiceTemplateSpecContainerEnvFromSecretRefLocalObjectReference[];
    optional: boolean;
}

export interface GetCloudRunServiceTemplateSpecContainerEnvFromSecretRefLocalObjectReference {
    name: string;
}

export interface GetCloudRunServiceTemplateSpecContainerEnvValueFrom {
    secretKeyReves: outputs.GetCloudRunServiceTemplateSpecContainerEnvValueFromSecretKeyRef[];
}

export interface GetCloudRunServiceTemplateSpecContainerEnvValueFromSecretKeyRef {
    key: string;
    name: string;
}

export interface GetCloudRunServiceTemplateSpecContainerLivenessProbe {
    failureThreshold: number;
    grpcs: outputs.GetCloudRunServiceTemplateSpecContainerLivenessProbeGrpc[];
    httpGets: outputs.GetCloudRunServiceTemplateSpecContainerLivenessProbeHttpGet[];
    initialDelaySeconds: number;
    periodSeconds: number;
    timeoutSeconds: number;
}

export interface GetCloudRunServiceTemplateSpecContainerLivenessProbeGrpc {
    port: number;
    service: string;
}

export interface GetCloudRunServiceTemplateSpecContainerLivenessProbeHttpGet {
    httpHeaders: outputs.GetCloudRunServiceTemplateSpecContainerLivenessProbeHttpGetHttpHeader[];
    path: string;
    port: number;
}

export interface GetCloudRunServiceTemplateSpecContainerLivenessProbeHttpGetHttpHeader {
    name: string;
    value: string;
}

export interface GetCloudRunServiceTemplateSpecContainerPort {
    containerPort: number;
    name: string;
    protocol: string;
}

export interface GetCloudRunServiceTemplateSpecContainerResource {
    limits: {[key: string]: string};
    requests: {[key: string]: string};
}

export interface GetCloudRunServiceTemplateSpecContainerStartupProbe {
    failureThreshold: number;
    grpcs: outputs.GetCloudRunServiceTemplateSpecContainerStartupProbeGrpc[];
    httpGets: outputs.GetCloudRunServiceTemplateSpecContainerStartupProbeHttpGet[];
    initialDelaySeconds: number;
    periodSeconds: number;
    tcpSockets: outputs.GetCloudRunServiceTemplateSpecContainerStartupProbeTcpSocket[];
    timeoutSeconds: number;
}

export interface GetCloudRunServiceTemplateSpecContainerStartupProbeGrpc {
    port: number;
    service: string;
}

export interface GetCloudRunServiceTemplateSpecContainerStartupProbeHttpGet {
    httpHeaders: outputs.GetCloudRunServiceTemplateSpecContainerStartupProbeHttpGetHttpHeader[];
    path: string;
    port: number;
}

export interface GetCloudRunServiceTemplateSpecContainerStartupProbeHttpGetHttpHeader {
    name: string;
    value: string;
}

export interface GetCloudRunServiceTemplateSpecContainerStartupProbeTcpSocket {
    port: number;
}

export interface GetCloudRunServiceTemplateSpecContainerVolumeMount {
    mountPath: string;
    name: string;
}

export interface GetCloudRunServiceTemplateSpecVolume {
    name: string;
    secrets: outputs.GetCloudRunServiceTemplateSpecVolumeSecret[];
}

export interface GetCloudRunServiceTemplateSpecVolumeSecret {
    defaultMode: number;
    items: outputs.GetCloudRunServiceTemplateSpecVolumeSecretItem[];
    secretName: string;
}

export interface GetCloudRunServiceTemplateSpecVolumeSecretItem {
    key: string;
    mode: number;
    path: string;
}

export interface GetCloudRunServiceTraffic {
    latestRevision: boolean;
    percent: number;
    revisionName: string;
    tag: string;
    url: string;
}

export interface GetCloudRunV2JobBinaryAuthorization {
    breakglassJustification: string;
    policy: string;
    useDefault: boolean;
}

export interface GetCloudRunV2JobCondition {
    executionReason: string;
    lastTransitionTime: string;
    message: string;
    reason: string;
    revisionReason: string;
    severity: string;
    state: string;
    type: string;
}

export interface GetCloudRunV2JobLatestCreatedExecution {
    completionTime: string;
    createTime: string;
    name: string;
}

export interface GetCloudRunV2JobTemplate {
    annotations: {[key: string]: string};
    labels: {[key: string]: string};
    parallelism: number;
    taskCount: number;
    templates: outputs.GetCloudRunV2JobTemplateTemplate[];
}

export interface GetCloudRunV2JobTemplateTemplate {
    containers: outputs.GetCloudRunV2JobTemplateTemplateContainer[];
    encryptionKey: string;
    executionEnvironment: string;
    maxRetries: number;
    serviceAccount: string;
    timeout: string;
    volumes: outputs.GetCloudRunV2JobTemplateTemplateVolume[];
    vpcAccesses: outputs.GetCloudRunV2JobTemplateTemplateVpcAccess[];
}

export interface GetCloudRunV2JobTemplateTemplateContainer {
    args: string[];
    commands: string[];
    envs: outputs.GetCloudRunV2JobTemplateTemplateContainerEnv[];
    image: string;
    name: string;
    ports: outputs.GetCloudRunV2JobTemplateTemplateContainerPort[];
    resources: outputs.GetCloudRunV2JobTemplateTemplateContainerResource[];
    volumeMounts: outputs.GetCloudRunV2JobTemplateTemplateContainerVolumeMount[];
    workingDir: string;
}

export interface GetCloudRunV2JobTemplateTemplateContainerEnv {
    name: string;
    value: string;
    valueSources: outputs.GetCloudRunV2JobTemplateTemplateContainerEnvValueSource[];
}

export interface GetCloudRunV2JobTemplateTemplateContainerEnvValueSource {
    secretKeyReves: outputs.GetCloudRunV2JobTemplateTemplateContainerEnvValueSourceSecretKeyRef[];
}

export interface GetCloudRunV2JobTemplateTemplateContainerEnvValueSourceSecretKeyRef {
    secret: string;
    version: string;
}

export interface GetCloudRunV2JobTemplateTemplateContainerPort {
    containerPort: number;
    name: string;
}

export interface GetCloudRunV2JobTemplateTemplateContainerResource {
    limits: {[key: string]: string};
}

export interface GetCloudRunV2JobTemplateTemplateContainerVolumeMount {
    mountPath: string;
    name: string;
}

export interface GetCloudRunV2JobTemplateTemplateVolume {
    cloudSqlInstances: outputs.GetCloudRunV2JobTemplateTemplateVolumeCloudSqlInstance[];
    name: string;
    secrets: outputs.GetCloudRunV2JobTemplateTemplateVolumeSecret[];
}

export interface GetCloudRunV2JobTemplateTemplateVolumeCloudSqlInstance {
    instances: string[];
}

export interface GetCloudRunV2JobTemplateTemplateVolumeSecret {
    defaultMode: number;
    items: outputs.GetCloudRunV2JobTemplateTemplateVolumeSecretItem[];
    secret: string;
}

export interface GetCloudRunV2JobTemplateTemplateVolumeSecretItem {
    mode: number;
    path: string;
    version: string;
}

export interface GetCloudRunV2JobTemplateTemplateVpcAccess {
    connector: string;
    egress: string;
    networkInterfaces: outputs.GetCloudRunV2JobTemplateTemplateVpcAccessNetworkInterface[];
}

export interface GetCloudRunV2JobTemplateTemplateVpcAccessNetworkInterface {
    network: string;
    subnetwork: string;
    tags: string[];
}

export interface GetCloudRunV2JobTerminalCondition {
    executionReason: string;
    lastTransitionTime: string;
    message: string;
    reason: string;
    revisionReason: string;
    severity: string;
    state: string;
    type: string;
}

export interface GetCloudRunV2ServiceBinaryAuthorization {
    breakglassJustification: string;
    policy: string;
    useDefault: boolean;
}

export interface GetCloudRunV2ServiceCondition {
    executionReason: string;
    lastTransitionTime: string;
    message: string;
    reason: string;
    revisionReason: string;
    severity: string;
    state: string;
    type: string;
}

export interface GetCloudRunV2ServiceTemplate {
    annotations: {[key: string]: string};
    containers: outputs.GetCloudRunV2ServiceTemplateContainer[];
    encryptionKey: string;
    executionEnvironment: string;
    labels: {[key: string]: string};
    maxInstanceRequestConcurrency: number;
    revision: string;
    scalings: outputs.GetCloudRunV2ServiceTemplateScaling[];
    serviceAccount: string;
    sessionAffinity: boolean;
    timeout: string;
    volumes: outputs.GetCloudRunV2ServiceTemplateVolume[];
    vpcAccesses: outputs.GetCloudRunV2ServiceTemplateVpcAccess[];
}

export interface GetCloudRunV2ServiceTemplateContainer {
    args: string[];
    commands: string[];
    dependsOns: string[];
    envs: outputs.GetCloudRunV2ServiceTemplateContainerEnv[];
    image: string;
    livenessProbes: outputs.GetCloudRunV2ServiceTemplateContainerLivenessProbe[];
    name: string;
    ports: outputs.GetCloudRunV2ServiceTemplateContainerPort[];
    resources: outputs.GetCloudRunV2ServiceTemplateContainerResource[];
    startupProbes: outputs.GetCloudRunV2ServiceTemplateContainerStartupProbe[];
    volumeMounts: outputs.GetCloudRunV2ServiceTemplateContainerVolumeMount[];
    workingDir: string;
}

export interface GetCloudRunV2ServiceTemplateContainerEnv {
    name: string;
    value: string;
    valueSources: outputs.GetCloudRunV2ServiceTemplateContainerEnvValueSource[];
}

export interface GetCloudRunV2ServiceTemplateContainerEnvValueSource {
    secretKeyReves: outputs.GetCloudRunV2ServiceTemplateContainerEnvValueSourceSecretKeyRef[];
}

export interface GetCloudRunV2ServiceTemplateContainerEnvValueSourceSecretKeyRef {
    secret: string;
    version: string;
}

export interface GetCloudRunV2ServiceTemplateContainerLivenessProbe {
    failureThreshold: number;
    grpcs: outputs.GetCloudRunV2ServiceTemplateContainerLivenessProbeGrpc[];
    httpGets: outputs.GetCloudRunV2ServiceTemplateContainerLivenessProbeHttpGet[];
    initialDelaySeconds: number;
    periodSeconds: number;
    tcpSockets: outputs.GetCloudRunV2ServiceTemplateContainerLivenessProbeTcpSocket[];
    timeoutSeconds: number;
}

export interface GetCloudRunV2ServiceTemplateContainerLivenessProbeGrpc {
    port: number;
    service: string;
}

export interface GetCloudRunV2ServiceTemplateContainerLivenessProbeHttpGet {
    httpHeaders: outputs.GetCloudRunV2ServiceTemplateContainerLivenessProbeHttpGetHttpHeader[];
    path: string;
    port: number;
}

export interface GetCloudRunV2ServiceTemplateContainerLivenessProbeHttpGetHttpHeader {
    name: string;
    value: string;
}

export interface GetCloudRunV2ServiceTemplateContainerLivenessProbeTcpSocket {
    port: number;
}

export interface GetCloudRunV2ServiceTemplateContainerPort {
    containerPort: number;
    name: string;
}

export interface GetCloudRunV2ServiceTemplateContainerResource {
    cpuIdle: boolean;
    limits: {[key: string]: string};
    startupCpuBoost: boolean;
}

export interface GetCloudRunV2ServiceTemplateContainerStartupProbe {
    failureThreshold: number;
    grpcs: outputs.GetCloudRunV2ServiceTemplateContainerStartupProbeGrpc[];
    httpGets: outputs.GetCloudRunV2ServiceTemplateContainerStartupProbeHttpGet[];
    initialDelaySeconds: number;
    periodSeconds: number;
    tcpSockets: outputs.GetCloudRunV2ServiceTemplateContainerStartupProbeTcpSocket[];
    timeoutSeconds: number;
}

export interface GetCloudRunV2ServiceTemplateContainerStartupProbeGrpc {
    port: number;
    service: string;
}

export interface GetCloudRunV2ServiceTemplateContainerStartupProbeHttpGet {
    httpHeaders: outputs.GetCloudRunV2ServiceTemplateContainerStartupProbeHttpGetHttpHeader[];
    path: string;
    port: number;
}

export interface GetCloudRunV2ServiceTemplateContainerStartupProbeHttpGetHttpHeader {
    name: string;
    value: string;
}

export interface GetCloudRunV2ServiceTemplateContainerStartupProbeTcpSocket {
    port: number;
}

export interface GetCloudRunV2ServiceTemplateContainerVolumeMount {
    mountPath: string;
    name: string;
}

export interface GetCloudRunV2ServiceTemplateScaling {
    maxInstanceCount: number;
    minInstanceCount: number;
}

export interface GetCloudRunV2ServiceTemplateVolume {
    cloudSqlInstances: outputs.GetCloudRunV2ServiceTemplateVolumeCloudSqlInstance[];
    gcs: outputs.GetCloudRunV2ServiceTemplateVolumeGc[];
    name: string;
    nfs: outputs.GetCloudRunV2ServiceTemplateVolumeNf[];
    secrets: outputs.GetCloudRunV2ServiceTemplateVolumeSecret[];
}

export interface GetCloudRunV2ServiceTemplateVolumeCloudSqlInstance {
    instances: string[];
}

export interface GetCloudRunV2ServiceTemplateVolumeGc {
    bucket: string;
    readOnly: boolean;
}

export interface GetCloudRunV2ServiceTemplateVolumeNf {
    path: string;
    readOnly: boolean;
    server: string;
}

export interface GetCloudRunV2ServiceTemplateVolumeSecret {
    defaultMode: number;
    items: outputs.GetCloudRunV2ServiceTemplateVolumeSecretItem[];
    secret: string;
}

export interface GetCloudRunV2ServiceTemplateVolumeSecretItem {
    mode: number;
    path: string;
    version: string;
}

export interface GetCloudRunV2ServiceTemplateVpcAccess {
    connector: string;
    egress: string;
    networkInterfaces: outputs.GetCloudRunV2ServiceTemplateVpcAccessNetworkInterface[];
}

export interface GetCloudRunV2ServiceTemplateVpcAccessNetworkInterface {
    network: string;
    subnetwork: string;
    tags: string[];
}

export interface GetCloudRunV2ServiceTerminalCondition {
    executionReason: string;
    lastTransitionTime: string;
    message: string;
    reason: string;
    revisionReason: string;
    severity: string;
    state: string;
    type: string;
}

export interface GetCloudRunV2ServiceTraffic {
    percent: number;
    revision: string;
    tag: string;
    type: string;
}

export interface GetCloudRunV2ServiceTrafficStatus {
    percent: number;
    revision: string;
    tag: string;
    type: string;
    uri: string;
}

export interface GetCloudbuildTriggerApprovalConfig {
    approvalRequired: boolean;
}

export interface GetCloudbuildTriggerBitbucketServerTriggerConfig {
    bitbucketServerConfigResource: string;
    projectKey: string;
    pullRequests: outputs.GetCloudbuildTriggerBitbucketServerTriggerConfigPullRequest[];
    pushes: outputs.GetCloudbuildTriggerBitbucketServerTriggerConfigPush[];
    repoSlug: string;
}

export interface GetCloudbuildTriggerBitbucketServerTriggerConfigPullRequest {
    branch: string;
    commentControl: string;
    invertRegex: boolean;
}

export interface GetCloudbuildTriggerBitbucketServerTriggerConfigPush {
    branch: string;
    invertRegex: boolean;
    tag: string;
}

export interface GetCloudbuildTriggerBuild {
    artifacts: outputs.GetCloudbuildTriggerBuildArtifact[];
    availableSecrets: outputs.GetCloudbuildTriggerBuildAvailableSecret[];
    images: string[];
    logsBucket: string;
    options: outputs.GetCloudbuildTriggerBuildOption[];
    queueTtl: string;
    secrets: outputs.GetCloudbuildTriggerBuildSecret[];
    sources: outputs.GetCloudbuildTriggerBuildSource[];
    steps: outputs.GetCloudbuildTriggerBuildStep[];
    substitutions: {[key: string]: string};
    tags: string[];
    timeout: string;
}

export interface GetCloudbuildTriggerBuildArtifact {
    images: string[];
    mavenArtifacts: outputs.GetCloudbuildTriggerBuildArtifactMavenArtifact[];
    npmPackages: outputs.GetCloudbuildTriggerBuildArtifactNpmPackage[];
    objects: outputs.GetCloudbuildTriggerBuildArtifactObject[];
    pythonPackages: outputs.GetCloudbuildTriggerBuildArtifactPythonPackage[];
}

export interface GetCloudbuildTriggerBuildArtifactMavenArtifact {
    artifactId: string;
    groupId: string;
    path: string;
    repository: string;
    version: string;
}

export interface GetCloudbuildTriggerBuildArtifactNpmPackage {
    packagePath: string;
    repository: string;
}

export interface GetCloudbuildTriggerBuildArtifactObject {
    location: string;
    paths: string[];
    timings: outputs.GetCloudbuildTriggerBuildArtifactObjectTiming[];
}

export interface GetCloudbuildTriggerBuildArtifactObjectTiming {
    endTime: string;
    startTime: string;
}

export interface GetCloudbuildTriggerBuildArtifactPythonPackage {
    paths: string[];
    repository: string;
}

export interface GetCloudbuildTriggerBuildAvailableSecret {
    secretManagers: outputs.GetCloudbuildTriggerBuildAvailableSecretSecretManager[];
}

export interface GetCloudbuildTriggerBuildAvailableSecretSecretManager {
    env: string;
    versionName: string;
}

export interface GetCloudbuildTriggerBuildOption {
    diskSizeGb: number;
    dynamicSubstitutions: boolean;
    envs: string[];
    logStreamingOption: string;
    logging: string;
    machineType: string;
    requestedVerifyOption: string;
    secretEnvs: string[];
    sourceProvenanceHashes: string[];
    substitutionOption: string;
    volumes: outputs.GetCloudbuildTriggerBuildOptionVolume[];
    workerPool: string;
}

export interface GetCloudbuildTriggerBuildOptionVolume {
    name: string;
    path: string;
}

export interface GetCloudbuildTriggerBuildSecret {
    kmsKeyName: string;
    secretEnv: {[key: string]: string};
}

export interface GetCloudbuildTriggerBuildSource {
    repoSources: outputs.GetCloudbuildTriggerBuildSourceRepoSource[];
    storageSources: outputs.GetCloudbuildTriggerBuildSourceStorageSource[];
}

export interface GetCloudbuildTriggerBuildSourceRepoSource {
    branchName: string;
    commitSha: string;
    dir: string;
    invertRegex: boolean;
    projectId: string;
    repoName: string;
    substitutions: {[key: string]: string};
    tagName: string;
}

export interface GetCloudbuildTriggerBuildSourceStorageSource {
    bucket: string;
    generation: string;
    object: string;
}

export interface GetCloudbuildTriggerBuildStep {
    allowExitCodes: number[];
    allowFailure: boolean;
    args: string[];
    dir: string;
    entrypoint: string;
    envs: string[];
    id: string;
    name: string;
    script: string;
    secretEnvs: string[];
    timeout: string;
    timing: string;
    volumes: outputs.GetCloudbuildTriggerBuildStepVolume[];
    waitFors: string[];
}

export interface GetCloudbuildTriggerBuildStepVolume {
    name: string;
    path: string;
}

export interface GetCloudbuildTriggerGitFileSource {
    bitbucketServerConfig: string;
    githubEnterpriseConfig: string;
    path: string;
    repoType: string;
    repository: string;
    revision: string;
    uri: string;
}

export interface GetCloudbuildTriggerGithub {
    enterpriseConfigResourceName: string;
    name: string;
    owner: string;
    pullRequests: outputs.GetCloudbuildTriggerGithubPullRequest[];
    pushes: outputs.GetCloudbuildTriggerGithubPush[];
}

export interface GetCloudbuildTriggerGithubPullRequest {
    branch: string;
    commentControl: string;
    invertRegex: boolean;
}

export interface GetCloudbuildTriggerGithubPush {
    branch: string;
    invertRegex: boolean;
    tag: string;
}

export interface GetCloudbuildTriggerPubsubConfig {
    serviceAccountEmail: string;
    state: string;
    subscription: string;
    topic: string;
}

export interface GetCloudbuildTriggerRepositoryEventConfig {
    pullRequests: outputs.GetCloudbuildTriggerRepositoryEventConfigPullRequest[];
    pushes: outputs.GetCloudbuildTriggerRepositoryEventConfigPush[];
    repository: string;
}

export interface GetCloudbuildTriggerRepositoryEventConfigPullRequest {
    branch: string;
    commentControl: string;
    invertRegex: boolean;
}

export interface GetCloudbuildTriggerRepositoryEventConfigPush {
    branch: string;
    invertRegex: boolean;
    tag: string;
}

export interface GetCloudbuildTriggerSourceToBuild {
    bitbucketServerConfig: string;
    githubEnterpriseConfig: string;
    ref: string;
    repoType: string;
    repository: string;
    uri: string;
}

export interface GetCloudbuildTriggerTriggerTemplate {
    branchName: string;
    commitSha: string;
    dir: string;
    invertRegex: boolean;
    projectId: string;
    repoName: string;
    tagName: string;
}

export interface GetCloudbuildTriggerWebhookConfig {
    secret: string;
    state: string;
}

export interface GetCloudfunctions2FunctionBuildConfig {
    automaticUpdatePolicies: outputs.GetCloudfunctions2FunctionBuildConfigAutomaticUpdatePolicy[];
    build: string;
    dockerRepository: string;
    entryPoint: string;
    environmentVariables: {[key: string]: string};
    onDeployUpdatePolicies: outputs.GetCloudfunctions2FunctionBuildConfigOnDeployUpdatePolicy[];
    runtime: string;
    serviceAccount: string;
    sources: outputs.GetCloudfunctions2FunctionBuildConfigSource[];
    workerPool: string;
}

export interface GetCloudfunctions2FunctionBuildConfigAutomaticUpdatePolicy {
}

export interface GetCloudfunctions2FunctionBuildConfigOnDeployUpdatePolicy {
    runtimeVersion: string;
}

export interface GetCloudfunctions2FunctionBuildConfigSource {
    repoSources: outputs.GetCloudfunctions2FunctionBuildConfigSourceRepoSource[];
    storageSources: outputs.GetCloudfunctions2FunctionBuildConfigSourceStorageSource[];
}

export interface GetCloudfunctions2FunctionBuildConfigSourceRepoSource {
    branchName: string;
    commitSha: string;
    dir: string;
    invertRegex: boolean;
    projectId: string;
    repoName: string;
    tagName: string;
}

export interface GetCloudfunctions2FunctionBuildConfigSourceStorageSource {
    bucket: string;
    generation: number;
    object: string;
}

export interface GetCloudfunctions2FunctionEventTrigger {
    eventFilters: outputs.GetCloudfunctions2FunctionEventTriggerEventFilter[];
    eventType: string;
    pubsubTopic: string;
    retryPolicy: string;
    serviceAccountEmail: string;
    trigger: string;
    triggerRegion: string;
}

export interface GetCloudfunctions2FunctionEventTriggerEventFilter {
    attribute: string;
    operator: string;
    value: string;
}

export interface GetCloudfunctions2FunctionServiceConfig {
    allTrafficOnLatestRevision: boolean;
    availableCpu: string;
    availableMemory: string;
    environmentVariables: {[key: string]: string};
    gcfUri: string;
    ingressSettings: string;
    maxInstanceCount: number;
    maxInstanceRequestConcurrency: number;
    minInstanceCount: number;
    secretEnvironmentVariables: outputs.GetCloudfunctions2FunctionServiceConfigSecretEnvironmentVariable[];
    secretVolumes: outputs.GetCloudfunctions2FunctionServiceConfigSecretVolume[];
    service: string;
    serviceAccountEmail: string;
    timeoutSeconds: number;
    uri: string;
    vpcConnector: string;
    vpcConnectorEgressSettings: string;
}

export interface GetCloudfunctions2FunctionServiceConfigSecretEnvironmentVariable {
    key: string;
    projectId: string;
    secret: string;
    version: string;
}

export interface GetCloudfunctions2FunctionServiceConfigSecretVolume {
    mountPath: string;
    projectId: string;
    secret: string;
    versions: outputs.GetCloudfunctions2FunctionServiceConfigSecretVolumeVersion[];
}

export interface GetCloudfunctions2FunctionServiceConfigSecretVolumeVersion {
    path: string;
    version: string;
}

export interface GetCloudfunctionsFunctionEventTrigger {
    eventType: string;
    failurePolicies: outputs.GetCloudfunctionsFunctionEventTriggerFailurePolicy[];
    resource: string;
}

export interface GetCloudfunctionsFunctionEventTriggerFailurePolicy {
    retry: boolean;
}

export interface GetCloudfunctionsFunctionSecretEnvironmentVariable {
    key: string;
    projectId: string;
    secret: string;
    version: string;
}

export interface GetCloudfunctionsFunctionSecretVolume {
    mountPath: string;
    projectId: string;
    secret: string;
    versions: outputs.GetCloudfunctionsFunctionSecretVolumeVersion[];
}

export interface GetCloudfunctionsFunctionSecretVolumeVersion {
    path: string;
    version: string;
}

export interface GetCloudfunctionsFunctionSourceRepository {
    deployedUrl: string;
    url: string;
}

export interface GetComposerEnvironmentConfig {
    airflowUri: string;
    dagGcsPrefix: string;
    dataRetentionConfigs: outputs.GetComposerEnvironmentConfigDataRetentionConfig[];
    databaseConfigs: outputs.GetComposerEnvironmentConfigDatabaseConfig[];
    encryptionConfigs: outputs.GetComposerEnvironmentConfigEncryptionConfig[];
    environmentSize: string;
    gkeCluster: string;
    maintenanceWindows: outputs.GetComposerEnvironmentConfigMaintenanceWindow[];
    masterAuthorizedNetworksConfigs: outputs.GetComposerEnvironmentConfigMasterAuthorizedNetworksConfig[];
    nodeConfigs: outputs.GetComposerEnvironmentConfigNodeConfig[];
    nodeCount: number;
    privateEnvironmentConfigs: outputs.GetComposerEnvironmentConfigPrivateEnvironmentConfig[];
    recoveryConfigs: outputs.GetComposerEnvironmentConfigRecoveryConfig[];
    resilienceMode: string;
    softwareConfigs: outputs.GetComposerEnvironmentConfigSoftwareConfig[];
    webServerConfigs: outputs.GetComposerEnvironmentConfigWebServerConfig[];
    webServerNetworkAccessControls: outputs.GetComposerEnvironmentConfigWebServerNetworkAccessControl[];
    workloadsConfigs: outputs.GetComposerEnvironmentConfigWorkloadsConfig[];
}

export interface GetComposerEnvironmentConfigDataRetentionConfig {
    taskLogsRetentionConfigs: outputs.GetComposerEnvironmentConfigDataRetentionConfigTaskLogsRetentionConfig[];
}

export interface GetComposerEnvironmentConfigDataRetentionConfigTaskLogsRetentionConfig {
    storageMode: string;
}

export interface GetComposerEnvironmentConfigDatabaseConfig {
    machineType: string;
    zone: string;
}

export interface GetComposerEnvironmentConfigEncryptionConfig {
    kmsKeyName: string;
}

export interface GetComposerEnvironmentConfigMaintenanceWindow {
    endTime: string;
    recurrence: string;
    startTime: string;
}

export interface GetComposerEnvironmentConfigMasterAuthorizedNetworksConfig {
    cidrBlocks: outputs.GetComposerEnvironmentConfigMasterAuthorizedNetworksConfigCidrBlock[];
    enabled: boolean;
}

export interface GetComposerEnvironmentConfigMasterAuthorizedNetworksConfigCidrBlock {
    cidrBlock: string;
    displayName: string;
}

export interface GetComposerEnvironmentConfigNodeConfig {
    diskSizeGb: number;
    enableIpMasqAgent: boolean;
    ipAllocationPolicies: outputs.GetComposerEnvironmentConfigNodeConfigIpAllocationPolicy[];
    machineType: string;
    network: string;
    oauthScopes: string[];
    serviceAccount: string;
    subnetwork: string;
    tags: string[];
    zone: string;
}

export interface GetComposerEnvironmentConfigNodeConfigIpAllocationPolicy {
    clusterIpv4CidrBlock: string;
    clusterSecondaryRangeName: string;
    servicesIpv4CidrBlock: string;
    servicesSecondaryRangeName: string;
    useIpAliases: boolean;
}

export interface GetComposerEnvironmentConfigPrivateEnvironmentConfig {
    cloudComposerConnectionSubnetwork: string;
    cloudComposerNetworkIpv4CidrBlock: string;
    cloudSqlIpv4CidrBlock: string;
    connectionType: string;
    enablePrivateEndpoint: boolean;
    enablePrivatelyUsedPublicIps: boolean;
    masterIpv4CidrBlock: string;
    webServerIpv4CidrBlock: string;
}

export interface GetComposerEnvironmentConfigRecoveryConfig {
    scheduledSnapshotsConfigs: outputs.GetComposerEnvironmentConfigRecoveryConfigScheduledSnapshotsConfig[];
}

export interface GetComposerEnvironmentConfigRecoveryConfigScheduledSnapshotsConfig {
    enabled: boolean;
    snapshotCreationSchedule: string;
    snapshotLocation: string;
    timeZone: string;
}

export interface GetComposerEnvironmentConfigSoftwareConfig {
    airflowConfigOverrides: {[key: string]: string};
    envVariables: {[key: string]: string};
    imageVersion: string;
    pypiPackages: {[key: string]: string};
    pythonVersion: string;
    schedulerCount: number;
}

export interface GetComposerEnvironmentConfigWebServerConfig {
    machineType: string;
}

export interface GetComposerEnvironmentConfigWebServerNetworkAccessControl {
    allowedIpRanges: outputs.GetComposerEnvironmentConfigWebServerNetworkAccessControlAllowedIpRange[];
}

export interface GetComposerEnvironmentConfigWebServerNetworkAccessControlAllowedIpRange {
    description: string;
    value: string;
}

export interface GetComposerEnvironmentConfigWorkloadsConfig {
    schedulers: outputs.GetComposerEnvironmentConfigWorkloadsConfigScheduler[];
    triggerers: outputs.GetComposerEnvironmentConfigWorkloadsConfigTriggerer[];
    webServers: outputs.GetComposerEnvironmentConfigWorkloadsConfigWebServer[];
    workers: outputs.GetComposerEnvironmentConfigWorkloadsConfigWorker[];
}

export interface GetComposerEnvironmentConfigWorkloadsConfigScheduler {
    count: number;
    cpu: number;
    memoryGb: number;
    storageGb: number;
}

export interface GetComposerEnvironmentConfigWorkloadsConfigTriggerer {
    count: number;
    cpu: number;
    memoryGb: number;
}

export interface GetComposerEnvironmentConfigWorkloadsConfigWebServer {
    cpu: number;
    memoryGb: number;
    storageGb: number;
}

export interface GetComposerEnvironmentConfigWorkloadsConfigWorker {
    cpu: number;
    maxCount: number;
    memoryGb: number;
    minCount: number;
    storageGb: number;
}

export interface GetComposerEnvironmentStorageConfig {
    bucket: string;
}

export interface GetComposerImageVersionsImageVersion {
    imageVersionId: string;
    supportedPythonVersions: string[];
}

export interface GetComputeAddressesAddress {
    address: string;
    addressType: string;
    description: string;
    name: string;
    region: string;
    selfLink: string;
    status: string;
}

export interface GetComputeBackendBucketCdnPolicy {
    bypassCacheOnRequestHeaders: outputs.GetComputeBackendBucketCdnPolicyBypassCacheOnRequestHeader[];
    cacheKeyPolicies: outputs.GetComputeBackendBucketCdnPolicyCacheKeyPolicy[];
    cacheMode: string;
    clientTtl: number;
    defaultTtl: number;
    maxTtl: number;
    negativeCaching: boolean;
    negativeCachingPolicies: outputs.GetComputeBackendBucketCdnPolicyNegativeCachingPolicy[];
    requestCoalescing: boolean;
    serveWhileStale: number;
    signedUrlCacheMaxAgeSec: number;
}

export interface GetComputeBackendBucketCdnPolicyBypassCacheOnRequestHeader {
    headerName: string;
}

export interface GetComputeBackendBucketCdnPolicyCacheKeyPolicy {
    includeHttpHeaders: string[];
    queryStringWhitelists: string[];
}

export interface GetComputeBackendBucketCdnPolicyNegativeCachingPolicy {
    code: number;
    ttl: number;
}

export interface GetComputeBackendServiceBackend {
    balancingMode: string;
    capacityScaler: number;
    description: string;
    group: string;
    maxConnections: number;
    maxConnectionsPerEndpoint: number;
    maxConnectionsPerInstance: number;
    maxRate: number;
    maxRatePerEndpoint: number;
    maxRatePerInstance: number;
    maxUtilization: number;
}

export interface GetComputeBackendServiceCdnPolicy {
    bypassCacheOnRequestHeaders: outputs.GetComputeBackendServiceCdnPolicyBypassCacheOnRequestHeader[];
    cacheKeyPolicies: outputs.GetComputeBackendServiceCdnPolicyCacheKeyPolicy[];
    cacheMode: string;
    clientTtl: number;
    defaultTtl: number;
    maxTtl: number;
    negativeCaching: boolean;
    negativeCachingPolicies: outputs.GetComputeBackendServiceCdnPolicyNegativeCachingPolicy[];
    serveWhileStale: number;
    signedUrlCacheMaxAgeSec: number;
}

export interface GetComputeBackendServiceCdnPolicyBypassCacheOnRequestHeader {
    headerName: string;
}

export interface GetComputeBackendServiceCdnPolicyCacheKeyPolicy {
    includeHost: boolean;
    includeHttpHeaders: string[];
    includeNamedCookies: string[];
    includeProtocol: boolean;
    includeQueryString: boolean;
    queryStringBlacklists: string[];
    queryStringWhitelists: string[];
}

export interface GetComputeBackendServiceCdnPolicyNegativeCachingPolicy {
    code: number;
    ttl: number;
}

export interface GetComputeBackendServiceCircuitBreaker {
    maxConnections: number;
    maxPendingRequests: number;
    maxRequests: number;
    maxRequestsPerConnection: number;
    maxRetries: number;
}

export interface GetComputeBackendServiceConsistentHash {
    httpCookies: outputs.GetComputeBackendServiceConsistentHashHttpCooky[];
    httpHeaderName: string;
    minimumRingSize: number;
}

export interface GetComputeBackendServiceConsistentHashHttpCooky {
    name: string;
    path: string;
    ttls: outputs.GetComputeBackendServiceConsistentHashHttpCookyTtl[];
}

export interface GetComputeBackendServiceConsistentHashHttpCookyTtl {
    nanos: number;
    seconds: number;
}

export interface GetComputeBackendServiceIap {
    enabled: boolean;
    oauth2ClientId: string;
    oauth2ClientSecret: string;
    oauth2ClientSecretSha256: string;
}

export interface GetComputeBackendServiceLocalityLbPolicy {
    customPolicies: outputs.GetComputeBackendServiceLocalityLbPolicyCustomPolicy[];
    policies: outputs.GetComputeBackendServiceLocalityLbPolicyPolicy[];
}

export interface GetComputeBackendServiceLocalityLbPolicyCustomPolicy {
    data: string;
    name: string;
}

export interface GetComputeBackendServiceLocalityLbPolicyPolicy {
    name: string;
}

export interface GetComputeBackendServiceLogConfig {
    enable: boolean;
    sampleRate: number;
}

export interface GetComputeBackendServiceOutlierDetection {
    baseEjectionTimes: outputs.GetComputeBackendServiceOutlierDetectionBaseEjectionTime[];
    consecutiveErrors: number;
    consecutiveGatewayFailure: number;
    enforcingConsecutiveErrors: number;
    enforcingConsecutiveGatewayFailure: number;
    enforcingSuccessRate: number;
    intervals: outputs.GetComputeBackendServiceOutlierDetectionInterval[];
    maxEjectionPercent: number;
    successRateMinimumHosts: number;
    successRateRequestVolume: number;
    successRateStdevFactor: number;
}

export interface GetComputeBackendServiceOutlierDetectionBaseEjectionTime {
    nanos: number;
    seconds: number;
}

export interface GetComputeBackendServiceOutlierDetectionInterval {
    nanos: number;
    seconds: number;
}

export interface GetComputeBackendServiceSecuritySetting {
    awsV4Authentications: outputs.GetComputeBackendServiceSecuritySettingAwsV4Authentication[];
    clientTlsPolicy: string;
    subjectAltNames: string[];
}

export interface GetComputeBackendServiceSecuritySettingAwsV4Authentication {
    accessKey: string;
    accessKeyId: string;
    accessKeyVersion: string;
    originRegion: string;
}

export interface GetComputeDiskAsyncPrimaryDisk {
    disk: string;
}

export interface GetComputeDiskDiskEncryptionKey {
    kmsKeySelfLink: string;
    kmsKeyServiceAccount: string;
    rawKey: string;
    rsaEncryptedKey: string;
    sha256: string;
}

export interface GetComputeDiskGuestOsFeature {
    type: string;
}

export interface GetComputeDiskSourceImageEncryptionKey {
    kmsKeySelfLink: string;
    kmsKeyServiceAccount: string;
    rawKey: string;
    sha256: string;
}

export interface GetComputeDiskSourceSnapshotEncryptionKey {
    kmsKeySelfLink: string;
    kmsKeyServiceAccount: string;
    rawKey: string;
    sha256: string;
}

export interface GetComputeForwardingRuleServiceDirectoryRegistration {
    namespace: string;
    service: string;
}

export interface GetComputeForwardingRulesRule {
    allPorts: boolean;
    allowGlobalAccess: boolean;
    allowPscGlobalAccess: boolean;
    backendService: string;
    baseForwardingRule: string;
    creationTimestamp: string;
    description: string;
    effectiveLabels: {[key: string]: string};
    forwardingRuleId: number;
    ipAddress: string;
    ipProtocol: string;
    ipVersion: string;
    isMirroringCollector: boolean;
    labelFingerprint: string;
    labels: {[key: string]: string};
    loadBalancingScheme: string;
    name: string;
    network: string;
    networkTier: string;
    noAutomateDnsZone: boolean;
    portRange: string;
    ports: string[];
    project: string;
    pscConnectionId: string;
    pscConnectionStatus: string;
    recreateClosedPsc: boolean;
    region: string;
    selfLink: string;
    serviceDirectoryRegistrations: outputs.GetComputeForwardingRulesRuleServiceDirectoryRegistration[];
    serviceLabel: string;
    serviceName: string;
    sourceIpRanges: string[];
    subnetwork: string;
    target: string;
    terraformLabels: {[key: string]: string};
}

export interface GetComputeForwardingRulesRuleServiceDirectoryRegistration {
    namespace: string;
    service: string;
}

export interface GetComputeGlobalForwardingRuleMetadataFilter {
    filterLabels: outputs.GetComputeGlobalForwardingRuleMetadataFilterFilterLabel[];
    filterMatchCriteria: string;
}

export interface GetComputeGlobalForwardingRuleMetadataFilterFilterLabel {
    name: string;
    value: string;
}

export interface GetComputeGlobalForwardingRuleServiceDirectoryRegistration {
    namespace: string;
    serviceDirectoryRegion: string;
}

export interface GetComputeHaVpnGatewayVpnInterface {
    id: number;
    interconnectAttachment: string;
    ipAddress: string;
}

export interface GetComputeHealthCheckGrpcHealthCheck {
    grpcServiceName: string;
    port: number;
    portName: string;
    portSpecification: string;
}

export interface GetComputeHealthCheckHttp2HealthCheck {
    host: string;
    port: number;
    portName: string;
    portSpecification: string;
    proxyHeader: string;
    requestPath: string;
    response: string;
}

export interface GetComputeHealthCheckHttpHealthCheck {
    host: string;
    port: number;
    portName: string;
    portSpecification: string;
    proxyHeader: string;
    requestPath: string;
    response: string;
}

export interface GetComputeHealthCheckHttpsHealthCheck {
    host: string;
    port: number;
    portName: string;
    portSpecification: string;
    proxyHeader: string;
    requestPath: string;
    response: string;
}

export interface GetComputeHealthCheckLogConfig {
    enable: boolean;
}

export interface GetComputeHealthCheckSslHealthCheck {
    port: number;
    portName: string;
    portSpecification: string;
    proxyHeader: string;
    request: string;
    response: string;
}

export interface GetComputeHealthCheckTcpHealthCheck {
    port: number;
    portName: string;
    portSpecification: string;
    proxyHeader: string;
    request: string;
    response: string;
}

export interface GetComputeInstanceAdvancedMachineFeature {
    enableNestedVirtualization: boolean;
    threadsPerCore: number;
    visibleCoreCount: number;
}

export interface GetComputeInstanceAttachedDisk {
    deviceName: string;
    diskEncryptionKeyRaw: string;
    diskEncryptionKeySha256: string;
    kmsKeySelfLink: string;
    mode: string;
    source: string;
}

export interface GetComputeInstanceBootDisk {
    autoDelete: boolean;
    deviceName: string;
    diskEncryptionKeyRaw: string;
    diskEncryptionKeySha256: string;
    initializeParams: outputs.GetComputeInstanceBootDiskInitializeParam[];
    kmsKeySelfLink: string;
    mode: string;
    source: string;
}

export interface GetComputeInstanceBootDiskInitializeParam {
    enableConfidentialCompute: boolean;
    image: string;
    labels: {[key: string]: string};
    provisionedIops: number;
    provisionedThroughput: number;
    resourceManagerTags: {[key: string]: string};
    size: number;
    storagePool: string;
    type: string;
}

export interface GetComputeInstanceConfidentialInstanceConfig {
    confidentialInstanceType: string;
    enableConfidentialCompute: boolean;
}

export interface GetComputeInstanceGroupManagerAllInstancesConfig {
    labels: {[key: string]: string};
    metadata: {[key: string]: string};
}

export interface GetComputeInstanceGroupManagerAutoHealingPolicy {
    healthCheck: string;
    initialDelaySec: number;
}

export interface GetComputeInstanceGroupManagerInstanceLifecyclePolicy {
    defaultActionOnFailure: string;
    forceUpdateOnRepair: string;
}

export interface GetComputeInstanceGroupManagerNamedPort {
    name: string;
    port: number;
}

export interface GetComputeInstanceGroupManagerStatefulDisk {
    deleteRule: string;
    deviceName: string;
}

export interface GetComputeInstanceGroupManagerStatefulExternalIp {
    deleteRule: string;
    interfaceName: string;
}

export interface GetComputeInstanceGroupManagerStatefulInternalIp {
    deleteRule: string;
    interfaceName: string;
}

export interface GetComputeInstanceGroupManagerStatus {
    allInstancesConfigs: outputs.GetComputeInstanceGroupManagerStatusAllInstancesConfig[];
    isStable: boolean;
    statefuls: outputs.GetComputeInstanceGroupManagerStatusStateful[];
    versionTargets: outputs.GetComputeInstanceGroupManagerStatusVersionTarget[];
}

export interface GetComputeInstanceGroupManagerStatusAllInstancesConfig {
    currentRevision: string;
    effective: boolean;
}

export interface GetComputeInstanceGroupManagerStatusStateful {
    hasStatefulConfig: boolean;
    perInstanceConfigs: outputs.GetComputeInstanceGroupManagerStatusStatefulPerInstanceConfig[];
}

export interface GetComputeInstanceGroupManagerStatusStatefulPerInstanceConfig {
    allEffective: boolean;
}

export interface GetComputeInstanceGroupManagerStatusVersionTarget {
    isReached: boolean;
}

export interface GetComputeInstanceGroupManagerUpdatePolicy {
    maxSurgeFixed: number;
    maxSurgePercent: number;
    maxUnavailableFixed: number;
    maxUnavailablePercent: number;
    minimalAction: string;
    mostDisruptiveAllowedAction: string;
    replacementMethod: string;
    type: string;
}

export interface GetComputeInstanceGroupManagerVersion {
    instanceTemplate: string;
    name: string;
    targetSizes: outputs.GetComputeInstanceGroupManagerVersionTargetSize[];
}

export interface GetComputeInstanceGroupManagerVersionTargetSize {
    fixed: number;
    percent: number;
}

export interface GetComputeInstanceGroupNamedPort {
    name: string;
    port: number;
}

export interface GetComputeInstanceGuestAccelerator {
    count: number;
    type: string;
}

export interface GetComputeInstanceNetworkInterface {
    accessConfigs: outputs.GetComputeInstanceNetworkInterfaceAccessConfig[];
    aliasIpRanges: outputs.GetComputeInstanceNetworkInterfaceAliasIpRange[];
    internalIpv6PrefixLength: number;
    ipv6AccessConfigs: outputs.GetComputeInstanceNetworkInterfaceIpv6AccessConfig[];
    ipv6AccessType: string;
    ipv6Address: string;
    name: string;
    network: string;
    networkIp: string;
    nicType: string;
    queueCount: number;
    stackType: string;
    subnetwork: string;
    subnetworkProject: string;
}

export interface GetComputeInstanceNetworkInterfaceAccessConfig {
    natIp: string;
    networkTier: string;
    publicPtrDomainName: string;
}

export interface GetComputeInstanceNetworkInterfaceAliasIpRange {
    ipCidrRange: string;
    subnetworkRangeName: string;
}

export interface GetComputeInstanceNetworkInterfaceIpv6AccessConfig {
    externalIpv6: string;
    externalIpv6PrefixLength: string;
    name: string;
    networkTier: string;
    publicPtrDomainName: string;
}

export interface GetComputeInstanceNetworkPerformanceConfig {
    totalEgressBandwidthTier: string;
}

export interface GetComputeInstanceParam {
    resourceManagerTags: {[key: string]: string};
}

export interface GetComputeInstanceReservationAffinity {
    specificReservations: outputs.GetComputeInstanceReservationAffinitySpecificReservation[];
    type: string;
}

export interface GetComputeInstanceReservationAffinitySpecificReservation {
    key: string;
    values: string[];
}

export interface GetComputeInstanceScheduling {
    automaticRestart: boolean;
    instanceTerminationAction: string;
    localSsdRecoveryTimeouts: outputs.GetComputeInstanceSchedulingLocalSsdRecoveryTimeout[];
    maxRunDurations: outputs.GetComputeInstanceSchedulingMaxRunDuration[];
    minNodeCpus: number;
    nodeAffinities: outputs.GetComputeInstanceSchedulingNodeAffinity[];
    onHostMaintenance: string;
    onInstanceStopActions: outputs.GetComputeInstanceSchedulingOnInstanceStopAction[];
    preemptible: boolean;
    provisioningModel: string;
}

export interface GetComputeInstanceSchedulingLocalSsdRecoveryTimeout {
    nanos: number;
    seconds: number;
}

export interface GetComputeInstanceSchedulingMaxRunDuration {
    nanos: number;
    seconds: number;
}

export interface GetComputeInstanceSchedulingNodeAffinity {
    key: string;
    operator: string;
    values: string[];
}

export interface GetComputeInstanceSchedulingOnInstanceStopAction {
    discardLocalSsd: boolean;
}

export interface GetComputeInstanceScratchDisk {
    deviceName: string;
    interface: string;
    size: number;
}

export interface GetComputeInstanceServiceAccount {
    email: string;
    scopes: string[];
}

export interface GetComputeInstanceShieldedInstanceConfig {
    enableIntegrityMonitoring: boolean;
    enableSecureBoot: boolean;
    enableVtpm: boolean;
}

export interface GetComputeInstanceTemplateAdvancedMachineFeature {
    enableNestedVirtualization: boolean;
    threadsPerCore: number;
    visibleCoreCount: number;
}

export interface GetComputeInstanceTemplateConfidentialInstanceConfig {
    confidentialInstanceType: string;
    enableConfidentialCompute: boolean;
}

export interface GetComputeInstanceTemplateDisk {
    autoDelete: boolean;
    boot: boolean;
    deviceName: string;
    diskEncryptionKeys: outputs.GetComputeInstanceTemplateDiskDiskEncryptionKey[];
    diskName: string;
    diskSizeGb: number;
    diskType: string;
    interface: string;
    labels: {[key: string]: string};
    mode: string;
    provisionedIops: number;
    resourceManagerTags: {[key: string]: string};
    resourcePolicies: string[];
    source: string;
    sourceImage: string;
    sourceImageEncryptionKeys: outputs.GetComputeInstanceTemplateDiskSourceImageEncryptionKey[];
    sourceSnapshot: string;
    sourceSnapshotEncryptionKeys: outputs.GetComputeInstanceTemplateDiskSourceSnapshotEncryptionKey[];
    type: string;
}

export interface GetComputeInstanceTemplateDiskDiskEncryptionKey {
    kmsKeySelfLink: string;
}

export interface GetComputeInstanceTemplateDiskSourceImageEncryptionKey {
    kmsKeySelfLink: string;
    kmsKeyServiceAccount: string;
}

export interface GetComputeInstanceTemplateDiskSourceSnapshotEncryptionKey {
    kmsKeySelfLink: string;
    kmsKeyServiceAccount: string;
}

export interface GetComputeInstanceTemplateGuestAccelerator {
    count: number;
    type: string;
}

export interface GetComputeInstanceTemplateNetworkInterface {
    accessConfigs: outputs.GetComputeInstanceTemplateNetworkInterfaceAccessConfig[];
    aliasIpRanges: outputs.GetComputeInstanceTemplateNetworkInterfaceAliasIpRange[];
    internalIpv6PrefixLength: number;
    ipv6AccessConfigs: outputs.GetComputeInstanceTemplateNetworkInterfaceIpv6AccessConfig[];
    ipv6AccessType: string;
    ipv6Address: string;
    name: string;
    network: string;
    networkIp: string;
    nicType: string;
    queueCount: number;
    stackType: string;
    subnetwork: string;
    subnetworkProject: string;
}

export interface GetComputeInstanceTemplateNetworkInterfaceAccessConfig {
    natIp: string;
    networkTier: string;
    publicPtrDomainName: string;
}

export interface GetComputeInstanceTemplateNetworkInterfaceAliasIpRange {
    ipCidrRange: string;
    subnetworkRangeName: string;
}

export interface GetComputeInstanceTemplateNetworkInterfaceIpv6AccessConfig {
    externalIpv6: string;
    externalIpv6PrefixLength: string;
    name: string;
    networkTier: string;
    publicPtrDomainName: string;
}

export interface GetComputeInstanceTemplateNetworkPerformanceConfig {
    totalEgressBandwidthTier: string;
}

export interface GetComputeInstanceTemplateReservationAffinity {
    specificReservations: outputs.GetComputeInstanceTemplateReservationAffinitySpecificReservation[];
    type: string;
}

export interface GetComputeInstanceTemplateReservationAffinitySpecificReservation {
    key: string;
    values: string[];
}

export interface GetComputeInstanceTemplateScheduling {
    automaticRestart: boolean;
    instanceTerminationAction: string;
    localSsdRecoveryTimeouts: outputs.GetComputeInstanceTemplateSchedulingLocalSsdRecoveryTimeout[];
    maxRunDurations: outputs.GetComputeInstanceTemplateSchedulingMaxRunDuration[];
    minNodeCpus: number;
    nodeAffinities: outputs.GetComputeInstanceTemplateSchedulingNodeAffinity[];
    onHostMaintenance: string;
    onInstanceStopActions: outputs.GetComputeInstanceTemplateSchedulingOnInstanceStopAction[];
    preemptible: boolean;
    provisioningModel: string;
}

export interface GetComputeInstanceTemplateSchedulingLocalSsdRecoveryTimeout {
    nanos: number;
    seconds: number;
}

export interface GetComputeInstanceTemplateSchedulingMaxRunDuration {
    nanos: number;
    seconds: number;
}

export interface GetComputeInstanceTemplateSchedulingNodeAffinity {
    key: string;
    operator: string;
    values: string[];
}

export interface GetComputeInstanceTemplateSchedulingOnInstanceStopAction {
    discardLocalSsd: boolean;
}

export interface GetComputeInstanceTemplateServiceAccount {
    email: string;
    scopes: string[];
}

export interface GetComputeInstanceTemplateShieldedInstanceConfig {
    enableIntegrityMonitoring: boolean;
    enableSecureBoot: boolean;
    enableVtpm: boolean;
}

export interface GetComputeMachineTypesMachineType {
    accelerators: outputs.GetComputeMachineTypesMachineTypeAccelerator[];
    deprecateds: outputs.GetComputeMachineTypesMachineTypeDeprecated[];
    description: string;
    guestCpus: number;
    isSharedCpus: boolean;
    maximumPersistentDisks: number;
    maximumPersistentDisksSizeGb: number;
    memoryMb: number;
    name: string;
    selfLink: string;
}

export interface GetComputeMachineTypesMachineTypeAccelerator {
    guestAcceleratorCount: number;
    guestAcceleratorType: string;
}

export interface GetComputeMachineTypesMachineTypeDeprecated {
    replacement: string;
    state: string;
}

export interface GetComputeNetworkPeeringTimeouts {
    read?: string;
}

export interface GetComputeRegionDiskAsyncPrimaryDisk {
    disk: string;
}

export interface GetComputeRegionDiskDiskEncryptionKey {
    kmsKeyName: string;
    rawKey: string;
    sha256: string;
}

export interface GetComputeRegionDiskGuestOsFeature {
    type: string;
}

export interface GetComputeRegionDiskSourceSnapshotEncryptionKey {
    rawKey: string;
    sha256: string;
}

export interface GetComputeRegionInstanceGroupInstance {
    instance: string;
    namedPorts: outputs.GetComputeRegionInstanceGroupInstanceNamedPort[];
    status: string;
}

export interface GetComputeRegionInstanceGroupInstanceNamedPort {
    name: string;
    port: number;
}

export interface GetComputeRegionInstanceTemplateAdvancedMachineFeature {
    enableNestedVirtualization: boolean;
    threadsPerCore: number;
    visibleCoreCount: number;
}

export interface GetComputeRegionInstanceTemplateConfidentialInstanceConfig {
    confidentialInstanceType: string;
    enableConfidentialCompute: boolean;
}

export interface GetComputeRegionInstanceTemplateDisk {
    autoDelete: boolean;
    boot: boolean;
    deviceName: string;
    diskEncryptionKeys: outputs.GetComputeRegionInstanceTemplateDiskDiskEncryptionKey[];
    diskName: string;
    diskSizeGb: number;
    diskType: string;
    interface: string;
    labels: {[key: string]: string};
    mode: string;
    provisionedIops: number;
    resourceManagerTags: {[key: string]: string};
    resourcePolicies: string[];
    source: string;
    sourceImage: string;
    sourceImageEncryptionKeys: outputs.GetComputeRegionInstanceTemplateDiskSourceImageEncryptionKey[];
    sourceSnapshot: string;
    sourceSnapshotEncryptionKeys: outputs.GetComputeRegionInstanceTemplateDiskSourceSnapshotEncryptionKey[];
    type: string;
}

export interface GetComputeRegionInstanceTemplateDiskDiskEncryptionKey {
    kmsKeySelfLink: string;
}

export interface GetComputeRegionInstanceTemplateDiskSourceImageEncryptionKey {
    kmsKeySelfLink: string;
    kmsKeyServiceAccount: string;
}

export interface GetComputeRegionInstanceTemplateDiskSourceSnapshotEncryptionKey {
    kmsKeySelfLink: string;
    kmsKeyServiceAccount: string;
}

export interface GetComputeRegionInstanceTemplateGuestAccelerator {
    count: number;
    type: string;
}

export interface GetComputeRegionInstanceTemplateNetworkInterface {
    accessConfigs: outputs.GetComputeRegionInstanceTemplateNetworkInterfaceAccessConfig[];
    aliasIpRanges: outputs.GetComputeRegionInstanceTemplateNetworkInterfaceAliasIpRange[];
    internalIpv6PrefixLength: number;
    ipv6AccessConfigs: outputs.GetComputeRegionInstanceTemplateNetworkInterfaceIpv6AccessConfig[];
    ipv6AccessType: string;
    ipv6Address: string;
    name: string;
    network: string;
    networkIp: string;
    nicType: string;
    queueCount: number;
    stackType: string;
    subnetwork: string;
    subnetworkProject: string;
}

export interface GetComputeRegionInstanceTemplateNetworkInterfaceAccessConfig {
    natIp: string;
    networkTier: string;
    publicPtrDomainName: string;
}

export interface GetComputeRegionInstanceTemplateNetworkInterfaceAliasIpRange {
    ipCidrRange: string;
    subnetworkRangeName: string;
}

export interface GetComputeRegionInstanceTemplateNetworkInterfaceIpv6AccessConfig {
    externalIpv6: string;
    externalIpv6PrefixLength: string;
    name: string;
    networkTier: string;
    publicPtrDomainName: string;
}

export interface GetComputeRegionInstanceTemplateNetworkPerformanceConfig {
    totalEgressBandwidthTier: string;
}

export interface GetComputeRegionInstanceTemplateReservationAffinity {
    specificReservations: outputs.GetComputeRegionInstanceTemplateReservationAffinitySpecificReservation[];
    type: string;
}

export interface GetComputeRegionInstanceTemplateReservationAffinitySpecificReservation {
    key: string;
    values: string[];
}

export interface GetComputeRegionInstanceTemplateScheduling {
    automaticRestart: boolean;
    instanceTerminationAction: string;
    localSsdRecoveryTimeouts: outputs.GetComputeRegionInstanceTemplateSchedulingLocalSsdRecoveryTimeout[];
    maxRunDurations: outputs.GetComputeRegionInstanceTemplateSchedulingMaxRunDuration[];
    minNodeCpus: number;
    nodeAffinities: outputs.GetComputeRegionInstanceTemplateSchedulingNodeAffinity[];
    onHostMaintenance: string;
    onInstanceStopActions: outputs.GetComputeRegionInstanceTemplateSchedulingOnInstanceStopAction[];
    preemptible: boolean;
    provisioningModel: string;
}

export interface GetComputeRegionInstanceTemplateSchedulingLocalSsdRecoveryTimeout {
    nanos: number;
    seconds: number;
}

export interface GetComputeRegionInstanceTemplateSchedulingMaxRunDuration {
    nanos: number;
    seconds: number;
}

export interface GetComputeRegionInstanceTemplateSchedulingNodeAffinity {
    key: string;
    operator: string;
    values: string[];
}

export interface GetComputeRegionInstanceTemplateSchedulingOnInstanceStopAction {
    discardLocalSsd: boolean;
}

export interface GetComputeRegionInstanceTemplateServiceAccount {
    email: string;
    scopes: string[];
}

export interface GetComputeRegionInstanceTemplateShieldedInstanceConfig {
    enableIntegrityMonitoring: boolean;
    enableSecureBoot: boolean;
    enableVtpm: boolean;
}

export interface GetComputeRegionNetworkEndpointGroupAppEngine {
    service: string;
    urlMask: string;
    version: string;
}

export interface GetComputeRegionNetworkEndpointGroupCloudFunction {
    function: string;
    urlMask: string;
}

export interface GetComputeRegionNetworkEndpointGroupCloudRun {
    service: string;
    tag: string;
    urlMask: string;
}

export interface GetComputeReservationShareSetting {
    projectMaps: outputs.GetComputeReservationShareSettingProjectMap[];
    shareType: string;
}

export interface GetComputeReservationShareSettingProjectMap {
    id: string;
    projectId: string;
}

export interface GetComputeReservationSpecificReservation {
    count: number;
    inUseCount: number;
    instanceProperties: outputs.GetComputeReservationSpecificReservationInstanceProperty[];
}

export interface GetComputeReservationSpecificReservationInstanceProperty {
    guestAccelerators: outputs.GetComputeReservationSpecificReservationInstancePropertyGuestAccelerator[];
    localSsds: outputs.GetComputeReservationSpecificReservationInstancePropertyLocalSsd[];
    machineType: string;
    minCpuPlatform: string;
}

export interface GetComputeReservationSpecificReservationInstancePropertyGuestAccelerator {
    acceleratorCount: number;
    acceleratorType: string;
}

export interface GetComputeReservationSpecificReservationInstancePropertyLocalSsd {
    diskSizeGb: number;
    interface: string;
}

export interface GetComputeResourcePolicyDiskConsistencyGroupPolicy {
    enabled: boolean;
}

export interface GetComputeResourcePolicyGroupPlacementPolicy {
    availabilityDomainCount: number;
    collocation: string;
    vmCount: number;
}

export interface GetComputeResourcePolicyInstanceSchedulePolicy {
    expirationTime: string;
    startTime: string;
    timeZone: string;
    vmStartSchedules: outputs.GetComputeResourcePolicyInstanceSchedulePolicyVmStartSchedule[];
    vmStopSchedules: outputs.GetComputeResourcePolicyInstanceSchedulePolicyVmStopSchedule[];
}

export interface GetComputeResourcePolicyInstanceSchedulePolicyVmStartSchedule {
    schedule: string;
}

export interface GetComputeResourcePolicyInstanceSchedulePolicyVmStopSchedule {
    schedule: string;
}

export interface GetComputeResourcePolicySnapshotSchedulePolicy {
    retentionPolicies: outputs.GetComputeResourcePolicySnapshotSchedulePolicyRetentionPolicy[];
    schedules: outputs.GetComputeResourcePolicySnapshotSchedulePolicySchedule[];
    snapshotProperties: outputs.GetComputeResourcePolicySnapshotSchedulePolicySnapshotProperty[];
}

export interface GetComputeResourcePolicySnapshotSchedulePolicyRetentionPolicy {
    maxRetentionDays: number;
    onSourceDiskDelete: string;
}

export interface GetComputeResourcePolicySnapshotSchedulePolicySchedule {
    dailySchedules: outputs.GetComputeResourcePolicySnapshotSchedulePolicyScheduleDailySchedule[];
    hourlySchedules: outputs.GetComputeResourcePolicySnapshotSchedulePolicyScheduleHourlySchedule[];
    weeklySchedules: outputs.GetComputeResourcePolicySnapshotSchedulePolicyScheduleWeeklySchedule[];
}

export interface GetComputeResourcePolicySnapshotSchedulePolicyScheduleDailySchedule {
    daysInCycle: number;
    startTime: string;
}

export interface GetComputeResourcePolicySnapshotSchedulePolicyScheduleHourlySchedule {
    hoursInCycle: number;
    startTime: string;
}

export interface GetComputeResourcePolicySnapshotSchedulePolicyScheduleWeeklySchedule {
    dayOfWeeks: outputs.GetComputeResourcePolicySnapshotSchedulePolicyScheduleWeeklyScheduleDayOfWeek[];
}

export interface GetComputeResourcePolicySnapshotSchedulePolicyScheduleWeeklyScheduleDayOfWeek {
    day: string;
    startTime: string;
}

export interface GetComputeResourcePolicySnapshotSchedulePolicySnapshotProperty {
    chainName: string;
    guestFlush: boolean;
    labels: {[key: string]: string};
    storageLocations: string[];
}

export interface GetComputeRouterBgp {
    advertiseMode: string;
    advertisedGroups: string[];
    advertisedIpRanges: outputs.GetComputeRouterBgpAdvertisedIpRange[];
    asn: number;
    identifierRange: string;
    keepaliveInterval: number;
}

export interface GetComputeRouterBgpAdvertisedIpRange {
    description: string;
    range: string;
}

export interface GetComputeRouterNatLogConfig {
    enable: boolean;
    filter: string;
}

export interface GetComputeRouterNatRule {
    actions: outputs.GetComputeRouterNatRuleAction[];
    description: string;
    match: string;
    ruleNumber: number;
}

export interface GetComputeRouterNatRuleAction {
    sourceNatActiveIps: string[];
    sourceNatDrainIps: string[];
}

export interface GetComputeRouterNatSubnetwork {
    name: string;
    secondaryIpRangeNames: string[];
    sourceIpRangesToNats: string[];
}

export interface GetComputeRouterStatusBestRoute {
    description: string;
    destRange: string;
    name: string;
    network: string;
    nextHopGateway: string;
    nextHopIlb: string;
    nextHopInstance: string;
    nextHopInstanceZone: string;
    nextHopIp: string;
    nextHopNetwork: string;
    nextHopVpnTunnel: string;
    priority: number;
    project: string;
    selfLink: string;
    tags: string[];
}

export interface GetComputeRouterStatusBestRoutesForRouter {
    description: string;
    destRange: string;
    name: string;
    network: string;
    nextHopGateway: string;
    nextHopIlb: string;
    nextHopInstance: string;
    nextHopInstanceZone: string;
    nextHopIp: string;
    nextHopNetwork: string;
    nextHopVpnTunnel: string;
    priority: number;
    project: string;
    selfLink: string;
    tags: string[];
}

export interface GetComputeSecurityPolicyAdaptiveProtectionConfig {
    layer7DdosDefenseConfigs: outputs.GetComputeSecurityPolicyAdaptiveProtectionConfigLayer7DdosDefenseConfig[];
}

export interface GetComputeSecurityPolicyAdaptiveProtectionConfigLayer7DdosDefenseConfig {
    enable: boolean;
    ruleVisibility: string;
}

export interface GetComputeSecurityPolicyAdvancedOptionsConfig {
    jsonCustomConfigs: outputs.GetComputeSecurityPolicyAdvancedOptionsConfigJsonCustomConfig[];
    jsonParsing: string;
    logLevel: string;
    userIpRequestHeaders: string[];
}

export interface GetComputeSecurityPolicyAdvancedOptionsConfigJsonCustomConfig {
    contentTypes: string[];
}

export interface GetComputeSecurityPolicyRecaptchaOptionsConfig {
    redirectSiteKey: string;
}

export interface GetComputeSecurityPolicyRule {
    action: string;
    description: string;
    headerActions: outputs.GetComputeSecurityPolicyRuleHeaderAction[];
    matches: outputs.GetComputeSecurityPolicyRuleMatch[];
    preview: boolean;
    priority: number;
    rateLimitOptions: outputs.GetComputeSecurityPolicyRuleRateLimitOption[];
    redirectOptions: outputs.GetComputeSecurityPolicyRuleRedirectOption[];
}

export interface GetComputeSecurityPolicyRuleHeaderAction {
    requestHeadersToAdds: outputs.GetComputeSecurityPolicyRuleHeaderActionRequestHeadersToAdd[];
}

export interface GetComputeSecurityPolicyRuleHeaderActionRequestHeadersToAdd {
    headerName: string;
    headerValue: string;
}

export interface GetComputeSecurityPolicyRuleMatch {
    configs: outputs.GetComputeSecurityPolicyRuleMatchConfig[];
    exprOptions: outputs.GetComputeSecurityPolicyRuleMatchExprOption[];
    exprs: outputs.GetComputeSecurityPolicyRuleMatchExpr[];
    versionedExpr: string;
}

export interface GetComputeSecurityPolicyRuleMatchConfig {
    srcIpRanges: string[];
}

export interface GetComputeSecurityPolicyRuleMatchExpr {
    expression: string;
}

export interface GetComputeSecurityPolicyRuleMatchExprOption {
    recaptchaOptions: outputs.GetComputeSecurityPolicyRuleMatchExprOptionRecaptchaOption[];
}

export interface GetComputeSecurityPolicyRuleMatchExprOptionRecaptchaOption {
    actionTokenSiteKeys: string[];
    sessionTokenSiteKeys: string[];
}

export interface GetComputeSecurityPolicyRuleRateLimitOption {
    banDurationSec: number;
    banThresholds: outputs.GetComputeSecurityPolicyRuleRateLimitOptionBanThreshold[];
    conformAction: string;
    enforceOnKey: string;
    enforceOnKeyName: string;
    exceedAction: string;
    exceedRedirectOptions: outputs.GetComputeSecurityPolicyRuleRateLimitOptionExceedRedirectOption[];
    rateLimitThresholds: outputs.GetComputeSecurityPolicyRuleRateLimitOptionRateLimitThreshold[];
}

export interface GetComputeSecurityPolicyRuleRateLimitOptionBanThreshold {
    count: number;
    intervalSec: number;
}

export interface GetComputeSecurityPolicyRuleRateLimitOptionExceedRedirectOption {
    target: string;
    type: string;
}

export interface GetComputeSecurityPolicyRuleRateLimitOptionRateLimitThreshold {
    count: number;
    intervalSec: number;
}

export interface GetComputeSecurityPolicyRuleRedirectOption {
    target: string;
    type: string;
}

export interface GetComputeSnapshotSnapshotEncryptionKey {
    kmsKeySelfLink: string;
    kmsKeyServiceAccount: string;
    rawKey: string;
    sha256: string;
}

export interface GetComputeSnapshotSourceDiskEncryptionKey {
    kmsKeyServiceAccount: string;
    rawKey: string;
}

export interface GetComputeSubnetworkSecondaryIpRange {
    ipCidrRange: string;
    rangeName: string;
}

export interface GetComputeSubnetworksSubnetwork {
    description: string;
    ipCidrRange: string;
    name: string;
    network: string;
    networkSelfLink: string;
    privateIpGoogleAccess: boolean;
    selfLink: string;
}

export interface GetContainerClusterAddonsConfig {
    cloudrunConfigs: outputs.GetContainerClusterAddonsConfigCloudrunConfig[];
    configConnectorConfigs: outputs.GetContainerClusterAddonsConfigConfigConnectorConfig[];
    dnsCacheConfigs: outputs.GetContainerClusterAddonsConfigDnsCacheConfig[];
    gcePersistentDiskCsiDriverConfigs: outputs.GetContainerClusterAddonsConfigGcePersistentDiskCsiDriverConfig[];
    gcpFilestoreCsiDriverConfigs: outputs.GetContainerClusterAddonsConfigGcpFilestoreCsiDriverConfig[];
    gcsFuseCsiDriverConfigs: outputs.GetContainerClusterAddonsConfigGcsFuseCsiDriverConfig[];
    gkeBackupAgentConfigs: outputs.GetContainerClusterAddonsConfigGkeBackupAgentConfig[];
    horizontalPodAutoscalings: outputs.GetContainerClusterAddonsConfigHorizontalPodAutoscaling[];
    httpLoadBalancings: outputs.GetContainerClusterAddonsConfigHttpLoadBalancing[];
    networkPolicyConfigs: outputs.GetContainerClusterAddonsConfigNetworkPolicyConfig[];
    rayOperatorConfigs: outputs.GetContainerClusterAddonsConfigRayOperatorConfig[];
    statefulHaConfigs: outputs.GetContainerClusterAddonsConfigStatefulHaConfig[];
}

export interface GetContainerClusterAddonsConfigCloudrunConfig {
    disabled: boolean;
    loadBalancerType: string;
}

export interface GetContainerClusterAddonsConfigConfigConnectorConfig {
    enabled: boolean;
}

export interface GetContainerClusterAddonsConfigDnsCacheConfig {
    enabled: boolean;
}

export interface GetContainerClusterAddonsConfigGcePersistentDiskCsiDriverConfig {
    enabled: boolean;
}

export interface GetContainerClusterAddonsConfigGcpFilestoreCsiDriverConfig {
    enabled: boolean;
}

export interface GetContainerClusterAddonsConfigGcsFuseCsiDriverConfig {
    enabled: boolean;
}

export interface GetContainerClusterAddonsConfigGkeBackupAgentConfig {
    enabled: boolean;
}

export interface GetContainerClusterAddonsConfigHorizontalPodAutoscaling {
    disabled: boolean;
}

export interface GetContainerClusterAddonsConfigHttpLoadBalancing {
    disabled: boolean;
}

export interface GetContainerClusterAddonsConfigNetworkPolicyConfig {
    disabled: boolean;
}

export interface GetContainerClusterAddonsConfigRayOperatorConfig {
    enabled: boolean;
    rayClusterLoggingConfigs: outputs.GetContainerClusterAddonsConfigRayOperatorConfigRayClusterLoggingConfig[];
    rayClusterMonitoringConfigs: outputs.GetContainerClusterAddonsConfigRayOperatorConfigRayClusterMonitoringConfig[];
}

export interface GetContainerClusterAddonsConfigRayOperatorConfigRayClusterLoggingConfig {
    enabled: boolean;
}

export interface GetContainerClusterAddonsConfigRayOperatorConfigRayClusterMonitoringConfig {
    enabled: boolean;
}

export interface GetContainerClusterAddonsConfigStatefulHaConfig {
    enabled: boolean;
}

export interface GetContainerClusterAuthenticatorGroupsConfig {
    securityGroup: string;
}

export interface GetContainerClusterBinaryAuthorization {
    enabled: boolean;
    evaluationMode: string;
}

export interface GetContainerClusterClusterAutoscaling {
    autoProvisioningDefaults: outputs.GetContainerClusterClusterAutoscalingAutoProvisioningDefault[];
    autoProvisioningLocations: string[];
    autoscalingProfile: string;
    enabled: boolean;
    resourceLimits: outputs.GetContainerClusterClusterAutoscalingResourceLimit[];
}

export interface GetContainerClusterClusterAutoscalingAutoProvisioningDefault {
    bootDiskKmsKey: string;
    diskSize: number;
    diskType: string;
    imageType: string;
    managements: outputs.GetContainerClusterClusterAutoscalingAutoProvisioningDefaultManagement[];
    minCpuPlatform: string;
    oauthScopes: string[];
    serviceAccount: string;
    shieldedInstanceConfigs: outputs.GetContainerClusterClusterAutoscalingAutoProvisioningDefaultShieldedInstanceConfig[];
    upgradeSettings: outputs.GetContainerClusterClusterAutoscalingAutoProvisioningDefaultUpgradeSetting[];
}

export interface GetContainerClusterClusterAutoscalingAutoProvisioningDefaultManagement {
    autoRepair: boolean;
    autoUpgrade: boolean;
    upgradeOptions: outputs.GetContainerClusterClusterAutoscalingAutoProvisioningDefaultManagementUpgradeOption[];
}

export interface GetContainerClusterClusterAutoscalingAutoProvisioningDefaultManagementUpgradeOption {
    autoUpgradeStartTime: string;
    description: string;
}

export interface GetContainerClusterClusterAutoscalingAutoProvisioningDefaultShieldedInstanceConfig {
    enableIntegrityMonitoring: boolean;
    enableSecureBoot: boolean;
}

export interface GetContainerClusterClusterAutoscalingAutoProvisioningDefaultUpgradeSetting {
    blueGreenSettings: outputs.GetContainerClusterClusterAutoscalingAutoProvisioningDefaultUpgradeSettingBlueGreenSetting[];
    maxSurge: number;
    maxUnavailable: number;
    strategy: string;
}

export interface GetContainerClusterClusterAutoscalingAutoProvisioningDefaultUpgradeSettingBlueGreenSetting {
    nodePoolSoakDuration: string;
    standardRolloutPolicies: outputs.GetContainerClusterClusterAutoscalingAutoProvisioningDefaultUpgradeSettingBlueGreenSettingStandardRolloutPolicy[];
}

export interface GetContainerClusterClusterAutoscalingAutoProvisioningDefaultUpgradeSettingBlueGreenSettingStandardRolloutPolicy {
    batchNodeCount: number;
    batchPercentage: number;
    batchSoakDuration: string;
}

export interface GetContainerClusterClusterAutoscalingResourceLimit {
    maximum: number;
    minimum: number;
    resourceType: string;
}

export interface GetContainerClusterConfidentialNode {
    enabled: boolean;
}

export interface GetContainerClusterCostManagementConfig {
    enabled: boolean;
}

export interface GetContainerClusterDatabaseEncryption {
    keyName: string;
    state: string;
}

export interface GetContainerClusterDefaultSnatStatus {
    disabled: boolean;
}

export interface GetContainerClusterDnsConfig {
    clusterDns: string;
    clusterDnsDomain: string;
    clusterDnsScope: string;
}

export interface GetContainerClusterEnableK8sBetaApi {
    enabledApis: string[];
}

export interface GetContainerClusterFleet {
    membership: string;
    membershipId: string;
    membershipLocation: string;
    preRegistered: boolean;
    project: string;
}

export interface GetContainerClusterGatewayApiConfig {
    channel: string;
}

export interface GetContainerClusterIdentityServiceConfig {
    enabled: boolean;
}

export interface GetContainerClusterIpAllocationPolicy {
    additionalPodRangesConfigs: outputs.GetContainerClusterIpAllocationPolicyAdditionalPodRangesConfig[];
    clusterIpv4CidrBlock: string;
    clusterSecondaryRangeName: string;
    podCidrOverprovisionConfigs: outputs.GetContainerClusterIpAllocationPolicyPodCidrOverprovisionConfig[];
    servicesIpv4CidrBlock: string;
    servicesSecondaryRangeName: string;
    stackType: string;
}

export interface GetContainerClusterIpAllocationPolicyAdditionalPodRangesConfig {
    podRangeNames: string[];
}

export interface GetContainerClusterIpAllocationPolicyPodCidrOverprovisionConfig {
    disabled: boolean;
}

export interface GetContainerClusterLoggingConfig {
    enableComponents: string[];
}

export interface GetContainerClusterMaintenancePolicy {
    dailyMaintenanceWindows: outputs.GetContainerClusterMaintenancePolicyDailyMaintenanceWindow[];
    maintenanceExclusions: outputs.GetContainerClusterMaintenancePolicyMaintenanceExclusion[];
    recurringWindows: outputs.GetContainerClusterMaintenancePolicyRecurringWindow[];
}

export interface GetContainerClusterMaintenancePolicyDailyMaintenanceWindow {
    duration: string;
    startTime: string;
}

export interface GetContainerClusterMaintenancePolicyMaintenanceExclusion {
    endTime: string;
    exclusionName: string;
    exclusionOptions: outputs.GetContainerClusterMaintenancePolicyMaintenanceExclusionExclusionOption[];
    startTime: string;
}

export interface GetContainerClusterMaintenancePolicyMaintenanceExclusionExclusionOption {
    scope: string;
}

export interface GetContainerClusterMaintenancePolicyRecurringWindow {
    endTime: string;
    recurrence: string;
    startTime: string;
}

export interface GetContainerClusterMasterAuth {
    clientCertificate: string;
    clientCertificateConfigs: outputs.GetContainerClusterMasterAuthClientCertificateConfig[];
    clientKey: string;
    clusterCaCertificate: string;
}

export interface GetContainerClusterMasterAuthClientCertificateConfig {
    issueClientCertificate: boolean;
}

export interface GetContainerClusterMasterAuthorizedNetworksConfig {
    cidrBlocks: outputs.GetContainerClusterMasterAuthorizedNetworksConfigCidrBlock[];
    gcpPublicCidrsAccessEnabled: boolean;
}

export interface GetContainerClusterMasterAuthorizedNetworksConfigCidrBlock {
    cidrBlock: string;
    displayName: string;
}

export interface GetContainerClusterMeshCertificate {
    enableCertificates: boolean;
}

export interface GetContainerClusterMonitoringConfig {
    advancedDatapathObservabilityConfigs: outputs.GetContainerClusterMonitoringConfigAdvancedDatapathObservabilityConfig[];
    enableComponents: string[];
    managedPrometheuses: outputs.GetContainerClusterMonitoringConfigManagedPrometheus[];
}

export interface GetContainerClusterMonitoringConfigAdvancedDatapathObservabilityConfig {
    enableMetrics: boolean;
    enableRelay: boolean;
}

export interface GetContainerClusterMonitoringConfigManagedPrometheus {
    enabled: boolean;
}

export interface GetContainerClusterNetworkPolicy {
    enabled: boolean;
    provider: string;
}

export interface GetContainerClusterNodeConfig {
    advancedMachineFeatures: outputs.GetContainerClusterNodeConfigAdvancedMachineFeature[];
    bootDiskKmsKey: string;
    confidentialNodes: outputs.GetContainerClusterNodeConfigConfidentialNode[];
    containerdConfigs: outputs.GetContainerClusterNodeConfigContainerdConfig[];
    diskSizeGb: number;
    diskType: string;
    effectiveTaints: outputs.GetContainerClusterNodeConfigEffectiveTaint[];
    enableConfidentialStorage: boolean;
    ephemeralStorageLocalSsdConfigs: outputs.GetContainerClusterNodeConfigEphemeralStorageLocalSsdConfig[];
    fastSockets: outputs.GetContainerClusterNodeConfigFastSocket[];
    gcfsConfigs: outputs.GetContainerClusterNodeConfigGcfsConfig[];
    guestAccelerators: outputs.GetContainerClusterNodeConfigGuestAccelerator[];
    gvnics: outputs.GetContainerClusterNodeConfigGvnic[];
    hostMaintenancePolicies: outputs.GetContainerClusterNodeConfigHostMaintenancePolicy[];
    imageType: string;
    kubeletConfigs: outputs.GetContainerClusterNodeConfigKubeletConfig[];
    labels: {[key: string]: string};
    linuxNodeConfigs: outputs.GetContainerClusterNodeConfigLinuxNodeConfig[];
    localNvmeSsdBlockConfigs: outputs.GetContainerClusterNodeConfigLocalNvmeSsdBlockConfig[];
    localSsdCount: number;
    loggingVariant: string;
    machineType: string;
    metadata: {[key: string]: string};
    minCpuPlatform: string;
    nodeGroup: string;
    oauthScopes: string[];
    preemptible: boolean;
    reservationAffinities: outputs.GetContainerClusterNodeConfigReservationAffinity[];
    resourceLabels: {[key: string]: string};
    resourceManagerTags: {[key: string]: string};
    secondaryBootDisks: outputs.GetContainerClusterNodeConfigSecondaryBootDisk[];
    serviceAccount: string;
    shieldedInstanceConfigs: outputs.GetContainerClusterNodeConfigShieldedInstanceConfig[];
    soleTenantConfigs: outputs.GetContainerClusterNodeConfigSoleTenantConfig[];
    spot: boolean;
    tags: string[];
    taints: outputs.GetContainerClusterNodeConfigTaint[];
    workloadMetadataConfigs: outputs.GetContainerClusterNodeConfigWorkloadMetadataConfig[];
}

export interface GetContainerClusterNodeConfigAdvancedMachineFeature {
    enableNestedVirtualization: boolean;
    threadsPerCore: number;
}

export interface GetContainerClusterNodeConfigConfidentialNode {
    enabled: boolean;
}

export interface GetContainerClusterNodeConfigContainerdConfig {
    privateRegistryAccessConfigs: outputs.GetContainerClusterNodeConfigContainerdConfigPrivateRegistryAccessConfig[];
}

export interface GetContainerClusterNodeConfigContainerdConfigPrivateRegistryAccessConfig {
    certificateAuthorityDomainConfigs: outputs.GetContainerClusterNodeConfigContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfig[];
    enabled: boolean;
}

export interface GetContainerClusterNodeConfigContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfig {
    fqdns: string[];
    gcpSecretManagerCertificateConfigs: outputs.GetContainerClusterNodeConfigContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfigGcpSecretManagerCertificateConfig[];
}

export interface GetContainerClusterNodeConfigContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfigGcpSecretManagerCertificateConfig {
    secretUri: string;
}

export interface GetContainerClusterNodeConfigEffectiveTaint {
    effect: string;
    key: string;
    value: string;
}

export interface GetContainerClusterNodeConfigEphemeralStorageLocalSsdConfig {
    localSsdCount: number;
}

export interface GetContainerClusterNodeConfigFastSocket {
    enabled: boolean;
}

export interface GetContainerClusterNodeConfigGcfsConfig {
    enabled: boolean;
}

export interface GetContainerClusterNodeConfigGuestAccelerator {
    count: number;
    gpuDriverInstallationConfigs: outputs.GetContainerClusterNodeConfigGuestAcceleratorGpuDriverInstallationConfig[];
    gpuPartitionSize: string;
    gpuSharingConfigs: outputs.GetContainerClusterNodeConfigGuestAcceleratorGpuSharingConfig[];
    type: string;
}

export interface GetContainerClusterNodeConfigGuestAcceleratorGpuDriverInstallationConfig {
    gpuDriverVersion: string;
}

export interface GetContainerClusterNodeConfigGuestAcceleratorGpuSharingConfig {
    gpuSharingStrategy: string;
    maxSharedClientsPerGpu: number;
}

export interface GetContainerClusterNodeConfigGvnic {
    enabled: boolean;
}

export interface GetContainerClusterNodeConfigHostMaintenancePolicy {
    maintenanceInterval: string;
}

export interface GetContainerClusterNodeConfigKubeletConfig {
    cpuCfsQuota: boolean;
    cpuCfsQuotaPeriod: string;
    cpuManagerPolicy: string;
    podPidsLimit: number;
}

export interface GetContainerClusterNodeConfigLinuxNodeConfig {
    cgroupMode: string;
    sysctls: {[key: string]: string};
}

export interface GetContainerClusterNodeConfigLocalNvmeSsdBlockConfig {
    localSsdCount: number;
}

export interface GetContainerClusterNodeConfigReservationAffinity {
    consumeReservationType: string;
    key: string;
    values: string[];
}

export interface GetContainerClusterNodeConfigSecondaryBootDisk {
    diskImage: string;
    mode: string;
}

export interface GetContainerClusterNodeConfigShieldedInstanceConfig {
    enableIntegrityMonitoring: boolean;
    enableSecureBoot: boolean;
}

export interface GetContainerClusterNodeConfigSoleTenantConfig {
    nodeAffinities: outputs.GetContainerClusterNodeConfigSoleTenantConfigNodeAffinity[];
}

export interface GetContainerClusterNodeConfigSoleTenantConfigNodeAffinity {
    key: string;
    operator: string;
    values: string[];
}

export interface GetContainerClusterNodeConfigTaint {
    effect: string;
    key: string;
    value: string;
}

export interface GetContainerClusterNodeConfigWorkloadMetadataConfig {
    mode: string;
}

export interface GetContainerClusterNodePool {
    autoscalings: outputs.GetContainerClusterNodePoolAutoscaling[];
    initialNodeCount: number;
    instanceGroupUrls: string[];
    managedInstanceGroupUrls: string[];
    managements: outputs.GetContainerClusterNodePoolManagement[];
    maxPodsPerNode: number;
    name: string;
    namePrefix: string;
    networkConfigs: outputs.GetContainerClusterNodePoolNetworkConfig[];
    nodeConfigs: outputs.GetContainerClusterNodePoolNodeConfig[];
    nodeCount: number;
    nodeLocations: string[];
    placementPolicies: outputs.GetContainerClusterNodePoolPlacementPolicy[];
    queuedProvisionings: outputs.GetContainerClusterNodePoolQueuedProvisioning[];
    upgradeSettings: outputs.GetContainerClusterNodePoolUpgradeSetting[];
    version: string;
}

export interface GetContainerClusterNodePoolAutoConfig {
    networkTags: outputs.GetContainerClusterNodePoolAutoConfigNetworkTag[];
    resourceManagerTags: {[key: string]: string};
}

export interface GetContainerClusterNodePoolAutoConfigNetworkTag {
    tags: string[];
}

export interface GetContainerClusterNodePoolAutoscaling {
    locationPolicy: string;
    maxNodeCount: number;
    minNodeCount: number;
    totalMaxNodeCount: number;
    totalMinNodeCount: number;
}

export interface GetContainerClusterNodePoolDefault {
    nodeConfigDefaults: outputs.GetContainerClusterNodePoolDefaultNodeConfigDefault[];
}

export interface GetContainerClusterNodePoolDefaultNodeConfigDefault {
    containerdConfigs: outputs.GetContainerClusterNodePoolDefaultNodeConfigDefaultContainerdConfig[];
    loggingVariant: string;
}

export interface GetContainerClusterNodePoolDefaultNodeConfigDefaultContainerdConfig {
    privateRegistryAccessConfigs: outputs.GetContainerClusterNodePoolDefaultNodeConfigDefaultContainerdConfigPrivateRegistryAccessConfig[];
}

export interface GetContainerClusterNodePoolDefaultNodeConfigDefaultContainerdConfigPrivateRegistryAccessConfig {
    certificateAuthorityDomainConfigs: outputs.GetContainerClusterNodePoolDefaultNodeConfigDefaultContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfig[];
    enabled: boolean;
}

export interface GetContainerClusterNodePoolDefaultNodeConfigDefaultContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfig {
    fqdns: string[];
    gcpSecretManagerCertificateConfigs: outputs.GetContainerClusterNodePoolDefaultNodeConfigDefaultContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfigGcpSecretManagerCertificateConfig[];
}

export interface GetContainerClusterNodePoolDefaultNodeConfigDefaultContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfigGcpSecretManagerCertificateConfig {
    secretUri: string;
}

export interface GetContainerClusterNodePoolManagement {
    autoRepair: boolean;
    autoUpgrade: boolean;
}

export interface GetContainerClusterNodePoolNetworkConfig {
    additionalNodeNetworkConfigs: outputs.GetContainerClusterNodePoolNetworkConfigAdditionalNodeNetworkConfig[];
    additionalPodNetworkConfigs: outputs.GetContainerClusterNodePoolNetworkConfigAdditionalPodNetworkConfig[];
    createPodRange: boolean;
    enablePrivateNodes: boolean;
    networkPerformanceConfigs: outputs.GetContainerClusterNodePoolNetworkConfigNetworkPerformanceConfig[];
    podCidrOverprovisionConfigs: outputs.GetContainerClusterNodePoolNetworkConfigPodCidrOverprovisionConfig[];
    podIpv4CidrBlock: string;
    podRange: string;
}

export interface GetContainerClusterNodePoolNetworkConfigAdditionalNodeNetworkConfig {
    network: string;
    subnetwork: string;
}

export interface GetContainerClusterNodePoolNetworkConfigAdditionalPodNetworkConfig {
    maxPodsPerNode: number;
    secondaryPodRange: string;
    subnetwork: string;
}

export interface GetContainerClusterNodePoolNetworkConfigNetworkPerformanceConfig {
    totalEgressBandwidthTier: string;
}

export interface GetContainerClusterNodePoolNetworkConfigPodCidrOverprovisionConfig {
    disabled: boolean;
}

export interface GetContainerClusterNodePoolNodeConfig {
    advancedMachineFeatures: outputs.GetContainerClusterNodePoolNodeConfigAdvancedMachineFeature[];
    bootDiskKmsKey: string;
    confidentialNodes: outputs.GetContainerClusterNodePoolNodeConfigConfidentialNode[];
    containerdConfigs: outputs.GetContainerClusterNodePoolNodeConfigContainerdConfig[];
    diskSizeGb: number;
    diskType: string;
    effectiveTaints: outputs.GetContainerClusterNodePoolNodeConfigEffectiveTaint[];
    enableConfidentialStorage: boolean;
    ephemeralStorageLocalSsdConfigs: outputs.GetContainerClusterNodePoolNodeConfigEphemeralStorageLocalSsdConfig[];
    fastSockets: outputs.GetContainerClusterNodePoolNodeConfigFastSocket[];
    gcfsConfigs: outputs.GetContainerClusterNodePoolNodeConfigGcfsConfig[];
    guestAccelerators: outputs.GetContainerClusterNodePoolNodeConfigGuestAccelerator[];
    gvnics: outputs.GetContainerClusterNodePoolNodeConfigGvnic[];
    hostMaintenancePolicies: outputs.GetContainerClusterNodePoolNodeConfigHostMaintenancePolicy[];
    imageType: string;
    kubeletConfigs: outputs.GetContainerClusterNodePoolNodeConfigKubeletConfig[];
    labels: {[key: string]: string};
    linuxNodeConfigs: outputs.GetContainerClusterNodePoolNodeConfigLinuxNodeConfig[];
    localNvmeSsdBlockConfigs: outputs.GetContainerClusterNodePoolNodeConfigLocalNvmeSsdBlockConfig[];
    localSsdCount: number;
    loggingVariant: string;
    machineType: string;
    metadata: {[key: string]: string};
    minCpuPlatform: string;
    nodeGroup: string;
    oauthScopes: string[];
    preemptible: boolean;
    reservationAffinities: outputs.GetContainerClusterNodePoolNodeConfigReservationAffinity[];
    resourceLabels: {[key: string]: string};
    resourceManagerTags: {[key: string]: string};
    secondaryBootDisks: outputs.GetContainerClusterNodePoolNodeConfigSecondaryBootDisk[];
    serviceAccount: string;
    shieldedInstanceConfigs: outputs.GetContainerClusterNodePoolNodeConfigShieldedInstanceConfig[];
    soleTenantConfigs: outputs.GetContainerClusterNodePoolNodeConfigSoleTenantConfig[];
    spot: boolean;
    tags: string[];
    taints: outputs.GetContainerClusterNodePoolNodeConfigTaint[];
    workloadMetadataConfigs: outputs.GetContainerClusterNodePoolNodeConfigWorkloadMetadataConfig[];
}

export interface GetContainerClusterNodePoolNodeConfigAdvancedMachineFeature {
    enableNestedVirtualization: boolean;
    threadsPerCore: number;
}

export interface GetContainerClusterNodePoolNodeConfigConfidentialNode {
    enabled: boolean;
}

export interface GetContainerClusterNodePoolNodeConfigContainerdConfig {
    privateRegistryAccessConfigs: outputs.GetContainerClusterNodePoolNodeConfigContainerdConfigPrivateRegistryAccessConfig[];
}

export interface GetContainerClusterNodePoolNodeConfigContainerdConfigPrivateRegistryAccessConfig {
    certificateAuthorityDomainConfigs: outputs.GetContainerClusterNodePoolNodeConfigContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfig[];
    enabled: boolean;
}

export interface GetContainerClusterNodePoolNodeConfigContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfig {
    fqdns: string[];
    gcpSecretManagerCertificateConfigs: outputs.GetContainerClusterNodePoolNodeConfigContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfigGcpSecretManagerCertificateConfig[];
}

export interface GetContainerClusterNodePoolNodeConfigContainerdConfigPrivateRegistryAccessConfigCertificateAuthorityDomainConfigGcpSecretManagerCertificateConfig {
    secretUri: string;
}

export interface GetContainerClusterNodePoolNodeConfigEffectiveTaint {
    effect: string;
    key: string;
    value: string;
}

export interface GetContainerClusterNodePoolNodeConfigEphemeralStorageLocalSsdConfig {
    localSsdCount: number;
}

export interface GetContainerClusterNodePoolNodeConfigFastSocket {
    enabled: boolean;
}

export interface GetContainerClusterNodePoolNodeConfigGcfsConfig {
    enabled: boolean;
}

export interface GetContainerClusterNodePoolNodeConfigGuestAccelerator {
    count: number;
    gpuDriverInstallationConfigs: outputs.GetContainerClusterNodePoolNodeConfigGuestAcceleratorGpuDriverInstallationConfig[];
    gpuPartitionSize: string;
    gpuSharingConfigs: outputs.GetContainerClusterNodePoolNodeConfigGuestAcceleratorGpuSharingConfig[];
    type: string;
}

export interface GetContainerClusterNodePoolNodeConfigGuestAcceleratorGpuDriverInstallationConfig {
    gpuDriverVersion: string;
}

export interface GetContainerClusterNodePoolNodeConfigGuestAcceleratorGpuSharingConfig {
    gpuSharingStrategy: string;
    maxSharedClientsPerGpu: number;
}

export interface GetContainerClusterNodePoolNodeConfigGvnic {
    enabled: boolean;
}

export interface GetContainerClusterNodePoolNodeConfigHostMaintenancePolicy {
    maintenanceInterval: string;
}

export interface GetContainerClusterNodePoolNodeConfigKubeletConfig {
    cpuCfsQuota: boolean;
    cpuCfsQuotaPeriod: string;
    cpuManagerPolicy: string;
    podPidsLimit: number;
}

export interface GetContainerClusterNodePoolNodeConfigLinuxNodeConfig {
    cgroupMode: string;
    sysctls: {[key: string]: string};
}

export interface GetContainerClusterNodePoolNodeConfigLocalNvmeSsdBlockConfig {
    localSsdCount: number;
}

export interface GetContainerClusterNodePoolNodeConfigReservationAffinity {
    consumeReservationType: string;
    key: string;
    values: string[];
}

export interface GetContainerClusterNodePoolNodeConfigSecondaryBootDisk {
    diskImage: string;
    mode: string;
}

export interface GetContainerClusterNodePoolNodeConfigShieldedInstanceConfig {
    enableIntegrityMonitoring: boolean;
    enableSecureBoot: boolean;
}

export interface GetContainerClusterNodePoolNodeConfigSoleTenantConfig {
    nodeAffinities: outputs.GetContainerClusterNodePoolNodeConfigSoleTenantConfigNodeAffinity[];
}

export interface GetContainerClusterNodePoolNodeConfigSoleTenantConfigNodeAffinity {
    key: string;
    operator: string;
    values: string[];
}

export interface GetContainerClusterNodePoolNodeConfigTaint {
    effect: string;
    key: string;
    value: string;
}

export interface GetContainerClusterNodePoolNodeConfigWorkloadMetadataConfig {
    mode: string;
}

export interface GetContainerClusterNodePoolPlacementPolicy {
    policyName: string;
    tpuTopology: string;
    type: string;
}

export interface GetContainerClusterNodePoolQueuedProvisioning {
    enabled: boolean;
}

export interface GetContainerClusterNodePoolUpgradeSetting {
    blueGreenSettings: outputs.GetContainerClusterNodePoolUpgradeSettingBlueGreenSetting[];
    maxSurge: number;
    maxUnavailable: number;
    strategy: string;
}

export interface GetContainerClusterNodePoolUpgradeSettingBlueGreenSetting {
    nodePoolSoakDuration: string;
    standardRolloutPolicies: outputs.GetContainerClusterNodePoolUpgradeSettingBlueGreenSettingStandardRolloutPolicy[];
}

export interface GetContainerClusterNodePoolUpgradeSettingBlueGreenSettingStandardRolloutPolicy {
    batchNodeCount: number;
    batchPercentage: number;
    batchSoakDuration: string;
}

export interface GetContainerClusterNotificationConfig {
    pubsubs: outputs.GetContainerClusterNotificationConfigPubsub[];
}

export interface GetContainerClusterNotificationConfigPubsub {
    enabled: boolean;
    filters: outputs.GetContainerClusterNotificationConfigPubsubFilter[];
    topic: string;
}

export interface GetContainerClusterNotificationConfigPubsubFilter {
    eventTypes: string[];
}

export interface GetContainerClusterPrivateClusterConfig {
    enablePrivateEndpoint: boolean;
    enablePrivateNodes: boolean;
    masterGlobalAccessConfigs: outputs.GetContainerClusterPrivateClusterConfigMasterGlobalAccessConfig[];
    masterIpv4CidrBlock: string;
    peeringName: string;
    privateEndpoint: string;
    privateEndpointSubnetwork: string;
    publicEndpoint: string;
}

export interface GetContainerClusterPrivateClusterConfigMasterGlobalAccessConfig {
    enabled: boolean;
}

export interface GetContainerClusterReleaseChannel {
    channel: string;
}

export interface GetContainerClusterResourceUsageExportConfig {
    bigqueryDestinations: outputs.GetContainerClusterResourceUsageExportConfigBigqueryDestination[];
    enableNetworkEgressMetering: boolean;
    enableResourceConsumptionMetering: boolean;
}

export interface GetContainerClusterResourceUsageExportConfigBigqueryDestination {
    datasetId: string;
}

export interface GetContainerClusterSecretManagerConfig {
    enabled: boolean;
}

export interface GetContainerClusterSecurityPostureConfig {
    mode: string;
    vulnerabilityMode: string;
}

export interface GetContainerClusterServiceExternalIpsConfig {
    enabled: boolean;
}

export interface GetContainerClusterVerticalPodAutoscaling {
    enabled: boolean;
}

export interface GetContainerClusterWorkloadIdentityConfig {
    workloadPool: string;
}

export interface GetDataprocMetastoreServiceEncryptionConfig {
    kmsKey: string;
}

export interface GetDataprocMetastoreServiceHiveMetastoreConfig {
    auxiliaryVersions: outputs.GetDataprocMetastoreServiceHiveMetastoreConfigAuxiliaryVersion[];
    configOverrides: {[key: string]: string};
    endpointProtocol: string;
    kerberosConfigs: outputs.GetDataprocMetastoreServiceHiveMetastoreConfigKerberosConfig[];
    version: string;
}

export interface GetDataprocMetastoreServiceHiveMetastoreConfigAuxiliaryVersion {
    configOverrides: {[key: string]: string};
    key: string;
    version: string;
}

export interface GetDataprocMetastoreServiceHiveMetastoreConfigKerberosConfig {
    keytabs: outputs.GetDataprocMetastoreServiceHiveMetastoreConfigKerberosConfigKeytab[];
    krb5ConfigGcsUri: string;
    principal: string;
}

export interface GetDataprocMetastoreServiceHiveMetastoreConfigKerberosConfigKeytab {
    cloudSecret: string;
}

export interface GetDataprocMetastoreServiceMaintenanceWindow {
    dayOfWeek: string;
    hourOfDay: number;
}

export interface GetDataprocMetastoreServiceMetadataIntegration {
    dataCatalogConfigs: outputs.GetDataprocMetastoreServiceMetadataIntegrationDataCatalogConfig[];
}

export interface GetDataprocMetastoreServiceMetadataIntegrationDataCatalogConfig {
    enabled: boolean;
}

export interface GetDataprocMetastoreServiceNetworkConfig {
    consumers: outputs.GetDataprocMetastoreServiceNetworkConfigConsumer[];
}

export interface GetDataprocMetastoreServiceNetworkConfigConsumer {
    endpointUri: string;
    subnetwork: string;
}

export interface GetDataprocMetastoreServiceScalingConfig {
    instanceSize: string;
    scalingFactor: number;
}

export interface GetDataprocMetastoreServiceScheduledBackup {
    backupLocation: string;
    cronSchedule: string;
    enabled: boolean;
    timeZone: string;
}

export interface GetDataprocMetastoreServiceTelemetryConfig {
    logFormat: string;
}

export interface GetDnsKeysKeySigningKey {
    algorithm: string;
    creationTime: string;
    description: string;
    digests: outputs.GetDnsKeysKeySigningKeyDigest[];
    dsRecord: string;
    id: string;
    isActive: boolean;
    keyLength: number;
    keyTag: number;
    publicKey: string;
}

export interface GetDnsKeysKeySigningKeyDigest {
    digest: string;
    type: string;
}

export interface GetDnsKeysZoneSigningKey {
    algorithm: string;
    creationTime: string;
    description: string;
    digests: outputs.GetDnsKeysZoneSigningKeyDigest[];
    id: string;
    isActive: boolean;
    keyLength: number;
    keyTag: number;
    publicKey: string;
}

export interface GetDnsKeysZoneSigningKeyDigest {
    digest: string;
    type: string;
}

export interface GetDnsManagedZonesManagedZone {
    description: string;
    dnsName: string;
    id: string;
    managedZoneId: number;
    name: string;
    nameServers: string[];
    project: string;
    visibility: string;
}

export interface GetFilestoreInstanceFileShare {
    capacityGb: number;
    name: string;
    nfsExportOptions: outputs.GetFilestoreInstanceFileShareNfsExportOption[];
    sourceBackup: string;
}

export interface GetFilestoreInstanceFileShareNfsExportOption {
    accessMode: string;
    anonGid: number;
    anonUid: number;
    ipRanges: string[];
    squashMode: string;
}

export interface GetFilestoreInstanceNetwork {
    connectMode: string;
    ipAddresses: string[];
    modes: string[];
    network: string;
    reservedIpRange: string;
}

export interface GetFolderOrganizationPolicyBooleanPolicy {
    enforced: boolean;
}

export interface GetFolderOrganizationPolicyListPolicy {
    allows: outputs.GetFolderOrganizationPolicyListPolicyAllow[];
    denies: outputs.GetFolderOrganizationPolicyListPolicyDeny[];
    inheritFromParent: boolean;
    suggestedValue: string;
}

export interface GetFolderOrganizationPolicyListPolicyAllow {
    all: boolean;
    values: string[];
}

export interface GetFolderOrganizationPolicyListPolicyDeny {
    all: boolean;
    values: string[];
}

export interface GetFolderOrganizationPolicyRestorePolicy {
    default: boolean;
}

export interface GetFoldersFolder {
    createTime: string;
    deleteTime: string;
    displayName: string;
    etag: string;
    name: string;
    parent: string;
    state: string;
    updateTime: string;
}

export interface GetGkeHubMembershipBindingState {
    code: string;
}

export interface GetIamPolicyAuditConfig {
    auditLogConfigs: outputs.GetIamPolicyAuditConfigAuditLogConfig[];
    service: string;
}

export interface GetIamPolicyAuditConfigAuditLogConfig {
    exemptedMembers?: string[];
    logType: string;
}

export interface GetIamPolicyBinding {
    condition?: outputs.GetIamPolicyBindingCondition;
    members: string[];
    role: string;
}

export interface GetIamPolicyBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface GetIamTestablePermissionsPermission {
    apiDisabled: boolean;
    customSupportLevel: string;
    name: string;
    stage: string;
    title: string;
}

export interface GetKmsCryptoKeyLatestVersionPublicKey {
    algorithm: string;
    pem: string;
}

export interface GetKmsCryptoKeyPrimary {
    name: string;
    state: string;
}

export interface GetKmsCryptoKeyVersionPublicKey {
    algorithm: string;
    pem: string;
}

export interface GetKmsCryptoKeyVersionTemplate {
    algorithm: string;
    protectionLevel: string;
}

export interface GetKmsCryptoKeyVersionsPublicKey {
    algorithm: string;
    pem: string;
}

export interface GetKmsCryptoKeyVersionsVersion {
    algorithm: string;
    cryptoKey: string;
    id: string;
    name: string;
    protectionLevel: string;
    publicKeys: outputs.GetKmsCryptoKeyVersionsVersionPublicKey[];
    state: string;
    version: number;
}

export interface GetKmsCryptoKeyVersionsVersionPublicKey {
    algorithm: string;
    pem: string;
}

export interface GetKmsCryptoKeysKey {
    cryptoKeyBackend: string;
    destroyScheduledDuration: string;
    effectiveLabels: {[key: string]: string};
    id: string;
    importOnly: boolean;
    keyRing: string;
    labels: {[key: string]: string};
    name: string;
    primaries: outputs.GetKmsCryptoKeysKeyPrimary[];
    purpose: string;
    rotationPeriod: string;
    skipInitialVersionCreation: boolean;
    terraformLabels: {[key: string]: string};
    versionTemplates: outputs.GetKmsCryptoKeysKeyVersionTemplate[];
}

export interface GetKmsCryptoKeysKeyPrimary {
    name: string;
    state: string;
}

export interface GetKmsCryptoKeysKeyVersionTemplate {
    algorithm: string;
    protectionLevel: string;
}

export interface GetKmsKeyRingsKeyRing {
    id: string;
    name: string;
}

export interface GetLoggingSinkBigqueryOption {
    usePartitionedTables: boolean;
}

export interface GetLoggingSinkExclusion {
    description: string;
    disabled: boolean;
    filter: string;
    name: string;
}

export interface GetMonitoringAppEngineServiceTelemetry {
    resourceName: string;
}

export interface GetMonitoringClusterIstioServiceTelemetry {
    resourceName: string;
}

export interface GetMonitoringIstioCanonicalServiceTelemetry {
    resourceName: string;
}

export interface GetMonitoringMeshIstioServiceTelemetry {
    resourceName: string;
}

export interface GetMonitoringNotificationChannelSensitiveLabel {
    authToken: string;
    password: string;
    serviceKey: string;
}

export interface GetMonitoringUptimeCheckIpsUptimeCheckIp {
    ipAddress: string;
    location: string;
    region: string;
}

export interface GetPrivatecaCertificateAuthorityAccessUrl {
    caCertificateAccessUrl: string;
    crlAccessUrls: string[];
}

export interface GetPrivatecaCertificateAuthorityConfig {
    subjectConfigs: outputs.GetPrivatecaCertificateAuthorityConfigSubjectConfig[];
    subjectKeyIds: outputs.GetPrivatecaCertificateAuthorityConfigSubjectKeyId[];
    x509Configs: outputs.GetPrivatecaCertificateAuthorityConfigX509Config[];
}

export interface GetPrivatecaCertificateAuthorityConfigSubjectConfig {
    subjectAltNames: outputs.GetPrivatecaCertificateAuthorityConfigSubjectConfigSubjectAltName[];
    subjects: outputs.GetPrivatecaCertificateAuthorityConfigSubjectConfigSubject[];
}

export interface GetPrivatecaCertificateAuthorityConfigSubjectConfigSubject {
    commonName: string;
    countryCode: string;
    locality: string;
    organization: string;
    organizationalUnit: string;
    postalCode: string;
    province: string;
    streetAddress: string;
}

export interface GetPrivatecaCertificateAuthorityConfigSubjectConfigSubjectAltName {
    dnsNames: string[];
    emailAddresses: string[];
    ipAddresses: string[];
    uris: string[];
}

export interface GetPrivatecaCertificateAuthorityConfigSubjectKeyId {
    keyId: string;
}

export interface GetPrivatecaCertificateAuthorityConfigX509Config {
    additionalExtensions: outputs.GetPrivatecaCertificateAuthorityConfigX509ConfigAdditionalExtension[];
    aiaOcspServers: string[];
    caOptions: outputs.GetPrivatecaCertificateAuthorityConfigX509ConfigCaOption[];
    keyUsages: outputs.GetPrivatecaCertificateAuthorityConfigX509ConfigKeyUsage[];
    nameConstraints: outputs.GetPrivatecaCertificateAuthorityConfigX509ConfigNameConstraint[];
    policyIds: outputs.GetPrivatecaCertificateAuthorityConfigX509ConfigPolicyId[];
}

export interface GetPrivatecaCertificateAuthorityConfigX509ConfigAdditionalExtension {
    critical: boolean;
    objectIds: outputs.GetPrivatecaCertificateAuthorityConfigX509ConfigAdditionalExtensionObjectId[];
    value: string;
}

export interface GetPrivatecaCertificateAuthorityConfigX509ConfigAdditionalExtensionObjectId {
    objectIdPaths: number[];
}

export interface GetPrivatecaCertificateAuthorityConfigX509ConfigCaOption {
    isCa: boolean;
    maxIssuerPathLength: number;
    nonCa: boolean;
    zeroMaxIssuerPathLength: boolean;
}

export interface GetPrivatecaCertificateAuthorityConfigX509ConfigKeyUsage {
    baseKeyUsages: outputs.GetPrivatecaCertificateAuthorityConfigX509ConfigKeyUsageBaseKeyUsage[];
    extendedKeyUsages: outputs.GetPrivatecaCertificateAuthorityConfigX509ConfigKeyUsageExtendedKeyUsage[];
    unknownExtendedKeyUsages: outputs.GetPrivatecaCertificateAuthorityConfigX509ConfigKeyUsageUnknownExtendedKeyUsage[];
}

export interface GetPrivatecaCertificateAuthorityConfigX509ConfigKeyUsageBaseKeyUsage {
    certSign: boolean;
    contentCommitment: boolean;
    crlSign: boolean;
    dataEncipherment: boolean;
    decipherOnly: boolean;
    digitalSignature: boolean;
    encipherOnly: boolean;
    keyAgreement: boolean;
    keyEncipherment: boolean;
}

export interface GetPrivatecaCertificateAuthorityConfigX509ConfigKeyUsageExtendedKeyUsage {
    clientAuth: boolean;
    codeSigning: boolean;
    emailProtection: boolean;
    ocspSigning: boolean;
    serverAuth: boolean;
    timeStamping: boolean;
}

export interface GetPrivatecaCertificateAuthorityConfigX509ConfigKeyUsageUnknownExtendedKeyUsage {
    objectIdPaths: number[];
}

export interface GetPrivatecaCertificateAuthorityConfigX509ConfigNameConstraint {
    critical: boolean;
    excludedDnsNames: string[];
    excludedEmailAddresses: string[];
    excludedIpRanges: string[];
    excludedUris: string[];
    permittedDnsNames: string[];
    permittedEmailAddresses: string[];
    permittedIpRanges: string[];
    permittedUris: string[];
}

export interface GetPrivatecaCertificateAuthorityConfigX509ConfigPolicyId {
    objectIdPaths: number[];
}

export interface GetPrivatecaCertificateAuthorityKeySpec {
    algorithm: string;
    cloudKmsKeyVersion: string;
}

export interface GetPrivatecaCertificateAuthoritySubordinateConfig {
    certificateAuthority: string;
    pemIssuerChains: outputs.GetPrivatecaCertificateAuthoritySubordinateConfigPemIssuerChain[];
}

export interface GetPrivatecaCertificateAuthoritySubordinateConfigPemIssuerChain {
    pemCertificates: string[];
}

export interface GetProjectOrganizationPolicyBooleanPolicy {
    enforced: boolean;
}

export interface GetProjectOrganizationPolicyListPolicy {
    allows: outputs.GetProjectOrganizationPolicyListPolicyAllow[];
    denies: outputs.GetProjectOrganizationPolicyListPolicyDeny[];
    inheritFromParent: boolean;
    suggestedValue: string;
}

export interface GetProjectOrganizationPolicyListPolicyAllow {
    all: boolean;
    values: string[];
}

export interface GetProjectOrganizationPolicyListPolicyDeny {
    all: boolean;
    values: string[];
}

export interface GetProjectOrganizationPolicyRestorePolicy {
    default: boolean;
}

export interface GetProjectsProject {
    createTime: string;
    labels: {[key: string]: string};
    lifecycleState: string;
    name: string;
    number: string;
    parent: {[key: string]: string};
    projectId: string;
}

export interface GetPubsubSubscriptionBigqueryConfig {
    dropUnknownFields: boolean;
    serviceAccountEmail: string;
    table: string;
    useTableSchema: boolean;
    useTopicSchema: boolean;
    writeMetadata: boolean;
}

export interface GetPubsubSubscriptionCloudStorageConfig {
    avroConfigs: outputs.GetPubsubSubscriptionCloudStorageConfigAvroConfig[];
    bucket: string;
    filenameDatetimeFormat: string;
    filenamePrefix: string;
    filenameSuffix: string;
    maxBytes: number;
    maxDuration: string;
    serviceAccountEmail: string;
    state: string;
}

export interface GetPubsubSubscriptionCloudStorageConfigAvroConfig {
    writeMetadata: boolean;
}

export interface GetPubsubSubscriptionDeadLetterPolicy {
    deadLetterTopic: string;
    maxDeliveryAttempts: number;
}

export interface GetPubsubSubscriptionExpirationPolicy {
    ttl: string;
}

export interface GetPubsubSubscriptionPushConfig {
    attributes: {[key: string]: string};
    noWrappers: outputs.GetPubsubSubscriptionPushConfigNoWrapper[];
    oidcTokens: outputs.GetPubsubSubscriptionPushConfigOidcToken[];
    pushEndpoint: string;
}

export interface GetPubsubSubscriptionPushConfigNoWrapper {
    writeMetadata: boolean;
}

export interface GetPubsubSubscriptionPushConfigOidcToken {
    audience: string;
    serviceAccountEmail: string;
}

export interface GetPubsubSubscriptionRetryPolicy {
    maximumBackoff: string;
    minimumBackoff: string;
}

export interface GetPubsubTopicIngestionDataSourceSetting {
    awsKineses: outputs.GetPubsubTopicIngestionDataSourceSettingAwsKinese[];
}

export interface GetPubsubTopicIngestionDataSourceSettingAwsKinese {
    awsRoleArn: string;
    consumerArn: string;
    gcpServiceAccount: string;
    streamArn: string;
}

export interface GetPubsubTopicMessageStoragePolicy {
    allowedPersistenceRegions: string[];
}

export interface GetPubsubTopicSchemaSetting {
    encoding: string;
    schema: string;
}

export interface GetRedisInstanceMaintenancePolicy {
    createTime: string;
    description: string;
    updateTime: string;
    weeklyMaintenanceWindows: outputs.GetRedisInstanceMaintenancePolicyWeeklyMaintenanceWindow[];
}

export interface GetRedisInstanceMaintenancePolicyWeeklyMaintenanceWindow {
    day: string;
    duration: string;
    startTimes: outputs.GetRedisInstanceMaintenancePolicyWeeklyMaintenanceWindowStartTime[];
}

export interface GetRedisInstanceMaintenancePolicyWeeklyMaintenanceWindowStartTime {
    hours: number;
    minutes: number;
    nanos: number;
    seconds: number;
}

export interface GetRedisInstanceMaintenanceSchedule {
    endTime: string;
    scheduleDeadlineTime: string;
    startTime: string;
}

export interface GetRedisInstanceNode {
    id: string;
    zone: string;
}

export interface GetRedisInstancePersistenceConfig {
    persistenceMode: string;
    rdbNextSnapshotTime: string;
    rdbSnapshotPeriod: string;
    rdbSnapshotStartTime: string;
}

export interface GetRedisInstanceServerCaCert {
    cert: string;
    createTime: string;
    expireTime: string;
    serialNumber: string;
    sha1Fingerprint: string;
}

export interface GetSecretManagerSecretReplication {
    autos: outputs.GetSecretManagerSecretReplicationAuto[];
    userManageds: outputs.GetSecretManagerSecretReplicationUserManaged[];
}

export interface GetSecretManagerSecretReplicationAuto {
    customerManagedEncryptions: outputs.GetSecretManagerSecretReplicationAutoCustomerManagedEncryption[];
}

export interface GetSecretManagerSecretReplicationAutoCustomerManagedEncryption {
    kmsKeyName: string;
}

export interface GetSecretManagerSecretReplicationUserManaged {
    replicas: outputs.GetSecretManagerSecretReplicationUserManagedReplica[];
}

export interface GetSecretManagerSecretReplicationUserManagedReplica {
    customerManagedEncryptions: outputs.GetSecretManagerSecretReplicationUserManagedReplicaCustomerManagedEncryption[];
    location: string;
}

export interface GetSecretManagerSecretReplicationUserManagedReplicaCustomerManagedEncryption {
    kmsKeyName: string;
}

export interface GetSecretManagerSecretRotation {
    nextRotationTime: string;
    rotationPeriod: string;
}

export interface GetSecretManagerSecretTopic {
    name: string;
}

export interface GetSecretManagerSecretsSecret {
    annotations: {[key: string]: string};
    createTime: string;
    effectiveAnnotations: {[key: string]: string};
    effectiveLabels: {[key: string]: string};
    expireTime: string;
    labels: {[key: string]: string};
    name: string;
    project: string;
    replications: outputs.GetSecretManagerSecretsSecretReplication[];
    rotations: outputs.GetSecretManagerSecretsSecretRotation[];
    secretId: string;
    terraformLabels: {[key: string]: string};
    topics: outputs.GetSecretManagerSecretsSecretTopic[];
    ttl: string;
    versionAliases: {[key: string]: string};
    versionDestroyTtl: string;
}

export interface GetSecretManagerSecretsSecretReplication {
    autos: outputs.GetSecretManagerSecretsSecretReplicationAuto[];
    userManageds: outputs.GetSecretManagerSecretsSecretReplicationUserManaged[];
}

export interface GetSecretManagerSecretsSecretReplicationAuto {
    customerManagedEncryptions: outputs.GetSecretManagerSecretsSecretReplicationAutoCustomerManagedEncryption[];
}

export interface GetSecretManagerSecretsSecretReplicationAutoCustomerManagedEncryption {
    kmsKeyName: string;
}

export interface GetSecretManagerSecretsSecretReplicationUserManaged {
    replicas: outputs.GetSecretManagerSecretsSecretReplicationUserManagedReplica[];
}

export interface GetSecretManagerSecretsSecretReplicationUserManagedReplica {
    customerManagedEncryptions: outputs.GetSecretManagerSecretsSecretReplicationUserManagedReplicaCustomerManagedEncryption[];
    location: string;
}

export interface GetSecretManagerSecretsSecretReplicationUserManagedReplicaCustomerManagedEncryption {
    kmsKeyName: string;
}

export interface GetSecretManagerSecretsSecretRotation {
    nextRotationTime: string;
    rotationPeriod: string;
}

export interface GetSecretManagerSecretsSecretTopic {
    name: string;
}

export interface GetSiteVerificationTokenTimeouts {
    read?: string;
}

export interface GetSourcerepoRepositoryPubsubConfig {
    messageFormat: string;
    serviceAccountEmail: string;
    topic: string;
}

export interface GetSpannerInstanceAutoscalingConfig {
    autoscalingLimits: outputs.GetSpannerInstanceAutoscalingConfigAutoscalingLimit[];
    autoscalingTargets: outputs.GetSpannerInstanceAutoscalingConfigAutoscalingTarget[];
}

export interface GetSpannerInstanceAutoscalingConfigAutoscalingLimit {
    maxNodes: number;
    maxProcessingUnits: number;
    minNodes: number;
    minProcessingUnits: number;
}

export interface GetSpannerInstanceAutoscalingConfigAutoscalingTarget {
    highPriorityCpuUtilizationPercent: number;
    storageUtilizationPercent: number;
}

export interface GetSqlCaCertsCert {
    cert: string;
    commonName: string;
    createTime: string;
    expirationTime: string;
    sha1Fingerprint: string;
}

export interface GetSqlDatabaseInstanceClone {
    allocatedIpRange: string;
    databaseNames: string[];
    pointInTime: string;
    preferredZone: string;
    sourceInstanceName: string;
}

export interface GetSqlDatabaseInstanceIpAddress {
    ipAddress: string;
    timeToRetire: string;
    type: string;
}

export interface GetSqlDatabaseInstanceReplicaConfiguration {
    caCertificate: string;
    clientCertificate: string;
    clientKey: string;
    connectRetryInterval: number;
    dumpFilePath: string;
    failoverTarget: boolean;
    masterHeartbeatPeriod: number;
    password: string;
    sslCipher: string;
    username: string;
    verifyServerCertificate: boolean;
}

export interface GetSqlDatabaseInstanceRestoreBackupContext {
    backupRunId: number;
    instanceId: string;
    project: string;
}

export interface GetSqlDatabaseInstanceServerCaCert {
    cert: string;
    commonName: string;
    createTime: string;
    expirationTime: string;
    sha1Fingerprint: string;
}

export interface GetSqlDatabaseInstanceSetting {
    activationPolicy: string;
    activeDirectoryConfigs: outputs.GetSqlDatabaseInstanceSettingActiveDirectoryConfig[];
    advancedMachineFeatures: outputs.GetSqlDatabaseInstanceSettingAdvancedMachineFeature[];
    availabilityType: string;
    backupConfigurations: outputs.GetSqlDatabaseInstanceSettingBackupConfiguration[];
    collation: string;
    connectorEnforcement: string;
    dataCacheConfigs: outputs.GetSqlDatabaseInstanceSettingDataCacheConfig[];
    databaseFlags: outputs.GetSqlDatabaseInstanceSettingDatabaseFlag[];
    deletionProtectionEnabled: boolean;
    denyMaintenancePeriods: outputs.GetSqlDatabaseInstanceSettingDenyMaintenancePeriod[];
    diskAutoresize: boolean;
    diskAutoresizeLimit: number;
    diskSize: number;
    diskType: string;
    edition: string;
    enableDataplexIntegration: boolean;
    enableGoogleMlIntegration: boolean;
    insightsConfigs: outputs.GetSqlDatabaseInstanceSettingInsightsConfig[];
    ipConfigurations: outputs.GetSqlDatabaseInstanceSettingIpConfiguration[];
    locationPreferences: outputs.GetSqlDatabaseInstanceSettingLocationPreference[];
    maintenanceWindows: outputs.GetSqlDatabaseInstanceSettingMaintenanceWindow[];
    passwordValidationPolicies: outputs.GetSqlDatabaseInstanceSettingPasswordValidationPolicy[];
    pricingPlan: string;
    sqlServerAuditConfigs: outputs.GetSqlDatabaseInstanceSettingSqlServerAuditConfig[];
    tier: string;
    timeZone: string;
    userLabels: {[key: string]: string};
    version: number;
}

export interface GetSqlDatabaseInstanceSettingActiveDirectoryConfig {
    domain: string;
}

export interface GetSqlDatabaseInstanceSettingAdvancedMachineFeature {
    threadsPerCore: number;
}

export interface GetSqlDatabaseInstanceSettingBackupConfiguration {
    backupRetentionSettings: outputs.GetSqlDatabaseInstanceSettingBackupConfigurationBackupRetentionSetting[];
    binaryLogEnabled: boolean;
    enabled: boolean;
    location: string;
    pointInTimeRecoveryEnabled: boolean;
    startTime: string;
    transactionLogRetentionDays: number;
}

export interface GetSqlDatabaseInstanceSettingBackupConfigurationBackupRetentionSetting {
    retainedBackups: number;
    retentionUnit: string;
}

export interface GetSqlDatabaseInstanceSettingDataCacheConfig {
    dataCacheEnabled: boolean;
}

export interface GetSqlDatabaseInstanceSettingDatabaseFlag {
    name: string;
    value: string;
}

export interface GetSqlDatabaseInstanceSettingDenyMaintenancePeriod {
    endDate: string;
    startDate: string;
    time: string;
}

export interface GetSqlDatabaseInstanceSettingInsightsConfig {
    queryInsightsEnabled: boolean;
    queryPlansPerMinute: number;
    queryStringLength: number;
    recordApplicationTags: boolean;
    recordClientAddress: boolean;
}

export interface GetSqlDatabaseInstanceSettingIpConfiguration {
    allocatedIpRange: string;
    authorizedNetworks: outputs.GetSqlDatabaseInstanceSettingIpConfigurationAuthorizedNetwork[];
    enablePrivatePathForGoogleCloudServices: boolean;
    ipv4Enabled: boolean;
    privateNetwork: string;
    pscConfigs: outputs.GetSqlDatabaseInstanceSettingIpConfigurationPscConfig[];
    serverCaMode: string;
    sslMode: string;
}

export interface GetSqlDatabaseInstanceSettingIpConfigurationAuthorizedNetwork {
    expirationTime: string;
    name: string;
    value: string;
}

export interface GetSqlDatabaseInstanceSettingIpConfigurationPscConfig {
    allowedConsumerProjects: string[];
    pscEnabled: boolean;
}

export interface GetSqlDatabaseInstanceSettingLocationPreference {
    followGaeApplication: string;
    secondaryZone: string;
    zone: string;
}

export interface GetSqlDatabaseInstanceSettingMaintenanceWindow {
    day: number;
    hour: number;
    updateTrack: string;
}

export interface GetSqlDatabaseInstanceSettingPasswordValidationPolicy {
    complexity: string;
    disallowUsernameSubstring: boolean;
    enablePasswordPolicy: boolean;
    minLength: number;
    passwordChangeInterval: string;
    reuseInterval: number;
}

export interface GetSqlDatabaseInstanceSettingSqlServerAuditConfig {
    bucket: string;
    retentionInterval: string;
    uploadInterval: string;
}

export interface GetSqlDatabaseInstancesInstance {
    availableMaintenanceVersions: string[];
    clones: outputs.GetSqlDatabaseInstancesInstanceClone[];
    connectionName: string;
    databaseVersion: string;
    deletionProtection: boolean;
    dnsName: string;
    encryptionKeyName: string;
    firstIpAddress: string;
    instanceType: string;
    ipAddresses: outputs.GetSqlDatabaseInstancesInstanceIpAddress[];
    maintenanceVersion: string;
    masterInstanceName: string;
    name: string;
    privateIpAddress: string;
    project: string;
    pscServiceAttachmentLink: string;
    publicIpAddress: string;
    region: string;
    replicaConfigurations: outputs.GetSqlDatabaseInstancesInstanceReplicaConfiguration[];
    restoreBackupContexts: outputs.GetSqlDatabaseInstancesInstanceRestoreBackupContext[];
    rootPassword: string;
    selfLink: string;
    serverCaCerts: outputs.GetSqlDatabaseInstancesInstanceServerCaCert[];
    serviceAccountEmailAddress: string;
    settings: outputs.GetSqlDatabaseInstancesInstanceSetting[];
}

export interface GetSqlDatabaseInstancesInstanceClone {
    allocatedIpRange: string;
    databaseNames: string[];
    pointInTime: string;
    preferredZone: string;
    sourceInstanceName: string;
}

export interface GetSqlDatabaseInstancesInstanceIpAddress {
    ipAddress: string;
    timeToRetire: string;
    type: string;
}

export interface GetSqlDatabaseInstancesInstanceReplicaConfiguration {
    caCertificate: string;
    clientCertificate: string;
    clientKey: string;
    connectRetryInterval: number;
    dumpFilePath: string;
    failoverTarget: boolean;
    masterHeartbeatPeriod: number;
    password: string;
    sslCipher: string;
    username: string;
    verifyServerCertificate: boolean;
}

export interface GetSqlDatabaseInstancesInstanceRestoreBackupContext {
    backupRunId: number;
    instanceId: string;
    project: string;
}

export interface GetSqlDatabaseInstancesInstanceServerCaCert {
    cert: string;
    commonName: string;
    createTime: string;
    expirationTime: string;
    sha1Fingerprint: string;
}

export interface GetSqlDatabaseInstancesInstanceSetting {
    activationPolicy: string;
    activeDirectoryConfigs: outputs.GetSqlDatabaseInstancesInstanceSettingActiveDirectoryConfig[];
    advancedMachineFeatures: outputs.GetSqlDatabaseInstancesInstanceSettingAdvancedMachineFeature[];
    availabilityType: string;
    backupConfigurations: outputs.GetSqlDatabaseInstancesInstanceSettingBackupConfiguration[];
    collation: string;
    connectorEnforcement: string;
    dataCacheConfigs: outputs.GetSqlDatabaseInstancesInstanceSettingDataCacheConfig[];
    databaseFlags: outputs.GetSqlDatabaseInstancesInstanceSettingDatabaseFlag[];
    deletionProtectionEnabled: boolean;
    denyMaintenancePeriods: outputs.GetSqlDatabaseInstancesInstanceSettingDenyMaintenancePeriod[];
    diskAutoresize: boolean;
    diskAutoresizeLimit: number;
    diskSize: number;
    diskType: string;
    edition: string;
    enableDataplexIntegration: boolean;
    enableGoogleMlIntegration: boolean;
    insightsConfigs: outputs.GetSqlDatabaseInstancesInstanceSettingInsightsConfig[];
    ipConfigurations: outputs.GetSqlDatabaseInstancesInstanceSettingIpConfiguration[];
    locationPreferences: outputs.GetSqlDatabaseInstancesInstanceSettingLocationPreference[];
    maintenanceWindows: outputs.GetSqlDatabaseInstancesInstanceSettingMaintenanceWindow[];
    passwordValidationPolicies: outputs.GetSqlDatabaseInstancesInstanceSettingPasswordValidationPolicy[];
    pricingPlan: string;
    sqlServerAuditConfigs: outputs.GetSqlDatabaseInstancesInstanceSettingSqlServerAuditConfig[];
    tier: string;
    timeZone: string;
    userLabels: {[key: string]: string};
    version: number;
}

export interface GetSqlDatabaseInstancesInstanceSettingActiveDirectoryConfig {
    domain: string;
}

export interface GetSqlDatabaseInstancesInstanceSettingAdvancedMachineFeature {
    threadsPerCore: number;
}

export interface GetSqlDatabaseInstancesInstanceSettingBackupConfiguration {
    backupRetentionSettings: outputs.GetSqlDatabaseInstancesInstanceSettingBackupConfigurationBackupRetentionSetting[];
    binaryLogEnabled: boolean;
    enabled: boolean;
    location: string;
    pointInTimeRecoveryEnabled: boolean;
    startTime: string;
    transactionLogRetentionDays: number;
}

export interface GetSqlDatabaseInstancesInstanceSettingBackupConfigurationBackupRetentionSetting {
    retainedBackups: number;
    retentionUnit: string;
}

export interface GetSqlDatabaseInstancesInstanceSettingDataCacheConfig {
    dataCacheEnabled: boolean;
}

export interface GetSqlDatabaseInstancesInstanceSettingDatabaseFlag {
    name: string;
    value: string;
}

export interface GetSqlDatabaseInstancesInstanceSettingDenyMaintenancePeriod {
    endDate: string;
    startDate: string;
    time: string;
}

export interface GetSqlDatabaseInstancesInstanceSettingInsightsConfig {
    queryInsightsEnabled: boolean;
    queryPlansPerMinute: number;
    queryStringLength: number;
    recordApplicationTags: boolean;
    recordClientAddress: boolean;
}

export interface GetSqlDatabaseInstancesInstanceSettingIpConfiguration {
    allocatedIpRange: string;
    authorizedNetworks: outputs.GetSqlDatabaseInstancesInstanceSettingIpConfigurationAuthorizedNetwork[];
    enablePrivatePathForGoogleCloudServices: boolean;
    ipv4Enabled: boolean;
    privateNetwork: string;
    pscConfigs: outputs.GetSqlDatabaseInstancesInstanceSettingIpConfigurationPscConfig[];
    serverCaMode: string;
    sslMode: string;
}

export interface GetSqlDatabaseInstancesInstanceSettingIpConfigurationAuthorizedNetwork {
    expirationTime: string;
    name: string;
    value: string;
}

export interface GetSqlDatabaseInstancesInstanceSettingIpConfigurationPscConfig {
    allowedConsumerProjects: string[];
    pscEnabled: boolean;
}

export interface GetSqlDatabaseInstancesInstanceSettingLocationPreference {
    followGaeApplication: string;
    secondaryZone: string;
    zone: string;
}

export interface GetSqlDatabaseInstancesInstanceSettingMaintenanceWindow {
    day: number;
    hour: number;
    updateTrack: string;
}

export interface GetSqlDatabaseInstancesInstanceSettingPasswordValidationPolicy {
    complexity: string;
    disallowUsernameSubstring: boolean;
    enablePasswordPolicy: boolean;
    minLength: number;
    passwordChangeInterval: string;
    reuseInterval: number;
}

export interface GetSqlDatabaseInstancesInstanceSettingSqlServerAuditConfig {
    bucket: string;
    retentionInterval: string;
    uploadInterval: string;
}

export interface GetSqlDatabasesDatabase {
    charset: string;
    collation: string;
    deletionPolicy: string;
    instance: string;
    name: string;
    project: string;
    selfLink: string;
}

export interface GetSqlTiersTier {
    diskQuota: number;
    ram: number;
    regions: string[];
    tier: string;
}

export interface GetStorageBucketAutoclass {
    enabled: boolean;
    terminalStorageClass: string;
}

export interface GetStorageBucketCor {
    maxAgeSeconds: number;
    methods: string[];
    origins: string[];
    responseHeaders: string[];
}

export interface GetStorageBucketCustomPlacementConfig {
    dataLocations: string[];
}

export interface GetStorageBucketEncryption {
    defaultKmsKeyName: string;
}

export interface GetStorageBucketLifecycleRule {
    actions: outputs.GetStorageBucketLifecycleRuleAction[];
    conditions: outputs.GetStorageBucketLifecycleRuleCondition[];
}

export interface GetStorageBucketLifecycleRuleAction {
    storageClass: string;
    type: string;
}

export interface GetStorageBucketLifecycleRuleCondition {
    age: number;
    createdBefore: string;
    customTimeBefore: string;
    daysSinceCustomTime: number;
    daysSinceNoncurrentTime: number;
    matchesPrefixes: string[];
    matchesStorageClasses: string[];
    matchesSuffixes: string[];
    noncurrentTimeBefore: string;
    numNewerVersions: number;
    sendAgeIfZero: boolean;
    sendDaysSinceCustomTimeIfZero: boolean;
    sendDaysSinceNoncurrentTimeIfZero: boolean;
    sendNumNewerVersionsIfZero: boolean;
    withState: string;
}

export interface GetStorageBucketLogging {
    logBucket: string;
    logObjectPrefix: string;
}

export interface GetStorageBucketObjectContentCustomerEncryption {
    encryptionAlgorithm: string;
    encryptionKey: string;
}

export interface GetStorageBucketObjectContentRetention {
    mode: string;
    retainUntilTime: string;
}

export interface GetStorageBucketObjectCustomerEncryption {
    encryptionAlgorithm: string;
    encryptionKey: string;
}

export interface GetStorageBucketObjectRetention {
    mode: string;
    retainUntilTime: string;
}

export interface GetStorageBucketObjectsBucketObject {
    contentType: string;
    mediaLink: string;
    name: string;
    selfLink: string;
    storageClass: string;
}

export interface GetStorageBucketRetentionPolicy {
    isLocked: boolean;
    retentionPeriod: number;
}

export interface GetStorageBucketSoftDeletePolicy {
    effectiveTime: string;
    retentionDurationSeconds: number;
}

export interface GetStorageBucketVersioning {
    enabled: boolean;
}

export interface GetStorageBucketWebsite {
    mainPageSuffix: string;
    notFoundPage: string;
}

export interface GetStorageBucketsBucket {
    labels: {[key: string]: string};
    location: string;
    name: string;
    selfLink: string;
    storageClass: string;
}

export interface GetTagsTagKeysKey {
    createTime: string;
    description: string;
    name: string;
    namespacedName: string;
    parent: string;
    purpose: string;
    purposeData: {[key: string]: string};
    shortName: string;
    updateTime: string;
}

export interface GetTagsTagValuesValue {
    createTime: string;
    description: string;
    name: string;
    namespacedName: string;
    parent: string;
    shortName: string;
    updateTime: string;
}

export interface GetVertexAiIndexDeployedIndex {
    deployedIndexId: string;
    indexEndpoint: string;
}

export interface GetVertexAiIndexIndexStat {
    shardsCount: number;
    vectorsCount: string;
}

export interface GetVertexAiIndexMetadata {
    configs: outputs.GetVertexAiIndexMetadataConfig[];
    contentsDeltaUri: string;
    isCompleteOverwrite: boolean;
}

export interface GetVertexAiIndexMetadataConfig {
    algorithmConfigs: outputs.GetVertexAiIndexMetadataConfigAlgorithmConfig[];
    approximateNeighborsCount: number;
    dimensions: number;
    distanceMeasureType: string;
    featureNormType: string;
    shardSize: string;
}

export interface GetVertexAiIndexMetadataConfigAlgorithmConfig {
    bruteForceConfigs: outputs.GetVertexAiIndexMetadataConfigAlgorithmConfigBruteForceConfig[];
    treeAhConfigs: outputs.GetVertexAiIndexMetadataConfigAlgorithmConfigTreeAhConfig[];
}

export interface GetVertexAiIndexMetadataConfigAlgorithmConfigBruteForceConfig {
}

export interface GetVertexAiIndexMetadataConfigAlgorithmConfigTreeAhConfig {
    leafNodeEmbeddingCount: number;
    leafNodesToSearchPercent: number;
}

export interface GetVmwareengineClusterNodeTypeConfig {
    customCoreCount: number;
    nodeCount: number;
    nodeTypeId: string;
}

export interface GetVmwareengineExternalAccessRuleDestinationIpRange {
    externalAddress: string;
    ipAddressRange: string;
}

export interface GetVmwareengineExternalAccessRuleSourceIpRange {
    ipAddress: string;
    ipAddressRange: string;
}

export interface GetVmwareengineNetworkPolicyExternalIp {
    enabled: boolean;
    state: string;
}

export interface GetVmwareengineNetworkPolicyInternetAccess {
    enabled: boolean;
    state: string;
}

export interface GetVmwareengineNetworkVpcNetwork {
    network: string;
    type: string;
}

export interface GetVmwareenginePrivateCloudHcx {
    fqdn: string;
    internalIp: string;
    state: string;
    version: string;
}

export interface GetVmwareenginePrivateCloudManagementCluster {
    clusterId: string;
    nodeTypeConfigs: outputs.GetVmwareenginePrivateCloudManagementClusterNodeTypeConfig[];
    stretchedClusterConfigs: outputs.GetVmwareenginePrivateCloudManagementClusterStretchedClusterConfig[];
}

export interface GetVmwareenginePrivateCloudManagementClusterNodeTypeConfig {
    customCoreCount: number;
    nodeCount: number;
    nodeTypeId: string;
}

export interface GetVmwareenginePrivateCloudManagementClusterStretchedClusterConfig {
    preferredLocation: string;
    secondaryLocation: string;
}

export interface GetVmwareenginePrivateCloudNetworkConfig {
    dnsServerIp: string;
    managementCidr: string;
    managementIpAddressLayoutVersion: number;
    vmwareEngineNetwork: string;
    vmwareEngineNetworkCanonical: string;
}

export interface GetVmwareenginePrivateCloudNsx {
    fqdn: string;
    internalIp: string;
    state: string;
    version: string;
}

export interface GetVmwareenginePrivateCloudVcenter {
    fqdn: string;
    internalIp: string;
    state: string;
    version: string;
}

export interface GetVmwareengineSubnetDhcpAddressRange {
    firstAddress: string;
    lastAddress: string;
}

export interface GetVpcAccessConnectorSubnet {
    name: string;
    projectId: string;
}

export interface GkeBackupBackupPlanBackupConfig {
    /**
     * If True, include all namespaced resources.
     */
    allNamespaces?: boolean;
    /**
     * This defines a customer managed encryption key that will be used to encrypt the "config"
     * portion (the Kubernetes resources) of Backups created via this plan.
     */
    encryptionKey?: outputs.GkeBackupBackupPlanBackupConfigEncryptionKey;
    /**
     * This flag specifies whether Kubernetes Secret resources should be included
     * when they fall into the scope of Backups.
     */
    includeSecrets: boolean;
    /**
     * This flag specifies whether volume data should be backed up when PVCs are
     * included in the scope of a Backup.
     */
    includeVolumeData: boolean;
    /**
     * This flag specifies whether Backups will not fail when
     * Backup for GKE detects Kubernetes configuration that is
     * non-standard or requires additional setup to restore.
     */
    permissiveMode?: boolean;
    /**
     * A list of namespaced Kubernetes Resources.
     */
    selectedApplications?: outputs.GkeBackupBackupPlanBackupConfigSelectedApplications;
    /**
     * If set, include just the resources in the listed namespaces.
     */
    selectedNamespaces?: outputs.GkeBackupBackupPlanBackupConfigSelectedNamespaces;
}

export interface GkeBackupBackupPlanBackupConfigEncryptionKey {
    /**
     * Google Cloud KMS encryption key. Format: projects/*&#47;locations/*&#47;keyRings/*&#47;cryptoKeys/*
     */
    gcpKmsEncryptionKey: string;
}

export interface GkeBackupBackupPlanBackupConfigSelectedApplications {
    /**
     * A list of namespaced Kubernetes resources.
     */
    namespacedNames: outputs.GkeBackupBackupPlanBackupConfigSelectedApplicationsNamespacedName[];
}

export interface GkeBackupBackupPlanBackupConfigSelectedApplicationsNamespacedName {
    /**
     * The name of a Kubernetes Resource.
     */
    name: string;
    /**
     * The namespace of a Kubernetes Resource.
     */
    namespace: string;
}

export interface GkeBackupBackupPlanBackupConfigSelectedNamespaces {
    /**
     * A list of Kubernetes Namespaces.
     */
    namespaces: string[];
}

export interface GkeBackupBackupPlanBackupSchedule {
    /**
     * A standard cron string that defines a repeating schedule for
     * creating Backups via this BackupPlan.
     * This is mutually exclusive with the rpoConfig field since at most one
     * schedule can be defined for a BackupPlan.
     * If this is defined, then backupRetainDays must also be defined.
     */
    cronSchedule?: string;
    /**
     * This flag denotes whether automatic Backup creation is paused for this BackupPlan.
     */
    paused: boolean;
    /**
     * Defines the RPO schedule configuration for this BackupPlan. This is mutually
     * exclusive with the cronSchedule field since at most one schedule can be defined
     * for a BackupPLan. If this is defined, then backupRetainDays must also be defined.
     */
    rpoConfig?: outputs.GkeBackupBackupPlanBackupScheduleRpoConfig;
}

export interface GkeBackupBackupPlanBackupScheduleRpoConfig {
    /**
     * User specified time windows during which backup can NOT happen for this BackupPlan.
     * Backups should start and finish outside of any given exclusion window. Note: backup
     * jobs will be scheduled to start and finish outside the duration of the window as
     * much as possible, but running jobs will not get canceled when it runs into the window.
     * All the time and date values in exclusionWindows entry in the API are in UTC. We
     * only allow <=1 recurrence (daily or weekly) exclusion window for a BackupPlan while no
     * restriction on number of single occurrence windows.
     */
    exclusionWindows?: outputs.GkeBackupBackupPlanBackupScheduleRpoConfigExclusionWindow[];
    /**
     * Defines the target RPO for the BackupPlan in minutes, which means the target
     * maximum data loss in time that is acceptable for this BackupPlan. This must be
     * at least 60, i.e., 1 hour, and at most 86400, i.e., 60 days.
     */
    targetRpoMinutes: number;
}

export interface GkeBackupBackupPlanBackupScheduleRpoConfigExclusionWindow {
    /**
     * The exclusion window occurs every day if set to "True".
     * Specifying this field to "False" is an error.
     * Only one of singleOccurrenceDate, daily and daysOfWeek may be set.
     */
    daily?: boolean;
    /**
     * The exclusion window occurs on these days of each week in UTC.
     * Only one of singleOccurrenceDate, daily and daysOfWeek may be set.
     */
    daysOfWeek?: outputs.GkeBackupBackupPlanBackupScheduleRpoConfigExclusionWindowDaysOfWeek;
    /**
     * Specifies duration of the window in seconds with up to nine fractional digits,
     * terminated by 's'. Example: "3.5s". Restrictions for duration based on the
     * recurrence type to allow some time for backup to happen:
     *   - single_occurrence_date:  no restriction
     *   - daily window: duration < 24 hours
     *   - weekly window:
     *     - days of week includes all seven days of a week: duration < 24 hours
     *     - all other weekly window: duration < 168 hours (i.e., 24 * 7 hours)
     */
    duration: string;
    /**
     * No recurrence. The exclusion window occurs only once and on this date in UTC.
     * Only one of singleOccurrenceDate, daily and daysOfWeek may be set.
     */
    singleOccurrenceDate?: outputs.GkeBackupBackupPlanBackupScheduleRpoConfigExclusionWindowSingleOccurrenceDate;
    /**
     * Specifies the start time of the window using time of the day in UTC.
     */
    startTime: outputs.GkeBackupBackupPlanBackupScheduleRpoConfigExclusionWindowStartTime;
}

export interface GkeBackupBackupPlanBackupScheduleRpoConfigExclusionWindowDaysOfWeek {
    /**
     * A list of days of week. Possible values: ["MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    daysOfWeeks?: string[];
}

export interface GkeBackupBackupPlanBackupScheduleRpoConfigExclusionWindowSingleOccurrenceDate {
    /**
     * Day of a month.
     */
    day?: number;
    /**
     * Month of a year.
     */
    month?: number;
    /**
     * Year of the date.
     */
    year?: number;
}

export interface GkeBackupBackupPlanBackupScheduleRpoConfigExclusionWindowStartTime {
    /**
     * Hours of day in 24 hour format.
     */
    hours?: number;
    /**
     * Minutes of hour of day.
     */
    minutes?: number;
    /**
     * Fractions of seconds in nanoseconds.
     */
    nanos?: number;
    /**
     * Seconds of minutes of the time.
     */
    seconds?: number;
}

export interface GkeBackupBackupPlanIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface GkeBackupBackupPlanIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface GkeBackupBackupPlanRetentionPolicy {
    /**
     * Minimum age for a Backup created via this BackupPlan (in days).
     * Must be an integer value between 0-90 (inclusive).
     * A Backup created under this BackupPlan will not be deletable
     * until it reaches Backup's (create time + backup_delete_lock_days).
     * Updating this field of a BackupPlan does not affect existing Backups.
     * Backups created after a successful update will inherit this new value.
     */
    backupDeleteLockDays: number;
    /**
     * The default maximum age of a Backup created via this BackupPlan.
     * This field MUST be an integer value >= 0 and <= 365. If specified,
     * a Backup created under this BackupPlan will be automatically deleted
     * after its age reaches (createTime + backupRetainDays).
     * If not specified, Backups created under this BackupPlan will NOT be
     * subject to automatic deletion. Updating this field does NOT affect
     * existing Backups under it. Backups created AFTER a successful update
     * will automatically pick up the new value.
     * NOTE: backupRetainDays must be >= backupDeleteLockDays.
     * If cronSchedule is defined, then this must be <= 360 * the creation interval.
     * If rpo_config is defined, then this must be
     * <= 360 * targetRpoMinutes/(1440minutes/day)
     */
    backupRetainDays: number;
    /**
     * This flag denotes whether the retention policy of this BackupPlan is locked.
     * If set to True, no further update is allowed on this policy, including
     * the locked field itself.
     */
    locked: boolean;
}

export interface GkeBackupBackupPlanTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface GkeBackupRestorePlanIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface GkeBackupRestorePlanIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface GkeBackupRestorePlanRestoreConfig {
    /**
     * If True, restore all namespaced resources in the Backup.
     * Setting this field to False will result in an error.
     */
    allNamespaces?: boolean;
    /**
     * Defines the behavior for handling the situation where cluster-scoped resources
     * being restored already exist in the target cluster.
     * This MUST be set to a value other than 'CLUSTER_RESOURCE_CONFLICT_POLICY_UNSPECIFIED'
     * if 'clusterResourceRestoreScope' is anyting other than 'noGroupKinds'.
     * See https://cloud.google.com/kubernetes-engine/docs/add-on/backup-for-gke/reference/rest/v1/RestoreConfig#clusterresourceconflictpolicy
     * for more information on each policy option. Possible values: ["USE_EXISTING_VERSION", "USE_BACKUP_VERSION"]
     */
    clusterResourceConflictPolicy?: string;
    /**
     * Identifies the cluster-scoped resources to restore from the Backup.
     */
    clusterResourceRestoreScope?: outputs.GkeBackupRestorePlanRestoreConfigClusterResourceRestoreScope;
    /**
     * A list of selected namespaces excluded from restoration.
     * All namespaces except those in this list will be restored.
     */
    excludedNamespaces?: outputs.GkeBackupRestorePlanRestoreConfigExcludedNamespaces;
    /**
     * Defines the behavior for handling the situation where sets of namespaced resources
     * being restored already exist in the target cluster.
     * This MUST be set to a value other than 'NAMESPACED_RESOURCE_RESTORE_MODE_UNSPECIFIED'
     * if the 'namespacedResourceRestoreScope' is anything other than 'noNamespaces'.
     * See https://cloud.google.com/kubernetes-engine/docs/add-on/backup-for-gke/reference/rest/v1/RestoreConfig#namespacedresourcerestoremode
     * for more information on each mode. Possible values: ["DELETE_AND_RESTORE", "FAIL_ON_CONFLICT", "MERGE_SKIP_ON_CONFLICT", "MERGE_REPLACE_VOLUME_ON_CONFLICT", "MERGE_REPLACE_ON_CONFLICT"]
     */
    namespacedResourceRestoreMode?: string;
    /**
     * Do not restore any namespaced resources if set to "True".
     * Specifying this field to "False" is not allowed.
     */
    noNamespaces?: boolean;
    /**
     * It contains custom ordering to use on a Restore.
     */
    restoreOrder?: outputs.GkeBackupRestorePlanRestoreConfigRestoreOrder;
    /**
     * A list of selected ProtectedApplications to restore.
     * The listed ProtectedApplications and all the resources
     * to which they refer will be restored.
     */
    selectedApplications?: outputs.GkeBackupRestorePlanRestoreConfigSelectedApplications;
    /**
     * A list of selected namespaces to restore from the Backup.
     * The listed Namespaces and all resources contained in them will be restored.
     */
    selectedNamespaces?: outputs.GkeBackupRestorePlanRestoreConfigSelectedNamespaces;
    /**
     * A list of transformation rules to be applied against Kubernetes
     * resources as they are selected for restoration from a Backup.
     * Rules are executed in order defined - this order matters,
     * as changes made by a rule may impact the filtering logic of subsequent
     * rules. An empty list means no transformation will occur.
     */
    transformationRules?: outputs.GkeBackupRestorePlanRestoreConfigTransformationRule[];
    /**
     * Specifies the mechanism to be used to restore volume data.
     * This should be set to a value other than 'NAMESPACED_RESOURCE_RESTORE_MODE_UNSPECIFIED'
     * if the 'namespacedResourceRestoreScope' is anything other than 'noNamespaces'.
     * If not specified, it will be treated as 'NO_VOLUME_DATA_RESTORATION'.
     * See https://cloud.google.com/kubernetes-engine/docs/add-on/backup-for-gke/reference/rest/v1/RestoreConfig#VolumeDataRestorePolicy
     * for more information on each policy option. Possible values: ["RESTORE_VOLUME_DATA_FROM_BACKUP", "REUSE_VOLUME_HANDLE_FROM_BACKUP", "NO_VOLUME_DATA_RESTORATION"]
     */
    volumeDataRestorePolicy?: string;
    /**
     * A table that binds volumes by their scope to a restore policy. Bindings
     * must have a unique scope. Any volumes not scoped in the bindings are
     * subject to the policy defined in volume_data_restore_policy.
     */
    volumeDataRestorePolicyBindings?: outputs.GkeBackupRestorePlanRestoreConfigVolumeDataRestorePolicyBinding[];
}

export interface GkeBackupRestorePlanRestoreConfigClusterResourceRestoreScope {
    /**
     * If True, all valid cluster-scoped resources will be restored.
     * Mutually exclusive to any other field in 'clusterResourceRestoreScope'.
     */
    allGroupKinds?: boolean;
    /**
     * A list of cluster-scoped resource group kinds to NOT restore from the backup.
     * If specified, all valid cluster-scoped resources will be restored except
     * for those specified in the list.
     * Mutually exclusive to any other field in 'clusterResourceRestoreScope'.
     */
    excludedGroupKinds?: outputs.GkeBackupRestorePlanRestoreConfigClusterResourceRestoreScopeExcludedGroupKind[];
    /**
     * If True, no cluster-scoped resources will be restored.
     * Mutually exclusive to any other field in 'clusterResourceRestoreScope'.
     */
    noGroupKinds?: boolean;
    /**
     * A list of cluster-scoped resource group kinds to restore from the backup.
     * If specified, only the selected resources will be restored.
     * Mutually exclusive to any other field in the 'clusterResourceRestoreScope'.
     */
    selectedGroupKinds?: outputs.GkeBackupRestorePlanRestoreConfigClusterResourceRestoreScopeSelectedGroupKind[];
}

export interface GkeBackupRestorePlanRestoreConfigClusterResourceRestoreScopeExcludedGroupKind {
    /**
     * API Group string of a Kubernetes resource, e.g.
     * "apiextensions.k8s.io", "storage.k8s.io", etc.
     * Use empty string for core group.
     */
    resourceGroup?: string;
    /**
     * Kind of a Kubernetes resource, e.g.
     * "CustomResourceDefinition", "StorageClass", etc.
     */
    resourceKind?: string;
}

export interface GkeBackupRestorePlanRestoreConfigClusterResourceRestoreScopeSelectedGroupKind {
    /**
     * API Group string of a Kubernetes resource, e.g.
     * "apiextensions.k8s.io", "storage.k8s.io", etc.
     * Use empty string for core group.
     */
    resourceGroup?: string;
    /**
     * Kind of a Kubernetes resource, e.g.
     * "CustomResourceDefinition", "StorageClass", etc.
     */
    resourceKind?: string;
}

export interface GkeBackupRestorePlanRestoreConfigExcludedNamespaces {
    /**
     * A list of Kubernetes Namespaces.
     */
    namespaces: string[];
}

export interface GkeBackupRestorePlanRestoreConfigRestoreOrder {
    /**
     * A list of group kind dependency pairs
     * that is used by Backup for GKE to
     * generate a group kind restore order.
     */
    groupKindDependencies: outputs.GkeBackupRestorePlanRestoreConfigRestoreOrderGroupKindDependency[];
}

export interface GkeBackupRestorePlanRestoreConfigRestoreOrderGroupKindDependency {
    /**
     * The requiring group kind requires that the satisfying
     * group kind be restored first.
     */
    requiring: outputs.GkeBackupRestorePlanRestoreConfigRestoreOrderGroupKindDependencyRequiring;
    /**
     * The satisfying group kind must be restored first
     * in order to satisfy the dependency.
     */
    satisfying: outputs.GkeBackupRestorePlanRestoreConfigRestoreOrderGroupKindDependencySatisfying;
}

export interface GkeBackupRestorePlanRestoreConfigRestoreOrderGroupKindDependencyRequiring {
    /**
     * API Group of a Kubernetes resource, e.g.
     * "apiextensions.k8s.io", "storage.k8s.io", etc.
     * Use empty string for core group.
     */
    resourceGroup?: string;
    /**
     * Kind of a Kubernetes resource, e.g.
     * "CustomResourceDefinition", "StorageClass", etc.
     */
    resourceKind?: string;
}

export interface GkeBackupRestorePlanRestoreConfigRestoreOrderGroupKindDependencySatisfying {
    /**
     * API Group of a Kubernetes resource, e.g.
     * "apiextensions.k8s.io", "storage.k8s.io", etc.
     * Use empty string for core group.
     */
    resourceGroup?: string;
    /**
     * Kind of a Kubernetes resource, e.g.
     * "CustomResourceDefinition", "StorageClass", etc.
     */
    resourceKind?: string;
}

export interface GkeBackupRestorePlanRestoreConfigSelectedApplications {
    /**
     * A list of namespaced Kubernetes resources.
     */
    namespacedNames: outputs.GkeBackupRestorePlanRestoreConfigSelectedApplicationsNamespacedName[];
}

export interface GkeBackupRestorePlanRestoreConfigSelectedApplicationsNamespacedName {
    /**
     * The name of a Kubernetes Resource.
     */
    name: string;
    /**
     * The namespace of a Kubernetes Resource.
     */
    namespace: string;
}

export interface GkeBackupRestorePlanRestoreConfigSelectedNamespaces {
    /**
     * A list of Kubernetes Namespaces.
     */
    namespaces: string[];
}

export interface GkeBackupRestorePlanRestoreConfigTransformationRule {
    /**
     * The description is a user specified string description
     * of the transformation rule.
     */
    description?: string;
    /**
     * A list of transformation rule actions to take against candidate
     * resources. Actions are executed in order defined - this order
     * matters, as they could potentially interfere with each other and
     * the first operation could affect the outcome of the second operation.
     */
    fieldActions: outputs.GkeBackupRestorePlanRestoreConfigTransformationRuleFieldAction[];
    /**
     * This field is used to specify a set of fields that should be used to
     * determine which resources in backup should be acted upon by the
     * supplied transformation rule actions, and this will ensure that only
     * specific resources are affected by transformation rule actions.
     */
    resourceFilter?: outputs.GkeBackupRestorePlanRestoreConfigTransformationRuleResourceFilter;
}

export interface GkeBackupRestorePlanRestoreConfigTransformationRuleFieldAction {
    /**
     * A string containing a JSON Pointer value that references the
     * location in the target document to move the value from.
     */
    fromPath?: string;
    /**
     * Specifies the operation to perform. Possible values: ["REMOVE", "MOVE", "COPY", "ADD", "TEST", "REPLACE"]
     */
    op: string;
    /**
     * A string containing a JSON-Pointer value that references a
     * location within the target document where the operation is performed.
     */
    path?: string;
    /**
     * A string that specifies the desired value in string format
     * to use for transformation.
     */
    value?: string;
}

export interface GkeBackupRestorePlanRestoreConfigTransformationRuleResourceFilter {
    /**
     * (Filtering parameter) Any resource subject to transformation must
     * belong to one of the listed "types". If this field is not provided,
     * no type filtering will be performed
     * (all resources of all types matching previous filtering parameters
     * will be candidates for transformation).
     */
    groupKinds?: outputs.GkeBackupRestorePlanRestoreConfigTransformationRuleResourceFilterGroupKind[];
    /**
     * This is a JSONPath expression that matches specific fields of
     * candidate resources and it operates as a filtering parameter
     * (resources that are not matched with this expression will not
     * be candidates for transformation).
     */
    jsonPath?: string;
    /**
     * (Filtering parameter) Any resource subject to transformation must
     * be contained within one of the listed Kubernetes Namespace in the
     * Backup. If this field is not provided, no namespace filtering will
     * be performed (all resources in all Namespaces, including all
     * cluster-scoped resources, will be candidates for transformation).
     * To mix cluster-scoped and namespaced resources in the same rule,
     * use an empty string ("") as one of the target namespaces.
     */
    namespaces?: string[];
}

export interface GkeBackupRestorePlanRestoreConfigTransformationRuleResourceFilterGroupKind {
    /**
     * API Group string of a Kubernetes resource, e.g.
     * "apiextensions.k8s.io", "storage.k8s.io", etc.
     * Use empty string for core group.
     */
    resourceGroup?: string;
    /**
     * Kind of a Kubernetes resource, e.g.
     * "CustomResourceDefinition", "StorageClass", etc.
     */
    resourceKind?: string;
}

export interface GkeBackupRestorePlanRestoreConfigVolumeDataRestorePolicyBinding {
    /**
     * Specifies the mechanism to be used to restore this volume data.
     * See https://cloud.google.com/kubernetes-engine/docs/add-on/backup-for-gke/reference/rest/v1/RestoreConfig#VolumeDataRestorePolicy
     * for more information on each policy option. Possible values: ["RESTORE_VOLUME_DATA_FROM_BACKUP", "REUSE_VOLUME_HANDLE_FROM_BACKUP", "NO_VOLUME_DATA_RESTORATION"]
     */
    policy: string;
    /**
     * The volume type, as determined by the PVC's
     * bound PV, to apply the policy to. Possible values: ["GCE_PERSISTENT_DISK"]
     */
    volumeType: string;
}

export interface GkeBackupRestorePlanTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface GkeHubFeatureFleetDefaultMemberConfig {
    /**
     * Config Management spec
     */
    configmanagement?: outputs.GkeHubFeatureFleetDefaultMemberConfigConfigmanagement;
    /**
     * Service Mesh spec
     */
    mesh?: outputs.GkeHubFeatureFleetDefaultMemberConfigMesh;
    /**
     * Policy Controller spec
     */
    policycontroller?: outputs.GkeHubFeatureFleetDefaultMemberConfigPolicycontroller;
}

export interface GkeHubFeatureFleetDefaultMemberConfigConfigmanagement {
    /**
     * ConfigSync configuration for the cluster
     */
    configSync?: outputs.GkeHubFeatureFleetDefaultMemberConfigConfigmanagementConfigSync;
    /**
     * Set this field to MANAGEMENT_AUTOMATIC to enable Config Sync auto-upgrades, and set this field to MANAGEMENT_MANUAL or MANAGEMENT_UNSPECIFIED to disable Config Sync auto-upgrades. Possible values: ["MANAGEMENT_UNSPECIFIED", "MANAGEMENT_AUTOMATIC", "MANAGEMENT_MANUAL"]
     */
    management?: string;
    /**
     * Version of ACM installed
     */
    version?: string;
}

export interface GkeHubFeatureFleetDefaultMemberConfigConfigmanagementConfigSync {
    /**
     * Enables the installation of ConfigSync. If set to true, ConfigSync resources will be created and the other ConfigSync fields will be applied if exist. If set to false, all other ConfigSync fields will be ignored, ConfigSync resources will be deleted. If omitted, ConfigSync resources will be managed depends on the presence of the git or oci field.
     */
    enabled?: boolean;
    /**
     * Git repo configuration for the cluster
     */
    git?: outputs.GkeHubFeatureFleetDefaultMemberConfigConfigmanagementConfigSyncGit;
    /**
     * OCI repo configuration for the cluster
     */
    oci?: outputs.GkeHubFeatureFleetDefaultMemberConfigConfigmanagementConfigSyncOci;
    /**
     * Set to true to enable the Config Sync admission webhook to prevent drifts. If set to 'false', disables the Config Sync admission webhook and does not prevent drifts.
     */
    preventDrift?: boolean;
    /**
     * Specifies whether the Config Sync Repo is in hierarchical or unstructured mode
     */
    sourceFormat?: string;
}

export interface GkeHubFeatureFleetDefaultMemberConfigConfigmanagementConfigSyncGit {
    /**
     * The Google Cloud Service Account Email used for auth when secretType is gcpServiceAccount
     */
    gcpServiceAccountEmail?: string;
    /**
     * URL for the HTTPS Proxy to be used when communicating with the Git repo
     */
    httpsProxy?: string;
    /**
     * The path within the Git repository that represents the top level of the repo to sync
     */
    policyDir?: string;
    /**
     * Type of secret configured for access to the Git repo
     */
    secretType: string;
    /**
     * The branch of the repository to sync from. Default: master
     */
    syncBranch?: string;
    /**
     * The URL of the Git repository to use as the source of truth
     */
    syncRepo?: string;
    /**
     * Git revision (tag or hash) to check out. Default HEAD
     */
    syncRev?: string;
    /**
     * Period in seconds between consecutive syncs. Default: 15
     */
    syncWaitSecs?: string;
}

export interface GkeHubFeatureFleetDefaultMemberConfigConfigmanagementConfigSyncOci {
    /**
     * The Google Cloud Service Account Email used for auth when secretType is gcpServiceAccount
     */
    gcpServiceAccountEmail?: string;
    /**
     * The absolute path of the directory that contains the local resources. Default: the root directory of the image
     */
    policyDir?: string;
    /**
     * Type of secret configured for access to the Git repo
     */
    secretType: string;
    /**
     * The OCI image repository URL for the package to sync from
     */
    syncRepo?: string;
    /**
     * Period in seconds between consecutive syncs. Default: 15
     */
    syncWaitSecs?: string;
    /**
     * Version of ACM installed
     *
     * @deprecated Deprecated
     */
    version?: string;
}

export interface GkeHubFeatureFleetDefaultMemberConfigMesh {
    /**
     * Whether to automatically manage Service Mesh Possible values: ["MANAGEMENT_UNSPECIFIED", "MANAGEMENT_AUTOMATIC", "MANAGEMENT_MANUAL"]
     */
    management: string;
}

export interface GkeHubFeatureFleetDefaultMemberConfigPolicycontroller {
    /**
     * Configuration of Policy Controller
     */
    policyControllerHubConfig: outputs.GkeHubFeatureFleetDefaultMemberConfigPolicycontrollerPolicyControllerHubConfig;
    /**
     * Configures the version of Policy Controller
     */
    version: string;
}

export interface GkeHubFeatureFleetDefaultMemberConfigPolicycontrollerPolicyControllerHubConfig {
    /**
     * Interval for Policy Controller Audit scans (in seconds). When set to 0, this disables audit functionality altogether.
     */
    auditIntervalSeconds?: number;
    /**
     * The maximum number of audit violations to be stored in a constraint. If not set, the internal default of 20 will be used.
     */
    constraintViolationLimit?: number;
    /**
     * Map of deployment configs to deployments ("admission", "audit", "mutation").
     */
    deploymentConfigs?: outputs.GkeHubFeatureFleetDefaultMemberConfigPolicycontrollerPolicyControllerHubConfigDeploymentConfig[];
    /**
     * The set of namespaces that are excluded from Policy Controller checks. Namespaces do not need to currently exist on the cluster.
     */
    exemptableNamespaces?: string[];
    /**
     * Configures the mode of the Policy Controller installation Possible values: ["INSTALL_SPEC_UNSPECIFIED", "INSTALL_SPEC_NOT_INSTALLED", "INSTALL_SPEC_ENABLED", "INSTALL_SPEC_SUSPENDED", "INSTALL_SPEC_DETACHED"]
     */
    installSpec: string;
    /**
     * Logs all denies and dry run failures.
     */
    logDeniesEnabled?: boolean;
    /**
     * Monitoring specifies the configuration of monitoring Policy Controller.
     */
    monitoring?: outputs.GkeHubFeatureFleetDefaultMemberConfigPolicycontrollerPolicyControllerHubConfigMonitoring;
    /**
     * Enables the ability to mutate resources using Policy Controller.
     */
    mutationEnabled?: boolean;
    /**
     * Specifies the desired policy content on the cluster.
     */
    policyContent?: outputs.GkeHubFeatureFleetDefaultMemberConfigPolicycontrollerPolicyControllerHubConfigPolicyContent;
    /**
     * Enables the ability to use Constraint Templates that reference to objects other than the object currently being evaluated.
     */
    referentialRulesEnabled?: boolean;
}

export interface GkeHubFeatureFleetDefaultMemberConfigPolicycontrollerPolicyControllerHubConfigDeploymentConfig {
    component: string;
    /**
     * Container resource requirements.
     */
    containerResources?: outputs.GkeHubFeatureFleetDefaultMemberConfigPolicycontrollerPolicyControllerHubConfigDeploymentConfigContainerResources;
    /**
     * Pod affinity configuration. Possible values: ["AFFINITY_UNSPECIFIED", "NO_AFFINITY", "ANTI_AFFINITY"]
     */
    podAffinity: string;
    /**
     * Pod tolerations of node taints.
     */
    podTolerations?: outputs.GkeHubFeatureFleetDefaultMemberConfigPolicycontrollerPolicyControllerHubConfigDeploymentConfigPodToleration[];
    /**
     * Pod replica count.
     */
    replicaCount: number;
}

export interface GkeHubFeatureFleetDefaultMemberConfigPolicycontrollerPolicyControllerHubConfigDeploymentConfigContainerResources {
    /**
     * Limits describes the maximum amount of compute resources allowed for use by the running container.
     */
    limits?: outputs.GkeHubFeatureFleetDefaultMemberConfigPolicycontrollerPolicyControllerHubConfigDeploymentConfigContainerResourcesLimits;
    /**
     * Requests describes the amount of compute resources reserved for the container by the kube-scheduler.
     */
    requests?: outputs.GkeHubFeatureFleetDefaultMemberConfigPolicycontrollerPolicyControllerHubConfigDeploymentConfigContainerResourcesRequests;
}

export interface GkeHubFeatureFleetDefaultMemberConfigPolicycontrollerPolicyControllerHubConfigDeploymentConfigContainerResourcesLimits {
    /**
     * CPU requirement expressed in Kubernetes resource units.
     */
    cpu?: string;
    /**
     * Memory requirement expressed in Kubernetes resource units.
     */
    memory?: string;
}

export interface GkeHubFeatureFleetDefaultMemberConfigPolicycontrollerPolicyControllerHubConfigDeploymentConfigContainerResourcesRequests {
    /**
     * CPU requirement expressed in Kubernetes resource units.
     */
    cpu?: string;
    /**
     * Memory requirement expressed in Kubernetes resource units.
     */
    memory?: string;
}

export interface GkeHubFeatureFleetDefaultMemberConfigPolicycontrollerPolicyControllerHubConfigDeploymentConfigPodToleration {
    /**
     * Matches a taint effect.
     */
    effect?: string;
    /**
     * Matches a taint key (not necessarily unique).
     */
    key?: string;
    /**
     * Matches a taint operator.
     */
    operator?: string;
    /**
     * Matches a taint value.
     */
    value?: string;
}

export interface GkeHubFeatureFleetDefaultMemberConfigPolicycontrollerPolicyControllerHubConfigMonitoring {
    /**
     * Specifies the list of backends Policy Controller will export to. An empty list would effectively disable metrics export. Possible values: ["MONITORING_BACKEND_UNSPECIFIED", "PROMETHEUS", "CLOUD_MONITORING"]
     */
    backends: string[];
}

export interface GkeHubFeatureFleetDefaultMemberConfigPolicycontrollerPolicyControllerHubConfigPolicyContent {
    /**
     * Configures which bundles to install and their corresponding install specs.
     */
    bundles?: outputs.GkeHubFeatureFleetDefaultMemberConfigPolicycontrollerPolicyControllerHubConfigPolicyContentBundle[];
    /**
     * Configures the installation of the Template Library.
     */
    templateLibrary?: outputs.GkeHubFeatureFleetDefaultMemberConfigPolicycontrollerPolicyControllerHubConfigPolicyContentTemplateLibrary;
}

export interface GkeHubFeatureFleetDefaultMemberConfigPolicycontrollerPolicyControllerHubConfigPolicyContentBundle {
    bundle: string;
    /**
     * The set of namespaces to be exempted from the bundle.
     */
    exemptedNamespaces?: string[];
}

export interface GkeHubFeatureFleetDefaultMemberConfigPolicycontrollerPolicyControllerHubConfigPolicyContentTemplateLibrary {
    /**
     * Configures the manner in which the template library is installed on the cluster. Possible values: ["INSTALATION_UNSPECIFIED", "NOT_INSTALLED", "ALL"]
     */
    installation?: string;
}

export interface GkeHubFeatureIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface GkeHubFeatureIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface GkeHubFeatureMembershipConfigmanagement {
    /**
     * **DEPRECATED** Binauthz configuration for the cluster. This field will be ignored and should not be set.
     */
    binauthz?: outputs.GkeHubFeatureMembershipConfigmanagementBinauthz;
    /**
     * Config Sync configuration for the cluster.
     */
    configSync?: outputs.GkeHubFeatureMembershipConfigmanagementConfigSync;
    /**
     * Hierarchy Controller configuration for the cluster.
     */
    hierarchyController?: outputs.GkeHubFeatureMembershipConfigmanagementHierarchyController;
    /**
     * Set this field to MANAGEMENT_AUTOMATIC to enable Config Sync auto-upgrades, and set this field to MANAGEMENT_MANUAL or MANAGEMENT_UNSPECIFIED to disable Config Sync auto-upgrades.
     */
    management: string;
    /**
     * **DEPRECATED** Configuring Policy Controller through the configmanagement feature is no longer recommended. Use the policycontroller feature instead.
     */
    policyController?: outputs.GkeHubFeatureMembershipConfigmanagementPolicyController;
    /**
     * Optional. Version of ACM to install. Defaults to the latest version.
     */
    version: string;
}

export interface GkeHubFeatureMembershipConfigmanagementBinauthz {
    /**
     * Whether binauthz is enabled in this cluster.
     */
    enabled?: boolean;
}

export interface GkeHubFeatureMembershipConfigmanagementConfigSync {
    /**
     * Enables the installation of ConfigSync. If set to true, ConfigSync resources will be created and the other ConfigSync fields will be applied if exist. If set to false, all other ConfigSync fields will be ignored, ConfigSync resources will be deleted. If omitted, ConfigSync resources will be managed depends on the presence of the git or oci field.
     */
    enabled?: boolean;
    git?: outputs.GkeHubFeatureMembershipConfigmanagementConfigSyncGit;
    /**
     * The Email of the Google Cloud Service Account (GSA) used for exporting Config Sync metrics to Cloud Monitoring. The GSA should have the Monitoring Metric Writer(roles/monitoring.metricWriter) IAM role. The Kubernetes ServiceAccount `default` in the namespace `config-management-monitoring` should be bound to the GSA.
     */
    metricsGcpServiceAccountEmail?: string;
    oci?: outputs.GkeHubFeatureMembershipConfigmanagementConfigSyncOci;
    /**
     * Set to true to enable the Config Sync admission webhook to prevent drifts. If set to `false`, disables the Config Sync admission webhook and does not prevent drifts.
     */
    preventDrift: boolean;
    /**
     * Specifies whether the Config Sync Repo is in "hierarchical" or "unstructured" mode.
     */
    sourceFormat?: string;
}

export interface GkeHubFeatureMembershipConfigmanagementConfigSyncGit {
    /**
     * The GCP Service Account Email used for auth when secretType is gcpServiceAccount.
     */
    gcpServiceAccountEmail?: string;
    /**
     * URL for the HTTPS proxy to be used when communicating with the Git repo.
     */
    httpsProxy?: string;
    /**
     * The path within the Git repository that represents the top level of the repo to sync. Default: the root directory of the repository.
     */
    policyDir?: string;
    /**
     * Type of secret configured for access to the Git repo. Must be one of ssh, cookiefile, gcenode, token, gcpserviceaccount or none. The validation of this is case-sensitive.
     */
    secretType?: string;
    /**
     * The branch of the repository to sync from. Default: master.
     */
    syncBranch?: string;
    /**
     * The URL of the Git repository to use as the source of truth.
     */
    syncRepo?: string;
    /**
     * Git revision (tag or hash) to check out. Default HEAD.
     */
    syncRev?: string;
    /**
     * Period in seconds between consecutive syncs. Default: 15.
     */
    syncWaitSecs?: string;
}

export interface GkeHubFeatureMembershipConfigmanagementConfigSyncOci {
    /**
     * The GCP Service Account Email used for auth when secret_type is gcpserviceaccount.
     */
    gcpServiceAccountEmail?: string;
    /**
     * The absolute path of the directory that contains the local resources. Default: the root directory of the image.
     */
    policyDir?: string;
    /**
     * Type of secret configured for access to the OCI Image. Must be one of gcenode, gcpserviceaccount or none. The validation of this is case-sensitive.
     */
    secretType?: string;
    /**
     * The OCI image repository URL for the package to sync from. e.g. LOCATION-docker.pkg.dev/PROJECT_ID/REPOSITORY_NAME/PACKAGE_NAME.
     */
    syncRepo?: string;
    /**
     * Period in seconds(int64 format) between consecutive syncs. Default: 15.
     */
    syncWaitSecs?: string;
}

export interface GkeHubFeatureMembershipConfigmanagementHierarchyController {
    /**
     * Whether hierarchical resource quota is enabled in this cluster.
     */
    enableHierarchicalResourceQuota?: boolean;
    /**
     * Whether pod tree labels are enabled in this cluster.
     */
    enablePodTreeLabels?: boolean;
    /**
     * Whether Hierarchy Controller is enabled in this cluster.
     */
    enabled?: boolean;
}

export interface GkeHubFeatureMembershipConfigmanagementPolicyController {
    /**
     * Sets the interval for Policy Controller Audit Scans (in seconds). When set to 0, this disables audit functionality altogether.
     */
    auditIntervalSeconds?: string;
    /**
     * Enables the installation of Policy Controller. If false, the rest of PolicyController fields take no effect.
     */
    enabled?: boolean;
    /**
     * The set of namespaces that are excluded from Policy Controller checks. Namespaces do not need to currently exist on the cluster.
     */
    exemptableNamespaces?: string[];
    /**
     * Logs all denies and dry run failures.
     */
    logDeniesEnabled?: boolean;
    /**
     * Specifies the backends Policy Controller should export metrics to. For example, to specify metrics should be exported to Cloud Monitoring and Prometheus, specify backends: ["cloudmonitoring", "prometheus"]. Default: ["cloudmonitoring", "prometheus"]
     */
    monitoring?: outputs.GkeHubFeatureMembershipConfigmanagementPolicyControllerMonitoring;
    /**
     * Enable or disable mutation in policy controller. If true, mutation CRDs, webhook and controller deployment will be deployed to the cluster.
     */
    mutationEnabled?: boolean;
    /**
     * Enables the ability to use Constraint Templates that reference to objects other than the object currently being evaluated.
     */
    referentialRulesEnabled?: boolean;
    /**
     * Installs the default template library along with Policy Controller.
     */
    templateLibraryInstalled?: boolean;
}

export interface GkeHubFeatureMembershipConfigmanagementPolicyControllerMonitoring {
    /**
     * Specifies the list of backends Policy Controller will export to. Specifying an empty value `[]` disables metrics export.
     */
    backends: string[];
}

export interface GkeHubFeatureMembershipMesh {
    /**
     * **DEPRECATED** Whether to automatically manage Service Mesh control planes. Possible values: CONTROL_PLANE_MANAGEMENT_UNSPECIFIED, AUTOMATIC, MANUAL
     *
     * @deprecated Deprecated
     */
    controlPlane?: string;
    /**
     * Whether to automatically manage Service Mesh. Possible values: MANAGEMENT_UNSPECIFIED, MANAGEMENT_AUTOMATIC, MANAGEMENT_MANUAL
     */
    management?: string;
}

export interface GkeHubFeatureMembershipPolicycontroller {
    /**
     * Policy Controller configuration for the cluster.
     */
    policyControllerHubConfig: outputs.GkeHubFeatureMembershipPolicycontrollerPolicyControllerHubConfig;
    /**
     * Optional. Version of Policy Controller to install. Defaults to the latest version.
     */
    version: string;
}

export interface GkeHubFeatureMembershipPolicycontrollerPolicyControllerHubConfig {
    /**
     * Sets the interval for Policy Controller Audit Scans (in seconds). When set to 0, this disables audit functionality altogether.
     */
    auditIntervalSeconds?: number;
    /**
     * The maximum number of audit violations to be stored in a constraint. If not set, the internal default of 20 will be used.
     */
    constraintViolationLimit?: number;
    /**
     * Map of deployment configs to deployments ("admission", "audit", "mutation").
     */
    deploymentConfigs?: outputs.GkeHubFeatureMembershipPolicycontrollerPolicyControllerHubConfigDeploymentConfig[];
    /**
     * The set of namespaces that are excluded from Policy Controller checks. Namespaces do not need to currently exist on the cluster.
     */
    exemptableNamespaces?: string[];
    /**
     * Configures the mode of the Policy Controller installation. Possible values: INSTALL_SPEC_UNSPECIFIED, INSTALL_SPEC_NOT_INSTALLED, INSTALL_SPEC_ENABLED, INSTALL_SPEC_SUSPENDED, INSTALL_SPEC_DETACHED
     */
    installSpec?: string;
    /**
     * Logs all denies and dry run failures.
     */
    logDeniesEnabled?: boolean;
    /**
     * Specifies the backends Policy Controller should export metrics to. For example, to specify metrics should be exported to Cloud Monitoring and Prometheus, specify backends: ["cloudmonitoring", "prometheus"]. Default: ["cloudmonitoring", "prometheus"]
     */
    monitoring?: outputs.GkeHubFeatureMembershipPolicycontrollerPolicyControllerHubConfigMonitoring;
    /**
     * Enables the ability to mutate resources using Policy Controller.
     */
    mutationEnabled?: boolean;
    /**
     * Specifies the desired policy content on the cluster.
     */
    policyContent?: outputs.GkeHubFeatureMembershipPolicycontrollerPolicyControllerHubConfigPolicyContent;
    /**
     * Enables the ability to use Constraint Templates that reference to objects other than the object currently being evaluated.
     */
    referentialRulesEnabled?: boolean;
}

export interface GkeHubFeatureMembershipPolicycontrollerPolicyControllerHubConfigDeploymentConfig {
    /**
     * The name for the key in the map for which this object is mapped to in the API
     */
    componentName: string;
    /**
     * Container resource requirements.
     */
    containerResources?: outputs.GkeHubFeatureMembershipPolicycontrollerPolicyControllerHubConfigDeploymentConfigContainerResources;
    /**
     * Pod affinity configuration. Possible values: AFFINITY_UNSPECIFIED, NO_AFFINITY, ANTI_AFFINITY
     */
    podAffinity?: string;
    /**
     * Pod tolerations of node taints.
     */
    podTolerations?: outputs.GkeHubFeatureMembershipPolicycontrollerPolicyControllerHubConfigDeploymentConfigPodToleration[];
    /**
     * Pod replica count.
     */
    replicaCount?: number;
}

export interface GkeHubFeatureMembershipPolicycontrollerPolicyControllerHubConfigDeploymentConfigContainerResources {
    /**
     * Limits describes the maximum amount of compute resources allowed for use by the running container.
     */
    limits?: outputs.GkeHubFeatureMembershipPolicycontrollerPolicyControllerHubConfigDeploymentConfigContainerResourcesLimits;
    /**
     * Requests describes the amount of compute resources reserved for the container by the kube-scheduler.
     */
    requests?: outputs.GkeHubFeatureMembershipPolicycontrollerPolicyControllerHubConfigDeploymentConfigContainerResourcesRequests;
}

export interface GkeHubFeatureMembershipPolicycontrollerPolicyControllerHubConfigDeploymentConfigContainerResourcesLimits {
    /**
     * CPU requirement expressed in Kubernetes resource units.
     */
    cpu?: string;
    /**
     * Memory requirement expressed in Kubernetes resource units.
     */
    memory?: string;
}

export interface GkeHubFeatureMembershipPolicycontrollerPolicyControllerHubConfigDeploymentConfigContainerResourcesRequests {
    /**
     * CPU requirement expressed in Kubernetes resource units.
     */
    cpu?: string;
    /**
     * Memory requirement expressed in Kubernetes resource units.
     */
    memory?: string;
}

export interface GkeHubFeatureMembershipPolicycontrollerPolicyControllerHubConfigDeploymentConfigPodToleration {
    /**
     * Matches a taint effect.
     */
    effect?: string;
    /**
     * Matches a taint key (not necessarily unique).
     */
    key?: string;
    /**
     * Matches a taint operator.
     */
    operator?: string;
    /**
     * Matches a taint value.
     */
    value?: string;
}

export interface GkeHubFeatureMembershipPolicycontrollerPolicyControllerHubConfigMonitoring {
    /**
     * Specifies the list of backends Policy Controller will export to. Specifying an empty value `[]` disables metrics export.
     */
    backends: string[];
}

export interface GkeHubFeatureMembershipPolicycontrollerPolicyControllerHubConfigPolicyContent {
    /**
     * map of bundle name to BundleInstallSpec. The bundle name maps to the `bundleName` key in the `policycontroller.gke.io/constraintData` annotation on a constraint.
     */
    bundles?: outputs.GkeHubFeatureMembershipPolicycontrollerPolicyControllerHubConfigPolicyContentBundle[];
    /**
     * Configures the installation of the Template Library.
     */
    templateLibrary?: outputs.GkeHubFeatureMembershipPolicycontrollerPolicyControllerHubConfigPolicyContentTemplateLibrary;
}

export interface GkeHubFeatureMembershipPolicycontrollerPolicyControllerHubConfigPolicyContentBundle {
    /**
     * The name for the key in the map for which this object is mapped to in the API
     */
    bundleName: string;
    /**
     * The set of namespaces to be exempted from the bundle.
     */
    exemptedNamespaces?: string[];
}

export interface GkeHubFeatureMembershipPolicycontrollerPolicyControllerHubConfigPolicyContentTemplateLibrary {
    /**
     * Configures the manner in which the template library is installed on the cluster. Possible values: INSTALLATION_UNSPECIFIED, NOT_INSTALLED, ALL
     */
    installation?: string;
}

export interface GkeHubFeatureMembershipTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface GkeHubFeatureResourceState {
    hasResources: boolean;
    state: string;
}

export interface GkeHubFeatureSpec {
    /**
     * Clusterupgrade feature spec.
     */
    clusterupgrade?: outputs.GkeHubFeatureSpecClusterupgrade;
    /**
     * Fleet Observability feature spec.
     */
    fleetobservability?: outputs.GkeHubFeatureSpecFleetobservability;
    /**
     * Multicluster Ingress-specific spec.
     */
    multiclusteringress?: outputs.GkeHubFeatureSpecMulticlusteringress;
}

export interface GkeHubFeatureSpecClusterupgrade {
    /**
     * Configuration overrides for individual upgrades.
     */
    gkeUpgradeOverrides?: outputs.GkeHubFeatureSpecClusterupgradeGkeUpgradeOverride[];
    /**
     * Post conditions to override for the specified upgrade.
     */
    postConditions?: outputs.GkeHubFeatureSpecClusterupgradePostConditions;
    /**
     * Specified if other fleet should be considered as a source of upgrades. Currently, at most one upstream fleet is allowed. The fleet name should be either fleet project number or id.
     */
    upstreamFleets: string[];
}

export interface GkeHubFeatureSpecClusterupgradeGkeUpgradeOverride {
    /**
     * Post conditions to override for the specified upgrade.
     */
    postConditions: outputs.GkeHubFeatureSpecClusterupgradeGkeUpgradeOverridePostConditions;
    /**
     * Which upgrade to override.
     */
    upgrade: outputs.GkeHubFeatureSpecClusterupgradeGkeUpgradeOverrideUpgrade;
}

export interface GkeHubFeatureSpecClusterupgradeGkeUpgradeOverridePostConditions {
    /**
     * Amount of time to "soak" after a rollout has been finished before marking it COMPLETE. Cannot exceed 30 days.
     */
    soaking: string;
}

export interface GkeHubFeatureSpecClusterupgradeGkeUpgradeOverrideUpgrade {
    /**
     * Name of the upgrade, e.g., "k8s_control_plane". It should be a valid upgrade name. It must not exceet 99 characters.
     */
    name: string;
    /**
     * Version of the upgrade, e.g., "1.22.1-gke.100". It should be a valid version. It must not exceet 99 characters.
     */
    version: string;
}

export interface GkeHubFeatureSpecClusterupgradePostConditions {
    /**
     * Amount of time to "soak" after a rollout has been finished before marking it COMPLETE. Cannot exceed 30 days.
     */
    soaking: string;
}

export interface GkeHubFeatureSpecFleetobservability {
    /**
     * Specified if fleet logging feature is enabled for the entire fleet. If UNSPECIFIED, fleet logging feature is disabled for the entire fleet.
     */
    loggingConfig?: outputs.GkeHubFeatureSpecFleetobservabilityLoggingConfig;
}

export interface GkeHubFeatureSpecFleetobservabilityLoggingConfig {
    /**
     * Specified if applying the default routing config to logs not specified in other configs.
     */
    defaultConfig?: outputs.GkeHubFeatureSpecFleetobservabilityLoggingConfigDefaultConfig;
    /**
     * Specified if applying the routing config to all logs for all fleet scopes.
     */
    fleetScopeLogsConfig?: outputs.GkeHubFeatureSpecFleetobservabilityLoggingConfigFleetScopeLogsConfig;
}

export interface GkeHubFeatureSpecFleetobservabilityLoggingConfigDefaultConfig {
    /**
     * Specified if fleet logging feature is enabled. Possible values: ["MODE_UNSPECIFIED", "COPY", "MOVE"]
     */
    mode?: string;
}

export interface GkeHubFeatureSpecFleetobservabilityLoggingConfigFleetScopeLogsConfig {
    /**
     * Specified if fleet logging feature is enabled. Possible values: ["MODE_UNSPECIFIED", "COPY", "MOVE"]
     */
    mode?: string;
}

export interface GkeHubFeatureSpecMulticlusteringress {
    /**
     * Fully-qualified Membership name which hosts the MultiClusterIngress CRD. Example: 'projects/foo-proj/locations/global/memberships/bar'
     */
    configMembership: string;
}

export interface GkeHubFeatureState {
    states: outputs.GkeHubFeatureStateState[];
}

export interface GkeHubFeatureStateState {
    code: string;
    description: string;
    updateTime: string;
}

export interface GkeHubFeatureTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface GkeHubFleetDefaultClusterConfig {
    /**
     * Enable/Disable binary authorization features for the cluster.
     */
    binaryAuthorizationConfig?: outputs.GkeHubFleetDefaultClusterConfigBinaryAuthorizationConfig;
    /**
     * Enable/Disable Security Posture features for the cluster.
     */
    securityPostureConfig?: outputs.GkeHubFleetDefaultClusterConfigSecurityPostureConfig;
}

export interface GkeHubFleetDefaultClusterConfigBinaryAuthorizationConfig {
    /**
     * Mode of operation for binauthz policy evaluation. Possible values: ["DISABLED", "POLICY_BINDINGS"]
     */
    evaluationMode?: string;
    /**
     * Binauthz policies that apply to this cluster.
     */
    policyBindings?: outputs.GkeHubFleetDefaultClusterConfigBinaryAuthorizationConfigPolicyBinding[];
}

export interface GkeHubFleetDefaultClusterConfigBinaryAuthorizationConfigPolicyBinding {
    /**
     * The relative resource name of the binauthz platform policy to audit. GKE
     * platform policies have the following format:
     * 'projects/{project_number}/platforms/gke/policies/{policy_id}'.
     */
    name?: string;
}

export interface GkeHubFleetDefaultClusterConfigSecurityPostureConfig {
    /**
     * Sets which mode to use for Security Posture features. Possible values: ["DISABLED", "BASIC", "ENTERPRISE"]
     */
    mode?: string;
    /**
     * Sets which mode to use for vulnerability scanning. Possible values: ["VULNERABILITY_DISABLED", "VULNERABILITY_BASIC", "VULNERABILITY_ENTERPRISE"]
     */
    vulnerabilityMode?: string;
}

export interface GkeHubFleetState {
    code: string;
}

export interface GkeHubFleetTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface GkeHubMembershipAuthority {
    issuer: string;
}

export interface GkeHubMembershipBindingState {
    code: string;
}

export interface GkeHubMembershipBindingTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface GkeHubMembershipEndpoint {
    /**
     * If this Membership is a Kubernetes API server hosted on GKE, this is a self link to its GCP resource.
     */
    gkeCluster?: outputs.GkeHubMembershipEndpointGkeCluster;
}

export interface GkeHubMembershipEndpointGkeCluster {
    resourceLink: string;
}

export interface GkeHubMembershipIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface GkeHubMembershipIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface GkeHubMembershipTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface GkeHubNamespaceState {
    code: string;
}

export interface GkeHubNamespaceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface GkeHubScopeIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface GkeHubScopeIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface GkeHubScopeRbacRoleBindingRole {
    /**
     * PredefinedRole is an ENUM representation of the default Kubernetes Roles Possible values: ["UNKNOWN", "ADMIN", "EDIT", "VIEW"]
     */
    predefinedRole?: string;
}

export interface GkeHubScopeRbacRoleBindingState {
    code: string;
}

export interface GkeHubScopeRbacRoleBindingTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface GkeHubScopeState {
    code: string;
}

export interface GkeHubScopeTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface GkeonpremBareMetalAdminClusterClusterOperations {
    /**
     * Whether collection of application logs/metrics should be enabled (in addition to system logs/metrics).
     */
    enableApplicationLogs?: boolean;
}

export interface GkeonpremBareMetalAdminClusterControlPlane {
    /**
     * Customizes the default API server args. Only a subset of
     * customized flags are supported. Please refer to the API server
     * documentation below to know the exact format:
     * https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
     */
    apiServerArgs?: outputs.GkeonpremBareMetalAdminClusterControlPlaneApiServerArg[];
    /**
     * Configures the node pool running the control plane. If specified the corresponding NodePool will be created for the cluster's control plane. The NodePool will have the same name and namespace as the cluster.
     */
    controlPlaneNodePoolConfig: outputs.GkeonpremBareMetalAdminClusterControlPlaneControlPlaneNodePoolConfig;
}

export interface GkeonpremBareMetalAdminClusterControlPlaneApiServerArg {
    /**
     * The argument name as it appears on the API Server command line please make sure to remove the leading dashes.
     */
    argument: string;
    /**
     * The value of the arg as it will be passed to the API Server command line.
     */
    value: string;
}

export interface GkeonpremBareMetalAdminClusterControlPlaneControlPlaneNodePoolConfig {
    /**
     * The generic configuration for a node pool running the control plane.
     */
    nodePoolConfig: outputs.GkeonpremBareMetalAdminClusterControlPlaneControlPlaneNodePoolConfigNodePoolConfig;
}

export interface GkeonpremBareMetalAdminClusterControlPlaneControlPlaneNodePoolConfigNodePoolConfig {
    /**
     * The map of Kubernetes labels (key/value pairs) to be applied to
     * each node. These will added in addition to any default label(s)
     * that Kubernetes may apply to the node. In case of conflict in
     * label keys, the applied set may differ depending on the Kubernetes
     * version -- it's best to assume the behavior is undefined and
     * conflicts should be avoided. For more information, including usage
     * and the valid values, see:
     *   - http://kubernetes.io/v1.1/docs/user-guide/labels.html
     * An object containing a list of "key": value pairs.
     * For example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
     */
    labels?: {[key: string]: string};
    /**
     * The list of machine addresses in the Bare Metal Node Pool.
     */
    nodeConfigs?: outputs.GkeonpremBareMetalAdminClusterControlPlaneControlPlaneNodePoolConfigNodePoolConfigNodeConfig[];
    /**
     * Specifies the nodes operating system (default: LINUX).
     */
    operatingSystem?: string;
    /**
     * The initial taints assigned to nodes of this node pool.
     */
    taints?: outputs.GkeonpremBareMetalAdminClusterControlPlaneControlPlaneNodePoolConfigNodePoolConfigTaint[];
}

export interface GkeonpremBareMetalAdminClusterControlPlaneControlPlaneNodePoolConfigNodePoolConfigNodeConfig {
    /**
     * The map of Kubernetes labels (key/value pairs) to be applied to
     * each node. These will added in addition to any default label(s)
     * that Kubernetes may apply to the node. In case of conflict in
     * label keys, the applied set may differ depending on the Kubernetes
     * version -- it's best to assume the behavior is undefined and
     * conflicts should be avoided. For more information, including usage
     * and the valid values, see:
     *   - http://kubernetes.io/v1.1/docs/user-guide/labels.html
     * An object containing a list of "key": value pairs.
     * For example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
     */
    labels?: {[key: string]: string};
    /**
     * The default IPv4 address for SSH access and Kubernetes node.
     * Example: 192.168.0.1
     */
    nodeIp?: string;
}

export interface GkeonpremBareMetalAdminClusterControlPlaneControlPlaneNodePoolConfigNodePoolConfigTaint {
    /**
     * Specifies the nodes operating system (default: LINUX). Possible values: ["EFFECT_UNSPECIFIED", "PREFER_NO_SCHEDULE", "NO_EXECUTE"]
     */
    effect?: string;
    /**
     * Key associated with the effect.
     */
    key?: string;
    /**
     * Value associated with the effect.
     */
    value?: string;
}

export interface GkeonpremBareMetalAdminClusterFleet {
    membership: string;
}

export interface GkeonpremBareMetalAdminClusterLoadBalancer {
    /**
     * A nested object resource
     */
    manualLbConfig?: outputs.GkeonpremBareMetalAdminClusterLoadBalancerManualLbConfig;
    /**
     * Specifies the load balancer ports.
     */
    portConfig: outputs.GkeonpremBareMetalAdminClusterLoadBalancerPortConfig;
    /**
     * Specified the Bare Metal Load Balancer Config
     */
    vipConfig: outputs.GkeonpremBareMetalAdminClusterLoadBalancerVipConfig;
}

export interface GkeonpremBareMetalAdminClusterLoadBalancerManualLbConfig {
    /**
     * Whether manual load balancing is enabled.
     */
    enabled: boolean;
}

export interface GkeonpremBareMetalAdminClusterLoadBalancerPortConfig {
    /**
     * The port that control plane hosted load balancers will listen on.
     */
    controlPlaneLoadBalancerPort: number;
}

export interface GkeonpremBareMetalAdminClusterLoadBalancerVipConfig {
    /**
     * The VIP which you previously set aside for the Kubernetes API of this Bare Metal Admin Cluster.
     */
    controlPlaneVip: string;
}

export interface GkeonpremBareMetalAdminClusterMaintenanceConfig {
    /**
     * All IPv4 address from these ranges will be placed into maintenance mode.
     * Nodes in maintenance mode will be cordoned and drained. When both of these
     * are true, the "baremetal.cluster.gke.io/maintenance" annotation will be set
     * on the node resource.
     */
    maintenanceAddressCidrBlocks: string[];
}

export interface GkeonpremBareMetalAdminClusterNetworkConfig {
    /**
     * A nested object resource
     */
    islandModeCidr?: outputs.GkeonpremBareMetalAdminClusterNetworkConfigIslandModeCidr;
}

export interface GkeonpremBareMetalAdminClusterNetworkConfigIslandModeCidr {
    /**
     * All pods in the cluster are assigned an RFC1918 IPv4 address from these ranges. This field cannot be changed after creation.
     */
    podAddressCidrBlocks: string[];
    /**
     * All services in the cluster are assigned an RFC1918 IPv4 address from these ranges. This field cannot be changed after creation.
     */
    serviceAddressCidrBlocks: string[];
}

export interface GkeonpremBareMetalAdminClusterNodeAccessConfig {
    /**
     * LoginUser is the user name used to access node machines.
     * It defaults to "root" if not set.
     */
    loginUser?: string;
}

export interface GkeonpremBareMetalAdminClusterNodeConfig {
    /**
     * The maximum number of pods a node can run. The size of the CIDR range
     * assigned to the node will be derived from this parameter.
     */
    maxPodsPerNode?: number;
}

export interface GkeonpremBareMetalAdminClusterProxy {
    /**
     * A list of IPs, hostnames, and domains that should skip the proxy.
     * For example: ["127.0.0.1", "example.com", ".corp", "localhost"].
     */
    noProxies?: string[];
    /**
     * Specifies the address of your proxy server.
     * For Example: http://domain
     * WARNING: Do not provide credentials in the format
     * of http://(username:password@)domain these will be rejected by the server.
     */
    uri: string;
}

export interface GkeonpremBareMetalAdminClusterSecurityConfig {
    /**
     * Configures user access to the Bare Metal User cluster.
     */
    authorization?: outputs.GkeonpremBareMetalAdminClusterSecurityConfigAuthorization;
}

export interface GkeonpremBareMetalAdminClusterSecurityConfigAuthorization {
    /**
     * Users that will be granted the cluster-admin role on the cluster, providing full access to the cluster.
     */
    adminUsers: outputs.GkeonpremBareMetalAdminClusterSecurityConfigAuthorizationAdminUser[];
}

export interface GkeonpremBareMetalAdminClusterSecurityConfigAuthorizationAdminUser {
    /**
     * The name of the user, e.g. 'my-gcp-id@gmail.com'.
     */
    username: string;
}

export interface GkeonpremBareMetalAdminClusterStatus {
    conditions: outputs.GkeonpremBareMetalAdminClusterStatusCondition[];
    errorMessage: string;
}

export interface GkeonpremBareMetalAdminClusterStatusCondition {
    lastTransitionTime: string;
    message: string;
    reason: string;
    state: string;
    type: string;
}

export interface GkeonpremBareMetalAdminClusterStorage {
    /**
     * Specifies the config for local PersistentVolumes backed
     * by mounted node disks. These disks need to be formatted and mounted by the
     * user, which can be done before or after cluster creation.
     */
    lvpNodeMountsConfig: outputs.GkeonpremBareMetalAdminClusterStorageLvpNodeMountsConfig;
    /**
     * Specifies the config for local PersistentVolumes backed by
     * subdirectories in a shared filesystem. These subdirectores are
     * automatically created during cluster creation.
     */
    lvpShareConfig: outputs.GkeonpremBareMetalAdminClusterStorageLvpShareConfig;
}

export interface GkeonpremBareMetalAdminClusterStorageLvpNodeMountsConfig {
    /**
     * The host machine path.
     */
    path: string;
    /**
     * The StorageClass name that PVs will be created with.
     */
    storageClass: string;
}

export interface GkeonpremBareMetalAdminClusterStorageLvpShareConfig {
    /**
     * Defines the machine path and storage class for the LVP Share.
     */
    lvpConfig: outputs.GkeonpremBareMetalAdminClusterStorageLvpShareConfigLvpConfig;
    /**
     * The number of subdirectories to create under path.
     */
    sharedPathPvCount?: number;
}

export interface GkeonpremBareMetalAdminClusterStorageLvpShareConfigLvpConfig {
    /**
     * The host machine path.
     */
    path: string;
    /**
     * The StorageClass name that PVs will be created with.
     */
    storageClass: string;
}

export interface GkeonpremBareMetalAdminClusterTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface GkeonpremBareMetalAdminClusterValidationCheck {
    options: string;
    scenario: string;
    statuses: outputs.GkeonpremBareMetalAdminClusterValidationCheckStatus[];
}

export interface GkeonpremBareMetalAdminClusterValidationCheckStatus {
    results: outputs.GkeonpremBareMetalAdminClusterValidationCheckStatusResult[];
}

export interface GkeonpremBareMetalAdminClusterValidationCheckStatusResult {
    category: string;
    description: string;
    details: string;
    options: string;
    reason: string;
}

export interface GkeonpremBareMetalClusterBinaryAuthorization {
    /**
     * Mode of operation for binauthz policy evaluation. If unspecified,
     * defaults to DISABLED. Possible values: ["DISABLED", "PROJECT_SINGLETON_POLICY_ENFORCE"]
     */
    evaluationMode?: string;
}

export interface GkeonpremBareMetalClusterClusterOperations {
    /**
     * Whether collection of application logs/metrics should be enabled (in addition to system logs/metrics).
     */
    enableApplicationLogs?: boolean;
}

export interface GkeonpremBareMetalClusterControlPlane {
    /**
     * Customizes the default API server args. Only a subset of
     * customized flags are supported. Please refer to the API server
     * documentation below to know the exact format:
     * https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
     */
    apiServerArgs?: outputs.GkeonpremBareMetalClusterControlPlaneApiServerArg[];
    /**
     * Configures the node pool running the control plane. If specified the corresponding NodePool will be created for the cluster's control plane. The NodePool will have the same name and namespace as the cluster.
     */
    controlPlaneNodePoolConfig: outputs.GkeonpremBareMetalClusterControlPlaneControlPlaneNodePoolConfig;
}

export interface GkeonpremBareMetalClusterControlPlaneApiServerArg {
    /**
     * The argument name as it appears on the API Server command line please make sure to remove the leading dashes.
     */
    argument: string;
    /**
     * The value of the arg as it will be passed to the API Server command line.
     */
    value: string;
}

export interface GkeonpremBareMetalClusterControlPlaneControlPlaneNodePoolConfig {
    /**
     * The generic configuration for a node pool running the control plane.
     */
    nodePoolConfig: outputs.GkeonpremBareMetalClusterControlPlaneControlPlaneNodePoolConfigNodePoolConfig;
}

export interface GkeonpremBareMetalClusterControlPlaneControlPlaneNodePoolConfigNodePoolConfig {
    /**
     * The map of Kubernetes labels (key/value pairs) to be applied to
     * each node. These will added in addition to any default label(s)
     * that Kubernetes may apply to the node. In case of conflict in
     * label keys, the applied set may differ depending on the Kubernetes
     * version -- it's best to assume the behavior is undefined and
     * conflicts should be avoided. For more information, including usage
     * and the valid values, see:
     *   - http://kubernetes.io/v1.1/docs/user-guide/labels.html
     * An object containing a list of "key": value pairs.
     * For example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
     */
    labels: {[key: string]: string};
    /**
     * The list of machine addresses in the Bare Metal Node Pool.
     */
    nodeConfigs?: outputs.GkeonpremBareMetalClusterControlPlaneControlPlaneNodePoolConfigNodePoolConfigNodeConfig[];
    /**
     * Specifies the nodes operating system (default: LINUX).
     */
    operatingSystem?: string;
    /**
     * The initial taints assigned to nodes of this node pool.
     */
    taints?: outputs.GkeonpremBareMetalClusterControlPlaneControlPlaneNodePoolConfigNodePoolConfigTaint[];
}

export interface GkeonpremBareMetalClusterControlPlaneControlPlaneNodePoolConfigNodePoolConfigNodeConfig {
    /**
     * The map of Kubernetes labels (key/value pairs) to be applied to
     * each node. These will added in addition to any default label(s)
     * that Kubernetes may apply to the node. In case of conflict in
     * label keys, the applied set may differ depending on the Kubernetes
     * version -- it's best to assume the behavior is undefined and
     * conflicts should be avoided. For more information, including usage
     * and the valid values, see:
     *   - http://kubernetes.io/v1.1/docs/user-guide/labels.html
     * An object containing a list of "key": value pairs.
     * Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
     */
    labels?: {[key: string]: string};
    /**
     * The default IPv4 address for SSH access and Kubernetes node.
     * Example: 192.168.0.1
     */
    nodeIp?: string;
}

export interface GkeonpremBareMetalClusterControlPlaneControlPlaneNodePoolConfigNodePoolConfigTaint {
    /**
     * Specifies the nodes operating system (default: LINUX). Possible values: ["EFFECT_UNSPECIFIED", "PREFER_NO_SCHEDULE", "NO_EXECUTE"]
     */
    effect?: string;
    /**
     * Key associated with the effect.
     */
    key?: string;
    /**
     * Value associated with the effect.
     */
    value?: string;
}

export interface GkeonpremBareMetalClusterFleet {
    membership: string;
}

export interface GkeonpremBareMetalClusterLoadBalancer {
    /**
     * Configuration for BGP typed load balancers.
     */
    bgpLbConfig?: outputs.GkeonpremBareMetalClusterLoadBalancerBgpLbConfig;
    /**
     * A nested object resource
     */
    manualLbConfig?: outputs.GkeonpremBareMetalClusterLoadBalancerManualLbConfig;
    /**
     * A nested object resource
     */
    metalLbConfig?: outputs.GkeonpremBareMetalClusterLoadBalancerMetalLbConfig;
    /**
     * Specifies the load balancer ports.
     */
    portConfig: outputs.GkeonpremBareMetalClusterLoadBalancerPortConfig;
    /**
     * Specified the Bare Metal Load Balancer Config
     */
    vipConfig: outputs.GkeonpremBareMetalClusterLoadBalancerVipConfig;
}

export interface GkeonpremBareMetalClusterLoadBalancerBgpLbConfig {
    /**
     * AddressPools is a list of non-overlapping IP pools used by load balancer
     * typed services. All addresses must be routable to load balancer nodes.
     * IngressVIP must be included in the pools.
     */
    addressPools: outputs.GkeonpremBareMetalClusterLoadBalancerBgpLbConfigAddressPool[];
    /**
     * BGP autonomous system number (ASN) of the cluster.
     * This field can be updated after cluster creation.
     */
    asn: number;
    /**
     * The list of BGP peers that the cluster will connect to.
     * At least one peer must be configured for each control plane node.
     * Control plane nodes will connect to these peers to advertise the control
     * plane VIP. The Services load balancer also uses these peers by default.
     * This field can be updated after cluster creation.
     */
    bgpPeerConfigs: outputs.GkeonpremBareMetalClusterLoadBalancerBgpLbConfigBgpPeerConfig[];
    /**
     * Specifies the node pool running data plane load balancing. L2 connectivity
     * is required among nodes in this pool. If missing, the control plane node
     * pool is used for data plane load balancing.
     */
    loadBalancerNodePoolConfig?: outputs.GkeonpremBareMetalClusterLoadBalancerBgpLbConfigLoadBalancerNodePoolConfig;
}

export interface GkeonpremBareMetalClusterLoadBalancerBgpLbConfigAddressPool {
    /**
     * The addresses that are part of this pool. Each address must be either in the CIDR form (1.2.3.0/24) or range form (1.2.3.1-1.2.3.5).
     */
    addresses: string[];
    /**
     * If true, avoid using IPs ending in .0 or .255.
     * This avoids buggy consumer devices mistakenly dropping IPv4 traffic for those special IP addresses.
     */
    avoidBuggyIps?: boolean;
    /**
     * If true, prevent IP addresses from being automatically assigned.
     */
    manualAssign?: string;
    /**
     * The name of the address pool.
     */
    pool: string;
}

export interface GkeonpremBareMetalClusterLoadBalancerBgpLbConfigBgpPeerConfig {
    /**
     * BGP autonomous system number (ASN) for the network that contains the
     * external peer device.
     */
    asn: number;
    /**
     * The IP address of the control plane node that connects to the external
     * peer.
     * If you don't specify any control plane nodes, all control plane nodes
     * can connect to the external peer. If you specify one or more IP addresses,
     * only the nodes specified participate in peering sessions.
     */
    controlPlaneNodes?: string[];
    /**
     * The IP address of the external peer device.
     */
    ipAddress: string;
}

export interface GkeonpremBareMetalClusterLoadBalancerBgpLbConfigLoadBalancerNodePoolConfig {
    /**
     * The generic configuration for a node pool running a load balancer.
     */
    nodePoolConfig?: outputs.GkeonpremBareMetalClusterLoadBalancerBgpLbConfigLoadBalancerNodePoolConfigNodePoolConfig;
}

export interface GkeonpremBareMetalClusterLoadBalancerBgpLbConfigLoadBalancerNodePoolConfigNodePoolConfig {
    /**
     * The modifiable kubelet configurations for the baremetal machines.
     */
    kubeletConfig?: outputs.GkeonpremBareMetalClusterLoadBalancerBgpLbConfigLoadBalancerNodePoolConfigNodePoolConfigKubeletConfig;
    /**
     * The map of Kubernetes labels (key/value pairs) to be applied to
     * each node. These will added in addition to any default label(s)
     * that Kubernetes may apply to the node. In case of conflict in
     * label keys, the applied set may differ depending on the Kubernetes
     * version -- it's best to assume the behavior is undefined and
     * conflicts should be avoided. For more information, including usage
     * and the valid values, see:
     *   - http://kubernetes.io/v1.1/docs/user-guide/labels.html
     * An object containing a list of "key": value pairs.
     * For example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
     */
    labels?: {[key: string]: string};
    /**
     * The list of machine addresses in the Bare Metal Node Pool.
     */
    nodeConfigs?: outputs.GkeonpremBareMetalClusterLoadBalancerBgpLbConfigLoadBalancerNodePoolConfigNodePoolConfigNodeConfig[];
    /**
     * Specifies the nodes operating system (default: LINUX).
     */
    operatingSystem?: string;
    /**
     * The initial taints assigned to nodes of this node pool.
     */
    taints?: outputs.GkeonpremBareMetalClusterLoadBalancerBgpLbConfigLoadBalancerNodePoolConfigNodePoolConfigTaint[];
}

export interface GkeonpremBareMetalClusterLoadBalancerBgpLbConfigLoadBalancerNodePoolConfigNodePoolConfigKubeletConfig {
    /**
     * The maximum size of bursty pulls, temporarily allows pulls to burst to this
     * number, while still not exceeding registry_pull_qps.
     * The value must not be a negative number.
     * Updating this field may impact scalability by changing the amount of
     * traffic produced by image pulls.
     * Defaults to 10.
     */
    registryBurst?: number;
    /**
     * The limit of registry pulls per second.
     * Setting this value to 0 means no limit.
     * Updating this field may impact scalability by changing the amount of
     * traffic produced by image pulls.
     * Defaults to 5.
     */
    registryPullQps?: number;
    /**
     * Prevents the Kubelet from pulling multiple images at a time.
     * We recommend *not* changing the default value on nodes that run docker
     * daemon with version  < 1.9 or an Another Union File System (Aufs) storage
     * backend. Issue https://github.com/kubernetes/kubernetes/issues/10959 has
     * more details.
     */
    serializeImagePullsDisabled?: boolean;
}

export interface GkeonpremBareMetalClusterLoadBalancerBgpLbConfigLoadBalancerNodePoolConfigNodePoolConfigNodeConfig {
    /**
     * The map of Kubernetes labels (key/value pairs) to be applied to
     * each node. These will added in addition to any default label(s)
     * that Kubernetes may apply to the node. In case of conflict in
     * label keys, the applied set may differ depending on the Kubernetes
     * version -- it's best to assume the behavior is undefined and
     * conflicts should be avoided. For more information, including usage
     * and the valid values, see:
     *   - http://kubernetes.io/v1.1/docs/user-guide/labels.html
     * An object containing a list of "key": value pairs.
     * For example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
     */
    labels?: {[key: string]: string};
    /**
     * The default IPv4 address for SSH access and Kubernetes node.
     * Example: 192.168.0.1
     */
    nodeIp?: string;
}

export interface GkeonpremBareMetalClusterLoadBalancerBgpLbConfigLoadBalancerNodePoolConfigNodePoolConfigTaint {
    /**
     * Specifies the nodes operating system (default: LINUX). Possible values: ["EFFECT_UNSPECIFIED", "PREFER_NO_SCHEDULE", "NO_EXECUTE"]
     */
    effect?: string;
    /**
     * Key associated with the effect.
     */
    key?: string;
    /**
     * Value associated with the effect.
     */
    value?: string;
}

export interface GkeonpremBareMetalClusterLoadBalancerManualLbConfig {
    /**
     * Whether manual load balancing is enabled.
     */
    enabled: boolean;
}

export interface GkeonpremBareMetalClusterLoadBalancerMetalLbConfig {
    /**
     * AddressPools is a list of non-overlapping IP pools used by load balancer
     * typed services. All addresses must be routable to load balancer nodes.
     * IngressVIP must be included in the pools.
     */
    addressPools: outputs.GkeonpremBareMetalClusterLoadBalancerMetalLbConfigAddressPool[];
    /**
     * Specifies the load balancer's node pool configuration.
     */
    loadBalancerNodePoolConfig?: outputs.GkeonpremBareMetalClusterLoadBalancerMetalLbConfigLoadBalancerNodePoolConfig;
}

export interface GkeonpremBareMetalClusterLoadBalancerMetalLbConfigAddressPool {
    /**
     * The addresses that are part of this pool. Each address must be either in the CIDR form (1.2.3.0/24) or range form (1.2.3.1-1.2.3.5).
     */
    addresses: string[];
    /**
     * If true, avoid using IPs ending in .0 or .255.
     * This avoids buggy consumer devices mistakenly dropping IPv4 traffic for those special IP addresses.
     */
    avoidBuggyIps?: boolean;
    /**
     * If true, prevent IP addresses from being automatically assigned.
     */
    manualAssign?: boolean;
    /**
     * The name of the address pool.
     */
    pool: string;
}

export interface GkeonpremBareMetalClusterLoadBalancerMetalLbConfigLoadBalancerNodePoolConfig {
    /**
     * The generic configuration for a node pool running a load balancer.
     */
    nodePoolConfig?: outputs.GkeonpremBareMetalClusterLoadBalancerMetalLbConfigLoadBalancerNodePoolConfigNodePoolConfig;
}

export interface GkeonpremBareMetalClusterLoadBalancerMetalLbConfigLoadBalancerNodePoolConfigNodePoolConfig {
    /**
     * The map of Kubernetes labels (key/value pairs) to be applied to
     * each node. These will added in addition to any default label(s)
     * that Kubernetes may apply to the node. In case of conflict in
     * label keys, the applied set may differ depending on the Kubernetes
     * version -- it's best to assume the behavior is undefined and
     * conflicts should be avoided. For more information, including usage
     * and the valid values, see:
     *   - http://kubernetes.io/v1.1/docs/user-guide/labels.html
     * An object containing a list of "key": value pairs.
     * For example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
     */
    labels: {[key: string]: string};
    /**
     * The list of machine addresses in the Bare Metal Node Pool.
     */
    nodeConfigs?: outputs.GkeonpremBareMetalClusterLoadBalancerMetalLbConfigLoadBalancerNodePoolConfigNodePoolConfigNodeConfig[];
    /**
     * Specifies the nodes operating system (default: LINUX).
     */
    operatingSystem: string;
    /**
     * The initial taints assigned to nodes of this node pool.
     */
    taints?: outputs.GkeonpremBareMetalClusterLoadBalancerMetalLbConfigLoadBalancerNodePoolConfigNodePoolConfigTaint[];
}

export interface GkeonpremBareMetalClusterLoadBalancerMetalLbConfigLoadBalancerNodePoolConfigNodePoolConfigNodeConfig {
    /**
     * The map of Kubernetes labels (key/value pairs) to be applied to
     * each node. These will added in addition to any default label(s)
     * that Kubernetes may apply to the node. In case of conflict in
     * label keys, the applied set may differ depending on the Kubernetes
     * version -- it's best to assume the behavior is undefined and
     * conflicts should be avoided. For more information, including usage
     * and the valid values, see:
     *   - http://kubernetes.io/v1.1/docs/user-guide/labels.html
     * An object containing a list of "key": value pairs.
     * For example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
     */
    labels?: {[key: string]: string};
    /**
     * The default IPv4 address for SSH access and Kubernetes node.
     * Example: 192.168.0.1
     */
    nodeIp?: string;
}

export interface GkeonpremBareMetalClusterLoadBalancerMetalLbConfigLoadBalancerNodePoolConfigNodePoolConfigTaint {
    /**
     * Specifies the nodes operating system (default: LINUX). Possible values: ["EFFECT_UNSPECIFIED", "PREFER_NO_SCHEDULE", "NO_EXECUTE"]
     */
    effect?: string;
    /**
     * Key associated with the effect.
     */
    key?: string;
    /**
     * Value associated with the effect.
     */
    value?: string;
}

export interface GkeonpremBareMetalClusterLoadBalancerPortConfig {
    /**
     * The port that control plane hosted load balancers will listen on.
     */
    controlPlaneLoadBalancerPort: number;
}

export interface GkeonpremBareMetalClusterLoadBalancerVipConfig {
    /**
     * The VIP which you previously set aside for the Kubernetes API of this Bare Metal User Cluster.
     */
    controlPlaneVip: string;
    /**
     * The VIP which you previously set aside for ingress traffic into this Bare Metal User Cluster.
     */
    ingressVip: string;
}

export interface GkeonpremBareMetalClusterMaintenanceConfig {
    /**
     * All IPv4 address from these ranges will be placed into maintenance mode.
     * Nodes in maintenance mode will be cordoned and drained. When both of these
     * are true, the "baremetal.cluster.gke.io/maintenance" annotation will be set
     * on the node resource.
     */
    maintenanceAddressCidrBlocks: string[];
}

export interface GkeonpremBareMetalClusterNetworkConfig {
    /**
     * Enables the use of advanced Anthos networking features, such as Bundled
     * Load Balancing with BGP or the egress NAT gateway.
     * Setting configuration for advanced networking features will automatically
     * set this flag.
     */
    advancedNetworking?: boolean;
    /**
     * A nested object resource
     */
    islandModeCidr?: outputs.GkeonpremBareMetalClusterNetworkConfigIslandModeCidr;
    /**
     * Configuration for multiple network interfaces.
     */
    multipleNetworkInterfacesConfig?: outputs.GkeonpremBareMetalClusterNetworkConfigMultipleNetworkInterfacesConfig;
    /**
     * Configuration for SR-IOV.
     */
    srIovConfig?: outputs.GkeonpremBareMetalClusterNetworkConfigSrIovConfig;
}

export interface GkeonpremBareMetalClusterNetworkConfigIslandModeCidr {
    /**
     * All pods in the cluster are assigned an RFC1918 IPv4 address from these ranges. This field cannot be changed after creation.
     */
    podAddressCidrBlocks: string[];
    /**
     * All services in the cluster are assigned an RFC1918 IPv4 address from these ranges. This field cannot be changed after creation.
     */
    serviceAddressCidrBlocks: string[];
}

export interface GkeonpremBareMetalClusterNetworkConfigMultipleNetworkInterfacesConfig {
    /**
     * Whether to enable multiple network interfaces for your pods.
     * When set network_config.advanced_networking is automatically
     * set to true.
     */
    enabled?: boolean;
}

export interface GkeonpremBareMetalClusterNetworkConfigSrIovConfig {
    /**
     * Whether to install the SR-IOV operator.
     */
    enabled?: boolean;
}

export interface GkeonpremBareMetalClusterNodeAccessConfig {
    /**
     * LoginUser is the user name used to access node machines.
     * It defaults to "root" if not set.
     */
    loginUser: string;
}

export interface GkeonpremBareMetalClusterNodeConfig {
    /**
     * The available runtimes that can be used to run containers in a Bare Metal User Cluster. Possible values: ["CONTAINER_RUNTIME_UNSPECIFIED", "DOCKER", "CONTAINERD"]
     */
    containerRuntime: string;
    /**
     * The maximum number of pods a node can run. The size of the CIDR range
     * assigned to the node will be derived from this parameter.
     */
    maxPodsPerNode: number;
}

export interface GkeonpremBareMetalClusterOsEnvironmentConfig {
    /**
     * Whether the package repo should not be included when initializing
     * bare metal machines.
     */
    packageRepoExcluded: boolean;
}

export interface GkeonpremBareMetalClusterProxy {
    /**
     * A list of IPs, hostnames, and domains that should skip the proxy.
     * For example ["127.0.0.1", "example.com", ".corp", "localhost"].
     */
    noProxies?: string[];
    /**
     * Specifies the address of your proxy server.
     * For example: http://domain
     * WARNING: Do not provide credentials in the format
     * of http://(username:password@)domain these will be rejected by the server.
     */
    uri: string;
}

export interface GkeonpremBareMetalClusterSecurityConfig {
    /**
     * Configures user access to the Bare Metal User cluster.
     */
    authorization?: outputs.GkeonpremBareMetalClusterSecurityConfigAuthorization;
}

export interface GkeonpremBareMetalClusterSecurityConfigAuthorization {
    /**
     * Users that will be granted the cluster-admin role on the cluster, providing full access to the cluster.
     */
    adminUsers: outputs.GkeonpremBareMetalClusterSecurityConfigAuthorizationAdminUser[];
}

export interface GkeonpremBareMetalClusterSecurityConfigAuthorizationAdminUser {
    /**
     * The name of the user, e.g. 'my-gcp-id@gmail.com'.
     */
    username: string;
}

export interface GkeonpremBareMetalClusterStatus {
    conditions: outputs.GkeonpremBareMetalClusterStatusCondition[];
    errorMessage: string;
}

export interface GkeonpremBareMetalClusterStatusCondition {
    lastTransitionTime: string;
    message: string;
    reason: string;
    state: string;
    type: string;
}

export interface GkeonpremBareMetalClusterStorage {
    /**
     * Specifies the config for local PersistentVolumes backed
     * by mounted node disks. These disks need to be formatted and mounted by the
     * user, which can be done before or after cluster creation.
     */
    lvpNodeMountsConfig: outputs.GkeonpremBareMetalClusterStorageLvpNodeMountsConfig;
    /**
     * Specifies the config for local PersistentVolumes backed by
     * subdirectories in a shared filesystem. These subdirectores are
     * automatically created during cluster creation.
     */
    lvpShareConfig: outputs.GkeonpremBareMetalClusterStorageLvpShareConfig;
}

export interface GkeonpremBareMetalClusterStorageLvpNodeMountsConfig {
    /**
     * The host machine path.
     */
    path: string;
    /**
     * The StorageClass name that PVs will be created with.
     */
    storageClass: string;
}

export interface GkeonpremBareMetalClusterStorageLvpShareConfig {
    /**
     * Defines the machine path and storage class for the LVP Share.
     */
    lvpConfig: outputs.GkeonpremBareMetalClusterStorageLvpShareConfigLvpConfig;
    /**
     * The number of subdirectories to create under path.
     */
    sharedPathPvCount?: number;
}

export interface GkeonpremBareMetalClusterStorageLvpShareConfigLvpConfig {
    /**
     * The host machine path.
     */
    path: string;
    /**
     * The StorageClass name that PVs will be created with.
     */
    storageClass: string;
}

export interface GkeonpremBareMetalClusterTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface GkeonpremBareMetalClusterUpgradePolicy {
    /**
     * Specifies which upgrade policy to use. Possible values: ["SERIAL", "CONCURRENT"]
     */
    policy?: string;
}

export interface GkeonpremBareMetalClusterValidationCheck {
    options: string;
    scenario: string;
    statuses: outputs.GkeonpremBareMetalClusterValidationCheckStatus[];
}

export interface GkeonpremBareMetalClusterValidationCheckStatus {
    results: outputs.GkeonpremBareMetalClusterValidationCheckStatusResult[];
}

export interface GkeonpremBareMetalClusterValidationCheckStatusResult {
    category: string;
    description: string;
    details: string;
    options: string;
    reason: string;
}

export interface GkeonpremBareMetalNodePoolNodePoolConfig {
    /**
     * The map of Kubernetes labels (key/value pairs) to be applied to
     * each node. These will added in addition to any default label(s)
     * that Kubernetes may apply to the node. In case of conflict in
     * label keys, the applied set may differ depending on the Kubernetes
     * version -- it's best to assume the behavior is undefined and
     * conflicts should be avoided. For more information, including usage
     * and the valid values, see:
     *   - http://kubernetes.io/v1.1/docs/user-guide/labels.html
     * An object containing a list of "key": value pairs.
     * For example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
     */
    labels: {[key: string]: string};
    /**
     * The list of machine addresses in the Bare Metal Node Pool.
     */
    nodeConfigs: outputs.GkeonpremBareMetalNodePoolNodePoolConfigNodeConfig[];
    /**
     * Specifies the nodes operating system (default: LINUX).
     */
    operatingSystem: string;
    /**
     * The initial taints assigned to nodes of this node pool.
     */
    taints?: outputs.GkeonpremBareMetalNodePoolNodePoolConfigTaint[];
}

export interface GkeonpremBareMetalNodePoolNodePoolConfigNodeConfig {
    /**
     * The map of Kubernetes labels (key/value pairs) to be applied to
     * each node. These will added in addition to any default label(s)
     * that Kubernetes may apply to the node. In case of conflict in
     * label keys, the applied set may differ depending on the Kubernetes
     * version -- it's best to assume the behavior is undefined and
     * conflicts should be avoided. For more information, including usage
     * and the valid values, see:
     *   - http://kubernetes.io/v1.1/docs/user-guide/labels.html
     * An object containing a list of "key": value pairs.
     * For example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
     */
    labels?: {[key: string]: string};
    /**
     * The default IPv4 address for SSH access and Kubernetes node.
     * Example: 192.168.0.1
     */
    nodeIp?: string;
}

export interface GkeonpremBareMetalNodePoolNodePoolConfigTaint {
    /**
     * Specifies the nodes operating system (default: LINUX). Possible values: ["EFFECT_UNSPECIFIED", "PREFER_NO_SCHEDULE", "NO_EXECUTE"]
     */
    effect?: string;
    /**
     * Key associated with the effect.
     */
    key?: string;
    /**
     * Value associated with the effect.
     */
    value?: string;
}

export interface GkeonpremBareMetalNodePoolStatus {
    conditions: outputs.GkeonpremBareMetalNodePoolStatusCondition[];
    errorMessage: string;
}

export interface GkeonpremBareMetalNodePoolStatusCondition {
    lastTransitionTime: string;
    message: string;
    reason: string;
    state: string;
    type: string;
}

export interface GkeonpremBareMetalNodePoolTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface GkeonpremVmwareClusterAntiAffinityGroups {
    /**
     * Spread nodes across at least three physical hosts (requires at least three
     * hosts).
     * Enabled by default.
     */
    aagConfigDisabled: boolean;
}

export interface GkeonpremVmwareClusterAuthorization {
    /**
     * Users that will be granted the cluster-admin role on the cluster, providing
     * full access to the cluster.
     */
    adminUsers?: outputs.GkeonpremVmwareClusterAuthorizationAdminUser[];
}

export interface GkeonpremVmwareClusterAuthorizationAdminUser {
    /**
     * The name of the user, e.g. 'my-gcp-id@gmail.com'.
     */
    username: string;
}

export interface GkeonpremVmwareClusterAutoRepairConfig {
    /**
     * Whether auto repair is enabled.
     */
    enabled: boolean;
}

export interface GkeonpremVmwareClusterControlPlaneNode {
    /**
     * AutoResizeConfig provides auto resizing configurations.
     */
    autoResizeConfig?: outputs.GkeonpremVmwareClusterControlPlaneNodeAutoResizeConfig;
    /**
     * The number of CPUs for each admin cluster node that serve as control planes
     * for this VMware User Cluster. (default: 4 CPUs)
     */
    cpus?: number;
    /**
     * The megabytes of memory for each admin cluster node that serves as a
     * control plane for this VMware User Cluster (default: 8192 MB memory).
     */
    memory?: number;
    /**
     * The number of control plane nodes for this VMware User Cluster.
     * (default: 1 replica).
     */
    replicas?: number;
    /**
     * Vsphere-specific config.
     */
    vsphereConfigs: outputs.GkeonpremVmwareClusterControlPlaneNodeVsphereConfig[];
}

export interface GkeonpremVmwareClusterControlPlaneNodeAutoResizeConfig {
    /**
     * Whether to enable control plane node auto resizing.
     */
    enabled: boolean;
}

export interface GkeonpremVmwareClusterControlPlaneNodeVsphereConfig {
    datastore: string;
    storagePolicyName: string;
}

export interface GkeonpremVmwareClusterDataplaneV2 {
    /**
     * Enable advanced networking which requires dataplane_v2_enabled to be set true.
     */
    advancedNetworking?: boolean;
    /**
     * Enables Dataplane V2.
     */
    dataplaneV2Enabled?: boolean;
    /**
     * Enable Dataplane V2 for clusters with Windows nodes.
     */
    windowsDataplaneV2Enabled?: boolean;
}

export interface GkeonpremVmwareClusterFleet {
    membership: string;
}

export interface GkeonpremVmwareClusterLoadBalancer {
    /**
     * Configuration for F5 Big IP typed load balancers.
     */
    f5Config?: outputs.GkeonpremVmwareClusterLoadBalancerF5Config;
    /**
     * Manually configured load balancers.
     */
    manualLbConfig?: outputs.GkeonpremVmwareClusterLoadBalancerManualLbConfig;
    /**
     * Configuration for MetalLB typed load balancers.
     */
    metalLbConfig?: outputs.GkeonpremVmwareClusterLoadBalancerMetalLbConfig;
    /**
     * The VIPs used by the load balancer.
     */
    vipConfig?: outputs.GkeonpremVmwareClusterLoadBalancerVipConfig;
}

export interface GkeonpremVmwareClusterLoadBalancerF5Config {
    /**
     * The load balancer's IP address.
     */
    address?: string;
    /**
     * he preexisting partition to be used by the load balancer. T
     * his partition is usually created for the admin cluster for example:
     * 'my-f5-admin-partition'.
     */
    partition?: string;
    /**
     * The pool name. Only necessary, if using SNAT.
     */
    snatPool: string;
}

export interface GkeonpremVmwareClusterLoadBalancerManualLbConfig {
    /**
     * NodePort for control plane service. The Kubernetes API server in the admin
     * cluster is implemented as a Service of type NodePort (ex. 30968).
     */
    controlPlaneNodePort: number;
    /**
     * NodePort for ingress service's http. The ingress service in the admin
     * cluster is implemented as a Service of type NodePort (ex. 32527).
     */
    ingressHttpNodePort: number;
    /**
     * NodePort for ingress service's https. The ingress service in the admin
     * cluster is implemented as a Service of type NodePort (ex. 30139).
     */
    ingressHttpsNodePort: number;
    /**
     * NodePort for konnectivity server service running as a sidecar in each
     * kube-apiserver pod (ex. 30564).
     */
    konnectivityServerNodePort: number;
}

export interface GkeonpremVmwareClusterLoadBalancerMetalLbConfig {
    /**
     * AddressPools is a list of non-overlapping IP pools used by load balancer
     * typed services. All addresses must be routable to load balancer nodes.
     * IngressVIP must be included in the pools.
     */
    addressPools: outputs.GkeonpremVmwareClusterLoadBalancerMetalLbConfigAddressPool[];
}

export interface GkeonpremVmwareClusterLoadBalancerMetalLbConfigAddressPool {
    /**
     * The addresses that are part of this pool. Each address
     * must be either in the CIDR form (1.2.3.0/24) or range
     * form (1.2.3.1-1.2.3.5).
     */
    addresses: string[];
    /**
     * If true, avoid using IPs ending in .0 or .255.
     * This avoids buggy consumer devices mistakenly dropping IPv4 traffic for
     * those special IP addresses.
     */
    avoidBuggyIps: boolean;
    /**
     * If true, prevent IP addresses from being automatically assigned.
     */
    manualAssign: boolean;
    /**
     * The name of the address pool.
     */
    pool: string;
}

export interface GkeonpremVmwareClusterLoadBalancerVipConfig {
    /**
     * The VIP which you previously set aside for the Kubernetes API of this cluster.
     */
    controlPlaneVip?: string;
    /**
     * The VIP which you previously set aside for ingress traffic into this cluster.
     */
    ingressVip?: string;
}

export interface GkeonpremVmwareClusterNetworkConfig {
    /**
     * Configuration for control plane V2 mode.
     */
    controlPlaneV2Config?: outputs.GkeonpremVmwareClusterNetworkConfigControlPlaneV2Config;
    /**
     * Configuration settings for a DHCP IP configuration.
     */
    dhcpIpConfig?: outputs.GkeonpremVmwareClusterNetworkConfigDhcpIpConfig;
    /**
     * Represents common network settings irrespective of the host's IP address.
     */
    hostConfig?: outputs.GkeonpremVmwareClusterNetworkConfigHostConfig;
    /**
     * All pods in the cluster are assigned an RFC1918 IPv4 address from these ranges.
     * Only a single range is supported. This field cannot be changed after creation.
     */
    podAddressCidrBlocks: string[];
    /**
     * All services in the cluster are assigned an RFC1918 IPv4 address
     * from these ranges. Only a single range is supported.. This field
     * cannot be changed after creation.
     */
    serviceAddressCidrBlocks: string[];
    /**
     * Configuration settings for a static IP configuration.
     */
    staticIpConfig?: outputs.GkeonpremVmwareClusterNetworkConfigStaticIpConfig;
    /**
     * vcenter_network specifies vCenter network name. Inherited from the admin cluster.
     */
    vcenterNetwork: string;
}

export interface GkeonpremVmwareClusterNetworkConfigControlPlaneV2Config {
    /**
     * Static IP addresses for the control plane nodes.
     */
    controlPlaneIpBlock?: outputs.GkeonpremVmwareClusterNetworkConfigControlPlaneV2ConfigControlPlaneIpBlock;
}

export interface GkeonpremVmwareClusterNetworkConfigControlPlaneV2ConfigControlPlaneIpBlock {
    /**
     * The network gateway used by the VMware User Cluster.
     */
    gateway?: string;
    /**
     * The node's network configurations used by the VMware User Cluster.
     */
    ips?: outputs.GkeonpremVmwareClusterNetworkConfigControlPlaneV2ConfigControlPlaneIpBlockIp[];
    /**
     * The netmask used by the VMware User Cluster.
     */
    netmask?: string;
}

export interface GkeonpremVmwareClusterNetworkConfigControlPlaneV2ConfigControlPlaneIpBlockIp {
    /**
     * Hostname of the machine. VM's name will be used if this field is empty.
     */
    hostname: string;
    /**
     * IP could be an IP address (like 1.2.3.4) or a CIDR (like 1.2.3.0/24).
     */
    ip?: string;
}

export interface GkeonpremVmwareClusterNetworkConfigDhcpIpConfig {
    /**
     * enabled is a flag to mark if DHCP IP allocation is
     * used for VMware user clusters.
     */
    enabled: boolean;
}

export interface GkeonpremVmwareClusterNetworkConfigHostConfig {
    /**
     * DNS search domains.
     */
    dnsSearchDomains?: string[];
    /**
     * DNS servers.
     */
    dnsServers?: string[];
    /**
     * NTP servers.
     */
    ntpServers?: string[];
}

export interface GkeonpremVmwareClusterNetworkConfigStaticIpConfig {
    /**
     * Represents the configuration values for static IP allocation to nodes.
     */
    ipBlocks: outputs.GkeonpremVmwareClusterNetworkConfigStaticIpConfigIpBlock[];
}

export interface GkeonpremVmwareClusterNetworkConfigStaticIpConfigIpBlock {
    /**
     * The network gateway used by the VMware User Cluster.
     */
    gateway: string;
    /**
     * The node's network configurations used by the VMware User Cluster.
     */
    ips: outputs.GkeonpremVmwareClusterNetworkConfigStaticIpConfigIpBlockIp[];
    /**
     * The netmask used by the VMware User Cluster.
     */
    netmask: string;
}

export interface GkeonpremVmwareClusterNetworkConfigStaticIpConfigIpBlockIp {
    /**
     * Hostname of the machine. VM's name will be used if this field is empty.
     */
    hostname: string;
    /**
     * IP could be an IP address (like 1.2.3.4) or a CIDR (like 1.2.3.0/24).
     */
    ip: string;
}

export interface GkeonpremVmwareClusterStatus {
    conditions: outputs.GkeonpremVmwareClusterStatusCondition[];
    errorMessage: string;
}

export interface GkeonpremVmwareClusterStatusCondition {
    lastTransitionTime: string;
    message: string;
    reason: string;
    state: string;
    type: string;
}

export interface GkeonpremVmwareClusterStorage {
    /**
     * Whether or not to deploy vSphere CSI components in the VMware User Cluster.
     * Enabled by default.
     */
    vsphereCsiDisabled: boolean;
}

export interface GkeonpremVmwareClusterTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface GkeonpremVmwareClusterUpgradePolicy {
    /**
     * Controls whether the upgrade applies to the control plane only.
     */
    controlPlaneOnly?: boolean;
}

export interface GkeonpremVmwareClusterValidationCheck {
    options: string;
    scenario: string;
    statuses: outputs.GkeonpremVmwareClusterValidationCheckStatus[];
}

export interface GkeonpremVmwareClusterValidationCheckStatus {
    results: outputs.GkeonpremVmwareClusterValidationCheckStatusResult[];
}

export interface GkeonpremVmwareClusterValidationCheckStatusResult {
    category: string;
    description: string;
    details: string;
    options: string;
    reason: string;
}

export interface GkeonpremVmwareClusterVcenter {
    /**
     * The vCenter IP address.
     */
    address: string;
    /**
     * Contains the vCenter CA certificate public key for SSL verification.
     */
    caCertData?: string;
    /**
     * The name of the vCenter cluster for the user cluster.
     */
    cluster?: string;
    /**
     * The name of the vCenter datacenter for the user cluster.
     */
    datacenter?: string;
    /**
     * The name of the vCenter datastore for the user cluster.
     */
    datastore?: string;
    /**
     * The name of the vCenter folder for the user cluster.
     */
    folder?: string;
    /**
     * The name of the vCenter resource pool for the user cluster.
     */
    resourcePool?: string;
    /**
     * The name of the vCenter storage policy for the user cluster.
     */
    storagePolicyName?: string;
}

export interface GkeonpremVmwareNodePoolConfig {
    /**
     * VMware disk size to be used during creation.
     */
    bootDiskSizeGb?: number;
    /**
     * The number of CPUs for each node in the node pool.
     */
    cpus?: number;
    /**
     * Allow node pool traffic to be load balanced. Only works for clusters with
     * MetalLB load balancers.
     */
    enableLoadBalancer?: boolean;
    /**
     * The OS image name in vCenter, only valid when using Windows.
     */
    image?: string;
    /**
     * The OS image to be used for each node in a node pool.
     * Currently 'cos', 'ubuntu', 'ubuntu_containerd' and 'windows' are supported.
     */
    imageType: string;
    /**
     * The map of Kubernetes labels (key/value pairs) to be applied to each node.
     * These will added in addition to any default label(s) that
     * Kubernetes may apply to the node.
     * In case of conflict in label keys, the applied set may differ depending on
     * the Kubernetes version -- it's best to assume the behavior is undefined
     * and conflicts should be avoided.
     */
    labels: {[key: string]: string};
    /**
     * The megabytes of memory for each node in the node pool.
     */
    memoryMb?: number;
    /**
     * The number of nodes in the node pool.
     */
    replicas?: number;
    /**
     * The initial taints assigned to nodes of this node pool.
     */
    taints?: outputs.GkeonpremVmwareNodePoolConfigTaint[];
    /**
     * Specifies the vSphere config for node pool.
     */
    vsphereConfig?: outputs.GkeonpremVmwareNodePoolConfigVsphereConfig;
}

export interface GkeonpremVmwareNodePoolConfigTaint {
    /**
     * Available taint effects. Possible values: ["EFFECT_UNSPECIFIED", "NO_SCHEDULE", "PREFER_NO_SCHEDULE", "NO_EXECUTE"]
     */
    effect?: string;
    /**
     * Key associated with the effect.
     */
    key: string;
    /**
     * Value associated with the effect.
     */
    value: string;
}

export interface GkeonpremVmwareNodePoolConfigVsphereConfig {
    /**
     * The name of the vCenter datastore. Inherited from the user cluster.
     */
    datastore?: string;
    /**
     * Vsphere host groups to apply to all VMs in the node pool
     */
    hostGroups?: string[];
    /**
     * Tags to apply to VMs.
     */
    tags?: outputs.GkeonpremVmwareNodePoolConfigVsphereConfigTag[];
}

export interface GkeonpremVmwareNodePoolConfigVsphereConfigTag {
    /**
     * The Vsphere tag category.
     */
    category?: string;
    /**
     * The Vsphere tag name.
     */
    tag?: string;
}

export interface GkeonpremVmwareNodePoolNodePoolAutoscaling {
    /**
     * Maximum number of replicas in the NodePool.
     */
    maxReplicas: number;
    /**
     * Minimum number of replicas in the NodePool.
     */
    minReplicas: number;
}

export interface GkeonpremVmwareNodePoolStatus {
    conditions: outputs.GkeonpremVmwareNodePoolStatusCondition[];
    errorMessage: string;
}

export interface GkeonpremVmwareNodePoolStatusCondition {
    lastTransitionTime: string;
    message: string;
    reason: string;
    state: string;
    type: string;
}

export interface GkeonpremVmwareNodePoolTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface HealthcareConsentStoreIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface HealthcareConsentStoreIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface HealthcareConsentStoreTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface HealthcareDatasetEncryptionSpec {
    /**
     * KMS encryption key that is used to secure this dataset and its sub-resources. The key used for
     * encryption and the dataset must be in the same location. If empty, the default Google encryption
     * key will be used to secure this dataset. The format is
     * projects/{projectId}/locations/{locationId}/keyRings/{keyRingId}/cryptoKeys/{keyId}.
     */
    kmsKeyName?: string;
}

export interface HealthcareDatasetIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface HealthcareDatasetIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface HealthcareDatasetTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface HealthcareDicomStoreIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface HealthcareDicomStoreIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface HealthcareDicomStoreNotificationConfig {
    /**
     * The Cloud Pub/Sub topic that notifications of changes are published on. Supplied by the client.
     * PubsubMessage.Data will contain the resource name. PubsubMessage.MessageId is the ID of this message.
     * It is guaranteed to be unique within the topic. PubsubMessage.PublishTime is the time at which the message
     * was published. Notifications are only sent if the topic is non-empty. Topic names must be scoped to a
     * project. service-PROJECT_NUMBER@gcp-sa-healthcare.iam.gserviceaccount.com must have publisher permissions on the given
     * Cloud Pub/Sub topic. Not having adequate permissions will cause the calls that send notifications to fail.
     */
    pubsubTopic: string;
    /**
     * Indicates whether or not to send Pub/Sub notifications on bulk import. Only supported for DICOM imports.
     */
    sendForBulkImport?: boolean;
}

export interface HealthcareDicomStoreTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface HealthcareFhirStoreIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface HealthcareFhirStoreIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface HealthcareFhirStoreNotificationConfig {
    /**
     * The Cloud Pub/Sub topic that notifications of changes are published on. Supplied by the client.
     * PubsubMessage.Data will contain the resource name. PubsubMessage.MessageId is the ID of this message.
     * It is guaranteed to be unique within the topic. PubsubMessage.PublishTime is the time at which the message
     * was published. Notifications are only sent if the topic is non-empty. Topic names must be scoped to a
     * project. service-PROJECT_NUMBER@gcp-sa-healthcare.iam.gserviceaccount.com must have publisher permissions on the given
     * Cloud Pub/Sub topic. Not having adequate permissions will cause the calls that send notifications to fail.
     */
    pubsubTopic: string;
    /**
     * Whether to send full FHIR resource to this Pub/Sub topic for Create and Update operation.
     * Note that setting this to true does not guarantee that all resources will be sent in the format of
     * full FHIR resource. When a resource change is too large or during heavy traffic, only the resource name will be
     * sent. Clients should always check the "payloadType" label from a Pub/Sub message to determine whether
     * it needs to fetch the full resource as a separate operation.
     */
    sendFullResource?: boolean;
    /**
     * Whether to send full FHIR resource to this Pub/Sub topic for deleting FHIR resource. Note that setting this to
     * true does not guarantee that all previous resources will be sent in the format of full FHIR resource. When a
     * resource change is too large or during heavy traffic, only the resource name will be sent. Clients should always
     * check the "payloadType" label from a Pub/Sub message to determine whether it needs to fetch the full previous
     * resource as a separate operation.
     */
    sendPreviousResourceOnDelete?: boolean;
}

export interface HealthcareFhirStoreStreamConfig {
    /**
     * The destination BigQuery structure that contains both the dataset location and corresponding schema config.
     * The output is organized in one table per resource type. The server reuses the existing tables (if any) that
     * are named after the resource types, e.g. "Patient", "Observation". When there is no existing table for a given
     * resource type, the server attempts to create one.
     * See the [streaming config reference](https://cloud.google.com/healthcare/docs/reference/rest/v1beta1/projects.locations.datasets.fhirStores#streamconfig) for more details.
     */
    bigqueryDestination: outputs.HealthcareFhirStoreStreamConfigBigqueryDestination;
    /**
     * Supply a FHIR resource type (such as "Patient" or "Observation"). See
     * https://www.hl7.org/fhir/valueset-resource-types.html for a list of all FHIR resource types. The server treats
     * an empty list as an intent to stream all the supported resource types in this FHIR store.
     */
    resourceTypes?: string[];
}

export interface HealthcareFhirStoreStreamConfigBigqueryDestination {
    /**
     * BigQuery URI to a dataset, up to 2000 characters long, in the format bq://projectId.bqDatasetId
     */
    datasetUri: string;
    /**
     * The configuration for the exported BigQuery schema.
     */
    schemaConfig: outputs.HealthcareFhirStoreStreamConfigBigqueryDestinationSchemaConfig;
}

export interface HealthcareFhirStoreStreamConfigBigqueryDestinationSchemaConfig {
    /**
     * The configuration for exported BigQuery tables to be partitioned by FHIR resource's last updated time column.
     */
    lastUpdatedPartitionConfig?: outputs.HealthcareFhirStoreStreamConfigBigqueryDestinationSchemaConfigLastUpdatedPartitionConfig;
    /**
     * The depth for all recursive structures in the output analytics schema. For example, concept in the CodeSystem
     * resource is a recursive structure; when the depth is 2, the CodeSystem table will have a column called
     * concept.concept but not concept.concept.concept. If not specified or set to 0, the server will use the default
     * value 2. The maximum depth allowed is 5.
     */
    recursiveStructureDepth: number;
    /**
     * Specifies the output schema type.
     *  * ANALYTICS: Analytics schema defined by the FHIR community.
     *   See https://github.com/FHIR/sql-on-fhir/blob/master/sql-on-fhir.md.
     *  * ANALYTICS_V2: Analytics V2, similar to schema defined by the FHIR community, with added support for extensions with one or more occurrences and contained resources in stringified JSON.
     *  * LOSSLESS: A data-driven schema generated from the fields present in the FHIR data being exported, with no additional simplification. Default value: "ANALYTICS" Possible values: ["ANALYTICS", "ANALYTICS_V2", "LOSSLESS"]
     */
    schemaType?: string;
}

export interface HealthcareFhirStoreStreamConfigBigqueryDestinationSchemaConfigLastUpdatedPartitionConfig {
    /**
     * Number of milliseconds for which to keep the storage for a partition.
     */
    expirationMs?: string;
    /**
     * Type of partitioning. Possible values: ["PARTITION_TYPE_UNSPECIFIED", "HOUR", "DAY", "MONTH", "YEAR"]
     */
    type: string;
}

export interface HealthcareFhirStoreTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface HealthcareHl7V2StoreIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface HealthcareHl7V2StoreIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface HealthcareHl7V2StoreNotificationConfig {
    /**
     * Restricts notifications sent for messages matching a filter. If this is empty, all messages
     * are matched. Syntax: https://cloud.google.com/appengine/docs/standard/python/search/query_strings
     *
     * Fields/functions available for filtering are:
     *
     * * messageType, from the MSH-9.1 field. For example, NOT messageType = "ADT".
     * * send_date or sendDate, the YYYY-MM-DD date the message was sent in the dataset's timeZone, from the MSH-7 segment. For example, send_date < "2017-01-02".
     * * sendTime, the timestamp when the message was sent, using the RFC3339 time format for comparisons, from the MSH-7 segment. For example, sendTime < "2017-01-02T00:00:00-05:00".
     * * sendFacility, the care center that the message came from, from the MSH-4 segment. For example, sendFacility = "ABC".
     * * PatientId(value, type), which matches if the message lists a patient having an ID of the given value and type in the PID-2, PID-3, or PID-4 segments. For example, PatientId("123456", "MRN").
     * * labels.x, a string value of the label with key x as set using the Message.labels map. For example, labels."priority"="high". The operator :* can be used to assert the existence of a label. For example, labels."priority":*.
     */
    filter?: string;
    /**
     * The Cloud Pub/Sub topic that notifications of changes are published on. Supplied by the client.
     * PubsubMessage.Data will contain the resource name. PubsubMessage.MessageId is the ID of this message.
     * It is guaranteed to be unique within the topic. PubsubMessage.PublishTime is the time at which the message
     * was published. Notifications are only sent if the topic is non-empty. Topic names must be scoped to a
     * project. service-PROJECT_NUMBER@gcp-sa-healthcare.iam.gserviceaccount.com must have publisher permissions on the given
     * Cloud Pub/Sub topic. Not having adequate permissions will cause the calls that send notifications to fail.
     *
     * If a notification cannot be published to Cloud Pub/Sub, errors will be logged to Stackdriver
     */
    pubsubTopic: string;
}

export interface HealthcareHl7V2StoreParserConfig {
    /**
     * Determines whether messages with no header are allowed.
     */
    allowNullHeader?: boolean;
    /**
     * JSON encoded string for schemas used to parse messages in this
     * store if schematized parsing is desired.
     */
    schema?: string;
    /**
     * Byte(s) to be used as the segment terminator. If this is unset, '\r' will be used as segment terminator.
     *
     * A base64-encoded string.
     */
    segmentTerminator?: string;
    /**
     * The version of the unschematized parser to be used when a custom 'schema' is not set. Default value: "V1" Possible values: ["V1", "V2", "V3"]
     */
    version?: string;
}

export interface HealthcareHl7V2StoreTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface IamAccessBoundaryPolicyRule {
    /**
     * An access boundary rule in an IAM policy.
     */
    accessBoundaryRule?: outputs.IamAccessBoundaryPolicyRuleAccessBoundaryRule;
    /**
     * The description of the rule.
     */
    description?: string;
}

export interface IamAccessBoundaryPolicyRuleAccessBoundaryRule {
    /**
     * The availability condition further constrains the access allowed by the access boundary rule.
     */
    availabilityCondition?: outputs.IamAccessBoundaryPolicyRuleAccessBoundaryRuleAvailabilityCondition;
    /**
     * A list of permissions that may be allowed for use on the specified resource.
     */
    availablePermissions?: string[];
    /**
     * The full resource name of a Google Cloud resource entity.
     */
    availableResource?: string;
}

export interface IamAccessBoundaryPolicyRuleAccessBoundaryRuleAvailabilityCondition {
    /**
     * Description of the expression. This is a longer text which describes the expression,
     * e.g. when hovered over it in a UI.
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression: string;
    /**
     * String indicating the location of the expression for error reporting,
     * e.g. a file name and a position in the file.
     */
    location?: string;
    /**
     * Title for the expression, i.e. a short string describing its purpose.
     * This can be used e.g. in UIs which allow to enter the expression.
     */
    title?: string;
}

export interface IamAccessBoundaryPolicyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface IamDenyPolicyRule {
    /**
     * A deny rule in an IAM deny policy.
     */
    denyRule?: outputs.IamDenyPolicyRuleDenyRule;
    /**
     * The description of the rule.
     */
    description?: string;
}

export interface IamDenyPolicyRuleDenyRule {
    /**
     * User defined CEVAL expression. A CEVAL expression is used to specify match criteria such as origin.ip, source.region_code and contents in the request header.
     */
    denialCondition?: outputs.IamDenyPolicyRuleDenyRuleDenialCondition;
    /**
     * The permissions that are explicitly denied by this rule. Each permission uses the format '{service-fqdn}/{resource}.{verb}',
     * where '{service-fqdn}' is the fully qualified domain name for the service. For example, 'iam.googleapis.com/roles.list'.
     */
    deniedPermissions?: string[];
    /**
     * The identities that are prevented from using one or more permissions on Google Cloud resources.
     */
    deniedPrincipals?: string[];
    /**
     * Specifies the permissions that this rule excludes from the set of denied permissions given by deniedPermissions.
     * If a permission appears in deniedPermissions and in exceptionPermissions then it will not be denied.
     * The excluded permissions can be specified using the same syntax as deniedPermissions.
     */
    exceptionPermissions?: string[];
    /**
     * The identities that are excluded from the deny rule, even if they are listed in the deniedPrincipals.
     * For example, you could add a Google group to the deniedPrincipals, then exclude specific users who belong to that group.
     */
    exceptionPrincipals?: string[];
}

export interface IamDenyPolicyRuleDenyRuleDenialCondition {
    /**
     * Description of the expression. This is a longer text which describes the expression,
     * e.g. when hovered over it in a UI.
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression: string;
    /**
     * String indicating the location of the expression for error reporting,
     * e.g. a file name and a position in the file.
     */
    location?: string;
    /**
     * Title for the expression, i.e. a short string describing its purpose.
     * This can be used e.g. in UIs which allow to enter the expression.
     */
    title?: string;
}

export interface IamDenyPolicyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface IamWorkforcePoolAccessRestrictions {
    /**
     * Services allowed for web sign-in with the workforce pool.
     * If not set by default there are no restrictions.
     */
    allowedServices?: outputs.IamWorkforcePoolAccessRestrictionsAllowedService[];
    /**
     * Disable programmatic sign-in by disabling token issue via the Security Token API endpoint.
     * See [Security Token Service API](https://cloud.google.com/iam/docs/reference/sts/rest).
     */
    disableProgrammaticSignin?: boolean;
}

export interface IamWorkforcePoolAccessRestrictionsAllowedService {
    /**
     * Domain name of the service.
     * Example: console.cloud.google
     */
    domain?: string;
}

export interface IamWorkforcePoolProviderExtraAttributesOauth2Client {
    /**
     * Represents the IdP and type of claims that should be fetched.
     * * AZURE_AD_GROUPS_MAIL: Used to get the user's group claims from the Azure AD identity provider using configuration provided
     * in ExtraAttributesOAuth2Client and 'mail' property of the 'microsoft.graph.group' object is used for claim mapping.
     * See https://learn.microsoft.com/en-us/graph/api/resources/group?view=graph-rest-1.0#properties for more details on
     * 'microsoft.graph.group' properties. The attributes obtained from idntity provider are mapped to 'assertion.groups'. Possible values: ["AZURE_AD_GROUPS_MAIL"]
     */
    attributesType: string;
    /**
     * The OAuth 2.0 client ID for retrieving extra attributes from the identity provider. Required to get the Access Token using client credentials grant flow.
     */
    clientId: string;
    /**
     * The OAuth 2.0 client secret for retrieving extra attributes from the identity provider. Required to get the Access Token using client credentials grant flow.
     */
    clientSecret: outputs.IamWorkforcePoolProviderExtraAttributesOauth2ClientClientSecret;
    /**
     * The OIDC identity provider's issuer URI. Must be a valid URI using the 'https' scheme. Required to get the OIDC discovery document.
     */
    issuerUri: string;
    /**
     * Represents the parameters to control which claims are fetched from an IdP.
     */
    queryParameters?: outputs.IamWorkforcePoolProviderExtraAttributesOauth2ClientQueryParameters;
}

export interface IamWorkforcePoolProviderExtraAttributesOauth2ClientClientSecret {
    /**
     * The value of the client secret.
     */
    value?: outputs.IamWorkforcePoolProviderExtraAttributesOauth2ClientClientSecretValue;
}

export interface IamWorkforcePoolProviderExtraAttributesOauth2ClientClientSecretValue {
    /**
     * The plain text of the client secret value.
     */
    plainText: string;
    /**
     * A thumbprint to represent the current client secret value.
     */
    thumbprint: string;
}

export interface IamWorkforcePoolProviderExtraAttributesOauth2ClientQueryParameters {
    /**
     * The filter used to request specific records from IdP. In case of attributes type as AZURE_AD_GROUPS_MAIL, it represents the
     * filter used to request specific groups for users from IdP. By default, all of the groups associated with the user are fetched. The
     * groups should be mail enabled and security enabled. See https://learn.microsoft.com/en-us/graph/search-query-parameter for more details.
     */
    filter?: string;
}

export interface IamWorkforcePoolProviderOidc {
    /**
     * The client ID. Must match the audience claim of the JWT issued by the identity provider.
     */
    clientId: string;
    /**
     * The optional client secret. Required to enable Authorization Code flow for web sign-in.
     */
    clientSecret?: outputs.IamWorkforcePoolProviderOidcClientSecret;
    /**
     * The OIDC issuer URI. Must be a valid URI using the 'https' scheme.
     */
    issuerUri: string;
    /**
     * OIDC JWKs in JSON String format. For details on definition of a
     * JWK, see https:tools.ietf.org/html/rfc7517. If not set, then we
     * use the 'jwks_uri' from the discovery document fetched from the
     * .well-known path for the 'issuer_uri'. Currently, RSA and EC asymmetric
     * keys are supported. The JWK must use following format and include only
     * the following fields:
     * '''
     * {
     *   "keys": [
     *     {
     *           "kty": "RSA/EC",
     *           "alg": "<algorithm>",
     *           "use": "sig",
     *           "kid": "<key-id>",
     *           "n": "",
     *           "e": "",
     *           "x": "",
     *           "y": "",
     *           "crv": ""
     *     }
     *   ]
     * }
     * '''
     */
    jwksJson?: string;
    /**
     * Configuration for web single sign-on for the OIDC provider. Here, web sign-in refers to console sign-in and gcloud sign-in through the browser.
     */
    webSsoConfig?: outputs.IamWorkforcePoolProviderOidcWebSsoConfig;
}

export interface IamWorkforcePoolProviderOidcClientSecret {
    /**
     * The value of the client secret.
     */
    value?: outputs.IamWorkforcePoolProviderOidcClientSecretValue;
}

export interface IamWorkforcePoolProviderOidcClientSecretValue {
    /**
     * The plain text of the client secret value.
     */
    plainText: string;
    /**
     * A thumbprint to represent the current client secret value.
     */
    thumbprint: string;
}

export interface IamWorkforcePoolProviderOidcWebSsoConfig {
    /**
     * Additional scopes to request for in the OIDC authentication request on top of scopes requested by default. By default, the 'openid', 'profile' and 'email' scopes that are supported by the identity provider are requested.
     * Each additional scope may be at most 256 characters. A maximum of 10 additional scopes may be configured.
     */
    additionalScopes?: string[];
    /**
     * The behavior for how OIDC Claims are included in the 'assertion' object used for attribute mapping and attribute condition.
     * * MERGE_USER_INFO_OVER_ID_TOKEN_CLAIMS: Merge the UserInfo Endpoint Claims with ID Token Claims, preferring UserInfo Claim Values for the same Claim Name. This option is available only for the Authorization Code Flow.
     * * ONLY_ID_TOKEN_CLAIMS: Only include ID Token Claims. Possible values: ["MERGE_USER_INFO_OVER_ID_TOKEN_CLAIMS", "ONLY_ID_TOKEN_CLAIMS"]
     */
    assertionClaimsBehavior: string;
    /**
     * The Response Type to request for in the OIDC Authorization Request for web sign-in.
     *
     * The 'CODE' Response Type is recommended to avoid the Implicit Flow, for security reasons.
     * * CODE: The 'response_type=code' selection uses the Authorization Code Flow for web sign-in. Requires a configured client secret.
     * * ID_TOKEN: The 'response_type=id_token' selection uses the Implicit Flow for web sign-in. Possible values: ["CODE", "ID_TOKEN"]
     */
    responseType: string;
}

export interface IamWorkforcePoolProviderSaml {
    /**
     * SAML Identity provider configuration metadata xml doc.
     * The xml document should comply with [SAML 2.0 specification](https://docs.oasis-open.org/security/saml/v2.0/saml-metadata-2.0-os.pdf).
     * The max size of the acceptable xml document will be bounded to 128k characters.
     *
     * The metadata xml document should satisfy the following constraints:
     * 1) Must contain an Identity Provider Entity ID.
     * 2) Must contain at least one non-expired signing key certificate.
     * 3) For each signing key:
     *   a) Valid from should be no more than 7 days from now.
     *   b) Valid to should be no more than 10 years in the future.
     * 4) Up to 3 IdP signing keys are allowed in the metadata xml.
     *
     * When updating the provider's metadata xml, at least one non-expired signing key
     * must overlap with the existing metadata. This requirement is skipped if there are
     * no non-expired signing keys present in the existing metadata.
     */
    idpMetadataXml: string;
}

export interface IamWorkforcePoolProviderTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface IamWorkforcePoolTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface IamWorkloadIdentityPoolProviderAws {
    /**
     * The AWS account ID.
     */
    accountId: string;
}

export interface IamWorkloadIdentityPoolProviderOidc {
    /**
     * Acceptable values for the 'aud' field (audience) in the OIDC token. Token exchange
     * requests are rejected if the token audience does not match one of the configured
     * values. Each audience may be at most 256 characters. A maximum of 10 audiences may
     * be configured.
     *
     * If this list is empty, the OIDC token audience must be equal to the full canonical
     * resource name of the WorkloadIdentityPoolProvider, with or without the HTTPS prefix.
     * For example:
     * '''
     * //iam.googleapis.com/projects/<project-number>/locations/<location>/workloadIdentityPools/<pool-id>/providers/<provider-id>
     * https://iam.googleapis.com/projects/<project-number>/locations/<location>/workloadIdentityPools/<pool-id>/providers/<provider-id>
     * '''
     */
    allowedAudiences?: string[];
    /**
     * The OIDC issuer URL.
     */
    issuerUri: string;
    /**
     * OIDC JWKs in JSON String format. For details on definition of a
     * JWK, see https:tools.ietf.org/html/rfc7517. If not set, then we
     * use the 'jwks_uri' from the discovery document fetched from the
     * .well-known path for the 'issuer_uri'. Currently, RSA and EC asymmetric
     * keys are supported. The JWK must use following format and include only
     * the following fields:
     * '''
     * {
     *   "keys": [
     *     {
     *           "kty": "RSA/EC",
     *           "alg": "<algorithm>",
     *           "use": "sig",
     *           "kid": "<key-id>",
     *           "n": "",
     *           "e": "",
     *           "x": "",
     *           "y": "",
     *           "crv": ""
     *     }
     *   ]
     * }
     * '''
     */
    jwksJson?: string;
}

export interface IamWorkloadIdentityPoolProviderSaml {
    /**
     * SAML Identity provider configuration metadata xml doc.
     */
    idpMetadataXml: string;
}

export interface IamWorkloadIdentityPoolProviderTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface IamWorkloadIdentityPoolTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface IapAppEngineServiceIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface IapAppEngineServiceIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface IapAppEngineVersionIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface IapAppEngineVersionIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface IapBrandTimeouts {
    create?: string;
    delete?: string;
}

export interface IapClientTimeouts {
    create?: string;
    delete?: string;
}

export interface IapTunnelDestGroupIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface IapTunnelDestGroupIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface IapTunnelDestGroupTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface IapTunnelIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface IapTunnelIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface IapTunnelInstanceIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface IapTunnelInstanceIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface IapWebBackendServiceIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface IapWebBackendServiceIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface IapWebIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface IapWebIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface IapWebRegionBackendServiceIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface IapWebRegionBackendServiceIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface IapWebTypeAppEngineIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface IapWebTypeAppEngineIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface IapWebTypeComputeIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface IapWebTypeComputeIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface IdentityPlatformConfigBlockingFunctions {
    /**
     * The user credentials to include in the JWT payload that is sent to the registered Blocking Functions.
     */
    forwardInboundCredentials?: outputs.IdentityPlatformConfigBlockingFunctionsForwardInboundCredentials;
    /**
     * Map of Trigger to event type. Key should be one of the supported event types: "beforeCreate", "beforeSignIn".
     */
    triggers: outputs.IdentityPlatformConfigBlockingFunctionsTrigger[];
}

export interface IdentityPlatformConfigBlockingFunctionsForwardInboundCredentials {
    /**
     * Whether to pass the user's OAuth identity provider's access token.
     */
    accessToken?: boolean;
    /**
     * Whether to pass the user's OIDC identity provider's ID token.
     */
    idToken?: boolean;
    /**
     * Whether to pass the user's OAuth identity provider's refresh token.
     */
    refreshToken?: boolean;
}

export interface IdentityPlatformConfigBlockingFunctionsTrigger {
    eventType: string;
    /**
     * HTTP URI trigger for the Cloud Function.
     */
    functionUri: string;
    /**
     * When the trigger was changed.
     */
    updateTime: string;
}

export interface IdentityPlatformConfigClient {
    /**
     * API key that can be used when making requests for this project.
     */
    apiKey: string;
    /**
     * Firebase subdomain.
     */
    firebaseSubdomain: string;
    /**
     * Configuration related to restricting a user's ability to affect their account.
     */
    permissions?: outputs.IdentityPlatformConfigClientPermissions;
}

export interface IdentityPlatformConfigClientPermissions {
    /**
     * When true, end users cannot delete their account on the associated project through any of our API methods
     */
    disabledUserDeletion?: boolean;
    /**
     * When true, end users cannot sign up for a new account on the associated project through any of our API methods
     */
    disabledUserSignup?: boolean;
}

export interface IdentityPlatformConfigMfa {
    /**
     * A list of usable second factors for this project. Possible values: ["PHONE_SMS"]
     */
    enabledProviders?: string[];
    /**
     * A list of usable second factors for this project along with their configurations.
     * This field does not support phone based MFA, for that use the 'enabledProviders' field.
     */
    providerConfigs?: outputs.IdentityPlatformConfigMfaProviderConfig[];
    /**
     * Whether MultiFactor Authentication has been enabled for this project. Possible values: ["DISABLED", "ENABLED", "MANDATORY"]
     */
    state: string;
}

export interface IdentityPlatformConfigMfaProviderConfig {
    /**
     * Whether MultiFactor Authentication has been enabled for this project. Possible values: ["DISABLED", "ENABLED", "MANDATORY"]
     */
    state: string;
    /**
     * TOTP MFA provider config for this project.
     */
    totpProviderConfig?: outputs.IdentityPlatformConfigMfaProviderConfigTotpProviderConfig;
}

export interface IdentityPlatformConfigMfaProviderConfigTotpProviderConfig {
    /**
     * The allowed number of adjacent intervals that will be used for verification to avoid clock skew.
     */
    adjacentIntervals?: number;
}

export interface IdentityPlatformConfigMonitoring {
    /**
     * Configuration for logging requests made to this project to Stackdriver Logging
     */
    requestLogging?: outputs.IdentityPlatformConfigMonitoringRequestLogging;
}

export interface IdentityPlatformConfigMonitoringRequestLogging {
    /**
     * Whether logging is enabled for this project or not.
     */
    enabled?: boolean;
}

export interface IdentityPlatformConfigMultiTenant {
    /**
     * Whether this project can have tenants or not.
     */
    allowTenants?: boolean;
    /**
     * The default cloud parent org or folder that the tenant project should be created under.
     * The parent resource name should be in the format of "/", such as "folders/123" or "organizations/456".
     * If the value is not set, the tenant will be created under the same organization or folder as the agent project.
     */
    defaultTenantLocation?: string;
}

export interface IdentityPlatformConfigQuota {
    /**
     * Quota for the Signup endpoint, if overwritten. Signup quota is measured in sign ups per project per hour per IP.
     */
    signUpQuotaConfig?: outputs.IdentityPlatformConfigQuotaSignUpQuotaConfig;
}

export interface IdentityPlatformConfigQuotaSignUpQuotaConfig {
    /**
     * A sign up APIs quota that customers can override temporarily.
     */
    quota?: number;
    /**
     * How long this quota will be active for. It is measurred in seconds, e.g., Example: "9.615s".
     */
    quotaDuration?: string;
    /**
     * When this quota will take affect.
     */
    startTime?: string;
}

export interface IdentityPlatformConfigSignIn {
    /**
     * Whether to allow more than one account to have the same email.
     */
    allowDuplicateEmails?: boolean;
    /**
     * Configuration options related to authenticating an anonymous user.
     */
    anonymous?: outputs.IdentityPlatformConfigSignInAnonymous;
    /**
     * Configuration options related to authenticating a user by their email address.
     */
    email?: outputs.IdentityPlatformConfigSignInEmail;
    /**
     * Output only. Hash config information.
     */
    hashConfigs: outputs.IdentityPlatformConfigSignInHashConfig[];
    /**
     * Configuration options related to authenticated a user by their phone number.
     */
    phoneNumber?: outputs.IdentityPlatformConfigSignInPhoneNumber;
}

export interface IdentityPlatformConfigSignInAnonymous {
    /**
     * Whether anonymous user auth is enabled for the project or not.
     */
    enabled: boolean;
}

export interface IdentityPlatformConfigSignInEmail {
    /**
     * Whether email auth is enabled for the project or not.
     */
    enabled: boolean;
    /**
     * Whether a password is required for email auth or not. If true, both an email and
     * password must be provided to sign in. If false, a user may sign in via either
     * email/password or email link.
     */
    passwordRequired?: boolean;
}

export interface IdentityPlatformConfigSignInHashConfig {
    algorithm: string;
    memoryCost: number;
    rounds: number;
    saltSeparator: string;
    signerKey: string;
}

export interface IdentityPlatformConfigSignInPhoneNumber {
    /**
     * Whether phone number auth is enabled for the project or not.
     */
    enabled: boolean;
    /**
     * A map of <test phone number, fake code> that can be used for phone auth testing.
     */
    testPhoneNumbers?: {[key: string]: string};
}

export interface IdentityPlatformConfigSmsRegionConfig {
    /**
     * A policy of allowing SMS to every region by default and adding disallowed regions to a disallow list.
     */
    allowByDefault?: outputs.IdentityPlatformConfigSmsRegionConfigAllowByDefault;
    /**
     * A policy of only allowing regions by explicitly adding them to an allowlist.
     */
    allowlistOnly?: outputs.IdentityPlatformConfigSmsRegionConfigAllowlistOnly;
}

export interface IdentityPlatformConfigSmsRegionConfigAllowByDefault {
    /**
     * Two letter unicode region codes to disallow as defined by https://cldr.unicode.org/ The full list of these region codes is here: https://github.com/unicode-cldr/cldr-localenames-full/blob/master/main/en/territories.json
     */
    disallowedRegions?: string[];
}

export interface IdentityPlatformConfigSmsRegionConfigAllowlistOnly {
    /**
     * Two letter unicode region codes to allow as defined by https://cldr.unicode.org/ The full list of these region codes is here: https://github.com/unicode-cldr/cldr-localenames-full/blob/master/main/en/territories.json
     */
    allowedRegions?: string[];
}

export interface IdentityPlatformConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface IdentityPlatformDefaultSupportedIdpConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface IdentityPlatformInboundSamlConfigIdpConfig {
    /**
     * The IdP's certificate data to verify the signature in the SAMLResponse issued by the IDP.
     */
    idpCertificates: outputs.IdentityPlatformInboundSamlConfigIdpConfigIdpCertificate[];
    /**
     * Unique identifier for all SAML entities
     */
    idpEntityId: string;
    /**
     * Indicates if outbounding SAMLRequest should be signed.
     */
    signRequest?: boolean;
    /**
     * URL to send Authentication request to.
     */
    ssoUrl: string;
}

export interface IdentityPlatformInboundSamlConfigIdpConfigIdpCertificate {
    /**
     * The IdP's x509 certificate.
     */
    x509Certificate?: string;
}

export interface IdentityPlatformInboundSamlConfigSpConfig {
    /**
     * Callback URI where responses from IDP are handled. Must start with 'https://'.
     */
    callbackUri?: string;
    /**
     * The IDP's certificate data to verify the signature in the SAMLResponse issued by the IDP.
     */
    spCertificates: outputs.IdentityPlatformInboundSamlConfigSpConfigSpCertificate[];
    /**
     * Unique identifier for all SAML entities.
     */
    spEntityId?: string;
}

export interface IdentityPlatformInboundSamlConfigSpConfigSpCertificate {
    x509Certificate: string;
}

export interface IdentityPlatformInboundSamlConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface IdentityPlatformOauthIdpConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface IdentityPlatformTenantDefaultSupportedIdpConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface IdentityPlatformTenantInboundSamlConfigIdpConfig {
    /**
     * The IDP's certificate data to verify the signature in the SAMLResponse issued by the IDP.
     */
    idpCertificates: outputs.IdentityPlatformTenantInboundSamlConfigIdpConfigIdpCertificate[];
    /**
     * Unique identifier for all SAML entities
     */
    idpEntityId: string;
    /**
     * Indicates if outbounding SAMLRequest should be signed.
     */
    signRequest?: boolean;
    /**
     * URL to send Authentication request to.
     */
    ssoUrl: string;
}

export interface IdentityPlatformTenantInboundSamlConfigIdpConfigIdpCertificate {
    /**
     * The x509 certificate
     */
    x509Certificate?: string;
}

export interface IdentityPlatformTenantInboundSamlConfigSpConfig {
    /**
     * Callback URI where responses from IDP are handled. Must start with 'https://'.
     */
    callbackUri: string;
    /**
     * The IDP's certificate data to verify the signature in the SAMLResponse issued by the IDP.
     */
    spCertificates: outputs.IdentityPlatformTenantInboundSamlConfigSpConfigSpCertificate[];
    /**
     * Unique identifier for all SAML entities.
     */
    spEntityId: string;
}

export interface IdentityPlatformTenantInboundSamlConfigSpConfigSpCertificate {
    x509Certificate: string;
}

export interface IdentityPlatformTenantInboundSamlConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface IdentityPlatformTenantOauthIdpConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface IdentityPlatformTenantTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface IntegrationConnectorsConnectionAuthConfig {
    /**
     * List containing additional auth configs.
     */
    additionalVariables?: outputs.IntegrationConnectorsConnectionAuthConfigAdditionalVariable[];
    /**
     * The type of authentication configured.
     */
    authKey?: string;
    /**
     * authType of the Connection Possible values: ["USER_PASSWORD", "OAUTH2_JWT_BEARER", "OAUTH2_CLIENT_CREDENTIALS", "SSH_PUBLIC_KEY", "OAUTH2_AUTH_CODE_FLOW"]
     */
    authType: string;
    /**
     * Parameters to support Oauth 2.0 Auth Code Grant Authentication.
     */
    oauth2AuthCodeFlow?: outputs.IntegrationConnectorsConnectionAuthConfigOauth2AuthCodeFlow;
    /**
     * OAuth3 Client Credentials for Authentication.
     */
    oauth2ClientCredentials?: outputs.IntegrationConnectorsConnectionAuthConfigOauth2ClientCredentials;
    /**
     * OAuth2 JWT Bearer for Authentication.
     */
    oauth2JwtBearer?: outputs.IntegrationConnectorsConnectionAuthConfigOauth2JwtBearer;
    /**
     * SSH Public Key for Authentication.
     */
    sshPublicKey?: outputs.IntegrationConnectorsConnectionAuthConfigSshPublicKey;
    /**
     * User password for Authentication.
     */
    userPassword?: outputs.IntegrationConnectorsConnectionAuthConfigUserPassword;
}

export interface IntegrationConnectorsConnectionAuthConfigAdditionalVariable {
    /**
     * Boolean Value of configVariable.
     */
    booleanValue?: boolean;
    /**
     * Encription key value of configVariable.
     */
    encryptionKeyValue?: outputs.IntegrationConnectorsConnectionAuthConfigAdditionalVariableEncryptionKeyValue;
    /**
     * Integer Value of configVariable.
     */
    integerValue?: number;
    /**
     * Key for the configVariable
     */
    key: string;
    /**
     * Secret value of configVariable.
     */
    secretValue?: outputs.IntegrationConnectorsConnectionAuthConfigAdditionalVariableSecretValue;
    /**
     * String Value of configVariabley.
     */
    stringValue?: string;
}

export interface IntegrationConnectorsConnectionAuthConfigAdditionalVariableEncryptionKeyValue {
    /**
     * The [KMS key name] with which the content of the Operation is encrypted. The
     * expected format: projects/*&#47;locations/*&#47;keyRings/*&#47;cryptoKeys/*.
     * Will be empty string if google managed.
     */
    kmsKeyName?: string;
    /**
     * Type of Encription Key Possible values: ["GOOGLE_MANAGED", "CUSTOMER_MANAGED"]
     */
    type: string;
}

export interface IntegrationConnectorsConnectionAuthConfigAdditionalVariableSecretValue {
    /**
     * Secret version of Secret Value for Config variable.
     */
    secretVersion: string;
}

export interface IntegrationConnectorsConnectionAuthConfigOauth2AuthCodeFlow {
    /**
     * Auth URL for Authorization Code Flow.
     */
    authUri?: string;
    /**
     * Client ID for user-provided OAuth app.
     */
    clientId?: string;
    /**
     * Client secret for user-provided OAuth app.
     */
    clientSecret?: outputs.IntegrationConnectorsConnectionAuthConfigOauth2AuthCodeFlowClientSecret;
    /**
     * Whether to enable PKCE when the user performs the auth code flow.
     */
    enablePkce?: boolean;
    /**
     * Scopes the connection will request when the user performs the auth code flow.
     */
    scopes?: string[];
}

export interface IntegrationConnectorsConnectionAuthConfigOauth2AuthCodeFlowClientSecret {
    /**
     * The resource name of the secret version in the format,
     * format as: projects/*&#47;secrets/*&#47;versions/*.
     */
    secretVersion: string;
}

export interface IntegrationConnectorsConnectionAuthConfigOauth2ClientCredentials {
    /**
     * Secret version of Password for Authentication.
     */
    clientId: string;
    /**
     * Secret version reference containing the client secret.
     */
    clientSecret?: outputs.IntegrationConnectorsConnectionAuthConfigOauth2ClientCredentialsClientSecret;
}

export interface IntegrationConnectorsConnectionAuthConfigOauth2ClientCredentialsClientSecret {
    /**
     * The resource name of the secret version in the format,
     * format as: projects/*&#47;secrets/*&#47;versions/*.
     */
    secretVersion: string;
}

export interface IntegrationConnectorsConnectionAuthConfigOauth2JwtBearer {
    /**
     * Secret version reference containing a PKCS#8 PEM-encoded private key associated with the Client Certificate.
     * This private key will be used to sign JWTs used for the jwt-bearer authorization grant.
     * Specified in the form as: projects/*&#47;secrets/*&#47;versions/*.
     */
    clientKey?: outputs.IntegrationConnectorsConnectionAuthConfigOauth2JwtBearerClientKey;
    /**
     * JwtClaims providers fields to generate the token.
     */
    jwtClaims?: outputs.IntegrationConnectorsConnectionAuthConfigOauth2JwtBearerJwtClaims;
}

export interface IntegrationConnectorsConnectionAuthConfigOauth2JwtBearerClientKey {
    /**
     * The resource name of the secret version in the format,
     * format as: projects/*&#47;secrets/*&#47;versions/*.
     */
    secretVersion: string;
}

export interface IntegrationConnectorsConnectionAuthConfigOauth2JwtBearerJwtClaims {
    /**
     * Value for the "aud" claim.
     */
    audience?: string;
    /**
     * Value for the "iss" claim.
     */
    issuer?: string;
    /**
     * Value for the "sub" claim.
     */
    subject?: string;
}

export interface IntegrationConnectorsConnectionAuthConfigSshPublicKey {
    /**
     * Format of SSH Client cert.
     */
    certType?: string;
    /**
     * SSH Client Cert. It should contain both public and private key.
     */
    sshClientCert?: outputs.IntegrationConnectorsConnectionAuthConfigSshPublicKeySshClientCert;
    /**
     * Password (passphrase) for ssh client certificate if it has one.
     */
    sshClientCertPass?: outputs.IntegrationConnectorsConnectionAuthConfigSshPublicKeySshClientCertPass;
    /**
     * The user account used to authenticate.
     */
    username: string;
}

export interface IntegrationConnectorsConnectionAuthConfigSshPublicKeySshClientCert {
    /**
     * The resource name of the secret version in the format,
     * format as: projects/*&#47;secrets/*&#47;versions/*.
     */
    secretVersion: string;
}

export interface IntegrationConnectorsConnectionAuthConfigSshPublicKeySshClientCertPass {
    /**
     * The resource name of the secret version in the format,
     * format as: projects/*&#47;secrets/*&#47;versions/*.
     */
    secretVersion: string;
}

export interface IntegrationConnectorsConnectionAuthConfigUserPassword {
    /**
     * Password for Authentication.
     */
    password?: outputs.IntegrationConnectorsConnectionAuthConfigUserPasswordPassword;
    /**
     * Username for Authentication.
     */
    username: string;
}

export interface IntegrationConnectorsConnectionAuthConfigUserPasswordPassword {
    /**
     * The resource name of the secret version in the format,
     * format as: projects/*&#47;secrets/*&#47;versions/*.
     */
    secretVersion: string;
}

export interface IntegrationConnectorsConnectionConfigVariable {
    /**
     * Boolean Value of configVariable
     */
    booleanValue?: boolean;
    /**
     * Encription key value of configVariable.
     */
    encryptionKeyValue?: outputs.IntegrationConnectorsConnectionConfigVariableEncryptionKeyValue;
    /**
     * Integer Value of configVariable
     */
    integerValue?: number;
    /**
     * Key for the configVariable
     */
    key: string;
    /**
     * Secret value of configVariable.
     */
    secretValue?: outputs.IntegrationConnectorsConnectionConfigVariableSecretValue;
    /**
     * String Value of configVariabley
     */
    stringValue?: string;
}

export interface IntegrationConnectorsConnectionConfigVariableEncryptionKeyValue {
    /**
     * The [KMS key name] with which the content of the Operation is encrypted. The
     * expected format: projects/*&#47;locations/*&#47;keyRings/*&#47;cryptoKeys/*.
     * Will be empty string if google managed.
     */
    kmsKeyName?: string;
    /**
     * Type of Encription Key Possible values: ["GOOGLE_MANAGED", "CUSTOMER_MANAGED"]
     */
    type: string;
}

export interface IntegrationConnectorsConnectionConfigVariableSecretValue {
    /**
     * Secret version of Secret Value for Config variable.
     */
    secretVersion: string;
}

export interface IntegrationConnectorsConnectionConnectorVersionInfraConfig {
    ratelimitThreshold: string;
}

export interface IntegrationConnectorsConnectionDestinationConfig {
    /**
     * The destinations for the key.
     */
    destinations?: outputs.IntegrationConnectorsConnectionDestinationConfigDestination[];
    /**
     * The key is the destination identifier that is supported by the Connector.
     */
    key: string;
}

export interface IntegrationConnectorsConnectionDestinationConfigDestination {
    /**
     * For publicly routable host.
     */
    host?: string;
    /**
     * The port is the target port number that is accepted by the destination.
     */
    port?: number;
    /**
     * PSC service attachments. Format: projects/*&#47;regions/*&#47;serviceAttachments/*
     */
    serviceAttachment?: string;
}

export interface IntegrationConnectorsConnectionEventingConfig {
    /**
     * List containing additional auth configs.
     */
    additionalVariables?: outputs.IntegrationConnectorsConnectionEventingConfigAdditionalVariable[];
    /**
     * authConfig for Eventing Configuration.
     */
    authConfig?: outputs.IntegrationConnectorsConnectionEventingConfigAuthConfig;
    /**
     * Enrichment Enabled.
     */
    enrichmentEnabled?: boolean;
    /**
     * registrationDestinationConfig
     */
    registrationDestinationConfig: outputs.IntegrationConnectorsConnectionEventingConfigRegistrationDestinationConfig;
}

export interface IntegrationConnectorsConnectionEventingConfigAdditionalVariable {
    /**
     * Boolean Value of configVariable.
     */
    booleanValue?: boolean;
    /**
     * Encription key value of configVariable.
     */
    encryptionKeyValue?: outputs.IntegrationConnectorsConnectionEventingConfigAdditionalVariableEncryptionKeyValue;
    /**
     * Integer Value of configVariable.
     */
    integerValue?: number;
    /**
     * Key for the configVariable
     */
    key: string;
    /**
     * Secret value of configVariable
     */
    secretValue?: outputs.IntegrationConnectorsConnectionEventingConfigAdditionalVariableSecretValue;
    /**
     * String Value of configVariabley.
     */
    stringValue?: string;
}

export interface IntegrationConnectorsConnectionEventingConfigAdditionalVariableEncryptionKeyValue {
    /**
     * The [KMS key name] with which the content of the Operation is encrypted. The
     * expected format: projects/*&#47;locations/*&#47;keyRings/*&#47;cryptoKeys/*.
     * Will be empty string if google managed.
     */
    kmsKeyName?: string;
    /**
     * Type of Encryption Key Possible values: ["GOOGLE_MANAGED", "CUSTOMER_MANAGED"]
     */
    type?: string;
}

export interface IntegrationConnectorsConnectionEventingConfigAdditionalVariableSecretValue {
    /**
     * Secret version of Secret Value for Config variable.
     */
    secretVersion: string;
}

export interface IntegrationConnectorsConnectionEventingConfigAuthConfig {
    /**
     * List containing additional auth configs.
     */
    additionalVariables?: outputs.IntegrationConnectorsConnectionEventingConfigAuthConfigAdditionalVariable[];
    /**
     * The type of authentication configured.
     */
    authKey?: string;
    /**
     * authType of the Connection Possible values: ["USER_PASSWORD"]
     */
    authType: string;
    /**
     * User password for Authentication.
     */
    userPassword: outputs.IntegrationConnectorsConnectionEventingConfigAuthConfigUserPassword;
}

export interface IntegrationConnectorsConnectionEventingConfigAuthConfigAdditionalVariable {
    /**
     * Boolean Value of configVariable.
     */
    booleanValue?: boolean;
    /**
     * Encription key value of configVariable
     */
    encryptionKeyValue?: outputs.IntegrationConnectorsConnectionEventingConfigAuthConfigAdditionalVariableEncryptionKeyValue;
    /**
     * Integer Value of configVariable.
     */
    integerValue?: number;
    /**
     * Key for the configVariable
     */
    key: string;
    /**
     * Secret value of configVariable
     */
    secretValue?: outputs.IntegrationConnectorsConnectionEventingConfigAuthConfigAdditionalVariableSecretValue;
    /**
     * String Value of configVariabley.
     */
    stringValue?: string;
}

export interface IntegrationConnectorsConnectionEventingConfigAuthConfigAdditionalVariableEncryptionKeyValue {
    /**
     * The [KMS key name] with which the content of the Operation is encrypted. The
     * expected format: projects/*&#47;locations/*&#47;keyRings/*&#47;cryptoKeys/*.
     * Will be empty string if google managed.
     */
    kmsKeyName?: string;
    /**
     * Type of Encription Key Possible values: ["GOOGLE_MANAGED", "CUSTOMER_MANAGED"]
     */
    type?: string;
}

export interface IntegrationConnectorsConnectionEventingConfigAuthConfigAdditionalVariableSecretValue {
    /**
     * Secret version of Secret Value for Config variable.
     */
    secretVersion: string;
}

export interface IntegrationConnectorsConnectionEventingConfigAuthConfigUserPassword {
    /**
     * Password for Authentication.
     */
    password?: outputs.IntegrationConnectorsConnectionEventingConfigAuthConfigUserPasswordPassword;
    /**
     * Username for Authentication.
     */
    username?: string;
}

export interface IntegrationConnectorsConnectionEventingConfigAuthConfigUserPasswordPassword {
    /**
     * The resource name of the secret version in the format,
     * format as: projects/*&#47;secrets/*&#47;versions/*.
     */
    secretVersion: string;
}

export interface IntegrationConnectorsConnectionEventingConfigRegistrationDestinationConfig {
    /**
     * destinations for the connection
     */
    destinations?: outputs.IntegrationConnectorsConnectionEventingConfigRegistrationDestinationConfigDestination[];
    /**
     * Key for the connection
     */
    key?: string;
}

export interface IntegrationConnectorsConnectionEventingConfigRegistrationDestinationConfigDestination {
    /**
     * Host
     */
    host?: string;
    /**
     * port number
     */
    port?: number;
    /**
     * Service Attachment
     */
    serviceAttachment?: string;
}

export interface IntegrationConnectorsConnectionEventingRuntimeData {
    eventsListenerEndpoint: string;
    statuses: outputs.IntegrationConnectorsConnectionEventingRuntimeDataStatus[];
}

export interface IntegrationConnectorsConnectionEventingRuntimeDataStatus {
    description: string;
    state: string;
}

export interface IntegrationConnectorsConnectionLockConfig {
    /**
     * Indicates whether or not the connection is locked.
     */
    locked: boolean;
    /**
     * Describes why a connection is locked.
     */
    reason?: string;
}

export interface IntegrationConnectorsConnectionLogConfig {
    /**
     * Enabled represents whether logging is enabled or not for a connection.
     */
    enabled: boolean;
}

export interface IntegrationConnectorsConnectionNodeConfig {
    /**
     * Minimum number of nodes in the runtime nodes.
     */
    maxNodeCount: number;
    /**
     * Minimum number of nodes in the runtime nodes.
     */
    minNodeCount: number;
}

export interface IntegrationConnectorsConnectionSslConfig {
    /**
     * Additional SSL related field values.
     */
    additionalVariables?: outputs.IntegrationConnectorsConnectionSslConfigAdditionalVariable[];
    /**
     * Type of Client Cert (PEM/JKS/.. etc.) Possible values: ["PEM"]
     */
    clientCertType?: string;
    /**
     * Client Certificate
     */
    clientCertificate?: outputs.IntegrationConnectorsConnectionSslConfigClientCertificate;
    /**
     * Client Private Key
     */
    clientPrivateKey?: outputs.IntegrationConnectorsConnectionSslConfigClientPrivateKey;
    /**
     * Secret containing the passphrase protecting the Client Private Key
     */
    clientPrivateKeyPass?: outputs.IntegrationConnectorsConnectionSslConfigClientPrivateKeyPass;
    /**
     * Private Server Certificate. Needs to be specified if trust model is PRIVATE.
     */
    privateServerCertificate?: outputs.IntegrationConnectorsConnectionSslConfigPrivateServerCertificate;
    /**
     * Type of Server Cert (PEM/JKS/.. etc.) Possible values: ["PEM"]
     */
    serverCertType?: string;
    /**
     * Enum for Trust Model Possible values: ["PUBLIC", "PRIVATE", "INSECURE"]
     */
    trustModel?: string;
    /**
     * Enum for controlling the SSL Type (TLS/MTLS) Possible values: ["TLS", "MTLS"]
     */
    type: string;
    /**
     * Bool for enabling SSL
     */
    useSsl?: boolean;
}

export interface IntegrationConnectorsConnectionSslConfigAdditionalVariable {
    /**
     * Boolean Value of configVariable.
     */
    booleanValue?: boolean;
    /**
     * Encription key value of configVariable
     */
    encryptionKeyValue?: outputs.IntegrationConnectorsConnectionSslConfigAdditionalVariableEncryptionKeyValue;
    /**
     * Integer Value of configVariable.
     */
    integerValue?: number;
    /**
     * Key for the configVariable
     */
    key: string;
    /**
     * Secret value of configVariable
     */
    secretValue?: outputs.IntegrationConnectorsConnectionSslConfigAdditionalVariableSecretValue;
    /**
     * String Value of configVariabley.
     */
    stringValue?: string;
}

export interface IntegrationConnectorsConnectionSslConfigAdditionalVariableEncryptionKeyValue {
    /**
     * The [KMS key name] with which the content of the Operation is encrypted. The
     * expected format: projects/*&#47;locations/*&#47;keyRings/*&#47;cryptoKeys/*.
     * Will be empty string if google managed.
     */
    kmsKeyName?: string;
    /**
     * Type of Encription Key Possible values: ["GOOGLE_MANAGED", "CUSTOMER_MANAGED"]
     */
    type?: string;
}

export interface IntegrationConnectorsConnectionSslConfigAdditionalVariableSecretValue {
    /**
     * Secret version of Secret Value for Config variable.
     */
    secretVersion: string;
}

export interface IntegrationConnectorsConnectionSslConfigClientCertificate {
    /**
     * Secret version of Secret Value for Config variable.
     */
    secretVersion: string;
}

export interface IntegrationConnectorsConnectionSslConfigClientPrivateKey {
    /**
     * Secret version of Secret Value for Config variable.
     */
    secretVersion: string;
}

export interface IntegrationConnectorsConnectionSslConfigClientPrivateKeyPass {
    /**
     * Secret version of Secret Value for Config variable.
     */
    secretVersion: string;
}

export interface IntegrationConnectorsConnectionSslConfigPrivateServerCertificate {
    /**
     * Secret version of Secret Value for Config variable.
     */
    secretVersion: string;
}

export interface IntegrationConnectorsConnectionStatus {
    description: string;
    state: string;
    status: string;
}

export interface IntegrationConnectorsConnectionTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface IntegrationConnectorsEndpointAttachmentTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface IntegrationConnectorsManagedZoneTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface IntegrationsAuthConfigClientCertificate {
    /**
     * The ssl certificate encoded in PEM format. This string must include the begin header and end footer lines.
     */
    encryptedPrivateKey: string;
    /**
     * 'passphrase' should be left unset if private key is not encrypted.
     * Note that 'passphrase' is not the password for web server, but an extra layer of security to protected private key.
     */
    passphrase?: string;
    /**
     * The ssl certificate encoded in PEM format. This string must include the begin header and end footer lines.
     */
    sslCertificate: string;
}

export interface IntegrationsAuthConfigDecryptedCredential {
    /**
     * Auth token credential.
     */
    authToken?: outputs.IntegrationsAuthConfigDecryptedCredentialAuthToken;
    /**
     * Credential type associated with auth configs.
     */
    credentialType: string;
    /**
     * JWT credential.
     */
    jwt?: outputs.IntegrationsAuthConfigDecryptedCredentialJwt;
    /**
     * OAuth2 authorization code credential.
     */
    oauth2AuthorizationCode?: outputs.IntegrationsAuthConfigDecryptedCredentialOauth2AuthorizationCode;
    /**
     * OAuth2 client credentials.
     */
    oauth2ClientCredentials?: outputs.IntegrationsAuthConfigDecryptedCredentialOauth2ClientCredentials;
    /**
     * Google OIDC ID Token.
     */
    oidcToken?: outputs.IntegrationsAuthConfigDecryptedCredentialOidcToken;
    /**
     * Service account credential.
     */
    serviceAccountCredentials?: outputs.IntegrationsAuthConfigDecryptedCredentialServiceAccountCredentials;
    /**
     * Username and password credential.
     */
    usernameAndPassword?: outputs.IntegrationsAuthConfigDecryptedCredentialUsernameAndPassword;
}

export interface IntegrationsAuthConfigDecryptedCredentialAuthToken {
    /**
     * The token for the auth type.
     */
    token?: string;
    /**
     * Authentication type, e.g. "Basic", "Bearer", etc.
     */
    type?: string;
}

export interface IntegrationsAuthConfigDecryptedCredentialJwt {
    /**
     * The token calculated by the header, payload and signature.
     */
    jwt: string;
    /**
     * Identifies which algorithm is used to generate the signature.
     */
    jwtHeader?: string;
    /**
     * Contains a set of claims. The JWT specification defines seven Registered Claim Names which are the standard fields commonly included in tokens. Custom claims are usually also included, depending on the purpose of the token.
     */
    jwtPayload?: string;
    /**
     * User's pre-shared secret to sign the token.
     */
    secret?: string;
}

export interface IntegrationsAuthConfigDecryptedCredentialOauth2AuthorizationCode {
    /**
     * The auth url endpoint to send the auth code request to.
     */
    authEndpoint?: string;
    /**
     * The client's id.
     */
    clientId?: string;
    /**
     * The client's secret.
     */
    clientSecret?: string;
    /**
     * A space-delimited list of requested scope permissions.
     */
    scope?: string;
    /**
     * The token url endpoint to send the token request to.
     */
    tokenEndpoint?: string;
}

export interface IntegrationsAuthConfigDecryptedCredentialOauth2ClientCredentials {
    /**
     * The client's ID.
     */
    clientId?: string;
    /**
     * The client's secret.
     */
    clientSecret?: string;
    /**
     * Represent how to pass parameters to fetch access token Possible values: ["REQUEST_TYPE_UNSPECIFIED", "REQUEST_BODY", "QUERY_PARAMETERS", "ENCODED_HEADER"]
     */
    requestType?: string;
    /**
     * A space-delimited list of requested scope permissions.
     */
    scope?: string;
    /**
     * The token endpoint is used by the client to obtain an access token by presenting its authorization grant or refresh token.
     */
    tokenEndpoint?: string;
    /**
     * Token parameters for the auth request.
     */
    tokenParams?: outputs.IntegrationsAuthConfigDecryptedCredentialOauth2ClientCredentialsTokenParams;
}

export interface IntegrationsAuthConfigDecryptedCredentialOauth2ClientCredentialsTokenParams {
    /**
     * A list of parameter map entries.
     */
    entries?: outputs.IntegrationsAuthConfigDecryptedCredentialOauth2ClientCredentialsTokenParamsEntry[];
}

export interface IntegrationsAuthConfigDecryptedCredentialOauth2ClientCredentialsTokenParamsEntry {
    /**
     * Key of the map entry.
     */
    key?: outputs.IntegrationsAuthConfigDecryptedCredentialOauth2ClientCredentialsTokenParamsEntryKey;
    /**
     * Value of the map entry.
     */
    value?: outputs.IntegrationsAuthConfigDecryptedCredentialOauth2ClientCredentialsTokenParamsEntryValue;
}

export interface IntegrationsAuthConfigDecryptedCredentialOauth2ClientCredentialsTokenParamsEntryKey {
    /**
     * Passing a literal value
     */
    literalValue?: outputs.IntegrationsAuthConfigDecryptedCredentialOauth2ClientCredentialsTokenParamsEntryKeyLiteralValue;
}

export interface IntegrationsAuthConfigDecryptedCredentialOauth2ClientCredentialsTokenParamsEntryKeyLiteralValue {
    /**
     * String.
     */
    stringValue?: string;
}

export interface IntegrationsAuthConfigDecryptedCredentialOauth2ClientCredentialsTokenParamsEntryValue {
    /**
     * Passing a literal value
     */
    literalValue?: outputs.IntegrationsAuthConfigDecryptedCredentialOauth2ClientCredentialsTokenParamsEntryValueLiteralValue;
}

export interface IntegrationsAuthConfigDecryptedCredentialOauth2ClientCredentialsTokenParamsEntryValueLiteralValue {
    /**
     * String.
     */
    stringValue?: string;
}

export interface IntegrationsAuthConfigDecryptedCredentialOidcToken {
    /**
     * Audience to be used when generating OIDC token. The audience claim identifies the recipients that the JWT is intended for.
     */
    audience?: string;
    /**
     * The service account email to be used as the identity for the token.
     */
    serviceAccountEmail?: string;
    /**
     * ID token obtained for the service account.
     */
    token: string;
    /**
     * The approximate time until the token retrieved is valid.
     *
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
     */
    tokenExpireTime: string;
}

export interface IntegrationsAuthConfigDecryptedCredentialServiceAccountCredentials {
    /**
     * A space-delimited list of requested scope permissions.
     */
    scope?: string;
    /**
     * Name of the service account that has the permission to make the request.
     */
    serviceAccount?: string;
}

export interface IntegrationsAuthConfigDecryptedCredentialUsernameAndPassword {
    /**
     * Password to be used.
     */
    password?: string;
    /**
     * Username to be used.
     */
    username?: string;
}

export interface IntegrationsAuthConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface IntegrationsClientCloudKmsConfig {
    /**
     * A Cloud KMS key is a named object containing one or more key versions, along
     * with metadata for the key. A key exists on exactly one key ring tied to a
     * specific location.
     */
    key: string;
    /**
     * Each version of a key contains key material used for encryption or signing.
     * A key's version is represented by an integer, starting at 1. To decrypt data
     * or verify a signature, you must use the same key version that was used to
     * encrypt or sign the data.
     */
    keyVersion?: string;
    /**
     * Location name of the key ring, e.g. "us-west1".
     */
    kmsLocation: string;
    /**
     * The Google Cloud project id of the project where the kms key stored. If empty,
     * the kms key is stored at the same project as customer's project and ecrypted
     * with CMEK, otherwise, the kms key is stored in the tenant project and
     * encrypted with GMEK.
     */
    kmsProjectId?: string;
    /**
     * A key ring organizes keys in a specific Google Cloud location and allows you to
     * manage access control on groups of keys. A key ring's name does not need to be
     * unique across a Google Cloud project, but must be unique within a given location.
     */
    kmsRing: string;
}

export interface IntegrationsClientTimeouts {
    create?: string;
    delete?: string;
}

export interface KmsCryptoKeyIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface KmsCryptoKeyIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface KmsCryptoKeyPrimary {
    name: string;
    state: string;
}

export interface KmsCryptoKeyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface KmsCryptoKeyVersionAttestation {
    certChains: outputs.KmsCryptoKeyVersionAttestationCertChain[];
    content: string;
    externalProtectionLevelOptions: outputs.KmsCryptoKeyVersionAttestationExternalProtectionLevelOption[];
    format: string;
}

export interface KmsCryptoKeyVersionAttestationCertChain {
    caviumCerts: string[];
    googleCardCerts: string[];
    googlePartitionCerts: string[];
}

export interface KmsCryptoKeyVersionAttestationExternalProtectionLevelOption {
    ekmConnectionKeyPath: string;
    externalKeyUri: string;
}

export interface KmsCryptoKeyVersionExternalProtectionLevelOptions {
    /**
     * The path to the external key material on the EKM when using EkmConnection e.g., "v0/my/key". Set this field instead of externalKeyUri when using an EkmConnection.
     */
    ekmConnectionKeyPath?: string;
    /**
     * The URI for an external resource that this CryptoKeyVersion represents.
     */
    externalKeyUri?: string;
}

export interface KmsCryptoKeyVersionTemplate {
    /**
     * The algorithm to use when creating a version based on this template.
     * See the [algorithm reference](https://cloud.google.com/kms/docs/reference/rest/v1/CryptoKeyVersionAlgorithm) for possible inputs.
     */
    algorithm: string;
    /**
     * The protection level to use when creating a version based on this template. Possible values include "SOFTWARE", "HSM", "EXTERNAL", "EXTERNAL_VPC". Defaults to "SOFTWARE".
     */
    protectionLevel?: string;
}

export interface KmsCryptoKeyVersionTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface KmsEkmConnectionIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface KmsEkmConnectionIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface KmsEkmConnectionServiceResolver {
    /**
     * Optional. The filter applied to the endpoints of the resolved service. If no filter is specified, all endpoints will be considered. An endpoint will be chosen arbitrarily from the filtered list for each request. For endpoint filter syntax and examples, see https://cloud.google.com/service-directory/docs/reference/rpc/google.cloud.servicedirectory.v1#resolveservicerequest.
     */
    endpointFilter: string;
    /**
     * Required. The hostname of the EKM replica used at TLS and HTTP layers.
     */
    hostname: string;
    /**
     * Required. A list of leaf server certificates used to authenticate HTTPS connections to the EKM replica. Currently, a maximum of 10 Certificate is supported.
     */
    serverCertificates: outputs.KmsEkmConnectionServiceResolverServerCertificate[];
    /**
     * Required. The resource name of the Service Directory service pointing to an EKM replica, in the format projects/*&#47;locations/*&#47;namespaces/*&#47;services/*
     */
    serviceDirectoryService: string;
}

export interface KmsEkmConnectionServiceResolverServerCertificate {
    /**
     * Output only. The issuer distinguished name in RFC 2253 format. Only present if parsed is true.
     */
    issuer: string;
    /**
     * Output only. The certificate is not valid after this time. Only present if parsed is true.
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
     */
    notAfterTime: string;
    /**
     * Output only. The certificate is not valid before this time. Only present if parsed is true.
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
     */
    notBeforeTime: string;
    /**
     * Output only. True if the certificate was parsed successfully.
     */
    parsed: boolean;
    /**
     * Required. The raw certificate bytes in DER format. A base64-encoded string.
     */
    rawDer: string;
    /**
     * Output only. The certificate serial number as a hex string. Only present if parsed is true.
     */
    serialNumber: string;
    /**
     * Output only. The SHA-256 certificate fingerprint as a hex string. Only present if parsed is true.
     */
    sha256Fingerprint: string;
    /**
     * Output only. The subject distinguished name in RFC 2253 format. Only present if parsed is true.
     */
    subject: string;
    /**
     * Output only. The subject Alternative DNS names. Only present if parsed is true.
     */
    subjectAlternativeDnsNames: string[];
}

export interface KmsEkmConnectionTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface KmsKeyRingIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface KmsKeyRingIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface KmsKeyRingImportJobAttestation {
    content: string;
    format: string;
}

export interface KmsKeyRingImportJobPublicKey {
    pem: string;
}

export interface KmsKeyRingImportJobTimeouts {
    create?: string;
    delete?: string;
}

export interface KmsKeyRingTimeouts {
    create?: string;
    delete?: string;
}

export interface KmsSecretCiphertextTimeouts {
    create?: string;
    delete?: string;
}

export interface LoggingBillingAccountBucketConfigCmekSettings {
    /**
     * The resource name for the configured Cloud KMS key.
     * KMS key name format:
     * "projects/[PROJECT_ID]/locations/[LOCATION]/keyRings/[KEYRING]/cryptoKeys/[KEY]"
     * To enable CMEK for the bucket, set this field to a valid kmsKeyName for which the associated service account has the required cloudkms.cryptoKeyEncrypterDecrypter roles assigned for the key.
     * The Cloud KMS key used by the bucket can be updated by changing the kmsKeyName to a new valid key name. Encryption operations that are in progress will be completed with the key that was in use when they started. Decryption operations will be completed using the key that was used at the time of encryption unless access to that key has been revoked.
     * See [Enabling CMEK for Logging Buckets](https://cloud.google.com/logging/docs/routing/managed-encryption-storage) for more information.
     */
    kmsKeyName: string;
    /**
     * The CryptoKeyVersion resource name for the configured Cloud KMS key.
     * KMS key name format:
     * "projects/[PROJECT_ID]/locations/[LOCATION]/keyRings/[KEYRING]/cryptoKeys/[KEY]/cryptoKeyVersions/[VERSION]"
     * For example:
     * "projects/my-project/locations/us-central1/keyRings/my-ring/cryptoKeys/my-key/cryptoKeyVersions/1"
     * This is a read-only field used to convey the specific configured CryptoKeyVersion of kms_key that has been configured. It will be populated in cases where the CMEK settings are bound to a single key version.
     */
    kmsKeyVersionName: string;
    /**
     * The resource name of the CMEK settings.
     */
    name: string;
    /**
     * The service account associated with a project for which CMEK will apply.
     * Before enabling CMEK for a logging bucket, you must first assign the cloudkms.cryptoKeyEncrypterDecrypter role to the service account associated with the project for which CMEK will apply. Use [v2.getCmekSettings](https://cloud.google.com/logging/docs/reference/v2/rest/v2/TopLevel/getCmekSettings#google.logging.v2.ConfigServiceV2.GetCmekSettings) to obtain the service account ID.
     * See [Enabling CMEK for Logging Buckets](https://cloud.google.com/logging/docs/routing/managed-encryption-storage) for more information.
     */
    serviceAccountId: string;
}

export interface LoggingBillingAccountBucketConfigIndexConfig {
    /**
     * The LogEntry field path to index.
     */
    fieldPath: string;
    /**
     * The type of data in this index
     * Note that some paths are automatically indexed, and other paths are not eligible for indexing. See indexing documentation for details.
     * For example: jsonPayload.request.status
     */
    type: string;
}

export interface LoggingBillingAccountSinkBigqueryOptions {
    /**
     * Whether to use BigQuery's partition tables. By default, Logging creates dated tables based on the log entries' timestamps, e.g. syslog_20170523. With partitioned tables the date suffix is no longer present and special query syntax has to be used instead. In both cases, tables are sharded based on UTC timezone.
     */
    usePartitionedTables: boolean;
}

export interface LoggingBillingAccountSinkExclusion {
    /**
     * A description of this exclusion.
     */
    description?: string;
    /**
     * If set to True, then this exclusion is disabled and it does not exclude any log entries
     */
    disabled?: boolean;
    /**
     * An advanced logs filter that matches the log entries to be excluded. By using the sample function, you can exclude less than 100% of the matching log entries
     */
    filter: string;
    /**
     * A client-assigned identifier, such as "load-balancer-exclusion". Identifiers are limited to 100 characters and can include only letters, digits, underscores, hyphens, and periods. First character has to be alphanumeric.
     */
    name: string;
}

export interface LoggingFolderBucketConfigCmekSettings {
    /**
     * The resource name for the configured Cloud KMS key.
     * KMS key name format:
     * "projects/[PROJECT_ID]/locations/[LOCATION]/keyRings/[KEYRING]/cryptoKeys/[KEY]"
     * To enable CMEK for the bucket, set this field to a valid kmsKeyName for which the associated service account has the required cloudkms.cryptoKeyEncrypterDecrypter roles assigned for the key.
     * The Cloud KMS key used by the bucket can be updated by changing the kmsKeyName to a new valid key name. Encryption operations that are in progress will be completed with the key that was in use when they started. Decryption operations will be completed using the key that was used at the time of encryption unless access to that key has been revoked.
     * See [Enabling CMEK for Logging Buckets](https://cloud.google.com/logging/docs/routing/managed-encryption-storage) for more information.
     */
    kmsKeyName: string;
    /**
     * The CryptoKeyVersion resource name for the configured Cloud KMS key.
     * KMS key name format:
     * "projects/[PROJECT_ID]/locations/[LOCATION]/keyRings/[KEYRING]/cryptoKeys/[KEY]/cryptoKeyVersions/[VERSION]"
     * For example:
     * "projects/my-project/locations/us-central1/keyRings/my-ring/cryptoKeys/my-key/cryptoKeyVersions/1"
     * This is a read-only field used to convey the specific configured CryptoKeyVersion of kms_key that has been configured. It will be populated in cases where the CMEK settings are bound to a single key version.
     */
    kmsKeyVersionName: string;
    /**
     * The resource name of the CMEK settings.
     */
    name: string;
    /**
     * The service account associated with a project for which CMEK will apply.
     * Before enabling CMEK for a logging bucket, you must first assign the cloudkms.cryptoKeyEncrypterDecrypter role to the service account associated with the project for which CMEK will apply. Use [v2.getCmekSettings](https://cloud.google.com/logging/docs/reference/v2/rest/v2/TopLevel/getCmekSettings#google.logging.v2.ConfigServiceV2.GetCmekSettings) to obtain the service account ID.
     * See [Enabling CMEK for Logging Buckets](https://cloud.google.com/logging/docs/routing/managed-encryption-storage) for more information.
     */
    serviceAccountId: string;
}

export interface LoggingFolderBucketConfigIndexConfig {
    /**
     * The LogEntry field path to index.
     */
    fieldPath: string;
    /**
     * The type of data in this index
     * Note that some paths are automatically indexed, and other paths are not eligible for indexing. See indexing documentation for details.
     * For example: jsonPayload.request.status
     */
    type: string;
}

export interface LoggingFolderSettingsTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface LoggingFolderSinkBigqueryOptions {
    /**
     * Whether to use BigQuery's partition tables. By default, Logging creates dated tables based on the log entries' timestamps, e.g. syslog_20170523. With partitioned tables the date suffix is no longer present and special query syntax has to be used instead. In both cases, tables are sharded based on UTC timezone.
     */
    usePartitionedTables: boolean;
}

export interface LoggingFolderSinkExclusion {
    /**
     * A description of this exclusion.
     */
    description?: string;
    /**
     * If set to True, then this exclusion is disabled and it does not exclude any log entries
     */
    disabled?: boolean;
    /**
     * An advanced logs filter that matches the log entries to be excluded. By using the sample function, you can exclude less than 100% of the matching log entries
     */
    filter: string;
    /**
     * A client-assigned identifier, such as "load-balancer-exclusion". Identifiers are limited to 100 characters and can include only letters, digits, underscores, hyphens, and periods. First character has to be alphanumeric.
     */
    name: string;
}

export interface LoggingLinkedDatasetBigqueryDataset {
    /**
     * Output only. The full resource name of the BigQuery dataset. The DATASET_ID will match the ID
     * of the link, so the link must match the naming restrictions of BigQuery datasets
     * (alphanumeric characters and underscores only). The dataset will have a resource path of
     * "bigquery.googleapis.com/projects/[PROJECT_ID]/datasets/[DATASET_ID]"
     */
    datasetId: string;
}

export interface LoggingLinkedDatasetTimeouts {
    create?: string;
    delete?: string;
}

export interface LoggingLogViewIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface LoggingLogViewIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface LoggingLogViewTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface LoggingMetricBucketOptions {
    /**
     * Specifies a set of buckets with arbitrary widths.
     */
    explicitBuckets?: outputs.LoggingMetricBucketOptionsExplicitBuckets;
    /**
     * Specifies an exponential sequence of buckets that have a width that is proportional to the value of
     * the lower bound. Each bucket represents a constant relative uncertainty on a specific value in the bucket.
     */
    exponentialBuckets?: outputs.LoggingMetricBucketOptionsExponentialBuckets;
    /**
     * Specifies a linear sequence of buckets that all have the same width (except overflow and underflow).
     * Each bucket represents a constant absolute uncertainty on the specific value in the bucket.
     */
    linearBuckets?: outputs.LoggingMetricBucketOptionsLinearBuckets;
}

export interface LoggingMetricBucketOptionsExplicitBuckets {
    /**
     * The values must be monotonically increasing.
     */
    bounds: number[];
}

export interface LoggingMetricBucketOptionsExponentialBuckets {
    /**
     * Must be greater than 1.
     */
    growthFactor: number;
    /**
     * Must be greater than 0.
     */
    numFiniteBuckets: number;
    /**
     * Must be greater than 0.
     */
    scale: number;
}

export interface LoggingMetricBucketOptionsLinearBuckets {
    /**
     * Must be greater than 0.
     */
    numFiniteBuckets: number;
    /**
     * Lower bound of the first bucket.
     */
    offset: number;
    /**
     * Must be greater than 0.
     */
    width: number;
}

export interface LoggingMetricMetricDescriptor {
    /**
     * A concise name for the metric, which can be displayed in user interfaces. Use sentence case
     * without an ending period, for example "Request count". This field is optional but it is
     * recommended to be set for any metrics associated with user-visible concepts, such as Quota.
     */
    displayName?: string;
    /**
     * The set of labels that can be used to describe a specific instance of this metric type. For
     * example, the appengine.googleapis.com/http/server/response_latencies metric type has a label
     * for the HTTP response code, response_code, so you can look at latencies for successful responses
     * or just for responses that failed.
     */
    labels?: outputs.LoggingMetricMetricDescriptorLabel[];
    /**
     * Whether the metric records instantaneous values, changes to a value, etc.
     * Some combinations of metricKind and valueType might not be supported.
     * For counter metrics, set this to DELTA. Possible values: ["DELTA", "GAUGE", "CUMULATIVE"]
     */
    metricKind: string;
    /**
     * The unit in which the metric value is reported. It is only applicable if the valueType is
     * 'INT64', 'DOUBLE', or 'DISTRIBUTION'. The supported units are a subset of
     * [The Unified Code for Units of Measure](http://unitsofmeasure.org/ucum.html) standard
     */
    unit?: string;
    /**
     * Whether the measurement is an integer, a floating-point number, etc.
     * Some combinations of metricKind and valueType might not be supported.
     * For counter metrics, set this to INT64. Possible values: ["BOOL", "INT64", "DOUBLE", "STRING", "DISTRIBUTION", "MONEY"]
     */
    valueType: string;
}

export interface LoggingMetricMetricDescriptorLabel {
    /**
     * A human-readable description for the label.
     */
    description?: string;
    /**
     * The label key.
     */
    key: string;
    /**
     * The type of data that can be assigned to the label. Default value: "STRING" Possible values: ["BOOL", "INT64", "STRING"]
     */
    valueType?: string;
}

export interface LoggingMetricTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface LoggingOrganizationBucketConfigCmekSettings {
    /**
     * The resource name for the configured Cloud KMS key.
     * KMS key name format:
     * "projects/[PROJECT_ID]/locations/[LOCATION]/keyRings/[KEYRING]/cryptoKeys/[KEY]"
     * To enable CMEK for the bucket, set this field to a valid kmsKeyName for which the associated service account has the required cloudkms.cryptoKeyEncrypterDecrypter roles assigned for the key.
     * The Cloud KMS key used by the bucket can be updated by changing the kmsKeyName to a new valid key name. Encryption operations that are in progress will be completed with the key that was in use when they started. Decryption operations will be completed using the key that was used at the time of encryption unless access to that key has been revoked.
     * See [Enabling CMEK for Logging Buckets](https://cloud.google.com/logging/docs/routing/managed-encryption-storage) for more information.
     */
    kmsKeyName: string;
    /**
     * The CryptoKeyVersion resource name for the configured Cloud KMS key.
     * KMS key name format:
     * "projects/[PROJECT_ID]/locations/[LOCATION]/keyRings/[KEYRING]/cryptoKeys/[KEY]/cryptoKeyVersions/[VERSION]"
     * For example:
     * "projects/my-project/locations/us-central1/keyRings/my-ring/cryptoKeys/my-key/cryptoKeyVersions/1"
     * This is a read-only field used to convey the specific configured CryptoKeyVersion of kms_key that has been configured. It will be populated in cases where the CMEK settings are bound to a single key version.
     */
    kmsKeyVersionName: string;
    /**
     * The resource name of the CMEK settings.
     */
    name: string;
    /**
     * The service account associated with a project for which CMEK will apply.
     * Before enabling CMEK for a logging bucket, you must first assign the cloudkms.cryptoKeyEncrypterDecrypter role to the service account associated with the project for which CMEK will apply. Use [v2.getCmekSettings](https://cloud.google.com/logging/docs/reference/v2/rest/v2/TopLevel/getCmekSettings#google.logging.v2.ConfigServiceV2.GetCmekSettings) to obtain the service account ID.
     * See [Enabling CMEK for Logging Buckets](https://cloud.google.com/logging/docs/routing/managed-encryption-storage) for more information.
     */
    serviceAccountId: string;
}

export interface LoggingOrganizationBucketConfigIndexConfig {
    /**
     * The LogEntry field path to index.
     */
    fieldPath: string;
    /**
     * The type of data in this index
     * Note that some paths are automatically indexed, and other paths are not eligible for indexing. See indexing documentation for details.
     * For example: jsonPayload.request.status
     */
    type: string;
}

export interface LoggingOrganizationSettingsTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface LoggingOrganizationSinkBigqueryOptions {
    /**
     * Whether to use BigQuery's partition tables. By default, Logging creates dated tables based on the log entries' timestamps, e.g. syslog_20170523. With partitioned tables the date suffix is no longer present and special query syntax has to be used instead. In both cases, tables are sharded based on UTC timezone.
     */
    usePartitionedTables: boolean;
}

export interface LoggingOrganizationSinkExclusion {
    /**
     * A description of this exclusion.
     */
    description?: string;
    /**
     * If set to True, then this exclusion is disabled and it does not exclude any log entries
     */
    disabled?: boolean;
    /**
     * An advanced logs filter that matches the log entries to be excluded. By using the sample function, you can exclude less than 100% of the matching log entries
     */
    filter: string;
    /**
     * A client-assigned identifier, such as "load-balancer-exclusion". Identifiers are limited to 100 characters and can include only letters, digits, underscores, hyphens, and periods. First character has to be alphanumeric.
     */
    name: string;
}

export interface LoggingProjectBucketConfigCmekSettings {
    /**
     * The resource name for the configured Cloud KMS key.
     * KMS key name format:
     * "projects/[PROJECT_ID]/locations/[LOCATION]/keyRings/[KEYRING]/cryptoKeys/[KEY]"
     * To enable CMEK for the bucket, set this field to a valid kmsKeyName for which the associated service account has the required cloudkms.cryptoKeyEncrypterDecrypter roles assigned for the key.
     * The Cloud KMS key used by the bucket can be updated by changing the kmsKeyName to a new valid key name. Encryption operations that are in progress will be completed with the key that was in use when they started. Decryption operations will be completed using the key that was used at the time of encryption unless access to that key has been revoked.
     * See [Enabling CMEK for Logging Buckets](https://cloud.google.com/logging/docs/routing/managed-encryption-storage) for more information.
     */
    kmsKeyName: string;
    /**
     * The CryptoKeyVersion resource name for the configured Cloud KMS key.
     * KMS key name format:
     * "projects/[PROJECT_ID]/locations/[LOCATION]/keyRings/[KEYRING]/cryptoKeys/[KEY]/cryptoKeyVersions/[VERSION]"
     * For example:
     * "projects/my-project/locations/us-central1/keyRings/my-ring/cryptoKeys/my-key/cryptoKeyVersions/1"
     * This is a read-only field used to convey the specific configured CryptoKeyVersion of kms_key that has been configured. It will be populated in cases where the CMEK settings are bound to a single key version.
     */
    kmsKeyVersionName: string;
    /**
     * The resource name of the CMEK settings.
     */
    name: string;
    /**
     * The service account associated with a project for which CMEK will apply.
     * Before enabling CMEK for a logging bucket, you must first assign the cloudkms.cryptoKeyEncrypterDecrypter role to the service account associated with the project for which CMEK will apply. Use [v2.getCmekSettings](https://cloud.google.com/logging/docs/reference/v2/rest/v2/TopLevel/getCmekSettings#google.logging.v2.ConfigServiceV2.GetCmekSettings) to obtain the service account ID.
     * See [Enabling CMEK for Logging Buckets](https://cloud.google.com/logging/docs/routing/managed-encryption-storage) for more information.
     */
    serviceAccountId: string;
}

export interface LoggingProjectBucketConfigIndexConfig {
    /**
     * The LogEntry field path to index.
     */
    fieldPath: string;
    /**
     * The type of data in this index
     * Note that some paths are automatically indexed, and other paths are not eligible for indexing. See indexing documentation for details.
     * For example: jsonPayload.request.status
     */
    type: string;
}

export interface LoggingProjectSinkBigqueryOptions {
    /**
     * Whether to use BigQuery's partition tables. By default, Logging creates dated tables based on the log entries' timestamps, e.g. syslog_20170523. With partitioned tables the date suffix is no longer present and special query syntax has to be used instead. In both cases, tables are sharded based on UTC timezone.
     */
    usePartitionedTables: boolean;
}

export interface LoggingProjectSinkExclusion {
    /**
     * A description of this exclusion.
     */
    description?: string;
    /**
     * If set to True, then this exclusion is disabled and it does not exclude any log entries
     */
    disabled?: boolean;
    /**
     * An advanced logs filter that matches the log entries to be excluded. By using the sample function, you can exclude less than 100% of the matching log entries
     */
    filter: string;
    /**
     * A client-assigned identifier, such as "load-balancer-exclusion". Identifiers are limited to 100 characters and can include only letters, digits, underscores, hyphens, and periods. First character has to be alphanumeric.
     */
    name: string;
}

export interface LookerInstanceAdminSettings {
    allowedEmailDomains?: string[];
}

export interface LookerInstanceCustomDomain {
    /**
     * Domain name
     */
    domain?: string;
    /**
     * Status of the custom domain.
     */
    state: string;
}

export interface LookerInstanceDenyMaintenancePeriod {
    /**
     * Required. Start date of the deny maintenance period
     */
    endDate: outputs.LookerInstanceDenyMaintenancePeriodEndDate;
    /**
     * Required. Start date of the deny maintenance period
     */
    startDate: outputs.LookerInstanceDenyMaintenancePeriodStartDate;
    /**
     * Required. Start time of the window in UTC time.
     */
    time: outputs.LookerInstanceDenyMaintenancePeriodTime;
}

export interface LookerInstanceDenyMaintenancePeriodEndDate {
    /**
     * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0
     * to specify a year by itself or a year and month where the day isn't significant.
     */
    day?: number;
    /**
     * Month of a year. Must be from 1 to 12, or 0 to specify a year without a
     * month and day.
     */
    month?: number;
    /**
     * Year of the date. Must be from 1 to 9999, or 0 to specify a date without
     * a year.
     */
    year?: number;
}

export interface LookerInstanceDenyMaintenancePeriodStartDate {
    /**
     * Day of a month. Must be from 1 to 31 and valid for the year and month, or 0
     * to specify a year by itself or a year and month where the day isn't significant.
     */
    day?: number;
    /**
     * Month of a year. Must be from 1 to 12, or 0 to specify a year without a
     * month and day.
     */
    month?: number;
    /**
     * Year of the date. Must be from 1 to 9999, or 0 to specify a date without
     * a year.
     */
    year?: number;
}

export interface LookerInstanceDenyMaintenancePeriodTime {
    /**
     * Hours of day in 24 hour format. Should be from 0 to 23.
     */
    hours?: number;
    /**
     * Minutes of hour of day. Must be from 0 to 59.
     */
    minutes?: number;
    /**
     * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
     */
    nanos?: number;
    /**
     * Seconds of minutes of the time. Must normally be from 0 to 59.
     */
    seconds?: number;
}

export interface LookerInstanceEncryptionConfig {
    /**
     * Name of the customer managed encryption key (CMEK) in KMS.
     */
    kmsKeyName?: string;
    /**
     * Full name and version of the CMEK key currently in use to encrypt Looker data.
     */
    kmsKeyNameVersion: string;
    /**
     * Status of the customer managed encryption key (CMEK) in KMS.
     */
    kmsKeyState: string;
}

export interface LookerInstanceMaintenanceWindow {
    /**
     * Required. Day of the week for this MaintenanceWindow (in UTC).
     *
     * - MONDAY: Monday
     * - TUESDAY: Tuesday
     * - WEDNESDAY: Wednesday
     * - THURSDAY: Thursday
     * - FRIDAY: Friday
     * - SATURDAY: Saturday
     * - SUNDAY: Sunday Possible values: ["MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    dayOfWeek: string;
    /**
     * Required. Start time of the window in UTC time.
     */
    startTime: outputs.LookerInstanceMaintenanceWindowStartTime;
}

export interface LookerInstanceMaintenanceWindowStartTime {
    /**
     * Hours of day in 24 hour format. Should be from 0 to 23.
     */
    hours?: number;
    /**
     * Minutes of hour of day. Must be from 0 to 59.
     */
    minutes?: number;
    /**
     * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
     */
    nanos?: number;
    /**
     * Seconds of minutes of the time. Must normally be from 0 to 59.
     */
    seconds?: number;
}

export interface LookerInstanceOauthConfig {
    /**
     * The client ID for the Oauth config.
     */
    clientId: string;
    /**
     * The client secret for the Oauth config.
     */
    clientSecret: string;
}

export interface LookerInstanceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface LookerInstanceUserMetadata {
    /**
     * Number of additional Developer Users to allocate to the Looker Instance.
     */
    additionalDeveloperUserCount?: number;
    /**
     * Number of additional Standard Users to allocate to the Looker Instance.
     */
    additionalStandardUserCount?: number;
    /**
     * Number of additional Viewer Users to allocate to the Looker Instance.
     */
    additionalViewerUserCount?: number;
}

export interface MemcacheInstanceMaintenancePolicy {
    /**
     * Output only. The time when the policy was created.
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond
     * resolution and up to nine fractional digits
     */
    createTime: string;
    /**
     * Optional. Description of what this policy is for.
     * Create/Update methods return INVALID_ARGUMENT if the
     * length is greater than 512.
     */
    description?: string;
    /**
     * Output only. The time when the policy was updated.
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond
     * resolution and up to nine fractional digits.
     */
    updateTime: string;
    /**
     * Required. Maintenance window that is applied to resources covered by this policy.
     * Minimum 1. For the current version, the maximum number of weekly_maintenance_windows
     * is expected to be one.
     */
    weeklyMaintenanceWindows: outputs.MemcacheInstanceMaintenancePolicyWeeklyMaintenanceWindow[];
}

export interface MemcacheInstanceMaintenancePolicyWeeklyMaintenanceWindow {
    /**
     * Required. The day of week that maintenance updates occur.
     * - DAY_OF_WEEK_UNSPECIFIED: The day of the week is unspecified.
     * - MONDAY: Monday
     * - TUESDAY: Tuesday
     * - WEDNESDAY: Wednesday
     * - THURSDAY: Thursday
     * - FRIDAY: Friday
     * - SATURDAY: Saturday
     * - SUNDAY: Sunday Possible values: ["DAY_OF_WEEK_UNSPECIFIED", "MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    day: string;
    /**
     * Required. The length of the maintenance window, ranging from 3 hours to 8 hours.
     * A duration in seconds with up to nine fractional digits,
     * terminated by 's'. Example: "3.5s".
     */
    duration: string;
    /**
     * Required. Start time of the window in UTC time.
     */
    startTime: outputs.MemcacheInstanceMaintenancePolicyWeeklyMaintenanceWindowStartTime;
}

export interface MemcacheInstanceMaintenancePolicyWeeklyMaintenanceWindowStartTime {
    /**
     * Hours of day in 24 hour format. Should be from 0 to 23.
     * An API may choose to allow the value "24:00:00" for scenarios like business closing time.
     */
    hours?: number;
    /**
     * Minutes of hour of day. Must be from 0 to 59.
     */
    minutes?: number;
    /**
     * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
     */
    nanos?: number;
    /**
     * Seconds of minutes of the time. Must normally be from 0 to 59.
     * An API may allow the value 60 if it allows leap-seconds.
     */
    seconds?: number;
}

export interface MemcacheInstanceMaintenanceSchedule {
    endTime: string;
    scheduleDeadlineTime: string;
    startTime: string;
}

export interface MemcacheInstanceMemcacheNode {
    host: string;
    nodeId: string;
    port: number;
    state: string;
    zone: string;
}

export interface MemcacheInstanceMemcacheParameters {
    /**
     * This is a unique ID associated with this set of parameters.
     */
    id: string;
    /**
     * User-defined set of parameters to use in the memcache process.
     */
    params?: {[key: string]: string};
}

export interface MemcacheInstanceNodeConfig {
    /**
     * Number of CPUs per node.
     */
    cpuCount: number;
    /**
     * Memory size in Mebibytes for each memcache node.
     */
    memorySizeMb: number;
}

export interface MemcacheInstanceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface MigrationCenterGroupTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface MigrationCenterPreferenceSetTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface MigrationCenterPreferenceSetVirtualMachinePreferences {
    /**
     * Commitment plan to consider when calculating costs for virtual machine insights and recommendations. If you are unsure which value to set, a 3 year commitment plan is often a good value to start with. Possible values: 'COMMITMENT_PLAN_UNSPECIFIED', 'COMMITMENT_PLAN_NONE', 'COMMITMENT_PLAN_ONE_YEAR', 'COMMITMENT_PLAN_THREE_YEARS'
     */
    commitmentPlan?: string;
    /**
     * The user preferences relating to Compute Engine target platform.
     */
    computeEnginePreferences?: outputs.MigrationCenterPreferenceSetVirtualMachinePreferencesComputeEnginePreferences;
    /**
     * The user preferences relating to target regions.
     */
    regionPreferences?: outputs.MigrationCenterPreferenceSetVirtualMachinePreferencesRegionPreferences;
    /**
     * Sizing optimization strategy specifies the preferred strategy used when extrapolating usage data to calculate insights and recommendations for a virtual machine. If you are unsure which value to set, a moderate sizing optimization strategy is often a good value to start with. Possible values: 'SIZING_OPTIMIZATION_STRATEGY_UNSPECIFIED', 'SIZING_OPTIMIZATION_STRATEGY_SAME_AS_SOURCE', 'SIZING_OPTIMIZATION_STRATEGY_MODERATE', 'SIZING_OPTIMIZATION_STRATEGY_AGGRESSIVE'
     */
    sizingOptimizationStrategy?: string;
    /**
     * Preferences concerning Sole Tenancy nodes and VMs.
     */
    soleTenancyPreferences?: outputs.MigrationCenterPreferenceSetVirtualMachinePreferencesSoleTenancyPreferences;
    /**
     * Target product for assets using this preference set. Specify either target product or business goal, but not both. Possible values: 'COMPUTE_MIGRATION_TARGET_PRODUCT_UNSPECIFIED', 'COMPUTE_MIGRATION_TARGET_PRODUCT_COMPUTE_ENGINE', 'COMPUTE_MIGRATION_TARGET_PRODUCT_VMWARE_ENGINE', 'COMPUTE_MIGRATION_TARGET_PRODUCT_SOLE_TENANCY'
     */
    targetProduct?: string;
    /**
     * The user preferences relating to Google Cloud VMware Engine target platform.
     */
    vmwareEnginePreferences?: outputs.MigrationCenterPreferenceSetVirtualMachinePreferencesVmwareEnginePreferences;
}

export interface MigrationCenterPreferenceSetVirtualMachinePreferencesComputeEnginePreferences {
    /**
     * License type to consider when calculating costs for virtual machine insights and recommendations. If unspecified, costs are calculated based on the default licensing plan. Possible values: 'LICENSE_TYPE_UNSPECIFIED', 'LICENSE_TYPE_DEFAULT', 'LICENSE_TYPE_BRING_YOUR_OWN_LICENSE'
     */
    licenseType?: string;
    /**
     * The type of machines to consider when calculating virtual machine migration insights and recommendations. Not all machine types are available in all zones and regions.
     */
    machinePreferences?: outputs.MigrationCenterPreferenceSetVirtualMachinePreferencesComputeEnginePreferencesMachinePreferences;
}

export interface MigrationCenterPreferenceSetVirtualMachinePreferencesComputeEnginePreferencesMachinePreferences {
    /**
     * Compute Engine machine series to consider for insights and recommendations. If empty, no restriction is applied on the machine series.
     */
    allowedMachineSeries?: outputs.MigrationCenterPreferenceSetVirtualMachinePreferencesComputeEnginePreferencesMachinePreferencesAllowedMachineSeries[];
}

export interface MigrationCenterPreferenceSetVirtualMachinePreferencesComputeEnginePreferencesMachinePreferencesAllowedMachineSeries {
    /**
     * Code to identify a Compute Engine machine series. Consult https://cloud.google.com/compute/docs/machine-resource#machine_type_comparison for more details on the available series.
     */
    code?: string;
}

export interface MigrationCenterPreferenceSetVirtualMachinePreferencesRegionPreferences {
    /**
     * A list of preferred regions, ordered by the most preferred region first. Set only valid Google Cloud region names. See https://cloud.google.com/compute/docs/regions-zones for available regions.
     */
    preferredRegions?: string[];
}

export interface MigrationCenterPreferenceSetVirtualMachinePreferencesSoleTenancyPreferences {
    /**
     * Commitment plan to consider when calculating costs for virtual machine insights and recommendations. If you are unsure which value to set, a 3 year commitment plan is often a good value to start with. Possible values: 'COMMITMENT_PLAN_UNSPECIFIED', 'ON_DEMAND', 'COMMITMENT_1_YEAR', 'COMMITMENT_3_YEAR'
     */
    commitmentPlan?: string;
    /**
     * CPU overcommit ratio. Acceptable values are between 1.0 and 2.0 inclusive.
     */
    cpuOvercommitRatio?: number;
    /**
     * Sole Tenancy nodes maintenance policy. Possible values: 'HOST_MAINTENANCE_POLICY_UNSPECIFIED', 'HOST_MAINTENANCE_POLICY_DEFAULT', 'HOST_MAINTENANCE_POLICY_RESTART_IN_PLACE', 'HOST_MAINTENANCE_POLICY_MIGRATE_WITHIN_NODE_GROUP'
     */
    hostMaintenancePolicy?: string;
    /**
     * A list of sole tenant node types. An empty list means that all possible node types will be considered.
     */
    nodeTypes?: outputs.MigrationCenterPreferenceSetVirtualMachinePreferencesSoleTenancyPreferencesNodeType[];
}

export interface MigrationCenterPreferenceSetVirtualMachinePreferencesSoleTenancyPreferencesNodeType {
    /**
     * Name of the Sole Tenant node. Consult https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes
     */
    nodeName?: string;
}

export interface MigrationCenterPreferenceSetVirtualMachinePreferencesVmwareEnginePreferences {
    /**
     * Commitment plan to consider when calculating costs for virtual machine insights and recommendations. If you are unsure which value to set, a 3 year commitment plan is often a good value to start with. Possible values: 'COMMITMENT_PLAN_UNSPECIFIED', 'ON_DEMAND', 'COMMITMENT_1_YEAR_MONTHLY_PAYMENTS', 'COMMITMENT_3_YEAR_MONTHLY_PAYMENTS', 'COMMITMENT_1_YEAR_UPFRONT_PAYMENT', 'COMMITMENT_3_YEAR_UPFRONT_PAYMENT',
     */
    commitmentPlan?: string;
    /**
     * CPU overcommit ratio. Acceptable values are between 1.0 and 8.0, with 0.1 increment.
     */
    cpuOvercommitRatio?: number;
    /**
     * Memory overcommit ratio. Acceptable values are 1.0, 1.25, 1.5, 1.75 and 2.0.
     */
    memoryOvercommitRatio?: number;
    /**
     * The Deduplication and Compression ratio is based on the logical (Used Before) space required to store data before applying deduplication and compression, in relation to the physical (Used After) space required after applying deduplication and compression. Specifically, the ratio is the Used Before space divided by the Used After space. For example, if the Used Before space is 3 GB, but the physical Used After space is 1 GB, the deduplication and compression ratio is 3x. Acceptable values are between 1.0 and 4.0.
     */
    storageDeduplicationCompressionRatio?: number;
}

export interface MlEngineModelDefaultVersion {
    /**
     * The name specified for the version when it was created.
     */
    name: string;
}

export interface MlEngineModelTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface MonitoringAlertPolicyAlertStrategy {
    /**
     * If an alert policy that was active has no data for this long, any open incidents will close.
     */
    autoClose?: string;
    /**
     * Control over how the notification channels in 'notification_channels'
     * are notified when this alert fires, on a per-channel basis.
     */
    notificationChannelStrategies?: outputs.MonitoringAlertPolicyAlertStrategyNotificationChannelStrategy[];
    /**
     * Required for alert policies with a LogMatch condition.
     * This limit is not implemented for alert policies that are not log-based.
     */
    notificationRateLimit?: outputs.MonitoringAlertPolicyAlertStrategyNotificationRateLimit;
}

export interface MonitoringAlertPolicyAlertStrategyNotificationChannelStrategy {
    /**
     * The notification channels that these settings apply to. Each of these
     * correspond to the name field in one of the NotificationChannel objects
     * referenced in the notification_channels field of this AlertPolicy. The format is
     * 'projects/[PROJECT_ID_OR_NUMBER]/notificationChannels/[CHANNEL_ID]'
     */
    notificationChannelNames?: string[];
    /**
     * The frequency at which to send reminder notifications for open incidents.
     */
    renotifyInterval?: string;
}

export interface MonitoringAlertPolicyAlertStrategyNotificationRateLimit {
    /**
     * Not more than one notification per period.
     * A duration in seconds with up to nine fractional digits, terminated by 's'. Example "60.5s".
     */
    period?: string;
}

export interface MonitoringAlertPolicyCondition {
    /**
     * A condition that checks that a time series
     * continues to receive new data points.
     */
    conditionAbsent?: outputs.MonitoringAlertPolicyConditionConditionAbsent;
    /**
     * A condition that checks for log messages matching given constraints.
     * If set, no other conditions can be present.
     */
    conditionMatchedLog?: outputs.MonitoringAlertPolicyConditionConditionMatchedLog;
    /**
     * A Monitoring Query Language query that outputs a boolean stream
     */
    conditionMonitoringQueryLanguage?: outputs.MonitoringAlertPolicyConditionConditionMonitoringQueryLanguage;
    /**
     * A condition type that allows alert policies to be defined using
     * Prometheus Query Language (PromQL).
     *
     * The PrometheusQueryLanguageCondition message contains information
     * from a Prometheus alerting rule and its associated rule group.
     */
    conditionPrometheusQueryLanguage?: outputs.MonitoringAlertPolicyConditionConditionPrometheusQueryLanguage;
    /**
     * A condition that compares a time series against a
     * threshold.
     */
    conditionThreshold?: outputs.MonitoringAlertPolicyConditionConditionThreshold;
    /**
     * A short name or phrase used to identify the
     * condition in dashboards, notifications, and
     * incidents. To avoid confusion, don't use the same
     * display name for multiple conditions in the same
     * policy.
     */
    displayName: string;
    /**
     * The unique resource name for this condition.
     * Its syntax is:
     * projects/[PROJECT_ID]/alertPolicies/[POLICY_ID]/conditions/[CONDITION_ID]
     * [CONDITION_ID] is assigned by Stackdriver Monitoring when
     * the condition is created as part of a new or updated alerting
     * policy.
     */
    name: string;
}

export interface MonitoringAlertPolicyConditionConditionAbsent {
    /**
     * Specifies the alignment of data points in
     * individual time series as well as how to
     * combine the retrieved time series together
     * (such as when aggregating multiple streams
     * on each resource to a single stream for each
     * resource or when aggregating streams across
     * all members of a group of resources).
     * Multiple aggregations are applied in the
     * order specified.
     */
    aggregations?: outputs.MonitoringAlertPolicyConditionConditionAbsentAggregation[];
    /**
     * The amount of time that a time series must
     * fail to report new data to be considered
     * failing. Currently, only values that are a
     * multiple of a minute--e.g. 60s, 120s, or 300s
     * --are supported.
     */
    duration: string;
    /**
     * A filter that identifies which time series
     * should be compared with the threshold.The
     * filter is similar to the one that is
     * specified in the
     * MetricService.ListTimeSeries request (that
     * call is useful to verify the time series
     * that will be retrieved / processed) and must
     * specify the metric type and optionally may
     * contain restrictions on resource type,
     * resource labels, and metric labels. This
     * field may not exceed 2048 Unicode characters
     * in length.
     */
    filter?: string;
    /**
     * The number/percent of time series for which
     * the comparison must hold in order for the
     * condition to trigger. If unspecified, then
     * the condition will trigger if the comparison
     * is true for any of the time series that have
     * been identified by filter and aggregations.
     */
    trigger?: outputs.MonitoringAlertPolicyConditionConditionAbsentTrigger;
}

export interface MonitoringAlertPolicyConditionConditionAbsentAggregation {
    /**
     * The alignment period for per-time
     * series alignment. If present,
     * alignmentPeriod must be at least
     * 60 seconds. After per-time series
     * alignment, each time series will
     * contain data points only on the
     * period boundaries. If
     * perSeriesAligner is not specified
     * or equals ALIGN_NONE, then this
     * field is ignored. If
     * perSeriesAligner is specified and
     * does not equal ALIGN_NONE, then
     * this field must be defined;
     * otherwise an error is returned.
     */
    alignmentPeriod?: string;
    /**
     * The approach to be used to combine
     * time series. Not all reducer
     * functions may be applied to all
     * time series, depending on the
     * metric type and the value type of
     * the original time series.
     * Reduction may change the metric
     * type of value type of the time
     * series.Time series data must be
     * aligned in order to perform cross-
     * time series reduction. If
     * crossSeriesReducer is specified,
     * then perSeriesAligner must be
     * specified and not equal ALIGN_NONE
     * and alignmentPeriod must be
     * specified; otherwise, an error is
     * returned. Possible values: ["REDUCE_NONE", "REDUCE_MEAN", "REDUCE_MIN", "REDUCE_MAX", "REDUCE_SUM", "REDUCE_STDDEV", "REDUCE_COUNT", "REDUCE_COUNT_TRUE", "REDUCE_COUNT_FALSE", "REDUCE_FRACTION_TRUE", "REDUCE_PERCENTILE_99", "REDUCE_PERCENTILE_95", "REDUCE_PERCENTILE_50", "REDUCE_PERCENTILE_05"]
     */
    crossSeriesReducer?: string;
    /**
     * The set of fields to preserve when
     * crossSeriesReducer is specified.
     * The groupByFields determine how
     * the time series are partitioned
     * into subsets prior to applying the
     * aggregation function. Each subset
     * contains time series that have the
     * same value for each of the
     * grouping fields. Each individual
     * time series is a member of exactly
     * one subset. The crossSeriesReducer
     * is applied to each subset of time
     * series. It is not possible to
     * reduce across different resource
     * types, so this field implicitly
     * contains resource.type. Fields not
     * specified in groupByFields are
     * aggregated away. If groupByFields
     * is not specified and all the time
     * series have the same resource
     * type, then the time series are
     * aggregated into a single output
     * time series. If crossSeriesReducer
     * is not defined, this field is
     * ignored.
     */
    groupByFields?: string[];
    /**
     * The approach to be used to align
     * individual time series. Not all
     * alignment functions may be applied
     * to all time series, depending on
     * the metric type and value type of
     * the original time series.
     * Alignment may change the metric
     * type or the value type of the time
     * series.Time series data must be
     * aligned in order to perform cross-
     * time series reduction. If
     * crossSeriesReducer is specified,
     * then perSeriesAligner must be
     * specified and not equal ALIGN_NONE
     * and alignmentPeriod must be
     * specified; otherwise, an error is
     * returned. Possible values: ["ALIGN_NONE", "ALIGN_DELTA", "ALIGN_RATE", "ALIGN_INTERPOLATE", "ALIGN_NEXT_OLDER", "ALIGN_MIN", "ALIGN_MAX", "ALIGN_MEAN", "ALIGN_COUNT", "ALIGN_SUM", "ALIGN_STDDEV", "ALIGN_COUNT_TRUE", "ALIGN_COUNT_FALSE", "ALIGN_FRACTION_TRUE", "ALIGN_PERCENTILE_99", "ALIGN_PERCENTILE_95", "ALIGN_PERCENTILE_50", "ALIGN_PERCENTILE_05", "ALIGN_PERCENT_CHANGE"]
     */
    perSeriesAligner?: string;
}

export interface MonitoringAlertPolicyConditionConditionAbsentTrigger {
    /**
     * The absolute number of time series
     * that must fail the predicate for the
     * condition to be triggered.
     */
    count?: number;
    /**
     * The percentage of time series that
     * must fail the predicate for the
     * condition to be triggered.
     */
    percent?: number;
}

export interface MonitoringAlertPolicyConditionConditionMatchedLog {
    /**
     * A logs-based filter.
     */
    filter: string;
    /**
     * A map from a label key to an extractor expression, which is used to
     * extract the value for this label key. Each entry in this map is
     * a specification for how data should be extracted from log entries that
     * match filter. Each combination of extracted values is treated as
     * a separate rule for the purposes of triggering notifications.
     * Label keys and corresponding values can be used in notifications
     * generated by this condition.
     */
    labelExtractors?: {[key: string]: string};
}

export interface MonitoringAlertPolicyConditionConditionMonitoringQueryLanguage {
    /**
     * The amount of time that a time series must
     * violate the threshold to be considered
     * failing. Currently, only values that are a
     * multiple of a minute--e.g., 0, 60, 120, or
     * 300 seconds--are supported. If an invalid
     * value is given, an error will be returned.
     * When choosing a duration, it is useful to
     * keep in mind the frequency of the underlying
     * time series data (which may also be affected
     * by any alignments specified in the
     * aggregations field); a good duration is long
     * enough so that a single outlier does not
     * generate spurious alerts, but short enough
     * that unhealthy states are detected and
     * alerted on quickly.
     */
    duration: string;
    /**
     * A condition control that determines how
     * metric-threshold conditions are evaluated when
     * data stops arriving. Possible values: ["EVALUATION_MISSING_DATA_INACTIVE", "EVALUATION_MISSING_DATA_ACTIVE", "EVALUATION_MISSING_DATA_NO_OP"]
     */
    evaluationMissingData?: string;
    /**
     * Monitoring Query Language query that outputs a boolean stream.
     */
    query: string;
    /**
     * The number/percent of time series for which
     * the comparison must hold in order for the
     * condition to trigger. If unspecified, then
     * the condition will trigger if the comparison
     * is true for any of the time series that have
     * been identified by filter and aggregations,
     * or by the ratio, if denominator_filter and
     * denominator_aggregations are specified.
     */
    trigger?: outputs.MonitoringAlertPolicyConditionConditionMonitoringQueryLanguageTrigger;
}

export interface MonitoringAlertPolicyConditionConditionMonitoringQueryLanguageTrigger {
    /**
     * The absolute number of time series
     * that must fail the predicate for the
     * condition to be triggered.
     */
    count?: number;
    /**
     * The percentage of time series that
     * must fail the predicate for the
     * condition to be triggered.
     */
    percent?: number;
}

export interface MonitoringAlertPolicyConditionConditionPrometheusQueryLanguage {
    /**
     * The alerting rule name of this alert in the corresponding Prometheus
     * configuration file.
     *
     * Some external tools may require this field to be populated correctly
     * in order to refer to the original Prometheus configuration file.
     * The rule group name and the alert name are necessary to update the
     * relevant AlertPolicies in case the definition of the rule group changes
     * in the future.
     *
     * This field is optional. If this field is not empty, then it must be a
     * valid Prometheus label name.
     */
    alertRule?: string;
    /**
     * Alerts are considered firing once their PromQL expression evaluated
     * to be "true" for this long. Alerts whose PromQL expression was not
     * evaluated to be "true" for long enough are considered pending. The
     * default value is zero. Must be zero or positive.
     */
    duration?: string;
    /**
     * How often this rule should be evaluated. Must be a positive multiple
     * of 30 seconds or missing. The default value is 30 seconds. If this
     * PrometheusQueryLanguageCondition was generated from a Prometheus
     * alerting rule, then this value should be taken from the enclosing
     * rule group.
     */
    evaluationInterval?: string;
    /**
     * Labels to add to or overwrite in the PromQL query result. Label names
     * must be valid.
     *
     * Label values can be templatized by using variables. The only available
     * variable names are the names of the labels in the PromQL result, including
     * "__name__" and "value". "labels" may be empty. This field is intended to be
     * used for organizing and identifying the AlertPolicy
     */
    labels?: {[key: string]: string};
    /**
     * The PromQL expression to evaluate. Every evaluation cycle this
     * expression is evaluated at the current time, and all resultant time
     * series become pending/firing alerts. This field must not be empty.
     */
    query: string;
    /**
     * The rule group name of this alert in the corresponding Prometheus
     * configuration file.
     *
     * Some external tools may require this field to be populated correctly
     * in order to refer to the original Prometheus configuration file.
     * The rule group name and the alert name are necessary to update the
     * relevant AlertPolicies in case the definition of the rule group changes
     * in the future. This field is optional.
     */
    ruleGroup?: string;
}

export interface MonitoringAlertPolicyConditionConditionThreshold {
    /**
     * Specifies the alignment of data points in
     * individual time series as well as how to
     * combine the retrieved time series together
     * (such as when aggregating multiple streams
     * on each resource to a single stream for each
     * resource or when aggregating streams across
     * all members of a group of resources).
     * Multiple aggregations are applied in the
     * order specified.This field is similar to the
     * one in the MetricService.ListTimeSeries
     * request. It is advisable to use the
     * ListTimeSeries method when debugging this
     * field.
     */
    aggregations?: outputs.MonitoringAlertPolicyConditionConditionThresholdAggregation[];
    /**
     * The comparison to apply between the time
     * series (indicated by filter and aggregation)
     * and the threshold (indicated by
     * threshold_value). The comparison is applied
     * on each time series, with the time series on
     * the left-hand side and the threshold on the
     * right-hand side. Only COMPARISON_LT and
     * COMPARISON_GT are supported currently. Possible values: ["COMPARISON_GT", "COMPARISON_GE", "COMPARISON_LT", "COMPARISON_LE", "COMPARISON_EQ", "COMPARISON_NE"]
     */
    comparison: string;
    /**
     * Specifies the alignment of data points in
     * individual time series selected by
     * denominatorFilter as well as how to combine
     * the retrieved time series together (such as
     * when aggregating multiple streams on each
     * resource to a single stream for each
     * resource or when aggregating streams across
     * all members of a group of resources).When
     * computing ratios, the aggregations and
     * denominator_aggregations fields must use the
     * same alignment period and produce time
     * series that have the same periodicity and
     * labels.This field is similar to the one in
     * the MetricService.ListTimeSeries request. It
     * is advisable to use the ListTimeSeries
     * method when debugging this field.
     */
    denominatorAggregations?: outputs.MonitoringAlertPolicyConditionConditionThresholdDenominatorAggregation[];
    /**
     * A filter that identifies a time series that
     * should be used as the denominator of a ratio
     * that will be compared with the threshold. If
     * a denominator_filter is specified, the time
     * series specified by the filter field will be
     * used as the numerator.The filter is similar
     * to the one that is specified in the
     * MetricService.ListTimeSeries request (that
     * call is useful to verify the time series
     * that will be retrieved / processed) and must
     * specify the metric type and optionally may
     * contain restrictions on resource type,
     * resource labels, and metric labels. This
     * field may not exceed 2048 Unicode characters
     * in length.
     */
    denominatorFilter?: string;
    /**
     * The amount of time that a time series must
     * violate the threshold to be considered
     * failing. Currently, only values that are a
     * multiple of a minute--e.g., 0, 60, 120, or
     * 300 seconds--are supported. If an invalid
     * value is given, an error will be returned.
     * When choosing a duration, it is useful to
     * keep in mind the frequency of the underlying
     * time series data (which may also be affected
     * by any alignments specified in the
     * aggregations field); a good duration is long
     * enough so that a single outlier does not
     * generate spurious alerts, but short enough
     * that unhealthy states are detected and
     * alerted on quickly.
     */
    duration: string;
    /**
     * A condition control that determines how
     * metric-threshold conditions are evaluated when
     * data stops arriving. Possible values: ["EVALUATION_MISSING_DATA_INACTIVE", "EVALUATION_MISSING_DATA_ACTIVE", "EVALUATION_MISSING_DATA_NO_OP"]
     */
    evaluationMissingData?: string;
    /**
     * A filter that identifies which time series
     * should be compared with the threshold.The
     * filter is similar to the one that is
     * specified in the
     * MetricService.ListTimeSeries request (that
     * call is useful to verify the time series
     * that will be retrieved / processed) and must
     * specify the metric type and optionally may
     * contain restrictions on resource type,
     * resource labels, and metric labels. This
     * field may not exceed 2048 Unicode characters
     * in length.
     */
    filter?: string;
    /**
     * When this field is present, the 'MetricThreshold'
     * condition forecasts whether the time series is
     * predicted to violate the threshold within the
     * 'forecastHorizon'. When this field is not set, the
     * 'MetricThreshold' tests the current value of the
     * timeseries against the threshold.
     */
    forecastOptions?: outputs.MonitoringAlertPolicyConditionConditionThresholdForecastOptions;
    /**
     * A value against which to compare the time
     * series.
     */
    thresholdValue?: number;
    /**
     * The number/percent of time series for which
     * the comparison must hold in order for the
     * condition to trigger. If unspecified, then
     * the condition will trigger if the comparison
     * is true for any of the time series that have
     * been identified by filter and aggregations,
     * or by the ratio, if denominator_filter and
     * denominator_aggregations are specified.
     */
    trigger?: outputs.MonitoringAlertPolicyConditionConditionThresholdTrigger;
}

export interface MonitoringAlertPolicyConditionConditionThresholdAggregation {
    /**
     * The alignment period for per-time
     * series alignment. If present,
     * alignmentPeriod must be at least
     * 60 seconds. After per-time series
     * alignment, each time series will
     * contain data points only on the
     * period boundaries. If
     * perSeriesAligner is not specified
     * or equals ALIGN_NONE, then this
     * field is ignored. If
     * perSeriesAligner is specified and
     * does not equal ALIGN_NONE, then
     * this field must be defined;
     * otherwise an error is returned.
     */
    alignmentPeriod?: string;
    /**
     * The approach to be used to combine
     * time series. Not all reducer
     * functions may be applied to all
     * time series, depending on the
     * metric type and the value type of
     * the original time series.
     * Reduction may change the metric
     * type of value type of the time
     * series.Time series data must be
     * aligned in order to perform cross-
     * time series reduction. If
     * crossSeriesReducer is specified,
     * then perSeriesAligner must be
     * specified and not equal ALIGN_NONE
     * and alignmentPeriod must be
     * specified; otherwise, an error is
     * returned. Possible values: ["REDUCE_NONE", "REDUCE_MEAN", "REDUCE_MIN", "REDUCE_MAX", "REDUCE_SUM", "REDUCE_STDDEV", "REDUCE_COUNT", "REDUCE_COUNT_TRUE", "REDUCE_COUNT_FALSE", "REDUCE_FRACTION_TRUE", "REDUCE_PERCENTILE_99", "REDUCE_PERCENTILE_95", "REDUCE_PERCENTILE_50", "REDUCE_PERCENTILE_05"]
     */
    crossSeriesReducer?: string;
    /**
     * The set of fields to preserve when
     * crossSeriesReducer is specified.
     * The groupByFields determine how
     * the time series are partitioned
     * into subsets prior to applying the
     * aggregation function. Each subset
     * contains time series that have the
     * same value for each of the
     * grouping fields. Each individual
     * time series is a member of exactly
     * one subset. The crossSeriesReducer
     * is applied to each subset of time
     * series. It is not possible to
     * reduce across different resource
     * types, so this field implicitly
     * contains resource.type. Fields not
     * specified in groupByFields are
     * aggregated away. If groupByFields
     * is not specified and all the time
     * series have the same resource
     * type, then the time series are
     * aggregated into a single output
     * time series. If crossSeriesReducer
     * is not defined, this field is
     * ignored.
     */
    groupByFields?: string[];
    /**
     * The approach to be used to align
     * individual time series. Not all
     * alignment functions may be applied
     * to all time series, depending on
     * the metric type and value type of
     * the original time series.
     * Alignment may change the metric
     * type or the value type of the time
     * series.Time series data must be
     * aligned in order to perform cross-
     * time series reduction. If
     * crossSeriesReducer is specified,
     * then perSeriesAligner must be
     * specified and not equal ALIGN_NONE
     * and alignmentPeriod must be
     * specified; otherwise, an error is
     * returned. Possible values: ["ALIGN_NONE", "ALIGN_DELTA", "ALIGN_RATE", "ALIGN_INTERPOLATE", "ALIGN_NEXT_OLDER", "ALIGN_MIN", "ALIGN_MAX", "ALIGN_MEAN", "ALIGN_COUNT", "ALIGN_SUM", "ALIGN_STDDEV", "ALIGN_COUNT_TRUE", "ALIGN_COUNT_FALSE", "ALIGN_FRACTION_TRUE", "ALIGN_PERCENTILE_99", "ALIGN_PERCENTILE_95", "ALIGN_PERCENTILE_50", "ALIGN_PERCENTILE_05", "ALIGN_PERCENT_CHANGE"]
     */
    perSeriesAligner?: string;
}

export interface MonitoringAlertPolicyConditionConditionThresholdDenominatorAggregation {
    /**
     * The alignment period for per-time
     * series alignment. If present,
     * alignmentPeriod must be at least
     * 60 seconds. After per-time series
     * alignment, each time series will
     * contain data points only on the
     * period boundaries. If
     * perSeriesAligner is not specified
     * or equals ALIGN_NONE, then this
     * field is ignored. If
     * perSeriesAligner is specified and
     * does not equal ALIGN_NONE, then
     * this field must be defined;
     * otherwise an error is returned.
     */
    alignmentPeriod?: string;
    /**
     * The approach to be used to combine
     * time series. Not all reducer
     * functions may be applied to all
     * time series, depending on the
     * metric type and the value type of
     * the original time series.
     * Reduction may change the metric
     * type of value type of the time
     * series.Time series data must be
     * aligned in order to perform cross-
     * time series reduction. If
     * crossSeriesReducer is specified,
     * then perSeriesAligner must be
     * specified and not equal ALIGN_NONE
     * and alignmentPeriod must be
     * specified; otherwise, an error is
     * returned. Possible values: ["REDUCE_NONE", "REDUCE_MEAN", "REDUCE_MIN", "REDUCE_MAX", "REDUCE_SUM", "REDUCE_STDDEV", "REDUCE_COUNT", "REDUCE_COUNT_TRUE", "REDUCE_COUNT_FALSE", "REDUCE_FRACTION_TRUE", "REDUCE_PERCENTILE_99", "REDUCE_PERCENTILE_95", "REDUCE_PERCENTILE_50", "REDUCE_PERCENTILE_05"]
     */
    crossSeriesReducer?: string;
    /**
     * The set of fields to preserve when
     * crossSeriesReducer is specified.
     * The groupByFields determine how
     * the time series are partitioned
     * into subsets prior to applying the
     * aggregation function. Each subset
     * contains time series that have the
     * same value for each of the
     * grouping fields. Each individual
     * time series is a member of exactly
     * one subset. The crossSeriesReducer
     * is applied to each subset of time
     * series. It is not possible to
     * reduce across different resource
     * types, so this field implicitly
     * contains resource.type. Fields not
     * specified in groupByFields are
     * aggregated away. If groupByFields
     * is not specified and all the time
     * series have the same resource
     * type, then the time series are
     * aggregated into a single output
     * time series. If crossSeriesReducer
     * is not defined, this field is
     * ignored.
     */
    groupByFields?: string[];
    /**
     * The approach to be used to align
     * individual time series. Not all
     * alignment functions may be applied
     * to all time series, depending on
     * the metric type and value type of
     * the original time series.
     * Alignment may change the metric
     * type or the value type of the time
     * series.Time series data must be
     * aligned in order to perform cross-
     * time series reduction. If
     * crossSeriesReducer is specified,
     * then perSeriesAligner must be
     * specified and not equal ALIGN_NONE
     * and alignmentPeriod must be
     * specified; otherwise, an error is
     * returned. Possible values: ["ALIGN_NONE", "ALIGN_DELTA", "ALIGN_RATE", "ALIGN_INTERPOLATE", "ALIGN_NEXT_OLDER", "ALIGN_MIN", "ALIGN_MAX", "ALIGN_MEAN", "ALIGN_COUNT", "ALIGN_SUM", "ALIGN_STDDEV", "ALIGN_COUNT_TRUE", "ALIGN_COUNT_FALSE", "ALIGN_FRACTION_TRUE", "ALIGN_PERCENTILE_99", "ALIGN_PERCENTILE_95", "ALIGN_PERCENTILE_50", "ALIGN_PERCENTILE_05", "ALIGN_PERCENT_CHANGE"]
     */
    perSeriesAligner?: string;
}

export interface MonitoringAlertPolicyConditionConditionThresholdForecastOptions {
    /**
     * The length of time into the future to forecast
     * whether a timeseries will violate the threshold.
     * If the predicted value is found to violate the
     * threshold, and the violation is observed in all
     * forecasts made for the Configured 'duration',
     * then the timeseries is considered to be failing.
     */
    forecastHorizon: string;
}

export interface MonitoringAlertPolicyConditionConditionThresholdTrigger {
    /**
     * The absolute number of time series
     * that must fail the predicate for the
     * condition to be triggered.
     */
    count?: number;
    /**
     * The percentage of time series that
     * must fail the predicate for the
     * condition to be triggered.
     */
    percent?: number;
}

export interface MonitoringAlertPolicyCreationRecord {
    mutateTime: string;
    mutatedBy: string;
}

export interface MonitoringAlertPolicyDocumentation {
    /**
     * The text of the documentation, interpreted according to mimeType.
     * The content may not exceed 8,192 Unicode characters and may not
     * exceed more than 10,240 bytes when encoded in UTF-8 format,
     * whichever is smaller.
     */
    content?: string;
    /**
     * Links to content such as playbooks, repositories, and other resources. This field can contain up to 3 entries.
     */
    links?: outputs.MonitoringAlertPolicyDocumentationLink[];
    /**
     * The format of the content field. Presently, only the value
     * "text/markdown" is supported.
     */
    mimeType?: string;
    /**
     * The subject line of the notification. The subject line may not
     * exceed 10,240 bytes. In notifications generated by this policy the contents
     * of the subject line after variable expansion will be truncated to 255 bytes
     * or shorter at the latest UTF-8 character boundary.
     */
    subject?: string;
}

export interface MonitoringAlertPolicyDocumentationLink {
    /**
     * A short display name for the link. The display name must not be empty or exceed 63 characters. Example: "playbook".
     */
    displayName?: string;
    /**
     * The url of a webpage. A url can be templatized by using variables in the path or the query parameters. The total length of a URL should not exceed 2083 characters before and after variable expansion. Example: "https://my_domain.com/playbook?name=${resource.name}".
     */
    url?: string;
}

export interface MonitoringAlertPolicyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface MonitoringCustomServiceTelemetry {
    /**
     * The full name of the resource that defines this service.
     * Formatted as described in
     * https://cloud.google.com/apis/design/resource_names.
     */
    resourceName?: string;
}

export interface MonitoringCustomServiceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface MonitoringDashboardTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface MonitoringGroupTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface MonitoringMetricDescriptorLabel {
    /**
     * A human-readable description for the label.
     */
    description?: string;
    /**
     * The key for this label. The key must not exceed 100 characters. The first character of the key must be an upper- or lower-case letter, the remaining characters must be letters, digits or underscores, and the key must match the regular expression [a-zA-Z][a-zA-Z0-9_]*
     */
    key: string;
    /**
     * The type of data that can be assigned to the label. Default value: "STRING" Possible values: ["STRING", "BOOL", "INT64"]
     */
    valueType?: string;
}

export interface MonitoringMetricDescriptorMetadata {
    /**
     * The delay of data points caused by ingestion. Data points older than this age are guaranteed to be ingested and available to be read, excluding data loss due to errors. In '[duration format](https://developers.google.com/protocol-buffers/docs/reference/google.protobuf?&_ga=2.264881487.1507873253.1593446723-935052455.1591817775#google.protobuf.Duration)'.
     */
    ingestDelay?: string;
    /**
     * The sampling period of metric data points. For metrics which are written periodically, consecutive data points are stored at this time interval, excluding data loss due to errors. Metrics with a higher granularity have a smaller sampling period. In '[duration format](https://developers.google.com/protocol-buffers/docs/reference/google.protobuf?&_ga=2.264881487.1507873253.1593446723-935052455.1591817775#google.protobuf.Duration)'.
     */
    samplePeriod?: string;
}

export interface MonitoringMetricDescriptorTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface MonitoringMonitoredProjectTimeouts {
    create?: string;
    delete?: string;
}

export interface MonitoringNotificationChannelSensitiveLabels {
    /**
     * An authorization token for a notification channel. Channel types that support this field include: slack
     */
    authToken?: string;
    /**
     * An password for a notification channel. Channel types that support this field include: webhook_basicauth
     */
    password?: string;
    /**
     * An servicekey token for a notification channel. Channel types that support this field include: pagerduty
     */
    serviceKey?: string;
}

export interface MonitoringNotificationChannelTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface MonitoringServiceBasicService {
    /**
     * Labels that specify the resource that emits the monitoring data
     * which is used for SLO reporting of this 'Service'.
     */
    serviceLabels?: {[key: string]: string};
    /**
     * The type of service that this basic service defines, e.g.
     * APP_ENGINE service type
     */
    serviceType?: string;
}

export interface MonitoringServiceTelemetry {
    resourceName: string;
}

export interface MonitoringServiceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface MonitoringSloBasicSli {
    /**
     * Availability based SLI, dervied from count of requests made to this service that return successfully.
     */
    availability?: outputs.MonitoringSloBasicSliAvailability;
    /**
     * Parameters for a latency threshold SLI.
     */
    latency?: outputs.MonitoringSloBasicSliLatency;
    /**
     * An optional set of locations to which this SLI is relevant.
     * Telemetry from other locations will not be used to calculate
     * performance for this SLI. If omitted, this SLI applies to all
     * locations in which the Service has activity. For service types
     * that don't support breaking down by location, setting this
     * field will result in an error.
     */
    locations?: string[];
    /**
     * An optional set of RPCs to which this SLI is relevant.
     * Telemetry from other methods will not be used to calculate
     * performance for this SLI. If omitted, this SLI applies to all
     * the Service's methods. For service types that don't support
     * breaking down by method, setting this field will result in an
     * error.
     */
    methods?: string[];
    /**
     * The set of API versions to which this SLI is relevant.
     * Telemetry from other API versions will not be used to
     * calculate performance for this SLI. If omitted,
     * this SLI applies to all API versions. For service types
     * that don't support breaking down by version, setting this
     * field will result in an error.
     */
    versions?: string[];
}

export interface MonitoringSloBasicSliAvailability {
    /**
     * Whether an availability SLI is enabled or not. Must be set to true. Defaults to 'true'.
     */
    enabled?: boolean;
}

export interface MonitoringSloBasicSliLatency {
    /**
     * A duration string, e.g. 10s.
     * Good service is defined to be the count of requests made to
     * this service that return in no more than threshold.
     */
    threshold: string;
}

export interface MonitoringSloRequestBasedSli {
    /**
     * Used when good_service is defined by a count of values aggregated in a
     * Distribution that fall into a good range. The total_service is the
     * total count of all values aggregated in the Distribution.
     * Defines a distribution TimeSeries filter and thresholds used for
     * measuring good service and total service.
     *
     * Exactly one of 'distribution_cut' or 'good_total_ratio' can be set.
     */
    distributionCut?: outputs.MonitoringSloRequestBasedSliDistributionCut;
    /**
     * A means to compute a ratio of 'good_service' to 'total_service'.
     * Defines computing this ratio with two TimeSeries [monitoring filters](https://cloud.google.com/monitoring/api/v3/filters)
     * Must specify exactly two of good, bad, and total service filters.
     * The relationship good_service + bad_service = total_service
     * will be assumed.
     *
     * Exactly one of 'distribution_cut' or 'good_total_ratio' can be set.
     */
    goodTotalRatio?: outputs.MonitoringSloRequestBasedSliGoodTotalRatio;
}

export interface MonitoringSloRequestBasedSliDistributionCut {
    /**
     * A TimeSeries [monitoring filter](https://cloud.google.com/monitoring/api/v3/filters)
     * aggregating values to quantify the good service provided.
     *
     * Must have ValueType = DISTRIBUTION and
     * MetricKind = DELTA or MetricKind = CUMULATIVE.
     */
    distributionFilter: string;
    /**
     * Range of numerical values. The computed good_service
     * will be the count of values x in the Distribution such
     * that range.min <= x <= range.max. inclusive of min and
     * max. Open ranges can be defined by setting
     * just one of min or max.
     */
    range: outputs.MonitoringSloRequestBasedSliDistributionCutRange;
}

export interface MonitoringSloRequestBasedSliDistributionCutRange {
    /**
     * max value for the range (inclusive). If not given,
     * will be set to 0
     */
    max?: number;
    /**
     * Min value for the range (inclusive). If not given,
     * will be set to 0
     */
    min?: number;
}

export interface MonitoringSloRequestBasedSliGoodTotalRatio {
    /**
     * A TimeSeries [monitoring filter](https://cloud.google.com/monitoring/api/v3/filters)
     * quantifying bad service provided, either demanded service that
     * was not provided or demanded service that was of inadequate
     * quality.
     *
     * Must have ValueType = DOUBLE or ValueType = INT64 and
     * must have MetricKind = DELTA or MetricKind = CUMULATIVE.
     *
     * Exactly two of 'good_service_filter','bad_service_filter','total_service_filter'
     * must be set (good + bad = total is assumed).
     */
    badServiceFilter?: string;
    /**
     * A TimeSeries [monitoring filter](https://cloud.google.com/monitoring/api/v3/filters)
     * quantifying good service provided.
     * Must have ValueType = DOUBLE or ValueType = INT64 and
     * must have MetricKind = DELTA or MetricKind = CUMULATIVE.
     *
     * Exactly two of 'good_service_filter','bad_service_filter','total_service_filter'
     * must be set (good + bad = total is assumed).
     */
    goodServiceFilter?: string;
    /**
     * A TimeSeries [monitoring filter](https://cloud.google.com/monitoring/api/v3/filters)
     * quantifying total demanded service.
     *
     * Must have ValueType = DOUBLE or ValueType = INT64 and
     * must have MetricKind = DELTA or MetricKind = CUMULATIVE.
     *
     * Exactly two of 'good_service_filter','bad_service_filter','total_service_filter'
     * must be set (good + bad = total is assumed).
     */
    totalServiceFilter?: string;
}

export interface MonitoringSloTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface MonitoringSloWindowsBasedSli {
    /**
     * A TimeSeries [monitoring filter](https://cloud.google.com/monitoring/api/v3/filters)
     * with ValueType = BOOL. The window is good if any true values
     * appear in the window. One of 'good_bad_metric_filter',
     * 'good_total_ratio_threshold', 'metric_mean_in_range',
     * 'metric_sum_in_range' must be set for 'windows_based_sli'.
     */
    goodBadMetricFilter?: string;
    /**
     * Criterion that describes a window as good if its performance is
     * high enough. One of 'good_bad_metric_filter',
     * 'good_total_ratio_threshold', 'metric_mean_in_range',
     * 'metric_sum_in_range' must be set for 'windows_based_sli'.
     */
    goodTotalRatioThreshold?: outputs.MonitoringSloWindowsBasedSliGoodTotalRatioThreshold;
    /**
     * Criterion that describes a window as good if the metric's value
     * is in a good range, *averaged* across returned streams.
     * One of 'good_bad_metric_filter',
     *
     * 'good_total_ratio_threshold', 'metric_mean_in_range',
     * 'metric_sum_in_range' must be set for 'windows_based_sli'.
     * Average value X of 'time_series' should satisfy
     * 'range.min <= X <= range.max' for a good window.
     */
    metricMeanInRange?: outputs.MonitoringSloWindowsBasedSliMetricMeanInRange;
    /**
     * Criterion that describes a window as good if the metric's value
     * is in a good range, *summed* across returned streams.
     * Summed value 'X' of 'time_series' should satisfy
     * 'range.min <= X <= range.max' for a good window.
     *
     * One of 'good_bad_metric_filter',
     * 'good_total_ratio_threshold', 'metric_mean_in_range',
     * 'metric_sum_in_range' must be set for 'windows_based_sli'.
     */
    metricSumInRange?: outputs.MonitoringSloWindowsBasedSliMetricSumInRange;
    /**
     * Duration over which window quality is evaluated, given as a
     * duration string "{X}s" representing X seconds. Must be an
     * integer fraction of a day and at least 60s.
     */
    windowPeriod?: string;
}

export interface MonitoringSloWindowsBasedSliGoodTotalRatioThreshold {
    /**
     * Basic SLI to evaluate to judge window quality.
     */
    basicSliPerformance?: outputs.MonitoringSloWindowsBasedSliGoodTotalRatioThresholdBasicSliPerformance;
    /**
     * Request-based SLI to evaluate to judge window quality.
     */
    performance?: outputs.MonitoringSloWindowsBasedSliGoodTotalRatioThresholdPerformance;
    /**
     * If window performance >= threshold, the window is counted
     * as good.
     */
    threshold?: number;
}

export interface MonitoringSloWindowsBasedSliGoodTotalRatioThresholdBasicSliPerformance {
    /**
     * Availability based SLI, dervied from count of requests made to this service that return successfully.
     */
    availability?: outputs.MonitoringSloWindowsBasedSliGoodTotalRatioThresholdBasicSliPerformanceAvailability;
    /**
     * Parameters for a latency threshold SLI.
     */
    latency?: outputs.MonitoringSloWindowsBasedSliGoodTotalRatioThresholdBasicSliPerformanceLatency;
    /**
     * An optional set of locations to which this SLI is relevant.
     * Telemetry from other locations will not be used to calculate
     * performance for this SLI. If omitted, this SLI applies to all
     * locations in which the Service has activity. For service types
     * that don't support breaking down by location, setting this
     * field will result in an error.
     */
    locations?: string[];
    /**
     * An optional set of RPCs to which this SLI is relevant.
     * Telemetry from other methods will not be used to calculate
     * performance for this SLI. If omitted, this SLI applies to all
     * the Service's methods. For service types that don't support
     * breaking down by method, setting this field will result in an
     * error.
     */
    methods?: string[];
    /**
     * The set of API versions to which this SLI is relevant.
     * Telemetry from other API versions will not be used to
     * calculate performance for this SLI. If omitted,
     * this SLI applies to all API versions. For service types
     * that don't support breaking down by version, setting this
     * field will result in an error.
     */
    versions?: string[];
}

export interface MonitoringSloWindowsBasedSliGoodTotalRatioThresholdBasicSliPerformanceAvailability {
    /**
     * Whether an availability SLI is enabled or not. Must be set to 'true. Defaults to 'true'.
     */
    enabled?: boolean;
}

export interface MonitoringSloWindowsBasedSliGoodTotalRatioThresholdBasicSliPerformanceLatency {
    /**
     * A duration string, e.g. 10s.
     * Good service is defined to be the count of requests made to
     * this service that return in no more than threshold.
     */
    threshold: string;
}

export interface MonitoringSloWindowsBasedSliGoodTotalRatioThresholdPerformance {
    /**
     * Used when good_service is defined by a count of values aggregated in a
     * Distribution that fall into a good range. The total_service is the
     * total count of all values aggregated in the Distribution.
     * Defines a distribution TimeSeries filter and thresholds used for
     * measuring good service and total service.
     */
    distributionCut?: outputs.MonitoringSloWindowsBasedSliGoodTotalRatioThresholdPerformanceDistributionCut;
    /**
     * A means to compute a ratio of 'good_service' to 'total_service'.
     * Defines computing this ratio with two TimeSeries [monitoring filters](https://cloud.google.com/monitoring/api/v3/filters)
     * Must specify exactly two of good, bad, and total service filters.
     * The relationship good_service + bad_service = total_service
     * will be assumed.
     */
    goodTotalRatio?: outputs.MonitoringSloWindowsBasedSliGoodTotalRatioThresholdPerformanceGoodTotalRatio;
}

export interface MonitoringSloWindowsBasedSliGoodTotalRatioThresholdPerformanceDistributionCut {
    /**
     * A TimeSeries [monitoring filter](https://cloud.google.com/monitoring/api/v3/filters)
     * aggregating values to quantify the good service provided.
     *
     * Must have ValueType = DISTRIBUTION and
     * MetricKind = DELTA or MetricKind = CUMULATIVE.
     */
    distributionFilter: string;
    /**
     * Range of numerical values. The computed good_service
     * will be the count of values x in the Distribution such
     * that range.min <= x <= range.max. inclusive of min and
     * max. Open ranges can be defined by setting
     * just one of min or max.
     */
    range: outputs.MonitoringSloWindowsBasedSliGoodTotalRatioThresholdPerformanceDistributionCutRange;
}

export interface MonitoringSloWindowsBasedSliGoodTotalRatioThresholdPerformanceDistributionCutRange {
    /**
     * max value for the range (inclusive). If not given,
     * will be set to 0
     */
    max?: number;
    /**
     * Min value for the range (inclusive). If not given,
     * will be set to 0
     */
    min?: number;
}

export interface MonitoringSloWindowsBasedSliGoodTotalRatioThresholdPerformanceGoodTotalRatio {
    /**
     * A TimeSeries [monitoring filter](https://cloud.google.com/monitoring/api/v3/filters)
     * quantifying bad service provided, either demanded service that
     * was not provided or demanded service that was of inadequate
     * quality. Exactly two of
     * good, bad, or total service filter must be defined (where
     * good + bad = total is assumed)
     *
     * Must have ValueType = DOUBLE or ValueType = INT64 and
     * must have MetricKind = DELTA or MetricKind = CUMULATIVE.
     */
    badServiceFilter?: string;
    /**
     * A TimeSeries [monitoring filter](https://cloud.google.com/monitoring/api/v3/filters)
     * quantifying good service provided. Exactly two of
     * good, bad, or total service filter must be defined (where
     * good + bad = total is assumed)
     *
     * Must have ValueType = DOUBLE or ValueType = INT64 and
     * must have MetricKind = DELTA or MetricKind = CUMULATIVE.
     */
    goodServiceFilter?: string;
    /**
     * A TimeSeries [monitoring filter](https://cloud.google.com/monitoring/api/v3/filters)
     * quantifying total demanded service. Exactly two of
     * good, bad, or total service filter must be defined (where
     * good + bad = total is assumed)
     *
     * Must have ValueType = DOUBLE or ValueType = INT64 and
     * must have MetricKind = DELTA or MetricKind = CUMULATIVE.
     */
    totalServiceFilter?: string;
}

export interface MonitoringSloWindowsBasedSliMetricMeanInRange {
    /**
     * Range of numerical values. The computed good_service
     * will be the count of values x in the Distribution such
     * that range.min <= x <= range.max. inclusive of min and
     * max. Open ranges can be defined by setting
     * just one of min or max. Mean value 'X' of 'time_series'
     * values should satisfy 'range.min <= X <= range.max' for a
     * good service.
     */
    range: outputs.MonitoringSloWindowsBasedSliMetricMeanInRangeRange;
    /**
     * A [monitoring filter](https://cloud.google.com/monitoring/api/v3/filters)
     * specifying the TimeSeries to use for evaluating window
     * The provided TimeSeries must have ValueType = INT64 or
     * ValueType = DOUBLE and MetricKind = GAUGE. Mean value 'X'
     * should satisfy 'range.min <= X <= range.max'
     * under good service.
     */
    timeSeries: string;
}

export interface MonitoringSloWindowsBasedSliMetricMeanInRangeRange {
    /**
     * max value for the range (inclusive). If not given,
     * will be set to "infinity", defining an open range
     * ">= range.min"
     */
    max?: number;
    /**
     * Min value for the range (inclusive). If not given,
     * will be set to "-infinity", defining an open range
     * "< range.max"
     */
    min?: number;
}

export interface MonitoringSloWindowsBasedSliMetricSumInRange {
    /**
     * Range of numerical values. The computed good_service
     * will be the count of values x in the Distribution such
     * that range.min <= x <= range.max. inclusive of min and
     * max. Open ranges can be defined by setting
     * just one of min or max. Summed value 'X' should satisfy
     * 'range.min <= X <= range.max' for a good window.
     */
    range: outputs.MonitoringSloWindowsBasedSliMetricSumInRangeRange;
    /**
     * A [monitoring filter](https://cloud.google.com/monitoring/api/v3/filters)
     * specifying the TimeSeries to use for evaluating window
     * quality. The provided TimeSeries must have
     * ValueType = INT64 or ValueType = DOUBLE and
     * MetricKind = GAUGE.
     *
     * Summed value 'X' should satisfy
     * 'range.min <= X <= range.max' for a good window.
     */
    timeSeries: string;
}

export interface MonitoringSloWindowsBasedSliMetricSumInRangeRange {
    /**
     * max value for the range (inclusive). If not given,
     * will be set to "infinity", defining an open range
     * ">= range.min"
     */
    max?: number;
    /**
     * Min value for the range (inclusive). If not given,
     * will be set to "-infinity", defining an open range
     * "< range.max"
     */
    min?: number;
}

export interface MonitoringUptimeCheckConfigContentMatcher {
    /**
     * String or regex content to match (max 1024 bytes)
     */
    content: string;
    /**
     * Information needed to perform a JSONPath content match. Used for 'ContentMatcherOption::MATCHES_JSON_PATH' and 'ContentMatcherOption::NOT_MATCHES_JSON_PATH'.
     */
    jsonPathMatcher?: outputs.MonitoringUptimeCheckConfigContentMatcherJsonPathMatcher;
    /**
     * The type of content matcher that will be applied to the server output, compared to the content string when the check is run. Default value: "CONTAINS_STRING" Possible values: ["CONTAINS_STRING", "NOT_CONTAINS_STRING", "MATCHES_REGEX", "NOT_MATCHES_REGEX", "MATCHES_JSON_PATH", "NOT_MATCHES_JSON_PATH"]
     */
    matcher?: string;
}

export interface MonitoringUptimeCheckConfigContentMatcherJsonPathMatcher {
    /**
     * Options to perform JSONPath content matching. Default value: "EXACT_MATCH" Possible values: ["EXACT_MATCH", "REGEX_MATCH"]
     */
    jsonMatcher?: string;
    /**
     * JSONPath within the response output pointing to the expected 'ContentMatcher::content' to match against.
     */
    jsonPath: string;
}

export interface MonitoringUptimeCheckConfigHttpCheck {
    /**
     * If present, the check will only pass if the HTTP response status code is in this set of status codes. If empty, the HTTP status code will only pass if the HTTP status code is 200-299.
     */
    acceptedResponseStatusCodes?: outputs.MonitoringUptimeCheckConfigHttpCheckAcceptedResponseStatusCode[];
    /**
     * The authentication information using username and password. Optional when creating an HTTP check; defaults to empty. Do not use with other authentication fields.
     */
    authInfo?: outputs.MonitoringUptimeCheckConfigHttpCheckAuthInfo;
    /**
     * The request body associated with the HTTP POST request. If 'content_type' is 'URL_ENCODED', the body passed in must be URL-encoded. Users can provide a 'Content-Length' header via the 'headers' field or the API will do so. If the 'request_method' is 'GET' and 'body' is not empty, the API will return an error. The maximum byte size is 1 megabyte. Note - As with all bytes fields JSON representations are base64 encoded. e.g. 'foo=bar' in URL-encoded form is 'foo%3Dbar' and in base64 encoding is 'Zm9vJTI1M0RiYXI='.
     */
    body?: string;
    /**
     * The content type to use for the check. Possible values: ["TYPE_UNSPECIFIED", "URL_ENCODED", "USER_PROVIDED"]
     */
    contentType?: string;
    /**
     * A user provided content type header to use for the check. The invalid configurations outlined in the 'content_type' field apply to custom_content_type', as well as the following 1. 'content_type' is 'URL_ENCODED' and 'custom_content_type' is set. 2. 'content_type' is 'USER_PROVIDED' and 'custom_content_type' is not set.
     */
    customContentType?: string;
    /**
     * The list of headers to send as part of the uptime check request. If two headers have the same key and different values, they should be entered as a single header, with the value being a comma-separated list of all the desired values as described in [RFC 2616 (page 31)](https://www.w3.org/Protocols/rfc2616/rfc2616.txt). Entering two separate headers with the same key in a Create call will cause the first to be overwritten by the second. The maximum number of headers allowed is 100.
     */
    headers: {[key: string]: string};
    /**
     * Boolean specifying whether to encrypt the header information. Encryption should be specified for any headers related to authentication that you do not wish to be seen when retrieving the configuration. The server will be responsible for encrypting the headers. On Get/List calls, if 'mask_headers' is set to 'true' then the headers will be obscured with '******'.
     */
    maskHeaders?: boolean;
    /**
     * The path to the page to run the check against. Will be combined with the host (specified within the MonitoredResource) and port to construct the full URL. If the provided path does not begin with '/', a '/' will be prepended automatically. Optional (defaults to '/').
     */
    path?: string;
    /**
     * Contains information needed to add pings to an HTTP check.
     */
    pingConfig?: outputs.MonitoringUptimeCheckConfigHttpCheckPingConfig;
    /**
     * The port to the page to run the check against. Will be combined with 'host' (specified within the 'monitored_resource') and path to construct the full URL. Optional (defaults to 80 without SSL, or 443 with SSL).
     */
    port: number;
    /**
     * The HTTP request method to use for the check. If set to 'METHOD_UNSPECIFIED' then 'request_method' defaults to 'GET'. Default value: "GET" Possible values: ["METHOD_UNSPECIFIED", "GET", "POST"]
     */
    requestMethod?: string;
    /**
     * The authentication information using the Monitoring Service Agent. Optional when creating an HTTPS check; defaults to empty. Do not use with other authentication fields.
     */
    serviceAgentAuthentication?: outputs.MonitoringUptimeCheckConfigHttpCheckServiceAgentAuthentication;
    /**
     * If true, use HTTPS instead of HTTP to run the check.
     */
    useSsl?: boolean;
    /**
     * Boolean specifying whether to include SSL certificate validation as a part of the Uptime check. Only applies to checks where 'monitored_resource' is set to 'uptime_url'. If 'use_ssl' is 'false', setting 'validate_ssl' to 'true' has no effect.
     */
    validateSsl?: boolean;
}

export interface MonitoringUptimeCheckConfigHttpCheckAcceptedResponseStatusCode {
    /**
     * A class of status codes to accept. Possible values: ["STATUS_CLASS_1XX", "STATUS_CLASS_2XX", "STATUS_CLASS_3XX", "STATUS_CLASS_4XX", "STATUS_CLASS_5XX", "STATUS_CLASS_ANY"]
     */
    statusClass?: string;
    /**
     * A status code to accept.
     */
    statusValue?: number;
}

export interface MonitoringUptimeCheckConfigHttpCheckAuthInfo {
    /**
     * The password to authenticate.
     */
    password: string;
    /**
     * The username to authenticate.
     */
    username: string;
}

export interface MonitoringUptimeCheckConfigHttpCheckPingConfig {
    /**
     * Number of ICMP pings. A maximum of 3 ICMP pings is currently supported.
     */
    pingsCount: number;
}

export interface MonitoringUptimeCheckConfigHttpCheckServiceAgentAuthentication {
    /**
     * The type of authentication to use. Possible values: ["SERVICE_AGENT_AUTHENTICATION_TYPE_UNSPECIFIED", "OIDC_TOKEN"]
     */
    type?: string;
}

export interface MonitoringUptimeCheckConfigMonitoredResource {
    /**
     * Values for all of the labels listed in the associated monitored resource descriptor. For example, Compute Engine VM instances use the labels 'project_id', 'instance_id', and 'zone'.
     */
    labels: {[key: string]: string};
    /**
     * The monitored resource type. This field must match the type field of a ['MonitoredResourceDescriptor'](https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.monitoredResourceDescriptors#MonitoredResourceDescriptor) object. For example, the type of a Compute Engine VM instance is 'gce_instance'. For a list of types, see [Monitoring resource types](https://cloud.google.com/monitoring/api/resources) and [Logging resource types](https://cloud.google.com/logging/docs/api/v2/resource-list).
     */
    type: string;
}

export interface MonitoringUptimeCheckConfigResourceGroup {
    /**
     * The group of resources being monitored. Should be the 'name' of a group
     */
    groupId?: string;
    /**
     * The resource type of the group members. Possible values: ["RESOURCE_TYPE_UNSPECIFIED", "INSTANCE", "AWS_ELB_LOAD_BALANCER"]
     */
    resourceType?: string;
}

export interface MonitoringUptimeCheckConfigSyntheticMonitor {
    /**
     * Target a Synthetic Monitor GCFv2 Instance
     */
    cloudFunctionV2: outputs.MonitoringUptimeCheckConfigSyntheticMonitorCloudFunctionV2;
}

export interface MonitoringUptimeCheckConfigSyntheticMonitorCloudFunctionV2 {
    /**
     * The fully qualified name of the cloud function resource.
     */
    name: string;
}

export interface MonitoringUptimeCheckConfigTcpCheck {
    /**
     * Contains information needed to add pings to a TCP check.
     */
    pingConfig?: outputs.MonitoringUptimeCheckConfigTcpCheckPingConfig;
    /**
     * The port to the page to run the check against. Will be combined with host (specified within the 'monitored_resource') to construct the full URL.
     */
    port: number;
}

export interface MonitoringUptimeCheckConfigTcpCheckPingConfig {
    /**
     * Number of ICMP pings. A maximum of 3 ICMP pings is currently supported.
     */
    pingsCount: number;
}

export interface MonitoringUptimeCheckConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetappActiveDirectoryTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetappBackupPolicyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetappBackupTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetappBackupVaultTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetappKmsconfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetappStoragePoolTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetappVolumeBackupConfig {
    /**
     * Specify a single backup policy ID for scheduled backups. Format: 'projects/{{projectId}}/locations/{{location}}/backupPolicies/{{backupPolicyName}}'
     */
    backupPolicies?: string[];
    /**
     * ID of the backup vault to use. A backup vault is reqired to create manual or scheduled backups.
     * Format: 'projects/{{projectId}}/locations/{{location}}/backupVaults/{{backupVaultName}}'
     */
    backupVault?: string;
    /**
     * When set to true, scheduled backup is enabled on the volume. Omit if no backup_policy is specified.
     */
    scheduledBackupEnabled?: boolean;
}

export interface NetappVolumeExportPolicy {
    /**
     * Export rules (up to 5) control NFS volume access.
     */
    rules: outputs.NetappVolumeExportPolicyRule[];
}

export interface NetappVolumeExportPolicyRule {
    /**
     * Defines the access type for clients matching the 'allowedClients' specification. Possible values: ["READ_ONLY", "READ_WRITE", "READ_NONE"]
     */
    accessType?: string;
    /**
     * Defines the client ingress specification (allowed clients) as a comma seperated list with IPv4 CIDRs or IPv4 host addresses.
     */
    allowedClients?: string;
    /**
     * If enabled, the root user (UID = 0) of the specified clients doesn't get mapped to nobody (UID = 65534). This is also known as no_root_squash.
     */
    hasRootAccess?: string;
    /**
     * If enabled (true) the rule defines a read only access for clients matching the 'allowedClients' specification. It enables nfs clients to mount using 'authentication' kerberos security mode.
     */
    kerberos5ReadOnly?: boolean;
    /**
     * If enabled (true) the rule defines read and write access for clients matching the 'allowedClients' specification. It enables nfs clients to mount using 'authentication' kerberos security mode. The 'kerberos5ReadOnly' value is ignored if this is enabled.
     */
    kerberos5ReadWrite?: boolean;
    /**
     * If enabled (true) the rule defines a read only access for clients matching the 'allowedClients' specification. It enables nfs clients to mount using 'integrity' kerberos security mode.
     */
    kerberos5iReadOnly?: boolean;
    /**
     * If enabled (true) the rule defines read and write access for clients matching the 'allowedClients' specification. It enables nfs clients to mount using 'integrity' kerberos security mode. The 'kerberos5iReadOnly' value is ignored if this is enabled.
     */
    kerberos5iReadWrite?: boolean;
    /**
     * If enabled (true) the rule defines a read only access for clients matching the 'allowedClients' specification. It enables nfs clients to mount using 'privacy' kerberos security mode.
     */
    kerberos5pReadOnly?: boolean;
    /**
     * If enabled (true) the rule defines read and write access for clients matching the 'allowedClients' specification. It enables nfs clients to mount using 'privacy' kerberos security mode. The 'kerberos5pReadOnly' value is ignored if this is enabled.
     */
    kerberos5pReadWrite?: boolean;
    /**
     * Enable to apply the export rule to NFSV3 clients.
     */
    nfsv3?: boolean;
    /**
     * Enable to apply the export rule to NFSV4.1 clients.
     */
    nfsv4?: boolean;
}

export interface NetappVolumeMountOption {
    export: string;
    exportFull: string;
    instructions: string;
    protocol: string;
}

export interface NetappVolumeReplicationDestinationVolumeParameters {
    /**
     * Description for the destination volume.
     */
    description?: string;
    /**
     * Share name for destination volume. If not specified, name of source volume's share name will be used.
     */
    shareName: string;
    /**
     * Name of an existing storage pool for the destination volume with format: 'projects/{{project}}/locations/{{location}}/storagePools/{{poolId}}'
     */
    storagePool: string;
    /**
     * Name for the destination volume to be created. If not specified, the name of the source volume will be used.
     */
    volumeId: string;
}

export interface NetappVolumeReplicationTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetappVolumeReplicationTransferStat {
    lagDuration: string;
    lastTransferBytes: string;
    lastTransferDuration: string;
    lastTransferEndTime: string;
    lastTransferError: string;
    totalTransferDuration: string;
    transferBytes: string;
    updateTime: string;
}

export interface NetappVolumeRestoreParameters {
    /**
     * Full name of the snapshot to use for creating this volume.
     * 'source_snapshot' and 'source_backup' cannot be used simultaneously.
     * Format: 'projects/{{project}}/locations/{{location}}/backupVaults/{{backupVaultId}}/backups/{{backup}}'.
     */
    sourceBackup?: string;
    /**
     * Full name of the snapshot to use for creating this volume.
     * 'source_snapshot' and 'source_backup' cannot be used simultaneously.
     * Format: 'projects/{{project}}/locations/{{location}}/volumes/{{volume}}/snapshots/{{snapshot}}'.
     */
    sourceSnapshot?: string;
}

export interface NetappVolumeSnapshotPolicy {
    /**
     * Daily schedule policy.
     */
    dailySchedule?: outputs.NetappVolumeSnapshotPolicyDailySchedule;
    /**
     * Enables automated snapshot creation according to defined schedule. Default is false.
     * To disable automatic snapshot creation you have to remove the whole snapshot_policy block.
     */
    enabled?: boolean;
    /**
     * Hourly schedule policy.
     */
    hourlySchedule?: outputs.NetappVolumeSnapshotPolicyHourlySchedule;
    /**
     * Monthly schedule policy.
     */
    monthlySchedule?: outputs.NetappVolumeSnapshotPolicyMonthlySchedule;
    /**
     * Weekly schedule policy.
     */
    weeklySchedule?: outputs.NetappVolumeSnapshotPolicyWeeklySchedule;
}

export interface NetappVolumeSnapshotPolicyDailySchedule {
    /**
     * Set the hour to create the snapshot (0-23), defaults to midnight (0).
     */
    hour?: number;
    /**
     * Set the minute of the hour to create the snapshot (0-59), defaults to the top of the hour (0).
     */
    minute?: number;
    /**
     * The maximum number of snapshots to keep for the daily schedule.
     */
    snapshotsToKeep: number;
}

export interface NetappVolumeSnapshotPolicyHourlySchedule {
    /**
     * Set the minute of the hour to create the snapshot (0-59), defaults to the top of the hour (0).
     */
    minute?: number;
    /**
     * The maximum number of snapshots to keep for the hourly schedule.
     */
    snapshotsToKeep: number;
}

export interface NetappVolumeSnapshotPolicyMonthlySchedule {
    /**
     * Set the day or days of the month to make a snapshot (1-31). Accepts a comma separated number of days. Defaults to '1'.
     */
    daysOfMonth?: string;
    /**
     * Set the hour to create the snapshot (0-23), defaults to midnight (0).
     */
    hour?: number;
    /**
     * Set the minute of the hour to create the snapshot (0-59), defaults to the top of the hour (0).
     */
    minute?: number;
    /**
     * The maximum number of snapshots to keep for the monthly schedule
     */
    snapshotsToKeep: number;
}

export interface NetappVolumeSnapshotPolicyWeeklySchedule {
    /**
     * Set the day or days of the week to make a snapshot. Accepts a comma separated days of the week. Defaults to 'Sunday'.
     */
    day?: string;
    /**
     * Set the hour to create the snapshot (0-23), defaults to midnight (0).
     */
    hour?: number;
    /**
     * Set the minute of the hour to create the snapshot (0-59), defaults to the top of the hour (0).
     */
    minute?: number;
    /**
     * The maximum number of snapshots to keep for the weekly schedule.
     */
    snapshotsToKeep: number;
}

export interface NetappVolumeSnapshotTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetappVolumeTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetworkConnectivityHubRoutingVpc {
    uri: string;
}

export interface NetworkConnectivityHubTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetworkConnectivityInternalRangeTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetworkConnectivityPolicyBasedRouteFilter {
    /**
     * The destination IP range of outgoing packets that this policy-based route applies to. Default is "0.0.0.0/0" if protocol version is IPv4.
     */
    destRange?: string;
    /**
     * The IP protocol that this policy-based route applies to. Valid values are 'TCP', 'UDP', and 'ALL'. Default is 'ALL'.
     */
    ipProtocol?: string;
    /**
     * Internet protocol versions this policy-based route applies to. Possible values: ["IPV4"]
     */
    protocolVersion: string;
    /**
     * The source IP range of outgoing packets that this policy-based route applies to. Default is "0.0.0.0/0" if protocol version is IPv4.
     */
    srcRange?: string;
}

export interface NetworkConnectivityPolicyBasedRouteInterconnectAttachment {
    /**
     * Cloud region to install this policy-based route on for Interconnect attachments. Use 'all' to install it on all Interconnect attachments.
     */
    region: string;
}

export interface NetworkConnectivityPolicyBasedRouteTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetworkConnectivityPolicyBasedRouteVirtualMachine {
    /**
     * A list of VM instance tags that this policy-based route applies to. VM instances that have ANY of tags specified here will install this PBR.
     */
    tags: string[];
}

export interface NetworkConnectivityPolicyBasedRouteWarning {
    code: string;
    data: {[key: string]: string};
    warningMessage: string;
}

export interface NetworkConnectivityRegionalEndpointTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetworkConnectivityServiceConnectionPolicyPscConfig {
    /**
     * Max number of PSC connections for this policy.
     */
    limit?: string;
    /**
     * IDs of the subnetworks or fully qualified identifiers for the subnetworks
     */
    subnetworks: string[];
}

export interface NetworkConnectivityServiceConnectionPolicyPscConnection {
    consumerAddress: string;
    consumerForwardingRule: string;
    consumerTargetProject: string;
    errorInfos: outputs.NetworkConnectivityServiceConnectionPolicyPscConnectionErrorInfo[];
    errorType: string;
    errors: outputs.NetworkConnectivityServiceConnectionPolicyPscConnectionError[];
    gceOperation: string;
    pscConnectionId: string;
    state: string;
}

export interface NetworkConnectivityServiceConnectionPolicyPscConnectionError {
    code: number;
    details: {[key: string]: string}[];
    message: string;
}

export interface NetworkConnectivityServiceConnectionPolicyPscConnectionErrorInfo {
    domain: string;
    metadata: {[key: string]: string};
    reason: string;
}

export interface NetworkConnectivityServiceConnectionPolicyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetworkConnectivitySpokeLinkedInterconnectAttachments {
    /**
     * A value that controls whether site-to-site data transfer is enabled for these resources. Note that data transfer is available only in supported locations.
     */
    siteToSiteDataTransfer: boolean;
    /**
     * The URIs of linked interconnect attachment resources
     */
    uris: string[];
}

export interface NetworkConnectivitySpokeLinkedRouterApplianceInstances {
    /**
     * The list of router appliance instances
     */
    instances: outputs.NetworkConnectivitySpokeLinkedRouterApplianceInstancesInstance[];
    /**
     * A value that controls whether site-to-site data transfer is enabled for these resources. Note that data transfer is available only in supported locations.
     */
    siteToSiteDataTransfer: boolean;
}

export interface NetworkConnectivitySpokeLinkedRouterApplianceInstancesInstance {
    /**
     * The IP address on the VM to use for peering.
     */
    ipAddress?: string;
    /**
     * The URI of the virtual machine resource
     */
    virtualMachine?: string;
}

export interface NetworkConnectivitySpokeLinkedVpcNetwork {
    /**
     * IP ranges encompassing the subnets to be excluded from peering.
     */
    excludeExportRanges?: string[];
    /**
     * The URI of the VPC network resource.
     */
    uri: string;
}

export interface NetworkConnectivitySpokeLinkedVpnTunnels {
    /**
     * A value that controls whether site-to-site data transfer is enabled for these resources. Note that data transfer is available only in supported locations.
     */
    siteToSiteDataTransfer: boolean;
    /**
     * The URIs of linked VPN tunnel resources.
     */
    uris: string[];
}

export interface NetworkConnectivitySpokeTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetworkManagementConnectivityTestDestination {
    /**
     * A Compute Engine instance URI.
     */
    instance?: string;
    /**
     * The IP address of the endpoint, which can be an external or
     * internal IP. An IPv6 address is only allowed when the test's
     * destination is a global load balancer VIP.
     */
    ipAddress?: string;
    /**
     * A Compute Engine network URI.
     */
    network?: string;
    /**
     * The IP protocol port of the endpoint. Only applicable when
     * protocol is TCP or UDP.
     */
    port?: number;
    /**
     * Project ID where the endpoint is located. The Project ID can be
     * derived from the URI if you provide a VM instance or network URI.
     * The following are two cases where you must provide the project ID:
     * 1. Only the IP address is specified, and the IP address is within
     * a GCP project. 2. When you are using Shared VPC and the IP address
     * that you provide is from the service project. In this case, the
     * network that the IP address resides in is defined in the host
     * project.
     */
    projectId?: string;
}

export interface NetworkManagementConnectivityTestSource {
    /**
     * A Compute Engine instance URI.
     */
    instance?: string;
    /**
     * The IP address of the endpoint, which can be an external or
     * internal IP. An IPv6 address is only allowed when the test's
     * destination is a global load balancer VIP.
     */
    ipAddress?: string;
    /**
     * A Compute Engine network URI.
     */
    network?: string;
    /**
     * Type of the network where the endpoint is located. Possible values: ["GCP_NETWORK", "NON_GCP_NETWORK"]
     */
    networkType?: string;
    /**
     * The IP protocol port of the endpoint. Only applicable when
     * protocol is TCP or UDP.
     */
    port?: number;
    /**
     * Project ID where the endpoint is located. The Project ID can be
     * derived from the URI if you provide a VM instance or network URI.
     * The following are two cases where you must provide the project ID:
     *
     * 1. Only the IP address is specified, and the IP address is
     *    within a GCP project.
     * 2. When you are using Shared VPC and the IP address
     *    that you provide is from the service project. In this case,
     *    the network that the IP address resides in is defined in the
     *    host project.
     */
    projectId?: string;
}

export interface NetworkManagementConnectivityTestTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetworkSecurityAddressGroupIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface NetworkSecurityAddressGroupIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface NetworkSecurityAddressGroupTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetworkSecurityClientTlsPolicyClientCertificate {
    /**
     * The certificate provider instance specification that will be passed to the data plane, which will be used to load necessary credential information.
     */
    certificateProviderInstance?: outputs.NetworkSecurityClientTlsPolicyClientCertificateCertificateProviderInstance;
    /**
     * gRPC specific configuration to access the gRPC server to obtain the cert and private key.
     */
    grpcEndpoint?: outputs.NetworkSecurityClientTlsPolicyClientCertificateGrpcEndpoint;
}

export interface NetworkSecurityClientTlsPolicyClientCertificateCertificateProviderInstance {
    /**
     * Plugin instance name, used to locate and load CertificateProvider instance configuration. Set to "google_cloud_private_spiffe" to use Certificate Authority Service certificate provider instance.
     */
    pluginInstance: string;
}

export interface NetworkSecurityClientTlsPolicyClientCertificateGrpcEndpoint {
    /**
     * The target URI of the gRPC endpoint. Only UDS path is supported, and should start with "unix:".
     */
    targetUri: string;
}

export interface NetworkSecurityClientTlsPolicyServerValidationCa {
    /**
     * The certificate provider instance specification that will be passed to the data plane, which will be used to load necessary credential information.
     */
    certificateProviderInstance?: outputs.NetworkSecurityClientTlsPolicyServerValidationCaCertificateProviderInstance;
    /**
     * gRPC specific configuration to access the gRPC server to obtain the cert and private key.
     */
    grpcEndpoint?: outputs.NetworkSecurityClientTlsPolicyServerValidationCaGrpcEndpoint;
}

export interface NetworkSecurityClientTlsPolicyServerValidationCaCertificateProviderInstance {
    /**
     * Plugin instance name, used to locate and load CertificateProvider instance configuration. Set to "google_cloud_private_spiffe" to use Certificate Authority Service certificate provider instance.
     */
    pluginInstance: string;
}

export interface NetworkSecurityClientTlsPolicyServerValidationCaGrpcEndpoint {
    /**
     * The target URI of the gRPC endpoint. Only UDS path is supported, and should start with "unix:".
     */
    targetUri: string;
}

export interface NetworkSecurityClientTlsPolicyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetworkSecurityFirewallEndpointAssociationTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetworkSecurityFirewallEndpointTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetworkSecurityGatewaySecurityPolicyRuleTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetworkSecurityGatewaySecurityPolicyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetworkSecuritySecurityProfileGroupTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetworkSecuritySecurityProfileThreatPreventionProfile {
    /**
     * The configuration for overriding threats actions by severity match.
     */
    severityOverrides?: outputs.NetworkSecuritySecurityProfileThreatPreventionProfileSeverityOverride[];
    /**
     * The configuration for overriding threats actions by threat id match.
     * If a threat is matched both by configuration provided in severity overrides
     * and threat overrides, the threat overrides action is applied.
     */
    threatOverrides?: outputs.NetworkSecuritySecurityProfileThreatPreventionProfileThreatOverride[];
}

export interface NetworkSecuritySecurityProfileThreatPreventionProfileSeverityOverride {
    /**
     * Threat action override. Possible values: ["ALERT", "ALLOW", "DEFAULT_ACTION", "DENY"]
     */
    action: string;
    /**
     * Severity level to match. Possible values: ["CRITICAL", "HIGH", "INFORMATIONAL", "LOW", "MEDIUM"]
     */
    severity: string;
}

export interface NetworkSecuritySecurityProfileThreatPreventionProfileThreatOverride {
    /**
     * Threat action. Possible values: ["ALERT", "ALLOW", "DEFAULT_ACTION", "DENY"]
     */
    action: string;
    /**
     * Vendor-specific ID of a threat to override.
     */
    threatId: string;
    /**
     * Type of threat.
     */
    type: string;
}

export interface NetworkSecuritySecurityProfileTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetworkSecurityTlsInspectionPolicyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetworkSecurityUrlListsTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetworkServicesEdgeCacheKeysetPublicKey {
    /**
     * The ID of the public key. The ID must be 1-63 characters long, and comply with RFC1035.
     * The name must be 1-64 characters long, and match the regular expression [a-zA-Z][a-zA-Z0-9_-]*
     * which means the first character must be a letter, and all following characters must be a dash, underscore, letter or digit.
     */
    id: string;
    /**
     * Set to true to have the CDN automatically manage this public key value.
     */
    managed?: boolean;
    /**
     * The base64-encoded value of the Ed25519 public key. The base64 encoding can be padded (44 bytes) or unpadded (43 bytes).
     * Representations or encodings of the public key other than this will be rejected with an error.
     */
    value?: string;
}

export interface NetworkServicesEdgeCacheKeysetTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetworkServicesEdgeCacheKeysetValidationSharedKey {
    /**
     * The name of the secret version in Secret Manager.
     *
     * The resource name of the secret version must be in the format 'projects/*&#47;secrets/*&#47;versions/*' where the '*' values are replaced by the secrets themselves.
     * The secrets must be at least 16 bytes large.  The recommended secret size depends on the signature algorithm you are using.
     * * If you are using HMAC-SHA1, we suggest 20-byte secrets.
     * * If you are using HMAC-SHA256, we suggest 32-byte secrets.
     * See RFC 2104, Section 3 for more details on these recommendations.
     */
    secretVersion: string;
}

export interface NetworkServicesEdgeCacheOriginAwsV4Authentication {
    /**
     * The access key ID your origin uses to identify the key.
     */
    accessKeyId: string;
    /**
     * The name of the AWS region that your origin is in.
     */
    originRegion: string;
    /**
     * The Secret Manager secret version of the secret access key used by your origin.
     *
     * This is the resource name of the secret version in the format 'projects/*&#47;secrets/*&#47;versions/*' where the '*' values are replaced by the project, secret, and version you require.
     */
    secretAccessKeyVersion: string;
}

export interface NetworkServicesEdgeCacheOriginOriginOverrideAction {
    /**
     * The header actions, including adding and removing
     * headers, for request handled by this origin.
     */
    headerAction?: outputs.NetworkServicesEdgeCacheOriginOriginOverrideActionHeaderAction;
    /**
     * The URL rewrite configuration for request that are
     * handled by this origin.
     */
    urlRewrite?: outputs.NetworkServicesEdgeCacheOriginOriginOverrideActionUrlRewrite;
}

export interface NetworkServicesEdgeCacheOriginOriginOverrideActionHeaderAction {
    /**
     * Describes a header to add.
     *
     * You may add a maximum of 25 request headers.
     */
    requestHeadersToAdds?: outputs.NetworkServicesEdgeCacheOriginOriginOverrideActionHeaderActionRequestHeadersToAdd[];
}

export interface NetworkServicesEdgeCacheOriginOriginOverrideActionHeaderActionRequestHeadersToAdd {
    /**
     * The name of the header to add.
     */
    headerName: string;
    /**
     * The value of the header to add.
     */
    headerValue: string;
    /**
     * Whether to replace all existing headers with the same name.
     *
     * By default, added header values are appended
     * to the response or request headers with the
     * same field names. The added values are
     * separated by commas.
     *
     * To overwrite existing values, set 'replace' to 'true'.
     */
    replace?: boolean;
}

export interface NetworkServicesEdgeCacheOriginOriginOverrideActionUrlRewrite {
    /**
     * Prior to forwarding the request to the selected
     * origin, the request's host header is replaced with
     * contents of the hostRewrite.
     *
     * This value must be between 1 and 255 characters.
     */
    hostRewrite?: string;
}

export interface NetworkServicesEdgeCacheOriginOriginRedirect {
    /**
     * The set of redirect response codes that the CDN
     * follows. Values of
     * [RedirectConditions](https://cloud.google.com/media-cdn/docs/reference/rest/v1/projects.locations.edgeCacheOrigins#redirectconditions)
     * are accepted.
     */
    redirectConditions?: string[];
}

export interface NetworkServicesEdgeCacheOriginTimeout {
    /**
     * The maximum duration to wait for a single origin connection to be established, including DNS lookup, TLS handshake and TCP/QUIC connection establishment.
     *
     * Defaults to 5 seconds. The timeout must be a value between 1s and 15s.
     *
     * The connectTimeout capped by the deadline set by the request's maxAttemptsTimeout.  The last connection attempt may have a smaller connectTimeout in order to adhere to the overall maxAttemptsTimeout.
     */
    connectTimeout?: string;
    /**
     * The maximum time across all connection attempts to the origin, including failover origins, before returning an error to the client. A HTTP 504 will be returned if the timeout is reached before a response is returned.
     *
     * Defaults to 15 seconds. The timeout must be a value between 1s and 30s.
     *
     * If a failoverOrigin is specified, the maxAttemptsTimeout of the first configured origin sets the deadline for all connection attempts across all failoverOrigins.
     */
    maxAttemptsTimeout?: string;
    /**
     * The maximum duration to wait between reads of a single HTTP connection/stream.
     *
     * Defaults to 15 seconds.  The timeout must be a value between 1s and 30s.
     *
     * The readTimeout is capped by the responseTimeout.  All reads of the HTTP connection/stream must be completed by the deadline set by the responseTimeout.
     *
     * If the response headers have already been written to the connection, the response will be truncated and logged.
     */
    readTimeout?: string;
    /**
     * The maximum duration to wait for the last byte of a response to arrive when reading from the HTTP connection/stream.
     *
     * Defaults to 30 seconds. The timeout must be a value between 1s and 120s.
     *
     * The responseTimeout starts after the connection has been established.
     *
     * This also applies to HTTP Chunked Transfer Encoding responses, and/or when an open-ended Range request is made to the origin. Origins that take longer to write additional bytes to the response than the configured responseTimeout will result in an error being returned to the client.
     *
     * If the response headers have already been written to the connection, the response will be truncated and logged.
     */
    responseTimeout?: string;
}

export interface NetworkServicesEdgeCacheOriginTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetworkServicesEdgeCacheServiceLogConfig {
    /**
     * Specifies whether to enable logging for traffic served by this service.
     */
    enable: boolean;
    /**
     * Configures the sampling rate of requests, where 1.0 means all logged requests are reported and 0.0 means no logged requests are reported. The default value is 1.0, and the value of the field must be in [0, 1].
     *
     * This field can only be specified if logging is enabled for this service.
     */
    sampleRate?: number;
}

export interface NetworkServicesEdgeCacheServiceRouting {
    /**
     * The list of hostRules to match against. These rules define which hostnames the EdgeCacheService will match against, and which route configurations apply.
     */
    hostRules: outputs.NetworkServicesEdgeCacheServiceRoutingHostRule[];
    /**
     * The list of pathMatchers referenced via name by hostRules. PathMatcher is used to match the path portion of the URL when a HostRule matches the URL's host portion.
     */
    pathMatchers: outputs.NetworkServicesEdgeCacheServiceRoutingPathMatcher[];
}

export interface NetworkServicesEdgeCacheServiceRoutingHostRule {
    /**
     * A human-readable description of the hostRule.
     */
    description?: string;
    /**
     * The list of host patterns to match.
     *
     * Host patterns must be valid hostnames. Ports are not allowed. Wildcard hosts are supported in the suffix or prefix form. * matches any string of ([a-z0-9-.]*). It does not match the empty string.
     *
     * When multiple hosts are specified, hosts are matched in the following priority:
     *
     *   1. Exact domain names: ''www.foo.com''.
     *   2. Suffix domain wildcards: ''*.foo.com'' or ''*-bar.foo.com''.
     *   3. Prefix domain wildcards: ''foo.*'' or ''foo-*''.
     *   4. Special wildcard ''*'' matching any domain.
     *
     *   Notes:
     *
     *     The wildcard will not match the empty string. e.g. ''*-bar.foo.com'' will match ''baz-bar.foo.com'' but not ''-bar.foo.com''. The longest wildcards match first. Only a single host in the entire service can match on ''*''. A domain must be unique across all configured hosts within a service.
     *
     *     Hosts are matched against the HTTP Host header, or for HTTP/2 and HTTP/3, the ":authority" header, from the incoming request.
     *
     *     You may specify up to 10 hosts.
     */
    hosts: string[];
    /**
     * The name of the pathMatcher associated with this hostRule.
     */
    pathMatcher: string;
}

export interface NetworkServicesEdgeCacheServiceRoutingPathMatcher {
    /**
     * A human-readable description of the resource.
     */
    description?: string;
    /**
     * The name to which this PathMatcher is referred by the HostRule.
     */
    name: string;
    /**
     * The routeRules to match against. routeRules support advanced routing behaviour, and can match on paths, headers and query parameters, as well as status codes and HTTP methods.
     */
    routeRules: outputs.NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRule[];
}

export interface NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRule {
    /**
     * A human-readable description of the routeRule.
     */
    description?: string;
    /**
     * The header actions, including adding & removing headers, for requests that match this route.
     */
    headerAction?: outputs.NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleHeaderAction;
    /**
     * The list of criteria for matching attributes of a request to this routeRule. This list has OR semantics: the request matches this routeRule when any of the matchRules are satisfied. However predicates
     * within a given matchRule have AND semantics. All predicates within a matchRule must match for the request to match the rule.
     */
    matchRules: outputs.NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleMatchRule[];
    /**
     * The Origin resource that requests to this route should fetch from when a matching response is not in cache. Origins can be defined as short names ("my-origin") or fully-qualified resource URLs - e.g. "networkservices.googleapis.com/projects/my-project/global/edgecacheorigins/my-origin"
     *
     * Only one of origin or urlRedirect can be set.
     */
    origin?: string;
    /**
     * The priority of this route rule, where 1 is the highest priority.
     *
     * You cannot configure two or more routeRules with the same priority. Priority for each rule must be set to a number between 1 and 999 inclusive.
     *
     * Priority numbers can have gaps, which enable you to add or remove rules in the future without affecting the rest of the rules. For example, 1, 2, 3, 4, 5, 9, 12, 16 is a valid series of priority numbers
     * to which you could add rules numbered from 6 to 8, 10 to 11, and 13 to 15 in the future without any impact on existing rules.
     */
    priority: string;
    /**
     * In response to a matching path, the routeAction performs advanced routing actions like URL rewrites, header transformations, etc. prior to forwarding the request to the selected origin.
     */
    routeAction?: outputs.NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleRouteAction;
    /**
     * The URL redirect configuration for requests that match this route.
     */
    urlRedirect?: outputs.NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleUrlRedirect;
}

export interface NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleHeaderAction {
    /**
     * Describes a header to add.
     */
    requestHeaderToAdds?: outputs.NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleHeaderActionRequestHeaderToAdd[];
    /**
     * A list of header names for headers that need to be removed from the request prior to forwarding the request to the origin.
     */
    requestHeaderToRemoves?: outputs.NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleHeaderActionRequestHeaderToRemove[];
    /**
     * Headers to add to the response prior to sending it back to the client.
     *
     * Response headers are only sent to the client, and do not have an effect on the cache serving the response.
     */
    responseHeaderToAdds?: outputs.NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleHeaderActionResponseHeaderToAdd[];
    /**
     * A list of header names for headers that need to be removed from the request prior to forwarding the request to the origin.
     */
    responseHeaderToRemoves?: outputs.NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleHeaderActionResponseHeaderToRemove[];
}

export interface NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleHeaderActionRequestHeaderToAdd {
    /**
     * The name of the header to add.
     */
    headerName: string;
    /**
     * The value of the header to add.
     */
    headerValue: string;
    /**
     * Whether to replace all existing headers with the same name.
     */
    replace: boolean;
}

export interface NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleHeaderActionRequestHeaderToRemove {
    /**
     * The name of the header to remove.
     */
    headerName: string;
}

export interface NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleHeaderActionResponseHeaderToAdd {
    /**
     * The name of the header to add.
     */
    headerName: string;
    /**
     * The value of the header to add.
     */
    headerValue: string;
    /**
     * Whether to replace all existing headers with the same name.
     */
    replace: boolean;
}

export interface NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleHeaderActionResponseHeaderToRemove {
    /**
     * Headers to remove from the response prior to sending it back to the client.
     *
     * Response headers are only sent to the client, and do not have an effect on the cache serving the response.
     */
    headerName: string;
}

export interface NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleMatchRule {
    /**
     * For satisfying the matchRule condition, the path of the request must exactly match the value specified in fullPathMatch after removing any query parameters and anchor that may be part of the original URL.
     */
    fullPathMatch?: string;
    /**
     * Specifies a list of header match criteria, all of which must match corresponding headers in the request.
     */
    headerMatches?: outputs.NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleMatchRuleHeaderMatch[];
    /**
     * Specifies that prefixMatch and fullPathMatch matches are case sensitive.
     */
    ignoreCase: boolean;
    /**
     * For satisfying the matchRule condition, the path of the request
     * must match the wildcard pattern specified in pathTemplateMatch
     * after removing any query parameters and anchor that may be part
     * of the original URL.
     *
     * pathTemplateMatch must be between 1 and 255 characters
     * (inclusive).  The pattern specified by pathTemplateMatch may
     * have at most 5 wildcard operators and at most 5 variable
     * captures in total.
     */
    pathTemplateMatch?: string;
    /**
     * For satisfying the matchRule condition, the request's path must begin with the specified prefixMatch. prefixMatch must begin with a /.
     */
    prefixMatch?: string;
    /**
     * Specifies a list of query parameter match criteria, all of which must match corresponding query parameters in the request.
     */
    queryParameterMatches?: outputs.NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleMatchRuleQueryParameterMatch[];
}

export interface NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleMatchRuleHeaderMatch {
    /**
     * The value of the header should exactly match contents of exactMatch.
     */
    exactMatch?: string;
    /**
     * The header name to match on.
     */
    headerName: string;
    /**
     * If set to false (default), the headerMatch is considered a match if the match criteria above are met.
     * If set to true, the headerMatch is considered a match if the match criteria above are NOT met.
     */
    invertMatch: boolean;
    /**
     * The value of the header must start with the contents of prefixMatch.
     */
    prefixMatch?: string;
    /**
     * A header with the contents of headerName must exist. The match takes place whether or not the request's header has a value.
     */
    presentMatch?: boolean;
    /**
     * The value of the header must end with the contents of suffixMatch.
     */
    suffixMatch?: string;
}

export interface NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleMatchRuleQueryParameterMatch {
    /**
     * The queryParameterMatch matches if the value of the parameter exactly matches the contents of exactMatch.
     */
    exactMatch?: string;
    /**
     * The name of the query parameter to match. The query parameter must exist in the request, in the absence of which the request match fails.
     */
    name: string;
    /**
     * Specifies that the queryParameterMatch matches if the request contains the query parameter, irrespective of whether the parameter has a value or not.
     */
    presentMatch?: boolean;
}

export interface NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleRouteAction {
    /**
     * The policy to use for defining caching and signed request behaviour for requests that match this route.
     */
    cdnPolicy?: outputs.NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleRouteActionCdnPolicy;
    /**
     * CORSPolicy defines Cross-Origin-Resource-Sharing configuration, including which CORS response headers will be set.
     */
    corsPolicy?: outputs.NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleRouteActionCorsPolicy;
    /**
     * The URL rewrite configuration for requests that match this route.
     */
    urlRewrite?: outputs.NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleRouteActionUrlRewrite;
}

export interface NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleRouteActionCdnPolicy {
    /**
     * Enable signature generation or propagation on this route.
     *
     * This field may only be specified when signedRequestMode is set to REQUIRE_TOKENS.
     */
    addSignatures?: outputs.NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleRouteActionCdnPolicyAddSignatures;
    /**
     * Defines the request parameters that contribute to the cache key.
     */
    cacheKeyPolicy?: outputs.NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleRouteActionCdnPolicyCacheKeyPolicy;
    /**
     * Cache modes allow users to control the behaviour of the cache, what content it should cache automatically, whether to respect origin headers, or whether to unconditionally cache all responses.
     *
     * For all cache modes, Cache-Control headers will be passed to the client. Use clientTtl to override what is sent to the client. Possible values: ["CACHE_ALL_STATIC", "USE_ORIGIN_HEADERS", "FORCE_CACHE_ALL", "BYPASS_CACHE"]
     */
    cacheMode: string;
    /**
     * Specifies a separate client (e.g. browser client) TTL, separate from the TTL used by the edge caches. Leaving this empty will use the same cache TTL for both the CDN and the client-facing response.
     *
     * - The TTL must be > 0 and <= 86400s (1 day)
     * - The clientTtl cannot be larger than the defaultTtl (if set)
     * - Fractions of a second are not allowed.
     *
     * Omit this field to use the defaultTtl, or the max-age set by the origin, as the client-facing TTL.
     *
     * When the cache mode is set to "USE_ORIGIN_HEADERS" or "BYPASS_CACHE", you must omit this field.
     * A duration in seconds terminated by 's'. Example: "3s".
     */
    clientTtl?: string;
    /**
     * Specifies the default TTL for cached content served by this origin for responses that do not have an existing valid TTL (max-age or s-max-age).
     *
     * Defaults to 3600s (1 hour).
     *
     * - The TTL must be >= 0 and <= 31,536,000 seconds (1 year)
     * - Setting a TTL of "0" means "always revalidate" (equivalent to must-revalidate)
     * - The value of defaultTTL cannot be set to a value greater than that of maxTTL.
     * - Fractions of a second are not allowed.
     * - When the cacheMode is set to FORCE_CACHE_ALL, the defaultTTL will overwrite the TTL set in all responses.
     *
     * Note that infrequently accessed objects may be evicted from the cache before the defined TTL. Objects that expire will be revalidated with the origin.
     *
     * When the cache mode is set to "USE_ORIGIN_HEADERS" or "BYPASS_CACHE", you must omit this field.
     *
     * A duration in seconds terminated by 's'. Example: "3s".
     */
    defaultTtl: string;
    /**
     * Specifies the maximum allowed TTL for cached content served by this origin.
     *
     * Defaults to 86400s (1 day).
     *
     * Cache directives that attempt to set a max-age or s-maxage higher than this, or an Expires header more than maxTtl seconds in the future will be capped at the value of maxTTL, as if it were the value of an s-maxage Cache-Control directive.
     *
     * - The TTL must be >= 0 and <= 31,536,000 seconds (1 year)
     * - Setting a TTL of "0" means "always revalidate"
     * - The value of maxTtl must be equal to or greater than defaultTtl.
     * - Fractions of a second are not allowed.
     *
     * When the cache mode is set to "USE_ORIGIN_HEADERS", "FORCE_CACHE_ALL", or "BYPASS_CACHE", you must omit this field.
     *
     * A duration in seconds terminated by 's'. Example: "3s".
     */
    maxTtl: string;
    /**
     * Negative caching allows per-status code TTLs to be set, in order to apply fine-grained caching for common errors or redirects. This can reduce the load on your origin and improve end-user experience by reducing response latency.
     *
     * By default, the CDNPolicy will apply the following default TTLs to these status codes:
     *
     * - HTTP 300 (Multiple Choice), 301, 308 (Permanent Redirects): 10m
     * - HTTP 404 (Not Found), 410 (Gone), 451 (Unavailable For Legal Reasons): 120s
     * - HTTP 405 (Method Not Found), 414 (URI Too Long), 501 (Not Implemented): 60s
     *
     * These defaults can be overridden in negativeCachingPolicy
     */
    negativeCaching?: boolean;
    /**
     * Sets a cache TTL for the specified HTTP status code. negativeCaching must be enabled to configure negativeCachingPolicy.
     *
     * - Omitting the policy and leaving negativeCaching enabled will use the default TTLs for each status code, defined in negativeCaching.
     * - TTLs must be >= 0 (where 0 is "always revalidate") and <= 86400s (1 day)
     *
     * Note that when specifying an explicit negativeCachingPolicy, you should take care to specify a cache TTL for all response codes that you wish to cache. The CDNPolicy will not apply any default negative caching when a policy exists.
     */
    negativeCachingPolicy?: {[key: string]: string};
    /**
     * The EdgeCacheKeyset containing the set of public keys used to validate signed requests at the edge.
     */
    signedRequestKeyset: string;
    /**
     * Limit how far into the future the expiration time of a signed request may be.
     *
     * When set, a signed request is rejected if its expiration time is later than now + signedRequestMaximumExpirationTtl, where now is the time at which the signed request is first handled by the CDN.
     *
     * - The TTL must be > 0.
     * - Fractions of a second are not allowed.
     *
     * By default, signedRequestMaximumExpirationTtl is not set and the expiration time of a signed request may be arbitrarily far into future.
     */
    signedRequestMaximumExpirationTtl?: string;
    /**
     * Whether to enforce signed requests. The default value is DISABLED, which means all content is public, and does not authorize access.
     *
     * You must also set a signedRequestKeyset to enable signed requests.
     *
     * When set to REQUIRE_SIGNATURES, all matching requests will have their signature validated. Requests that were not signed with the corresponding private key, or that are otherwise invalid (expired, do not match the signature, IP address, or header) will be rejected with a HTTP 403 and (if enabled) logged. Possible values: ["DISABLED", "REQUIRE_SIGNATURES", "REQUIRE_TOKENS"]
     */
    signedRequestMode: string;
    /**
     * Additional options for signed tokens.
     *
     * signedTokenOptions may only be specified when signedRequestMode is REQUIRE_TOKENS.
     */
    signedTokenOptions?: outputs.NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleRouteActionCdnPolicySignedTokenOptions;
}

export interface NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleRouteActionCdnPolicyAddSignatures {
    /**
     * The actions to take to add signatures to responses. Possible values: ["GENERATE_COOKIE", "GENERATE_TOKEN_HLS_COOKIELESS", "PROPAGATE_TOKEN_HLS_COOKIELESS"]
     */
    actions: string[];
    /**
     * The parameters to copy from the verified token to the generated token.
     *
     * Only the following parameters may be copied:
     *
     *   * 'PathGlobs'
     *   * 'paths'
     *   * 'acl'
     *   * 'URLPrefix'
     *   * 'IPRanges'
     *   * 'SessionID'
     *   * 'id'
     *   * 'Data'
     *   * 'data'
     *   * 'payload'
     *   * 'Headers'
     *
     * You may specify up to 6 parameters to copy.  A given parameter is be copied only if the parameter exists in the verified token.  Parameter names are matched exactly as specified.  The order of the parameters does not matter.  Duplicates are not allowed.
     *
     * This field may only be specified when the GENERATE_COOKIE or GENERATE_TOKEN_HLS_COOKIELESS actions are specified.
     */
    copiedParameters?: string[];
    /**
     * The keyset to use for signature generation.
     *
     * The following are both valid paths to an EdgeCacheKeyset resource:
     *
     *   * 'projects/project/locations/global/edgeCacheKeysets/yourKeyset'
     *   * 'yourKeyset'
     *
     * This must be specified when the GENERATE_COOKIE or GENERATE_TOKEN_HLS_COOKIELESS actions are specified.  This field may not be specified otherwise.
     */
    keyset?: string;
    /**
     * The query parameter in which to put the generated token.
     *
     * If not specified, defaults to 'edge-cache-token'.
     *
     * If specified, the name must be 1-64 characters long and match the regular expression 'a-zA-Z*' which means the first character must be a letter, and all following characters must be a dash, underscore, letter or digit.
     *
     * This field may only be set when the GENERATE_TOKEN_HLS_COOKIELESS or PROPAGATE_TOKEN_HLS_COOKIELESS actions are specified.
     */
    tokenQueryParameter?: string;
    /**
     * The duration the token is valid starting from the moment the token is first generated.
     *
     * Defaults to '86400s' (1 day).
     *
     * The TTL must be >= 0 and <= 604,800 seconds (1 week).
     *
     * This field may only be specified when the GENERATE_COOKIE or GENERATE_TOKEN_HLS_COOKIELESS actions are specified.
     *
     * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
     */
    tokenTtl?: string;
}

export interface NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleRouteActionCdnPolicyCacheKeyPolicy {
    /**
     * If true, requests to different hosts will be cached separately.
     *
     * Note: this should only be enabled if hosts share the same origin and content. Removing the host from the cache key may inadvertently result in different objects being cached than intended, depending on which route the first user matched.
     */
    excludeHost: boolean;
    /**
     * If true, exclude query string parameters from the cache key
     *
     * If false (the default), include the query string parameters in
     * the cache key according to includeQueryParameters and
     * excludeQueryParameters. If neither includeQueryParameters nor
     * excludeQueryParameters is set, the entire query string will be
     * included.
     */
    excludeQueryString?: boolean;
    /**
     * Names of query string parameters to exclude from cache keys. All other parameters will be included.
     *
     * Either specify includedQueryParameters or excludedQueryParameters, not both. '&' and '=' will be percent encoded and not treated as delimiters.
     */
    excludedQueryParameters?: string[];
    /**
     * If true, http and https requests will be cached separately.
     */
    includeProtocol: boolean;
    /**
     * Names of Cookies to include in cache keys.  The cookie name and cookie value of each cookie named will be used as part of the cache key.
     *
     * Cookie names:
     *   - must be valid RFC 6265 "cookie-name" tokens
     *   - are case sensitive
     *   - cannot start with "Edge-Cache-" (case insensitive)
     *
     *   Note that specifying several cookies, and/or cookies that have a large range of values (e.g., per-user) will dramatically impact the cache hit rate, and may result in a higher eviction rate and reduced performance.
     *
     *   You may specify up to three cookie names.
     */
    includedCookieNames?: string[];
    /**
     * Names of HTTP request headers to include in cache keys. The value of the header field will be used as part of the cache key.
     *
     * - Header names must be valid HTTP RFC 7230 header field values.
     * - Header field names are case insensitive
     * - To include the HTTP method, use ":method"
     *
     * Note that specifying several headers, and/or headers that have a large range of values (e.g. per-user) will dramatically impact the cache hit rate, and may result in a higher eviction rate and reduced performance.
     */
    includedHeaderNames?: string[];
    /**
     * Names of query string parameters to include in cache keys. All other parameters will be excluded.
     *
     * Either specify includedQueryParameters or excludedQueryParameters, not both. '&' and '=' will be percent encoded and not treated as delimiters.
     */
    includedQueryParameters?: string[];
}

export interface NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleRouteActionCdnPolicySignedTokenOptions {
    /**
     * The allowed signature algorithms to use.
     *
     * Defaults to using only ED25519.
     *
     * You may specify up to 3 signature algorithms to use. Possible values: ["ED25519", "HMAC_SHA_256", "HMAC_SHA1"]
     */
    allowedSignatureAlgorithms?: string[];
    /**
     * The query parameter in which to find the token.
     *
     * The name must be 1-64 characters long and match the regular expression 'a-zA-Z*' which means the first character must be a letter, and all following characters must be a dash, underscore, letter or digit.
     *
     * Defaults to 'edge-cache-token'.
     */
    tokenQueryParameter?: string;
}

export interface NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleRouteActionCorsPolicy {
    /**
     * In response to a preflight request, setting this to true indicates that the actual request can include user credentials.
     *
     * This translates to the Access-Control-Allow-Credentials response header.
     */
    allowCredentials?: boolean;
    /**
     * Specifies the content for the Access-Control-Allow-Headers response header.
     */
    allowHeaders?: string[];
    /**
     * Specifies the content for the Access-Control-Allow-Methods response header.
     */
    allowMethods?: string[];
    /**
     * Specifies the list of origins that will be allowed to do CORS requests.
     *
     * This translates to the Access-Control-Allow-Origin response header.
     */
    allowOrigins?: string[];
    /**
     * If true, specifies the CORS policy is disabled. The default value is false, which indicates that the CORS policy is in effect.
     */
    disabled?: boolean;
    /**
     * Specifies the content for the Access-Control-Allow-Headers response header.
     */
    exposeHeaders?: string[];
    /**
     * Specifies how long results of a preflight request can be cached by a client in seconds. Note that many browser clients enforce a maximum TTL of 600s (10 minutes).
     *
     * - Setting the value to -1 forces a pre-flight check for all requests (not recommended)
     * - A maximum TTL of 86400s can be set, but note that (as above) some clients may force pre-flight checks at a more regular interval.
     * - This translates to the Access-Control-Max-Age header.
     *
     * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
     */
    maxAge: string;
}

export interface NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleRouteActionUrlRewrite {
    /**
     * Prior to forwarding the request to the selected origin, the request's host header is replaced with contents of hostRewrite.
     */
    hostRewrite?: string;
    /**
     * Prior to forwarding the request to the selected origin, the matching portion of the request's path is replaced by pathPrefixRewrite.
     */
    pathPrefixRewrite?: string;
    /**
     * Prior to forwarding the request to the selected origin, if the
     * request matched a pathTemplateMatch, the matching portion of the
     * request's path is replaced re-written using the pattern specified
     * by pathTemplateRewrite.
     *
     * pathTemplateRewrite must be between 1 and 255 characters
     * (inclusive), must start with a '/', and must only use variables
     * captured by the route's pathTemplate matchers.
     *
     * pathTemplateRewrite may only be used when all of a route's
     * MatchRules specify pathTemplate.
     *
     * Only one of pathPrefixRewrite and pathTemplateRewrite may be
     * specified.
     */
    pathTemplateRewrite?: string;
}

export interface NetworkServicesEdgeCacheServiceRoutingPathMatcherRouteRuleUrlRedirect {
    /**
     * The host that will be used in the redirect response instead of the one that was supplied in the request.
     */
    hostRedirect?: string;
    /**
     * If set to true, the URL scheme in the redirected request is set to https. If set to false, the URL scheme of the redirected request will remain the same as that of the request.
     *
     * This can only be set if there is at least one (1) edgeSslCertificate set on the service.
     */
    httpsRedirect: boolean;
    /**
     * The path that will be used in the redirect response instead of the one that was supplied in the request.
     *
     * pathRedirect cannot be supplied together with prefixRedirect. Supply one alone or neither. If neither is supplied, the path of the original request will be used for the redirect.
     *
     * The path value must be between 1 and 1024 characters.
     */
    pathRedirect?: string;
    /**
     * The prefix that replaces the prefixMatch specified in the routeRule, retaining the remaining portion of the URL before redirecting the request.
     *
     * prefixRedirect cannot be supplied together with pathRedirect. Supply one alone or neither. If neither is supplied, the path of the original request will be used for the redirect.
     */
    prefixRedirect?: string;
    /**
     * The HTTP Status code to use for this RedirectAction.
     *
     * The supported values are:
     *
     * - 'MOVED_PERMANENTLY_DEFAULT', which is the default value and corresponds to 301.
     * - 'FOUND', which corresponds to 302.
     * - 'SEE_OTHER' which corresponds to 303.
     * - 'TEMPORARY_REDIRECT', which corresponds to 307. in this case, the request method will be retained.
     * - 'PERMANENT_REDIRECT', which corresponds to 308. in this case, the request method will be retained. Possible values: ["MOVED_PERMANENTLY_DEFAULT", "FOUND", "SEE_OTHER", "TEMPORARY_REDIRECT", "PERMANENT_REDIRECT"]
     */
    redirectResponseCode: string;
    /**
     * If set to true, any accompanying query portion of the original URL is removed prior to redirecting the request. If set to false, the query portion of the original URL is retained.
     */
    stripQuery: boolean;
}

export interface NetworkServicesEdgeCacheServiceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetworkServicesGatewayTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetworkServicesLbRouteExtensionExtensionChain {
    /**
     * A set of extensions to execute for the matching request.
     * At least one extension is required. Up to 3 extensions can be defined for each extension chain for
     * LbTrafficExtension resource. LbRouteExtension chains are limited to 1 extension per extension chain.
     */
    extensions: outputs.NetworkServicesLbRouteExtensionExtensionChainExtension[];
    /**
     * Conditions under which this chain is invoked for a request.
     */
    matchCondition: outputs.NetworkServicesLbRouteExtensionExtensionChainMatchCondition;
    /**
     * The name for this extension chain. The name is logged as part of the HTTP request logs.
     * The name must conform with RFC-1034, is restricted to lower-cased letters, numbers and hyphens,
     * and can have a maximum length of 63 characters. Additionally, the first character must be a letter
     * and the last character must be a letter or a number.
     */
    name: string;
}

export interface NetworkServicesLbRouteExtensionExtensionChainExtension {
    /**
     * The :authority header in the gRPC request sent from Envoy to the extension service.
     */
    authority?: string;
    /**
     * Determines how the proxy behaves if the call to the extension fails or times out.
     * When set to TRUE, request or response processing continues without error.
     * Any subsequent extensions in the extension chain are also executed.
     * When set to FALSE: * If response headers have not been delivered to the downstream client,
     * a generic 500 error is returned to the client. The error response can be tailored by
     * configuring a custom error response in the load balancer.
     */
    failOpen?: boolean;
    /**
     * List of the HTTP headers to forward to the extension (from the client or backend).
     * If omitted, all headers are sent. Each element is a string indicating the header name.
     */
    forwardHeaders?: string[];
    /**
     * The name for this extension. The name is logged as part of the HTTP request logs.
     * The name must conform with RFC-1034, is restricted to lower-cased letters, numbers and hyphens,
     * and can have a maximum length of 63 characters. Additionally, the first character must be a letter
     * and the last a letter or a number.
     */
    name: string;
    /**
     * The reference to the service that runs the extension. Must be a reference to a backend service
     */
    service: string;
    /**
     * Specifies the timeout for each individual message on the stream. The timeout must be between 10-1000 milliseconds.
     * A duration in seconds with up to nine fractional digits, ending with 's'. Example: "3.5s".
     */
    timeout?: string;
}

export interface NetworkServicesLbRouteExtensionExtensionChainMatchCondition {
    /**
     * A Common Expression Language (CEL) expression that is used to match requests for which the extension chain is executed.
     */
    celExpression: string;
}

export interface NetworkServicesLbRouteExtensionTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NetworkServicesLbTrafficExtensionExtensionChain {
    /**
     * A set of extensions to execute for the matching request.
     * At least one extension is required. Up to 3 extensions can be defined for each extension chain for
     * LbTrafficExtension resource. LbRouteExtension chains are limited to 1 extension per extension chain.
     */
    extensions: outputs.NetworkServicesLbTrafficExtensionExtensionChainExtension[];
    /**
     * Conditions under which this chain is invoked for a request.
     */
    matchCondition: outputs.NetworkServicesLbTrafficExtensionExtensionChainMatchCondition;
    /**
     * The name for this extension chain. The name is logged as part of the HTTP request logs.
     * The name must conform with RFC-1034, is restricted to lower-cased letters, numbers and hyphens,
     * and can have a maximum length of 63 characters. Additionally, the first character must be a letter
     * and the last a letter or a number.
     */
    name: string;
}

export interface NetworkServicesLbTrafficExtensionExtensionChainExtension {
    /**
     * The :authority header in the gRPC request sent from Envoy to the extension service.
     */
    authority?: string;
    /**
     * Determines how the proxy behaves if the call to the extension fails or times out.
     * When set to TRUE, request or response processing continues without error.
     * Any subsequent extensions in the extension chain are also executed.
     * When set to FALSE: * If response headers have not been delivered to the downstream client,
     * a generic 500 error is returned to the client. The error response can be tailored by
     * configuring a custom error response in the load balancer.
     */
    failOpen?: boolean;
    /**
     * List of the HTTP headers to forward to the extension (from the client or backend).
     * If omitted, all headers are sent. Each element is a string indicating the header name.
     */
    forwardHeaders?: string[];
    /**
     * The name for this extension. The name is logged as part of the HTTP request logs.
     * The name must conform with RFC-1034, is restricted to lower-cased letters, numbers and hyphens,
     * and can have a maximum length of 63 characters. Additionally, the first character must be a letter
     * and the last a letter or a number.
     */
    name: string;
    /**
     * The reference to the service that runs the extension. Must be a reference to a backend service
     */
    service: string;
    /**
     * A set of events during request or response processing for which this extension is called.
     * This field is required for the LbTrafficExtension resource. It's not relevant for the LbRouteExtension
     * resource. Possible values:'EVENT_TYPE_UNSPECIFIED', 'REQUEST_HEADERS', 'REQUEST_BODY', 'RESPONSE_HEADERS',
     * 'RESPONSE_BODY', 'RESPONSE_BODY' and 'RESPONSE_BODY'.
     */
    supportedEvents?: string[];
    /**
     * Specifies the timeout for each individual message on the stream. The timeout must be between 10-1000 milliseconds.
     * A duration in seconds with up to nine fractional digits, ending with 's'. Example: "3.5s".
     */
    timeout?: string;
}

export interface NetworkServicesLbTrafficExtensionExtensionChainMatchCondition {
    /**
     * A Common Expression Language (CEL) expression that is used to match requests for which the extension chain is executed.
     */
    celExpression: string;
}

export interface NetworkServicesLbTrafficExtensionTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NotebooksEnvironmentContainerImage {
    /**
     * The path to the container image repository.
     * For example: gcr.io/{project_id}/{imageName}
     */
    repository: string;
    /**
     * The tag of the container image. If not specified, this defaults to the latest tag.
     */
    tag?: string;
}

export interface NotebooksEnvironmentTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NotebooksEnvironmentVmImage {
    /**
     * Use this VM image family to find the image; the newest image in this family will be used.
     */
    imageFamily?: string;
    /**
     * Use VM image name to find the image.
     */
    imageName?: string;
    /**
     * The name of the Google Cloud project that this VM image belongs to.
     * Format: projects/{project_id}
     */
    project: string;
}

export interface NotebooksInstanceAcceleratorConfig {
    /**
     * Count of cores of this accelerator.
     */
    coreCount: number;
    /**
     * Type of this accelerator. Possible values: ["ACCELERATOR_TYPE_UNSPECIFIED", "NVIDIA_TESLA_K80", "NVIDIA_TESLA_P100", "NVIDIA_TESLA_V100", "NVIDIA_TESLA_P4", "NVIDIA_TESLA_T4", "NVIDIA_TESLA_T4_VWS", "NVIDIA_TESLA_P100_VWS", "NVIDIA_TESLA_P4_VWS", "NVIDIA_TESLA_A100", "TPU_V2", "TPU_V3"]
     */
    type: string;
}

export interface NotebooksInstanceContainerImage {
    /**
     * The path to the container image repository.
     * For example: gcr.io/{project_id}/{imageName}
     */
    repository: string;
    /**
     * The tag of the container image. If not specified, this defaults to the latest tag.
     */
    tag?: string;
}

export interface NotebooksInstanceIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface NotebooksInstanceIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface NotebooksInstanceReservationAffinity {
    /**
     * The type of Compute Reservation. Possible values: ["NO_RESERVATION", "ANY_RESERVATION", "SPECIFIC_RESERVATION"]
     */
    consumeReservationType: string;
    /**
     * Corresponds to the label key of reservation resource.
     */
    key?: string;
    /**
     * Corresponds to the label values of reservation resource.
     */
    values?: string[];
}

export interface NotebooksInstanceShieldedInstanceConfig {
    /**
     * Defines whether the instance has integrity monitoring enabled. Enables monitoring and attestation of the
     * boot integrity of the instance. The attestation is performed against the integrity policy baseline.
     * This baseline is initially derived from the implicitly trusted boot image when the instance is created.
     * Enabled by default.
     */
    enableIntegrityMonitoring?: boolean;
    /**
     * Defines whether the instance has Secure Boot enabled. Secure Boot helps ensure that the system only runs
     * authentic software by verifying the digital signature of all boot components, and halting the boot process
     * if signature verification fails.
     * Disabled by default.
     */
    enableSecureBoot?: boolean;
    /**
     * Defines whether the instance has the vTPM enabled.
     * Enabled by default.
     */
    enableVtpm?: boolean;
}

export interface NotebooksInstanceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NotebooksInstanceVmImage {
    /**
     * Use this VM image family to find the image; the newest image in this family will be used.
     */
    imageFamily?: string;
    /**
     * Use VM image name to find the image.
     */
    imageName?: string;
    /**
     * The name of the Google Cloud project that this VM image belongs to.
     * Format: projects/{project_id}
     */
    project: string;
}

export interface NotebooksLocationTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NotebooksRuntimeAccessConfig {
    /**
     * The type of access mode this instance. For valid values, see
     * 'https://cloud.google.com/vertex-ai/docs/workbench/reference/
     * rest/v1/projects.locations.runtimes#RuntimeAccessType'.
     */
    accessType?: string;
    /**
     * The proxy endpoint that is used to access the runtime.
     */
    proxyUri: string;
    /**
     * The owner of this runtime after creation. Format: 'alias@example.com'.
     * Currently supports one owner only.
     */
    runtimeOwner?: string;
}

export interface NotebooksRuntimeIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface NotebooksRuntimeIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface NotebooksRuntimeMetric {
    systemMetrics: {[key: string]: string};
}

export interface NotebooksRuntimeSoftwareConfig {
    /**
     * Specify a custom Cloud Storage path where the GPU driver is stored.
     * If not specified, we'll automatically choose from official GPU drivers.
     */
    customGpuDriverPath?: string;
    /**
     * Verifies core internal services are running. Default: True.
     */
    enableHealthMonitoring?: boolean;
    /**
     * Runtime will automatically shutdown after idle_shutdown_time.
     * Default: True
     */
    idleShutdown?: boolean;
    /**
     * Time in minutes to wait before shuting down runtime.
     * Default: 180 minutes
     */
    idleShutdownTimeout?: number;
    /**
     * Install Nvidia Driver automatically.
     */
    installGpuDriver?: boolean;
    /**
     * Use a list of container images to use as Kernels in the notebook instance.
     */
    kernels?: outputs.NotebooksRuntimeSoftwareConfigKernel[];
    /**
     * Cron expression in UTC timezone for schedule instance auto upgrade.
     * Please follow the [cron format](https://en.wikipedia.org/wiki/Cron).
     */
    notebookUpgradeSchedule?: string;
    /**
     * Path to a Bash script that automatically runs after a notebook instance
     * fully boots up. The path must be a URL or
     * Cloud Storage path (gs://path-to-file/file-name).
     */
    postStartupScript?: string;
    /**
     * Behavior for the post startup script. Possible values: ["POST_STARTUP_SCRIPT_BEHAVIOR_UNSPECIFIED", "RUN_EVERY_START", "DOWNLOAD_AND_RUN_EVERY_START"]
     */
    postStartupScriptBehavior?: string;
    /**
     * Bool indicating whether an newer image is available in an image family.
     */
    upgradeable: boolean;
}

export interface NotebooksRuntimeSoftwareConfigKernel {
    /**
     * The path to the container image repository.
     * For example: gcr.io/{project_id}/{imageName}
     */
    repository: string;
    /**
     * The tag of the container image. If not specified, this defaults to the latest tag.
     */
    tag?: string;
}

export interface NotebooksRuntimeTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface NotebooksRuntimeVirtualMachine {
    /**
     * The unique identifier of the Managed Compute Engine instance.
     */
    instanceId: string;
    /**
     * The user-friendly name of the Managed Compute Engine instance.
     */
    instanceName: string;
    /**
     * Virtual Machine configuration settings.
     */
    virtualMachineConfig?: outputs.NotebooksRuntimeVirtualMachineVirtualMachineConfig;
}

export interface NotebooksRuntimeVirtualMachineVirtualMachineConfig {
    /**
     * The Compute Engine accelerator configuration for this runtime.
     */
    acceleratorConfig?: outputs.NotebooksRuntimeVirtualMachineVirtualMachineConfigAcceleratorConfig;
    /**
     * Use a list of container images to start the notebook instance.
     */
    containerImages?: outputs.NotebooksRuntimeVirtualMachineVirtualMachineConfigContainerImage[];
    /**
     * Data disk option configuration settings.
     */
    dataDisk: outputs.NotebooksRuntimeVirtualMachineVirtualMachineConfigDataDisk;
    /**
     * Encryption settings for virtual machine data disk.
     */
    encryptionConfig?: outputs.NotebooksRuntimeVirtualMachineVirtualMachineConfigEncryptionConfig;
    /**
     * The Compute Engine guest attributes. (see [Project and instance
     * guest attributes](https://cloud.google.com/compute/docs/
     * storing-retrieving-metadata#guest_attributes)).
     */
    guestAttributes: {[key: string]: string};
    /**
     * If true, runtime will only have internal IP addresses. By default,
     * runtimes are not restricted to internal IP addresses, and will
     * have ephemeral external IP addresses assigned to each vm. This
     * 'internal_ip_only' restriction can only be enabled for subnetwork
     * enabled networks, and all dependencies must be configured to be
     * accessible without external IP addresses.
     */
    internalIpOnly?: boolean;
    /**
     * The labels to associate with this runtime. Label **keys** must
     * contain 1 to 63 characters, and must conform to [RFC 1035]
     * (https://www.ietf.org/rfc/rfc1035.txt). Label **values** may be
     * empty, but, if present, must contain 1 to 63 characters, and must
     * conform to [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt). No
     * more than 32 labels can be associated with a cluster.
     */
    labels: {[key: string]: string};
    /**
     * The Compute Engine machine type used for runtimes.
     */
    machineType: string;
    /**
     * The Compute Engine metadata entries to add to virtual machine.
     * (see [Project and instance metadata](https://cloud.google.com
     * /compute/docs/storing-retrieving-metadata#project_and_instance
     * _metadata)).
     */
    metadata: {[key: string]: string};
    /**
     * The Compute Engine network to be used for machine communications.
     * Cannot be specified with subnetwork. If neither 'network' nor
     * 'subnet' is specified, the "default" network of the project is
     * used, if it exists. A full URL or partial URI. Examples:
     *   * 'https://www.googleapis.com/compute/v1/projects/[project_id]/
     *   regions/global/default'
     *   * 'projects/[project_id]/regions/global/default'
     * Runtimes are managed resources inside Google Infrastructure.
     * Runtimes support the following network configurations:
     *   * Google Managed Network (Network & subnet are empty)
     *   * Consumer Project VPC (network & subnet are required). Requires
     *   configuring Private Service Access.
     *   * Shared VPC (network & subnet are required). Requires
     *   configuring Private Service Access.
     */
    network?: string;
    /**
     * The type of vNIC to be used on this interface. This may be gVNIC
     * or VirtioNet. Possible values: ["UNSPECIFIED_NIC_TYPE", "VIRTIO_NET", "GVNIC"]
     */
    nicType?: string;
    /**
     * Reserved IP Range name is used for VPC Peering. The
     * subnetwork allocation will use the range *name* if it's assigned.
     */
    reservedIpRange?: string;
    /**
     * Shielded VM Instance configuration settings.
     */
    shieldedInstanceConfig?: outputs.NotebooksRuntimeVirtualMachineVirtualMachineConfigShieldedInstanceConfig;
    /**
     * The Compute Engine subnetwork to be used for machine
     * communications. Cannot be specified with network. A full URL or
     * partial URI are valid. Examples:
     *   * 'https://www.googleapis.com/compute/v1/projects/[project_id]/
     *   regions/us-east1/subnetworks/sub0'
     *   * 'projects/[project_id]/regions/us-east1/subnetworks/sub0'
     */
    subnet?: string;
    /**
     * The Compute Engine tags to add to runtime (see [Tagging instances]
     * (https://cloud.google.com/compute/docs/
     * label-or-tag-resources#tags)).
     */
    tags: string[];
    /**
     * The zone where the virtual machine is located.
     */
    zone: string;
}

export interface NotebooksRuntimeVirtualMachineVirtualMachineConfigAcceleratorConfig {
    /**
     * Count of cores of this accelerator.
     */
    coreCount?: number;
    /**
     * Accelerator model. For valid values, see
     * 'https://cloud.google.com/vertex-ai/docs/workbench/reference/
     * rest/v1/projects.locations.runtimes#AcceleratorType'
     */
    type?: string;
}

export interface NotebooksRuntimeVirtualMachineVirtualMachineConfigContainerImage {
    /**
     * The path to the container image repository.
     * For example: gcr.io/{project_id}/{imageName}
     */
    repository: string;
    /**
     * The tag of the container image. If not specified, this defaults to the latest tag.
     */
    tag?: string;
}

export interface NotebooksRuntimeVirtualMachineVirtualMachineConfigDataDisk {
    /**
     * Optional. Specifies whether the disk will be auto-deleted
     * when the instance is deleted (but not when the disk is
     * detached from the instance).
     */
    autoDelete: boolean;
    /**
     * Optional. Indicates that this is a boot disk. The virtual
     * machine will use the first partition of the disk for its
     * root filesystem.
     */
    boot: boolean;
    /**
     * Optional. Specifies a unique device name of your choice
     * that is reflected into the /dev/disk/by-id/google-* tree
     * of a Linux operating system running within the instance.
     * This name can be used to reference the device for mounting,
     * resizing, and so on, from within the instance.
     * If not specified, the server chooses a default device name
     * to apply to this disk, in the form persistent-disk-x, where
     * x is a number assigned by Google Compute Engine. This field
     * is only applicable for persistent disks.
     */
    deviceName: string;
    /**
     * Indicates a list of features to enable on the guest operating
     * system. Applicable only for bootable images. To see a list of
     * available features, read 'https://cloud.google.com/compute/docs/
     * images/create-delete-deprecate-private-images#guest-os-features'
     * options. ''
     */
    guestOsFeatures: string[];
    /**
     * Output only. A zero-based index to this disk, where 0 is
     * reserved for the boot disk. If you have many disks attached
     * to an instance, each disk would have a unique index number.
     */
    index: number;
    /**
     * Input only. Specifies the parameters for a new disk that will
     * be created alongside the new instance. Use initialization
     * parameters to create boot disks or local SSDs attached to the
     * new instance. This property is mutually exclusive with the
     * source property; you can only define one or the other, but not
     * both.
     */
    initializeParams?: outputs.NotebooksRuntimeVirtualMachineVirtualMachineConfigDataDiskInitializeParams;
    /**
     * "Specifies the disk interface to use for attaching this disk,
     * which is either SCSI or NVME. The default is SCSI. Persistent
     * disks must always use SCSI and the request will fail if you attempt
     * to attach a persistent disk in any other format than SCSI. Local SSDs
     * can use either NVME or SCSI. For performance characteristics of SCSI
     * over NVMe, see Local SSD performance. Valid values: * NVME * SCSI".
     */
    interface?: string;
    /**
     * Type of the resource. Always compute#attachedDisk for attached
     * disks.
     */
    kind: string;
    /**
     * Output only. Any valid publicly visible licenses.
     */
    licenses: string[];
    /**
     * The mode in which to attach this disk, either READ_WRITE
     * or READ_ONLY. If not specified, the default is to attach
     * the disk in READ_WRITE mode.
     */
    mode?: string;
    /**
     * Specifies a valid partial or full URL to an existing
     * Persistent Disk resource.
     */
    source?: string;
    /**
     * Specifies the type of the disk, either SCRATCH or PERSISTENT.
     * If not specified, the default is PERSISTENT.
     */
    type?: string;
}

export interface NotebooksRuntimeVirtualMachineVirtualMachineConfigDataDiskInitializeParams {
    /**
     * Provide this property when creating the disk.
     */
    description?: string;
    /**
     * Specifies the disk name. If not specified, the default is
     * to use the name of the instance. If the disk with the
     * instance name exists already in the given zone/region, a
     * new name will be automatically generated.
     */
    diskName?: string;
    /**
     * Specifies the size of the disk in base-2 GB. If not
     * specified, the disk will be the same size as the image
     * (usually 10GB). If specified, the size must be equal to
     * or larger than 10GB. Default 100 GB.
     */
    diskSizeGb?: number;
    /**
     * The type of the boot disk attached to this runtime,
     * defaults to standard persistent disk. For valid values,
     * see 'https://cloud.google.com/vertex-ai/docs/workbench/
     * reference/rest/v1/projects.locations.runtimes#disktype'
     */
    diskType?: string;
    /**
     * Labels to apply to this disk. These can be later modified
     * by the disks.setLabels method. This field is only
     * applicable for persistent disks.
     */
    labels: {[key: string]: string};
}

export interface NotebooksRuntimeVirtualMachineVirtualMachineConfigEncryptionConfig {
    /**
     * The Cloud KMS resource identifier of the customer-managed
     * encryption key used to protect a resource, such as a disks.
     * It has the following format:
     * 'projects/{PROJECT_ID}/locations/{REGION}/keyRings/
     * {KEY_RING_NAME}/cryptoKeys/{KEY_NAME}'
     */
    kmsKey?: string;
}

export interface NotebooksRuntimeVirtualMachineVirtualMachineConfigShieldedInstanceConfig {
    /**
     * Defines whether the instance has integrity monitoring enabled.
     * Enables monitoring and attestation of the boot integrity of
     * the instance. The attestation is performed against the
     * integrity policy baseline. This baseline is initially derived
     * from the implicitly trusted boot image when the instance is
     * created. Enabled by default.
     */
    enableIntegrityMonitoring?: boolean;
    /**
     * Defines whether the instance has Secure Boot enabled.Secure
     * Boot helps ensure that the system only runs authentic software
     * by verifying the digital signature of all boot components, and
     * halting the boot process if signature verification fails.
     * Disabled by default.
     */
    enableSecureBoot?: boolean;
    /**
     * Defines whether the instance has the vTPM enabled. Enabled by
     * default.
     */
    enableVtpm?: boolean;
}

export interface OrgPolicyCustomConstraintTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface OrgPolicyPolicyDryRunSpec {
    /**
     * An opaque tag indicating the current version of the policy, used for concurrency control. This field is ignored if used in a 'CreatePolicy' request. When the policy' is returned from either a 'GetPolicy' or a 'ListPolicies' request, this 'etag' indicates the version of the current policy to use when executing a read-modify-write loop. When the policy is returned from a 'GetEffectivePolicy' request, the 'etag' will be unset.
     */
    etag: string;
    /**
     * Determines the inheritance behavior for this policy. If 'inherit_from_parent' is true, policy rules set higher up in the hierarchy (up to the closest root) are inherited and present in the effective policy. If it is false, then no rules are inherited, and this policy becomes the new root for evaluation. This field can be set only for policies which configure list constraints.
     */
    inheritFromParent?: boolean;
    /**
     * Ignores policies set above this resource and restores the 'constraint_default' enforcement behavior of the specific constraint at this resource. This field can be set in policies for either list or boolean constraints. If set, 'rules' must be empty and 'inherit_from_parent' must be set to false.
     */
    reset?: boolean;
    /**
     * In policies for boolean constraints, the following requirements apply: - There must be one and only one policy rule where condition is unset. - Boolean policy rules with conditions must set 'enforced' to the opposite of the policy rule without a condition. - During policy evaluation, policy rules with conditions that are true for a target resource take precedence.
     */
    rules?: outputs.OrgPolicyPolicyDryRunSpecRule[];
    /**
     * Output only. The time stamp this was previously updated. This represents the last time a call to 'CreatePolicy' or 'UpdatePolicy' was made for that policy.
     */
    updateTime: string;
}

export interface OrgPolicyPolicyDryRunSpecRule {
    /**
     * Setting this to '"TRUE"' means that all values are allowed. This field can be set only in Policies for list constraints.
     */
    allowAll?: string;
    /**
     * A condition which determines whether this rule is used in the evaluation of the policy. When set, the 'expression' field in the 'Expr' must include from 1 to 10 subexpressions, joined by the "||" or "&&" operators. Each subexpression must be of the form "resource.matchTag('/tag_key_short_name, 'tag_value_short_name')". or "resource.matchTagId('tagKeys/key_id', 'tagValues/value_id')". where key_name and value_name are the resource names for Label Keys and Values. These names are available from the Tag Manager Service. An example expression is: "resource.matchTag('123456789/environment, 'prod')". or "resource.matchTagId('tagKeys/123', 'tagValues/456')".
     */
    condition?: outputs.OrgPolicyPolicyDryRunSpecRuleCondition;
    /**
     * Setting this to '"TRUE"' means that all values are denied. This field can be set only in Policies for list constraints.
     */
    denyAll?: string;
    /**
     * If '"TRUE"', then the 'Policy' is enforced. If '"FALSE"', then any configuration is acceptable. This field can be set only in Policies for boolean constraints.
     */
    enforce?: string;
    /**
     * List of values to be used for this policy rule. This field can be set only in policies for list constraints.
     */
    values?: outputs.OrgPolicyPolicyDryRunSpecRuleValues;
}

export interface OrgPolicyPolicyDryRunSpecRuleCondition {
    /**
     * Optional. Description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression?: string;
    /**
     * Optional. String indicating the location of the expression for error reporting, e.g. a file name and a position in the file.
     */
    location?: string;
    /**
     * Optional. Title for the expression, i.e. a short string describing its purpose. This can be used e.g. in UIs which allow to enter the expression.
     */
    title?: string;
}

export interface OrgPolicyPolicyDryRunSpecRuleValues {
    /**
     * List of values allowed at this resource.
     */
    allowedValues?: string[];
    /**
     * List of values denied at this resource.
     */
    deniedValues?: string[];
}

export interface OrgPolicyPolicySpec {
    /**
     * An opaque tag indicating the current version of the 'Policy', used for concurrency control. This field is ignored if used in a 'CreatePolicy' request. When the 'Policy' is returned from either a 'GetPolicy' or a 'ListPolicies' request, this 'etag' indicates the version of the current 'Policy' to use when executing a read-modify-write loop. When the 'Policy' is returned from a 'GetEffectivePolicy' request, the 'etag' will be unset.
     */
    etag: string;
    /**
     * Determines the inheritance behavior for this 'Policy'. If 'inherit_from_parent' is true, PolicyRules set higher up in the hierarchy (up to the closest root) are inherited and present in the effective policy. If it is false, then no rules are inherited, and this Policy becomes the new root for evaluation. This field can be set only for Policies which configure list constraints.
     */
    inheritFromParent?: boolean;
    /**
     * Ignores policies set above this resource and restores the 'constraint_default' enforcement behavior of the specific 'Constraint' at this resource. This field can be set in policies for either list or boolean constraints. If set, 'rules' must be empty and 'inherit_from_parent' must be set to false.
     */
    reset?: boolean;
    /**
     * Up to 10 PolicyRules are allowed. In Policies for boolean constraints, the following requirements apply: - There must be one and only one PolicyRule where condition is unset. - BooleanPolicyRules with conditions must set 'enforced' to the opposite of the PolicyRule without a condition. - During policy evaluation, PolicyRules with conditions that are true for a target resource take precedence.
     */
    rules?: outputs.OrgPolicyPolicySpecRule[];
    /**
     * Output only. The time stamp this was previously updated. This represents the last time a call to 'CreatePolicy' or 'UpdatePolicy' was made for that 'Policy'.
     */
    updateTime: string;
}

export interface OrgPolicyPolicySpecRule {
    /**
     * Setting this to '"TRUE"' means that all values are allowed. This field can be set only in Policies for list constraints.
     */
    allowAll?: string;
    /**
     * A condition which determines whether this rule is used in the evaluation of the policy. When set, the 'expression' field in the 'Expr' must include from 1 to 10 subexpressions, joined by the "||" or "&&" operators. Each subexpression must be of the form "resource.matchTag('/tag_key_short_name, 'tag_value_short_name')". or "resource.matchTagId('tagKeys/key_id', 'tagValues/value_id')". where key_name and value_name are the resource names for Label Keys and Values. These names are available from the Tag Manager Service. An example expression is: "resource.matchTag('123456789/environment, 'prod')". or "resource.matchTagId('tagKeys/123', 'tagValues/456')".
     */
    condition?: outputs.OrgPolicyPolicySpecRuleCondition;
    /**
     * Setting this to '"TRUE"' means that all values are denied. This field can be set only in Policies for list constraints.
     */
    denyAll?: string;
    /**
     * If '"TRUE"', then the 'Policy' is enforced. If '"FALSE"', then any configuration is acceptable. This field can be set only in Policies for boolean constraints.
     */
    enforce?: string;
    /**
     * List of values to be used for this PolicyRule. This field can be set only in Policies for list constraints.
     */
    values?: outputs.OrgPolicyPolicySpecRuleValues;
}

export interface OrgPolicyPolicySpecRuleCondition {
    /**
     * Optional. Description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression?: string;
    /**
     * Optional. String indicating the location of the expression for error reporting, e.g. a file name and a position in the file.
     */
    location?: string;
    /**
     * Optional. Title for the expression, i.e. a short string describing its purpose. This can be used e.g. in UIs which allow to enter the expression.
     */
    title?: string;
}

export interface OrgPolicyPolicySpecRuleValues {
    /**
     * List of values allowed at this resource.
     */
    allowedValues?: string[];
    /**
     * List of values denied at this resource.
     */
    deniedValues?: string[];
}

export interface OrgPolicyPolicyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface OrganizationAccessApprovalSettingsEnrolledService {
    /**
     * The product for which Access Approval will be enrolled. Allowed values are listed (case-sensitive):
     *   all
     *   appengine.googleapis.com
     *   bigquery.googleapis.com
     *   bigtable.googleapis.com
     *   cloudkms.googleapis.com
     *   compute.googleapis.com
     *   dataflow.googleapis.com
     *   iam.googleapis.com
     *   pubsub.googleapis.com
     *   storage.googleapis.com
     */
    cloudProduct: string;
    /**
     * The enrollment level of the service. Default value: "BLOCK_ALL" Possible values: ["BLOCK_ALL"]
     */
    enrollmentLevel?: string;
}

export interface OrganizationAccessApprovalSettingsTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface OrganizationIamAuditConfigAuditLogConfig {
    /**
     * Identities that do not cause logging for this type of permission. Each entry can have one of the following values:user:{emailid}: An email address that represents a specific Google account. For example, alice@gmail.com or joe@example.com. serviceAccount:{emailid}: An email address that represents a service account. For example, my-other-app@appspot.gserviceaccount.com. group:{emailid}: An email address that represents a Google group. For example, admins@example.com. domain:{domain}: A G Suite domain (primary, instead of alias) name that represents all the users of that domain. For example, google.com or example.com.
     */
    exemptedMembers?: string[];
    /**
     * Permission type for which logging is to be configured. Must be one of DATA_READ, DATA_WRITE, or ADMIN_READ.
     */
    logType: string;
}

export interface OrganizationIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface OrganizationIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface OrganizationPolicyBooleanPolicy {
    /**
     * If true, then the Policy is enforced. If false, then any configuration is acceptable.
     */
    enforced: boolean;
}

export interface OrganizationPolicyListPolicy {
    /**
     * One or the other must be set.
     */
    allow?: outputs.OrganizationPolicyListPolicyAllow;
    /**
     * One or the other must be set.
     */
    deny?: outputs.OrganizationPolicyListPolicyDeny;
    /**
     * If set to true, the values from the effective Policy of the parent resource are inherited, meaning the values set in this Policy are added to the values inherited up the hierarchy.
     */
    inheritFromParent?: boolean;
    /**
     * The Google Cloud Console will try to default to a configuration that matches the value specified in this field.
     */
    suggestedValue: string;
}

export interface OrganizationPolicyListPolicyAllow {
    /**
     * The policy allows or denies all values.
     */
    all?: boolean;
    /**
     * The policy can define specific values that are allowed or denied.
     */
    values?: string[];
}

export interface OrganizationPolicyListPolicyDeny {
    /**
     * The policy allows or denies all values.
     */
    all?: boolean;
    /**
     * The policy can define specific values that are allowed or denied.
     */
    values?: string[];
}

export interface OrganizationPolicyRestorePolicy {
    /**
     * May only be set to true. If set, then the default Policy is restored.
     */
    default: boolean;
}

export interface OrganizationPolicyTimeouts {
    create?: string;
    delete?: string;
    read?: string;
    update?: string;
}

export interface OsConfigOsPolicyAssignmentInstanceFilter {
    /**
     * Target all VMs in the project. If true, no other criteria is permitted.
     */
    all?: boolean;
    /**
     * List of label sets used for VM exclusion.
     * If the list has more than one label set, the VM is excluded if any of the label sets are applicable for the VM.
     */
    exclusionLabels?: outputs.OsConfigOsPolicyAssignmentInstanceFilterExclusionLabel[];
    /**
     * List of label sets used for VM inclusion.
     * If the list has more than one 'LabelSet', the VM is included if any of the label sets are applicable for the VM.
     */
    inclusionLabels?: outputs.OsConfigOsPolicyAssignmentInstanceFilterInclusionLabel[];
    /**
     * List of inventories to select VMs.
     * A VM is selected if its inventory data matches at least one of the following inventories.
     */
    inventories?: outputs.OsConfigOsPolicyAssignmentInstanceFilterInventory[];
}

export interface OsConfigOsPolicyAssignmentInstanceFilterExclusionLabel {
    /**
     * Labels are identified by key/value pairs in this map. A VM should contain all the key/value pairs specified in this map to be selected.
     */
    labels?: {[key: string]: string};
}

export interface OsConfigOsPolicyAssignmentInstanceFilterInclusionLabel {
    /**
     * Labels are identified by key/value pairs in this map. A VM should contain all the key/value pairs specified in this map to be selected.
     */
    labels?: {[key: string]: string};
}

export interface OsConfigOsPolicyAssignmentInstanceFilterInventory {
    /**
     * The OS short name
     */
    osShortName: string;
    /**
     * The OS version Prefix matches are supported if asterisk(*) is provided as the last character. For example, to match all versions with a major version of '7', specify the following value for this field '7.*' An empty string matches all OS versions.
     */
    osVersion?: string;
}

export interface OsConfigOsPolicyAssignmentOsPolicy {
    /**
     * This flag determines the OS policy compliance status when none of the resource groups within the policy are applicable for a VM. Set this value to 'true' if the policy needs to be reported as compliant even if the policy has nothing to validate or enforce.
     */
    allowNoResourceGroupMatch?: boolean;
    /**
     * Policy description. Length of the description is limited to 1024 characters.
     */
    description?: string;
    /**
     * The id of the OS policy with the following restrictions:
     * * Must contain only lowercase letters, numbers, and hyphens.
     * * Must start with a letter.
     * * Must be between 1-63 characters.
     * * Must end with a number or a letter.
     * * Must be unique within the assignment.
     */
    id: string;
    /**
     * Policy mode Possible values: ["MODE_UNSPECIFIED", "VALIDATION", "ENFORCEMENT"]
     */
    mode: string;
    /**
     * List of resource groups for the policy. For a particular VM, resource groups are evaluated in the order specified and the first resource group that is applicable is selected and the rest are ignored.
     * If none of the resource groups are applicable for a VM, the VM is considered to be non-compliant w.r.t this policy. This behavior can be toggled by the flag 'allow_no_resource_group_match'
     */
    resourceGroups: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroup[];
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroup {
    /**
     * List of inventory filters for the resource group.
     * The resources in this resource group are applied to the target VM if it satisfies at least one of the following inventory filters.
     * For example, to apply this resource group to VMs running either 'RHEL' or 'CentOS' operating systems, specify 2 items for the list with following values: inventory_filters[0].os_short_name='rhel' and inventory_filters[1].os_short_name='centos'
     * If the list is empty, this resource group will be applied to the target VM unconditionally.
     */
    inventoryFilters?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupInventoryFilter[];
    /**
     * List of resources configured for this resource group. The resources are executed in the exact order specified here.
     */
    resources: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResource[];
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupInventoryFilter {
    /**
     * The OS short name
     */
    osShortName: string;
    /**
     * The OS version
     * Prefix matches are supported if asterisk(*) is provided as the last character. For example, to match all versions with a major version of '7', specify the following value for this field '7.*'
     * An empty string matches all OS versions.
     */
    osVersion?: string;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResource {
    /**
     * Exec resource
     */
    exec?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceExec;
    /**
     * File resource
     */
    file?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceFile;
    /**
     * The id of the resource with the following restrictions:
     * * Must contain only lowercase letters, numbers, and hyphens.
     * * Must start with a letter.
     * * Must be between 1-63 characters.
     * * Must end with a number or a letter.
     * * Must be unique within the OS policy.
     */
    id: string;
    /**
     * Package resource
     */
    pkg?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkg;
    /**
     * Package repository resource
     */
    repository?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceRepository;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceExec {
    /**
     * What to run to bring this resource into the desired state. An exit code of 100 indicates "success", any other exit code indicates a failure running enforce.
     */
    enforce?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceExecEnforce;
    /**
     * What to run to validate this resource is in the desired state. An exit code of 100 indicates "in desired state", and exit code of 101 indicates "not in desired state". Any other exit code indicates a failure running validate.
     */
    validate: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceExecValidate;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceExecEnforce {
    /**
     * Optional arguments to pass to the source during execution.
     */
    args?: string[];
    /**
     * A remote or local file.
     */
    file?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceExecEnforceFile;
    /**
     * The script interpreter to use. Possible values: ["INTERPRETER_UNSPECIFIED", "NONE", "SHELL", "POWERSHELL"]
     */
    interpreter: string;
    /**
     * Only recorded for enforce Exec. Path to an output file (that is created by this Exec) whose content will be recorded in OSPolicyResourceCompliance after a successful run. Absence or failure to read this file will result in this ExecResource being non-compliant. Output file size is limited to 100K bytes.
     */
    outputFilePath?: string;
    /**
     * An inline script. The size of the script is limited to 1024 characters.
     */
    script?: string;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceExecEnforceFile {
    /**
     * Defaults to false. When false, files are subject to validations based on the file type: Remote: A checksum must be specified. Cloud Storage: An object generation number must be specified.
     */
    allowInsecure?: boolean;
    /**
     * A Cloud Storage object.
     */
    gcs?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceExecEnforceFileGcs;
    /**
     * A local path within the VM to use.
     */
    localPath?: string;
    /**
     * A generic remote file.
     */
    remote?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceExecEnforceFileRemote;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceExecEnforceFileGcs {
    /**
     * Bucket of the Cloud Storage object.
     */
    bucket: string;
    /**
     * Generation number of the Cloud Storage object.
     */
    generation?: number;
    /**
     * Name of the Cloud Storage object.
     */
    object: string;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceExecEnforceFileRemote {
    /**
     * SHA256 checksum of the remote file.
     */
    sha256Checksum?: string;
    /**
     * URI from which to fetch the object. It should contain both the protocol and path following the format '{protocol}://{location}'.
     */
    uri: string;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceExecValidate {
    /**
     * Optional arguments to pass to the source during execution.
     */
    args?: string[];
    /**
     * A remote or local file.
     */
    file?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceExecValidateFile;
    /**
     * The script interpreter to use. Possible values: ["INTERPRETER_UNSPECIFIED", "NONE", "SHELL", "POWERSHELL"]
     */
    interpreter: string;
    /**
     * Only recorded for enforce Exec. Path to an output file (that is created by this Exec) whose content will be recorded in OSPolicyResourceCompliance after a successful run. Absence or failure to read this file will result in this ExecResource being non-compliant. Output file size is limited to 100K bytes.
     */
    outputFilePath?: string;
    /**
     * An inline script. The size of the script is limited to 1024 characters.
     */
    script?: string;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceExecValidateFile {
    /**
     * Defaults to false. When false, files are subject to validations based on the file type:
     * Remote: A checksum must be specified. Cloud Storage: An object generation number must be specified.
     */
    allowInsecure?: boolean;
    /**
     * A Cloud Storage object.
     */
    gcs?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceExecValidateFileGcs;
    /**
     * A local path within the VM to use.
     */
    localPath?: string;
    /**
     * A generic remote file.
     */
    remote?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceExecValidateFileRemote;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceExecValidateFileGcs {
    /**
     * Bucket of the Cloud Storage object.
     */
    bucket: string;
    /**
     * Generation number of the Cloud Storage object.
     */
    generation?: number;
    /**
     * Name of the Cloud Storage object.
     */
    object: string;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceExecValidateFileRemote {
    /**
     * SHA256 checksum of the remote file.
     */
    sha256Checksum?: string;
    /**
     * URI from which to fetch the object. It should contain both the protocol and path following the format '{protocol}://{location}'.
     */
    uri: string;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceFile {
    /**
     * A a file with this content. The size of the content is limited to 1024 characters.
     */
    content?: string;
    /**
     * A remote or local source.
     */
    file?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceFileFile;
    /**
     * The absolute path of the file within the VM.
     */
    path: string;
    /**
     * Consists of three octal digits which represent, in order, the permissions of the owner, group, and other users for the file (similarly to the numeric mode used in the linux chmod utility). Each digit represents a three bit number with the 4 bit corresponding to the read permissions, the 2 bit corresponds to the write bit, and the one bit corresponds to the execute permission. Default behavior is 755.
     * Below are some examples of permissions and their associated values: read, write, and execute: 7 read and execute: 5 read and write: 6 read only: 4
     */
    permissions: string;
    /**
     * Desired state of the file. Possible values: ["DESIRED_STATE_UNSPECIFIED", "PRESENT", "ABSENT", "CONTENTS_MATCH"]
     */
    state: string;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceFileFile {
    /**
     * Defaults to false. When false, files are subject to validations based on the file type: Remote: A checksum must be specified. Cloud Storage: An object generation number must be specified.
     */
    allowInsecure?: boolean;
    /**
     * A Cloud Storage object.
     */
    gcs?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceFileFileGcs;
    /**
     * A local path within the VM to use.
     */
    localPath?: string;
    /**
     * A generic remote file.
     */
    remote?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceFileFileRemote;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceFileFileGcs {
    /**
     * Bucket of the Cloud Storage object.
     */
    bucket: string;
    /**
     * Generation number of the Cloud Storage object.
     */
    generation?: number;
    /**
     * Name of the Cloud Storage object.
     */
    object: string;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceFileFileRemote {
    /**
     * SHA256 checksum of the remote file.
     */
    sha256Checksum?: string;
    /**
     * URI from which to fetch the object. It should contain both the protocol and path following the format '{protocol}://{location}'.
     */
    uri: string;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkg {
    /**
     * A package managed by Apt.
     */
    apt?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgApt;
    /**
     * A deb package file.
     */
    deb?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgDeb;
    /**
     * The desired state the agent should maintain for this package. Possible values: ["DESIRED_STATE_UNSPECIFIED", "INSTALLED", "REMOVED"]
     */
    desiredState: string;
    /**
     * A package managed by GooGet.
     */
    googet?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgGooget;
    /**
     * An MSI package.
     */
    msi?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgMsi;
    /**
     * An rpm package file.
     */
    rpm?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgRpm;
    /**
     * A package managed by YUM.
     */
    yum?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgYum;
    /**
     * A package managed by Zypper.
     */
    zypper?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgZypper;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgApt {
    /**
     * Package name.
     */
    name: string;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgDeb {
    /**
     * Whether dependencies should also be installed. - install when false: 'dpkg -i package' - install when true: 'apt-get update && apt-get -y install package.deb'
     */
    pullDeps?: boolean;
    /**
     * A deb package.
     */
    source: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgDebSource;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgDebSource {
    /**
     * Defaults to false. When false, files are subject to validations based on the file type:
     * Remote: A checksum must be specified. Cloud Storage: An object generation number must be specified.
     */
    allowInsecure?: boolean;
    /**
     * A Cloud Storage object.
     */
    gcs?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgDebSourceGcs;
    /**
     * A local path within the VM to use.
     */
    localPath?: string;
    /**
     * A generic remote file.
     */
    remote?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgDebSourceRemote;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgDebSourceGcs {
    /**
     * Bucket of the Cloud Storage object.
     */
    bucket: string;
    /**
     * Generation number of the Cloud Storage object.
     */
    generation?: number;
    /**
     * Name of the Cloud Storage object.
     */
    object: string;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgDebSourceRemote {
    /**
     * SHA256 checksum of the remote file.
     */
    sha256Checksum?: string;
    /**
     * URI from which to fetch the object. It should contain both the protocol and path following the format '{protocol}://{location}'.
     */
    uri: string;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgGooget {
    /**
     * Package name.
     */
    name: string;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgMsi {
    /**
     * Additional properties to use during installation. This should be in the format of Property=Setting. Appended to the defaults of 'ACTION=INSTALL REBOOT=ReallySuppress'.
     */
    properties?: string[];
    /**
     * The MSI package.
     */
    source: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgMsiSource;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgMsiSource {
    /**
     * Defaults to false. When false, files are subject to validations based on the file type:
     * Remote: A checksum must be specified. Cloud Storage: An object generation number must be specified.
     */
    allowInsecure?: boolean;
    /**
     * A Cloud Storage object.
     */
    gcs?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgMsiSourceGcs;
    /**
     * A local path within the VM to use.
     */
    localPath?: string;
    /**
     * A generic remote file.
     */
    remote?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgMsiSourceRemote;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgMsiSourceGcs {
    /**
     * Bucket of the Cloud Storage object.
     */
    bucket: string;
    /**
     * Generation number of the Cloud Storage object.
     */
    generation?: number;
    /**
     * Name of the Cloud Storage object.
     */
    object: string;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgMsiSourceRemote {
    /**
     * SHA256 checksum of the remote file.
     */
    sha256Checksum?: string;
    /**
     * URI from which to fetch the object. It should contain both the protocol and path following the format '{protocol}://{location}'.
     */
    uri: string;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgRpm {
    /**
     * Whether dependencies should also be installed. - install when false: 'rpm --upgrade --replacepkgs package.rpm' - install when true: 'yum -y install package.rpm' or 'zypper -y install package.rpm'
     */
    pullDeps?: boolean;
    /**
     * An rpm package.
     */
    source: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgRpmSource;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgRpmSource {
    /**
     * Defaults to false. When false, files are subject to validations based on the file type:
     * Remote: A checksum must be specified. Cloud Storage: An object generation number must be specified.
     */
    allowInsecure?: boolean;
    /**
     * A Cloud Storage object.
     */
    gcs?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgRpmSourceGcs;
    /**
     * A local path within the VM to use.
     */
    localPath?: string;
    /**
     * A generic remote file.
     */
    remote?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgRpmSourceRemote;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgRpmSourceGcs {
    /**
     * Bucket of the Cloud Storage object.
     */
    bucket: string;
    /**
     * Generation number of the Cloud Storage object.
     */
    generation?: number;
    /**
     * Name of the Cloud Storage object.
     */
    object: string;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgRpmSourceRemote {
    /**
     * SHA256 checksum of the remote file.
     */
    sha256Checksum?: string;
    /**
     * URI from which to fetch the object. It should contain both the protocol and path following the format '{protocol}://{location}'.
     */
    uri: string;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgYum {
    /**
     * Package name.
     */
    name: string;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourcePkgZypper {
    /**
     * Package name.
     */
    name: string;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceRepository {
    /**
     * An Apt Repository.
     */
    apt?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceRepositoryApt;
    /**
     * A Goo Repository.
     */
    goo?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceRepositoryGoo;
    /**
     * A Yum Repository.
     */
    yum?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceRepositoryYum;
    /**
     * A Zypper Repository.
     */
    zypper?: outputs.OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceRepositoryZypper;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceRepositoryApt {
    /**
     * Type of archive files in this repository. Possible values: ["ARCHIVE_TYPE_UNSPECIFIED", "DEB", "DEB_SRC"]
     */
    archiveType: string;
    /**
     * List of components for this repository. Must contain at least one item.
     */
    components: string[];
    /**
     * Distribution of this repository.
     */
    distribution: string;
    /**
     * URI of the key file for this repository. The agent maintains a keyring at '/etc/apt/trusted.gpg.d/osconfig_agent_managed.gpg'.
     */
    gpgKey?: string;
    /**
     * URI for this repository.
     */
    uri: string;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceRepositoryGoo {
    /**
     * The name of the repository.
     */
    name: string;
    /**
     * The url of the repository.
     */
    url: string;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceRepositoryYum {
    /**
     * The location of the repository directory.
     */
    baseUrl: string;
    /**
     * The display name of the repository.
     */
    displayName?: string;
    /**
     * URIs of GPG keys.
     */
    gpgKeys?: string[];
    /**
     * A one word, unique name for this repository. This is the 'repo id' in the yum config file and also the 'display_name' if 'display_name' is omitted. This id is also used as the unique identifier when checking for resource conflicts.
     */
    id: string;
}

export interface OsConfigOsPolicyAssignmentOsPolicyResourceGroupResourceRepositoryZypper {
    /**
     * The location of the repository directory.
     */
    baseUrl: string;
    /**
     * The display name of the repository.
     */
    displayName?: string;
    /**
     * URIs of GPG keys.
     */
    gpgKeys?: string[];
    /**
     * A one word, unique name for this repository. This is the 'repo id' in the zypper config file and also the 'display_name' if 'display_name' is omitted. This id is also used as the unique identifier when checking for GuestPolicy conflicts.
     */
    id: string;
}

export interface OsConfigOsPolicyAssignmentRollout {
    /**
     * The maximum number (or percentage) of VMs per zone to disrupt at any given moment.
     */
    disruptionBudget: outputs.OsConfigOsPolicyAssignmentRolloutDisruptionBudget;
    /**
     * This determines the minimum duration of time to wait after the configuration changes are applied through the current rollout. A VM continues to count towards the 'disruption_budget' at least until this duration of time has passed after configuration changes are applied.
     */
    minWaitDuration: string;
}

export interface OsConfigOsPolicyAssignmentRolloutDisruptionBudget {
    /**
     * Specifies a fixed value.
     */
    fixed?: number;
    /**
     * Specifies the relative value defined as a percentage, which will be multiplied by a reference value.
     */
    percent?: number;
}

export interface OsConfigOsPolicyAssignmentTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface OsConfigPatchDeploymentInstanceFilter {
    /**
     * Target all VM instances in the project. If true, no other criteria is permitted.
     */
    all?: boolean;
    /**
     * Targets VM instances matching ANY of these GroupLabels. This allows targeting of disparate groups of VM instances.
     */
    groupLabels?: outputs.OsConfigPatchDeploymentInstanceFilterGroupLabel[];
    /**
     * Targets VMs whose name starts with one of these prefixes. Similar to labels, this is another way to group
     * VMs when targeting configs, for example prefix="prod-".
     */
    instanceNamePrefixes?: string[];
    /**
     * Targets any of the VM instances specified. Instances are specified by their URI in the 'form zones/{{zone}}/instances/{{instance_name}}',
     * 'projects/{{project_id}}/zones/{{zone}}/instances/{{instance_name}}', or
     * 'https://www.googleapis.com/compute/v1/projects/{{project_id}}/zones/{{zone}}/instances/{{instance_name}}'
     */
    instances?: string[];
    /**
     * Targets VM instances in ANY of these zones. Leave empty to target VM instances in any zone.
     */
    zones?: string[];
}

export interface OsConfigPatchDeploymentInstanceFilterGroupLabel {
    /**
     * Compute Engine instance labels that must be present for a VM instance to be targeted by this filter
     */
    labels: {[key: string]: string};
}

export interface OsConfigPatchDeploymentOneTimeSchedule {
    /**
     * The desired patch job execution time. A timestamp in RFC3339 UTC "Zulu" format,
     * accurate to nanoseconds. Example: "2014-10-02T15:01:23.045123456Z".
     */
    executeTime: string;
}

export interface OsConfigPatchDeploymentPatchConfig {
    /**
     * Apt update settings. Use this setting to override the default apt patch rules.
     */
    apt?: outputs.OsConfigPatchDeploymentPatchConfigApt;
    /**
     * goo update settings. Use this setting to override the default goo patch rules.
     */
    goo?: outputs.OsConfigPatchDeploymentPatchConfigGoo;
    /**
     * Allows the patch job to run on Managed instance groups (MIGs).
     */
    migInstancesAllowed?: boolean;
    /**
     * The ExecStep to run after the patch update.
     */
    postStep?: outputs.OsConfigPatchDeploymentPatchConfigPostStep;
    /**
     * The ExecStep to run before the patch update.
     */
    preStep?: outputs.OsConfigPatchDeploymentPatchConfigPreStep;
    /**
     * Post-patch reboot settings. Possible values: ["DEFAULT", "ALWAYS", "NEVER"]
     */
    rebootConfig?: string;
    /**
     * Windows update settings. Use this setting to override the default Windows patch rules.
     */
    windowsUpdate?: outputs.OsConfigPatchDeploymentPatchConfigWindowsUpdate;
    /**
     * Yum update settings. Use this setting to override the default yum patch rules.
     */
    yum?: outputs.OsConfigPatchDeploymentPatchConfigYum;
    /**
     * zypper update settings. Use this setting to override the default zypper patch rules.
     */
    zypper?: outputs.OsConfigPatchDeploymentPatchConfigZypper;
}

export interface OsConfigPatchDeploymentPatchConfigApt {
    /**
     * List of packages to exclude from update. These packages will be excluded.
     */
    excludes?: string[];
    /**
     * An exclusive list of packages to be updated. These are the only packages that will be updated.
     * If these packages are not installed, they will be ignored. This field cannot be specified with
     * any other patch configuration fields.
     */
    exclusivePackages?: string[];
    /**
     * By changing the type to DIST, the patching is performed using apt-get dist-upgrade instead. Possible values: ["DIST", "UPGRADE"]
     */
    type?: string;
}

export interface OsConfigPatchDeploymentPatchConfigGoo {
    /**
     * goo update settings. Use this setting to override the default goo patch rules.
     */
    enabled: boolean;
}

export interface OsConfigPatchDeploymentPatchConfigPostStep {
    /**
     * The ExecStepConfig for all Linux VMs targeted by the PatchJob.
     */
    linuxExecStepConfig?: outputs.OsConfigPatchDeploymentPatchConfigPostStepLinuxExecStepConfig;
    /**
     * The ExecStepConfig for all Windows VMs targeted by the PatchJob.
     */
    windowsExecStepConfig?: outputs.OsConfigPatchDeploymentPatchConfigPostStepWindowsExecStepConfig;
}

export interface OsConfigPatchDeploymentPatchConfigPostStepLinuxExecStepConfig {
    /**
     * Defaults to [0]. A list of possible return values that the execution can return to indicate a success.
     */
    allowedSuccessCodes?: number[];
    /**
     * A Cloud Storage object containing the executable.
     */
    gcsObject?: outputs.OsConfigPatchDeploymentPatchConfigPostStepLinuxExecStepConfigGcsObject;
    /**
     * The script interpreter to use to run the script. If no interpreter is specified the script will
     * be executed directly, which will likely only succeed for scripts with shebang lines. Possible values: ["SHELL", "POWERSHELL"]
     */
    interpreter?: string;
    /**
     * An absolute path to the executable on the VM.
     */
    localPath?: string;
}

export interface OsConfigPatchDeploymentPatchConfigPostStepLinuxExecStepConfigGcsObject {
    /**
     * Bucket of the Cloud Storage object.
     */
    bucket: string;
    /**
     * Generation number of the Cloud Storage object. This is used to ensure that the ExecStep specified by this PatchJob does not change.
     */
    generationNumber: string;
    /**
     * Name of the Cloud Storage object.
     */
    object: string;
}

export interface OsConfigPatchDeploymentPatchConfigPostStepWindowsExecStepConfig {
    /**
     * Defaults to [0]. A list of possible return values that the execution can return to indicate a success.
     */
    allowedSuccessCodes?: number[];
    /**
     * A Cloud Storage object containing the executable.
     */
    gcsObject?: outputs.OsConfigPatchDeploymentPatchConfigPostStepWindowsExecStepConfigGcsObject;
    /**
     * The script interpreter to use to run the script. If no interpreter is specified the script will
     * be executed directly, which will likely only succeed for scripts with shebang lines. Possible values: ["SHELL", "POWERSHELL"]
     */
    interpreter?: string;
    /**
     * An absolute path to the executable on the VM.
     */
    localPath?: string;
}

export interface OsConfigPatchDeploymentPatchConfigPostStepWindowsExecStepConfigGcsObject {
    /**
     * Bucket of the Cloud Storage object.
     */
    bucket: string;
    /**
     * Generation number of the Cloud Storage object. This is used to ensure that the ExecStep specified by this PatchJob does not change.
     */
    generationNumber: string;
    /**
     * Name of the Cloud Storage object.
     */
    object: string;
}

export interface OsConfigPatchDeploymentPatchConfigPreStep {
    /**
     * The ExecStepConfig for all Linux VMs targeted by the PatchJob.
     */
    linuxExecStepConfig?: outputs.OsConfigPatchDeploymentPatchConfigPreStepLinuxExecStepConfig;
    /**
     * The ExecStepConfig for all Windows VMs targeted by the PatchJob.
     */
    windowsExecStepConfig?: outputs.OsConfigPatchDeploymentPatchConfigPreStepWindowsExecStepConfig;
}

export interface OsConfigPatchDeploymentPatchConfigPreStepLinuxExecStepConfig {
    /**
     * Defaults to [0]. A list of possible return values that the execution can return to indicate a success.
     */
    allowedSuccessCodes?: number[];
    /**
     * A Cloud Storage object containing the executable.
     */
    gcsObject?: outputs.OsConfigPatchDeploymentPatchConfigPreStepLinuxExecStepConfigGcsObject;
    /**
     * The script interpreter to use to run the script. If no interpreter is specified the script will
     * be executed directly, which will likely only succeed for scripts with shebang lines. Possible values: ["SHELL", "POWERSHELL"]
     */
    interpreter?: string;
    /**
     * An absolute path to the executable on the VM.
     */
    localPath?: string;
}

export interface OsConfigPatchDeploymentPatchConfigPreStepLinuxExecStepConfigGcsObject {
    /**
     * Bucket of the Cloud Storage object.
     */
    bucket: string;
    /**
     * Generation number of the Cloud Storage object. This is used to ensure that the ExecStep specified by this PatchJob does not change.
     */
    generationNumber: string;
    /**
     * Name of the Cloud Storage object.
     */
    object: string;
}

export interface OsConfigPatchDeploymentPatchConfigPreStepWindowsExecStepConfig {
    /**
     * Defaults to [0]. A list of possible return values that the execution can return to indicate a success.
     */
    allowedSuccessCodes?: number[];
    /**
     * A Cloud Storage object containing the executable.
     */
    gcsObject?: outputs.OsConfigPatchDeploymentPatchConfigPreStepWindowsExecStepConfigGcsObject;
    /**
     * The script interpreter to use to run the script. If no interpreter is specified the script will
     * be executed directly, which will likely only succeed for scripts with shebang lines. Possible values: ["SHELL", "POWERSHELL"]
     */
    interpreter?: string;
    /**
     * An absolute path to the executable on the VM.
     */
    localPath?: string;
}

export interface OsConfigPatchDeploymentPatchConfigPreStepWindowsExecStepConfigGcsObject {
    /**
     * Bucket of the Cloud Storage object.
     */
    bucket: string;
    /**
     * Generation number of the Cloud Storage object. This is used to ensure that the ExecStep specified by this PatchJob does not change.
     */
    generationNumber: string;
    /**
     * Name of the Cloud Storage object.
     */
    object: string;
}

export interface OsConfigPatchDeploymentPatchConfigWindowsUpdate {
    /**
     * Only apply updates of these windows update classifications. If empty, all updates are applied. Possible values: ["CRITICAL", "SECURITY", "DEFINITION", "DRIVER", "FEATURE_PACK", "SERVICE_PACK", "TOOL", "UPDATE_ROLLUP", "UPDATE"]
     */
    classifications?: string[];
    /**
     * List of KBs to exclude from update.
     */
    excludes?: string[];
    /**
     * An exclusive list of kbs to be updated. These are the only patches that will be updated.
     * This field must not be used with other patch configurations.
     */
    exclusivePatches?: string[];
}

export interface OsConfigPatchDeploymentPatchConfigYum {
    /**
     * List of packages to exclude from update. These packages will be excluded.
     */
    excludes?: string[];
    /**
     * An exclusive list of packages to be updated. These are the only packages that will be updated.
     * If these packages are not installed, they will be ignored. This field cannot be specified with
     * any other patch configuration fields.
     */
    exclusivePackages?: string[];
    /**
     * Will cause patch to run yum update-minimal instead.
     */
    minimal?: boolean;
    /**
     * Adds the --security flag to yum update. Not supported on all platforms.
     */
    security?: boolean;
}

export interface OsConfigPatchDeploymentPatchConfigZypper {
    /**
     * Install only patches with these categories. Common categories include security, recommended, and feature.
     */
    categories?: string[];
    /**
     * List of packages to exclude from update.
     */
    excludes?: string[];
    /**
     * An exclusive list of patches to be updated. These are the only patches that will be installed using 'zypper patch patch:' command.
     * This field must not be used with any other patch configuration fields.
     */
    exclusivePatches?: string[];
    /**
     * Install only patches with these severities. Common severities include critical, important, moderate, and low.
     */
    severities?: string[];
    /**
     * Adds the --with-optional flag to zypper patch.
     */
    withOptional?: boolean;
    /**
     * Adds the --with-update flag, to zypper patch.
     */
    withUpdate?: boolean;
}

export interface OsConfigPatchDeploymentRecurringSchedule {
    /**
     * The end time at which a recurring patch deployment schedule is no longer active.
     * A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds. Example: "2014-10-02T15:01:23.045123456Z".
     */
    endTime?: string;
    /**
     * The time the last patch job ran successfully.
     * A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds. Example: "2014-10-02T15:01:23.045123456Z".
     */
    lastExecuteTime: string;
    /**
     * Schedule with monthly executions.
     */
    monthly?: outputs.OsConfigPatchDeploymentRecurringScheduleMonthly;
    /**
     * The time the next patch job is scheduled to run.
     * A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds. Example: "2014-10-02T15:01:23.045123456Z".
     */
    nextExecuteTime: string;
    /**
     * The time that the recurring schedule becomes effective. Defaults to createTime of the patch deployment.
     * A timestamp in RFC3339 UTC "Zulu" format, accurate to nanoseconds. Example: "2014-10-02T15:01:23.045123456Z".
     */
    startTime?: string;
    /**
     * Time of the day to run a recurring deployment.
     */
    timeOfDay: outputs.OsConfigPatchDeploymentRecurringScheduleTimeOfDay;
    /**
     * Defines the time zone that timeOfDay is relative to. The rules for daylight saving time are
     * determined by the chosen time zone.
     */
    timeZone: outputs.OsConfigPatchDeploymentRecurringScheduleTimeZone;
    /**
     * Schedule with weekly executions.
     */
    weekly?: outputs.OsConfigPatchDeploymentRecurringScheduleWeekly;
}

export interface OsConfigPatchDeploymentRecurringScheduleMonthly {
    /**
     * One day of the month. 1-31 indicates the 1st to the 31st day. -1 indicates the last day of the month.
     * Months without the target day will be skipped. For example, a schedule to run "every month on the 31st"
     * will not run in February, April, June, etc.
     */
    monthDay?: number;
    /**
     * Week day in a month.
     */
    weekDayOfMonth?: outputs.OsConfigPatchDeploymentRecurringScheduleMonthlyWeekDayOfMonth;
}

export interface OsConfigPatchDeploymentRecurringScheduleMonthlyWeekDayOfMonth {
    /**
     * A day of the week. Possible values: ["MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    dayOfWeek: string;
    /**
     * Represents the number of days before or after the given week day of month that the patch deployment is scheduled for.
     */
    dayOffset?: number;
    /**
     * Week number in a month. 1-4 indicates the 1st to 4th week of the month. -1 indicates the last week of the month.
     */
    weekOrdinal: number;
}

export interface OsConfigPatchDeploymentRecurringScheduleTimeOfDay {
    /**
     * Hours of day in 24 hour format. Should be from 0 to 23.
     * An API may choose to allow the value "24:00:00" for scenarios like business closing time.
     */
    hours?: number;
    /**
     * Minutes of hour of day. Must be from 0 to 59.
     */
    minutes?: number;
    /**
     * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
     */
    nanos?: number;
    /**
     * Seconds of minutes of the time. Must normally be from 0 to 59. An API may allow the value 60 if it allows leap-seconds.
     */
    seconds?: number;
}

export interface OsConfigPatchDeploymentRecurringScheduleTimeZone {
    /**
     * IANA Time Zone Database time zone, e.g. "America/New_York".
     */
    id: string;
    /**
     * IANA Time Zone Database version number, e.g. "2019a".
     */
    version?: string;
}

export interface OsConfigPatchDeploymentRecurringScheduleWeekly {
    /**
     * IANA Time Zone Database time zone, e.g. "America/New_York". Possible values: ["MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    dayOfWeek: string;
}

export interface OsConfigPatchDeploymentRollout {
    /**
     * The maximum number (or percentage) of VMs per zone to disrupt at any given moment. The number of VMs calculated from multiplying the percentage by the total number of VMs in a zone is rounded up.
     * During patching, a VM is considered disrupted from the time the agent is notified to begin until patching has completed. This disruption time includes the time to complete reboot and any post-patch steps.
     * A VM contributes to the disruption budget if its patching operation fails either when applying the patches, running pre or post patch steps, or if it fails to respond with a success notification before timing out. VMs that are not running or do not have an active agent do not count toward this disruption budget.
     * For zone-by-zone rollouts, if the disruption budget in a zone is exceeded, the patch job stops, because continuing to the next zone requires completion of the patch process in the previous zone.
     * For example, if the disruption budget has a fixed value of 10, and 8 VMs fail to patch in the current zone, the patch job continues to patch 2 VMs at a time until the zone is completed. When that zone is completed successfully, patching begins with 10 VMs at a time in the next zone. If 10 VMs in the next zone fail to patch, the patch job stops.
     */
    disruptionBudget: outputs.OsConfigPatchDeploymentRolloutDisruptionBudget;
    /**
     * Mode of the patch rollout. Possible values: ["ZONE_BY_ZONE", "CONCURRENT_ZONES"]
     */
    mode: string;
}

export interface OsConfigPatchDeploymentRolloutDisruptionBudget {
    /**
     * Specifies a fixed value.
     */
    fixed?: number;
    /**
     * Specifies the relative value defined as a percentage, which will be multiplied by a reference value.
     */
    percentage?: number;
}

export interface OsConfigPatchDeploymentTimeouts {
    create?: string;
    delete?: string;
}

export interface OsLoginSshPublicKeyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface PrivatecaCaPoolIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface PrivatecaCaPoolIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface PrivatecaCaPoolIssuancePolicy {
    /**
     * IssuanceModes specifies the allowed ways in which Certificates may be requested from this CaPool.
     */
    allowedIssuanceModes?: outputs.PrivatecaCaPoolIssuancePolicyAllowedIssuanceModes;
    /**
     * If any AllowedKeyType is specified, then the certificate request's public key must match one of the key types listed here.
     * Otherwise, any key may be used.
     */
    allowedKeyTypes?: outputs.PrivatecaCaPoolIssuancePolicyAllowedKeyType[];
    /**
     * A set of X.509 values that will be applied to all certificates issued through this CaPool. If a certificate request
     * includes conflicting values for the same properties, they will be overwritten by the values defined here. If a certificate
     * request uses a CertificateTemplate that defines conflicting predefinedValues for the same properties, the certificate
     * issuance request will fail.
     */
    baselineValues?: outputs.PrivatecaCaPoolIssuancePolicyBaselineValues;
    /**
     * Describes constraints on identities that may appear in Certificates issued through this CaPool.
     * If this is omitted, then this CaPool will not add restrictions on a certificate's identity.
     */
    identityConstraints?: outputs.PrivatecaCaPoolIssuancePolicyIdentityConstraints;
    /**
     * The maximum lifetime allowed for issued Certificates. Note that if the issuing CertificateAuthority
     * expires before a Certificate's requested maximumLifetime, the effective lifetime will be explicitly truncated to match it.
     */
    maximumLifetime?: string;
}

export interface PrivatecaCaPoolIssuancePolicyAllowedIssuanceModes {
    /**
     * When true, allows callers to create Certificates by specifying a CertificateConfig.
     */
    allowConfigBasedIssuance: boolean;
    /**
     * When true, allows callers to create Certificates by specifying a CSR.
     */
    allowCsrBasedIssuance: boolean;
}

export interface PrivatecaCaPoolIssuancePolicyAllowedKeyType {
    /**
     * Represents an allowed Elliptic Curve key type.
     */
    ellipticCurve?: outputs.PrivatecaCaPoolIssuancePolicyAllowedKeyTypeEllipticCurve;
    /**
     * Describes an RSA key that may be used in a Certificate issued from a CaPool.
     */
    rsa?: outputs.PrivatecaCaPoolIssuancePolicyAllowedKeyTypeRsa;
}

export interface PrivatecaCaPoolIssuancePolicyAllowedKeyTypeEllipticCurve {
    /**
     * The algorithm used. Possible values: ["ECDSA_P256", "ECDSA_P384", "EDDSA_25519"]
     */
    signatureAlgorithm: string;
}

export interface PrivatecaCaPoolIssuancePolicyAllowedKeyTypeRsa {
    /**
     * The maximum allowed RSA modulus size, in bits. If this is not set, or if set to zero, the
     * service will not enforce an explicit upper bound on RSA modulus sizes.
     */
    maxModulusSize?: string;
    /**
     * The minimum allowed RSA modulus size, in bits. If this is not set, or if set to zero, the
     * service-level min RSA modulus size will continue to apply.
     */
    minModulusSize?: string;
}

export interface PrivatecaCaPoolIssuancePolicyBaselineValues {
    /**
     * Specifies an X.509 extension, which may be used in different parts of X.509 objects like certificates, CSRs, and CRLs.
     */
    additionalExtensions?: outputs.PrivatecaCaPoolIssuancePolicyBaselineValuesAdditionalExtension[];
    /**
     * Describes Online Certificate Status Protocol (OCSP) endpoint addresses that appear in the
     * "Authority Information Access" extension in the certificate.
     */
    aiaOcspServers?: string[];
    /**
     * Describes values that are relevant in a CA certificate.
     */
    caOptions: outputs.PrivatecaCaPoolIssuancePolicyBaselineValuesCaOptions;
    /**
     * Indicates the intended use for keys that correspond to a certificate.
     */
    keyUsage: outputs.PrivatecaCaPoolIssuancePolicyBaselineValuesKeyUsage;
    /**
     * Describes the X.509 name constraints extension.
     */
    nameConstraints?: outputs.PrivatecaCaPoolIssuancePolicyBaselineValuesNameConstraints;
    /**
     * Describes the X.509 certificate policy object identifiers, per https://tools.ietf.org/html/rfc5280#section-4.2.1.4.
     */
    policyIds?: outputs.PrivatecaCaPoolIssuancePolicyBaselineValuesPolicyId[];
}

export interface PrivatecaCaPoolIssuancePolicyBaselineValuesAdditionalExtension {
    /**
     * Indicates whether or not this extension is critical (i.e., if the client does not know how to
     * handle this extension, the client should consider this to be an error).
     */
    critical: boolean;
    /**
     * Describes values that are relevant in a CA certificate.
     */
    objectId: outputs.PrivatecaCaPoolIssuancePolicyBaselineValuesAdditionalExtensionObjectId;
    /**
     * The value of this X.509 extension. A base64-encoded string.
     */
    value: string;
}

export interface PrivatecaCaPoolIssuancePolicyBaselineValuesAdditionalExtensionObjectId {
    /**
     * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
     */
    objectIdPaths: number[];
}

export interface PrivatecaCaPoolIssuancePolicyBaselineValuesCaOptions {
    /**
     * When true, the "CA" in Basic Constraints extension will be set to true.
     */
    isCa?: boolean;
    /**
     * Refers to the "path length constraint" in Basic Constraints extension. For a CA certificate, this value describes the depth of
     * subordinate CA certificates that are allowed. If this value is less than 0, the request will fail.
     */
    maxIssuerPathLength?: number;
    /**
     * When true, the "CA" in Basic Constraints extension will be set to false.
     * If both 'is_ca' and 'non_ca' are unset, the extension will be omitted from the CA certificate.
     */
    nonCa?: boolean;
    /**
     * When true, the "path length constraint" in Basic Constraints extension will be set to 0.
     * if both 'max_issuer_path_length' and 'zero_max_issuer_path_length' are unset,
     * the max path length will be omitted from the CA certificate.
     */
    zeroMaxIssuerPathLength?: boolean;
}

export interface PrivatecaCaPoolIssuancePolicyBaselineValuesKeyUsage {
    /**
     * Describes high-level ways in which a key may be used.
     */
    baseKeyUsage: outputs.PrivatecaCaPoolIssuancePolicyBaselineValuesKeyUsageBaseKeyUsage;
    /**
     * Describes high-level ways in which a key may be used.
     */
    extendedKeyUsage: outputs.PrivatecaCaPoolIssuancePolicyBaselineValuesKeyUsageExtendedKeyUsage;
    /**
     * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
     */
    unknownExtendedKeyUsages?: outputs.PrivatecaCaPoolIssuancePolicyBaselineValuesKeyUsageUnknownExtendedKeyUsage[];
}

export interface PrivatecaCaPoolIssuancePolicyBaselineValuesKeyUsageBaseKeyUsage {
    /**
     * The key may be used to sign certificates.
     */
    certSign?: boolean;
    /**
     * The key may be used for cryptographic commitments. Note that this may also be referred to as "non-repudiation".
     */
    contentCommitment?: boolean;
    /**
     * The key may be used sign certificate revocation lists.
     */
    crlSign?: boolean;
    /**
     * The key may be used to encipher data.
     */
    dataEncipherment?: boolean;
    /**
     * The key may be used to decipher only.
     */
    decipherOnly?: boolean;
    /**
     * The key may be used for digital signatures.
     */
    digitalSignature?: boolean;
    /**
     * The key may be used to encipher only.
     */
    encipherOnly?: boolean;
    /**
     * The key may be used in a key agreement protocol.
     */
    keyAgreement?: boolean;
    /**
     * The key may be used to encipher other keys.
     */
    keyEncipherment?: boolean;
}

export interface PrivatecaCaPoolIssuancePolicyBaselineValuesKeyUsageExtendedKeyUsage {
    /**
     * Corresponds to OID 1.3.6.1.5.5.7.3.2. Officially described as "TLS WWW client authentication", though regularly used for non-WWW TLS.
     */
    clientAuth?: boolean;
    /**
     * Corresponds to OID 1.3.6.1.5.5.7.3.3. Officially described as "Signing of downloadable executable code client authentication".
     */
    codeSigning?: boolean;
    /**
     * Corresponds to OID 1.3.6.1.5.5.7.3.4. Officially described as "Email protection".
     */
    emailProtection?: boolean;
    /**
     * Corresponds to OID 1.3.6.1.5.5.7.3.9. Officially described as "Signing OCSP responses".
     */
    ocspSigning?: boolean;
    /**
     * Corresponds to OID 1.3.6.1.5.5.7.3.1. Officially described as "TLS WWW server authentication", though regularly used for non-WWW TLS.
     */
    serverAuth?: boolean;
    /**
     * Corresponds to OID 1.3.6.1.5.5.7.3.8. Officially described as "Binding the hash of an object to a time".
     */
    timeStamping?: boolean;
}

export interface PrivatecaCaPoolIssuancePolicyBaselineValuesKeyUsageUnknownExtendedKeyUsage {
    /**
     * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
     */
    objectIdPaths: number[];
}

export interface PrivatecaCaPoolIssuancePolicyBaselineValuesNameConstraints {
    /**
     * Indicates whether or not the name constraints are marked critical.
     */
    critical: boolean;
    /**
     * Contains excluded DNS names. Any DNS name that can be
     * constructed by simply adding zero or more labels to
     * the left-hand side of the name satisfies the name constraint.
     * For example, 'example.com', 'www.example.com', 'www.sub.example.com'
     * would satisfy 'example.com' while 'example1.com' does not.
     */
    excludedDnsNames?: string[];
    /**
     * Contains the excluded email addresses. The value can be a particular
     * email address, a hostname to indicate all email addresses on that host or
     * a domain with a leading period (e.g. '.example.com') to indicate
     * all email addresses in that domain.
     */
    excludedEmailAddresses?: string[];
    /**
     * Contains the excluded IP ranges. For IPv4 addresses, the ranges
     * are expressed using CIDR notation as specified in RFC 4632.
     * For IPv6 addresses, the ranges are expressed in similar encoding as IPv4
     * addresses.
     */
    excludedIpRanges?: string[];
    /**
     * Contains the excluded URIs that apply to the host part of the name.
     * The value can be a hostname or a domain with a
     * leading period (like '.example.com')
     */
    excludedUris?: string[];
    /**
     * Contains permitted DNS names. Any DNS name that can be
     * constructed by simply adding zero or more labels to
     * the left-hand side of the name satisfies the name constraint.
     * For example, 'example.com', 'www.example.com', 'www.sub.example.com'
     * would satisfy 'example.com' while 'example1.com' does not.
     */
    permittedDnsNames?: string[];
    /**
     * Contains the permitted email addresses. The value can be a particular
     * email address, a hostname to indicate all email addresses on that host or
     * a domain with a leading period (e.g. '.example.com') to indicate
     * all email addresses in that domain.
     */
    permittedEmailAddresses?: string[];
    /**
     * Contains the permitted IP ranges. For IPv4 addresses, the ranges
     * are expressed using CIDR notation as specified in RFC 4632.
     * For IPv6 addresses, the ranges are expressed in similar encoding as IPv4
     * addresses.
     */
    permittedIpRanges?: string[];
    /**
     * Contains the permitted URIs that apply to the host part of the name.
     * The value can be a hostname or a domain with a
     * leading period (like '.example.com')
     */
    permittedUris?: string[];
}

export interface PrivatecaCaPoolIssuancePolicyBaselineValuesPolicyId {
    /**
     * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
     */
    objectIdPaths: number[];
}

export interface PrivatecaCaPoolIssuancePolicyIdentityConstraints {
    /**
     * If this is set, the SubjectAltNames extension may be copied from a certificate request into the signed certificate.
     * Otherwise, the requested SubjectAltNames will be discarded.
     */
    allowSubjectAltNamesPassthrough: boolean;
    /**
     * If this is set, the Subject field may be copied from a certificate request into the signed certificate.
     * Otherwise, the requested Subject will be discarded.
     */
    allowSubjectPassthrough: boolean;
    /**
     * A CEL expression that may be used to validate the resolved X.509 Subject and/or Subject Alternative Name before a
     * certificate is signed. To see the full allowed syntax and some examples,
     * see https://cloud.google.com/certificate-authority-service/docs/cel-guide
     */
    celExpression?: outputs.PrivatecaCaPoolIssuancePolicyIdentityConstraintsCelExpression;
}

export interface PrivatecaCaPoolIssuancePolicyIdentityConstraintsCelExpression {
    /**
     * Description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression: string;
    /**
     * String indicating the location of the expression for error reporting, e.g. a file name and a position in the file.
     */
    location?: string;
    /**
     * Title for the expression, i.e. a short string describing its purpose. This can be used e.g. in UIs which allow to enter the expression.
     */
    title?: string;
}

export interface PrivatecaCaPoolPublishingOptions {
    /**
     * Specifies the encoding format of each CertificateAuthority's CA
     * certificate and CRLs. If this is omitted, CA certificates and CRLs
     * will be published in PEM. Possible values: ["PEM", "DER"]
     */
    encodingFormat?: string;
    /**
     * When true, publishes each CertificateAuthority's CA certificate and includes its URL in the "Authority Information Access"
     * X.509 extension in all issued Certificates. If this is false, the CA certificate will not be published and the corresponding
     * X.509 extension will not be written in issued certificates.
     */
    publishCaCert: boolean;
    /**
     * When true, publishes each CertificateAuthority's CRL and includes its URL in the "CRL Distribution Points" X.509 extension
     * in all issued Certificates. If this is false, CRLs will not be published and the corresponding X.509 extension will not
     * be written in issued certificates. CRLs will expire 7 days from their creation. However, we will rebuild daily. CRLs are
     * also rebuilt shortly after a certificate is revoked.
     */
    publishCrl: boolean;
}

export interface PrivatecaCaPoolTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface PrivatecaCertificateAuthorityAccessUrl {
    caCertificateAccessUrl: string;
    crlAccessUrls: string[];
}

export interface PrivatecaCertificateAuthorityConfig {
    /**
     * Specifies some of the values in a certificate that are related to the subject.
     */
    subjectConfig: outputs.PrivatecaCertificateAuthorityConfigSubjectConfig;
    /**
     * When specified this provides a custom SKI to be used in the certificate. This should only be used to maintain a SKI of an existing CA originally created outside CA service, which was not generated using method (1) described in RFC 5280 section 4.2.1.2..
     */
    subjectKeyId?: outputs.PrivatecaCertificateAuthorityConfigSubjectKeyId;
    /**
     * Describes how some of the technical X.509 fields in a certificate should be populated.
     */
    x509Config: outputs.PrivatecaCertificateAuthorityConfigX509Config;
}

export interface PrivatecaCertificateAuthorityConfigSubjectConfig {
    /**
     * Contains distinguished name fields such as the location and organization.
     */
    subject: outputs.PrivatecaCertificateAuthorityConfigSubjectConfigSubject;
    /**
     * The subject alternative name fields.
     */
    subjectAltName?: outputs.PrivatecaCertificateAuthorityConfigSubjectConfigSubjectAltName;
}

export interface PrivatecaCertificateAuthorityConfigSubjectConfigSubject {
    /**
     * The common name of the distinguished name.
     */
    commonName: string;
    /**
     * The country code of the subject.
     */
    countryCode?: string;
    /**
     * The locality or city of the subject.
     */
    locality?: string;
    /**
     * The organization of the subject.
     */
    organization: string;
    /**
     * The organizational unit of the subject.
     */
    organizationalUnit?: string;
    /**
     * The postal code of the subject.
     */
    postalCode?: string;
    /**
     * The province, territory, or regional state of the subject.
     */
    province?: string;
    /**
     * The street address of the subject.
     */
    streetAddress?: string;
}

export interface PrivatecaCertificateAuthorityConfigSubjectConfigSubjectAltName {
    /**
     * Contains only valid, fully-qualified host names.
     */
    dnsNames?: string[];
    /**
     * Contains only valid RFC 2822 E-mail addresses.
     */
    emailAddresses?: string[];
    /**
     * Contains only valid 32-bit IPv4 addresses or RFC 4291 IPv6 addresses.
     */
    ipAddresses?: string[];
    /**
     * Contains only valid RFC 3986 URIs.
     */
    uris?: string[];
}

export interface PrivatecaCertificateAuthorityConfigSubjectKeyId {
    /**
     * The value of the KeyId in lowercase hexidecimal.
     */
    keyId?: string;
}

export interface PrivatecaCertificateAuthorityConfigX509Config {
    /**
     * Specifies an X.509 extension, which may be used in different parts of X.509 objects like certificates, CSRs, and CRLs.
     */
    additionalExtensions?: outputs.PrivatecaCertificateAuthorityConfigX509ConfigAdditionalExtension[];
    /**
     * Describes Online Certificate Status Protocol (OCSP) endpoint addresses that appear in the
     * "Authority Information Access" extension in the certificate.
     */
    aiaOcspServers?: string[];
    /**
     * Describes values that are relevant in a CA certificate.
     */
    caOptions: outputs.PrivatecaCertificateAuthorityConfigX509ConfigCaOptions;
    /**
     * Indicates the intended use for keys that correspond to a certificate.
     */
    keyUsage: outputs.PrivatecaCertificateAuthorityConfigX509ConfigKeyUsage;
    /**
     * Describes the X.509 name constraints extension.
     */
    nameConstraints?: outputs.PrivatecaCertificateAuthorityConfigX509ConfigNameConstraints;
    /**
     * Describes the X.509 certificate policy object identifiers, per https://tools.ietf.org/html/rfc5280#section-4.2.1.4.
     */
    policyIds?: outputs.PrivatecaCertificateAuthorityConfigX509ConfigPolicyId[];
}

export interface PrivatecaCertificateAuthorityConfigX509ConfigAdditionalExtension {
    /**
     * Indicates whether or not this extension is critical (i.e., if the client does not know how to
     * handle this extension, the client should consider this to be an error).
     */
    critical: boolean;
    /**
     * Describes values that are relevant in a CA certificate.
     */
    objectId: outputs.PrivatecaCertificateAuthorityConfigX509ConfigAdditionalExtensionObjectId;
    /**
     * The value of this X.509 extension. A base64-encoded string.
     */
    value: string;
}

export interface PrivatecaCertificateAuthorityConfigX509ConfigAdditionalExtensionObjectId {
    /**
     * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
     */
    objectIdPaths: number[];
}

export interface PrivatecaCertificateAuthorityConfigX509ConfigCaOptions {
    /**
     * When true, the "CA" in Basic Constraints extension will be set to true.
     */
    isCa: boolean;
    /**
     * Refers to the "path length constraint" in Basic Constraints extension. For a CA certificate, this value describes the depth of
     * subordinate CA certificates that are allowed. If this value is less than 0, the request will fail. Setting the value to 0
     * requires setting 'zero_max_issuer_path_length = true'.
     */
    maxIssuerPathLength?: number;
    /**
     * When true, the "CA" in Basic Constraints extension will be set to false.
     * If both 'is_ca' and 'non_ca' are unset, the extension will be omitted from the CA certificate.
     */
    nonCa?: boolean;
    /**
     * When true, the "path length constraint" in Basic Constraints extension will be set to 0.
     * If both 'max_issuer_path_length' and 'zero_max_issuer_path_length' are unset,
     * the max path length will be omitted from the CA certificate.
     */
    zeroMaxIssuerPathLength?: boolean;
}

export interface PrivatecaCertificateAuthorityConfigX509ConfigKeyUsage {
    /**
     * Describes high-level ways in which a key may be used.
     */
    baseKeyUsage: outputs.PrivatecaCertificateAuthorityConfigX509ConfigKeyUsageBaseKeyUsage;
    /**
     * Describes high-level ways in which a key may be used.
     */
    extendedKeyUsage: outputs.PrivatecaCertificateAuthorityConfigX509ConfigKeyUsageExtendedKeyUsage;
    /**
     * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
     */
    unknownExtendedKeyUsages?: outputs.PrivatecaCertificateAuthorityConfigX509ConfigKeyUsageUnknownExtendedKeyUsage[];
}

export interface PrivatecaCertificateAuthorityConfigX509ConfigKeyUsageBaseKeyUsage {
    /**
     * The key may be used to sign certificates.
     */
    certSign?: boolean;
    /**
     * The key may be used for cryptographic commitments. Note that this may also be referred to as "non-repudiation".
     */
    contentCommitment?: boolean;
    /**
     * The key may be used sign certificate revocation lists.
     */
    crlSign?: boolean;
    /**
     * The key may be used to encipher data.
     */
    dataEncipherment?: boolean;
    /**
     * The key may be used to decipher only.
     */
    decipherOnly?: boolean;
    /**
     * The key may be used for digital signatures.
     */
    digitalSignature?: boolean;
    /**
     * The key may be used to encipher only.
     */
    encipherOnly?: boolean;
    /**
     * The key may be used in a key agreement protocol.
     */
    keyAgreement?: boolean;
    /**
     * The key may be used to encipher other keys.
     */
    keyEncipherment?: boolean;
}

export interface PrivatecaCertificateAuthorityConfigX509ConfigKeyUsageExtendedKeyUsage {
    /**
     * Corresponds to OID 1.3.6.1.5.5.7.3.2. Officially described as "TLS WWW client authentication", though regularly used for non-WWW TLS.
     */
    clientAuth?: boolean;
    /**
     * Corresponds to OID 1.3.6.1.5.5.7.3.3. Officially described as "Signing of downloadable executable code client authentication".
     */
    codeSigning?: boolean;
    /**
     * Corresponds to OID 1.3.6.1.5.5.7.3.4. Officially described as "Email protection".
     */
    emailProtection?: boolean;
    /**
     * Corresponds to OID 1.3.6.1.5.5.7.3.9. Officially described as "Signing OCSP responses".
     */
    ocspSigning?: boolean;
    /**
     * Corresponds to OID 1.3.6.1.5.5.7.3.1. Officially described as "TLS WWW server authentication", though regularly used for non-WWW TLS.
     */
    serverAuth?: boolean;
    /**
     * Corresponds to OID 1.3.6.1.5.5.7.3.8. Officially described as "Binding the hash of an object to a time".
     */
    timeStamping?: boolean;
}

export interface PrivatecaCertificateAuthorityConfigX509ConfigKeyUsageUnknownExtendedKeyUsage {
    /**
     * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
     */
    objectIdPaths: number[];
}

export interface PrivatecaCertificateAuthorityConfigX509ConfigNameConstraints {
    /**
     * Indicates whether or not the name constraints are marked critical.
     */
    critical: boolean;
    /**
     * Contains excluded DNS names. Any DNS name that can be
     * constructed by simply adding zero or more labels to
     * the left-hand side of the name satisfies the name constraint.
     * For example, 'example.com', 'www.example.com', 'www.sub.example.com'
     * would satisfy 'example.com' while 'example1.com' does not.
     */
    excludedDnsNames?: string[];
    /**
     * Contains the excluded email addresses. The value can be a particular
     * email address, a hostname to indicate all email addresses on that host or
     * a domain with a leading period (e.g. '.example.com') to indicate
     * all email addresses in that domain.
     */
    excludedEmailAddresses?: string[];
    /**
     * Contains the excluded IP ranges. For IPv4 addresses, the ranges
     * are expressed using CIDR notation as specified in RFC 4632.
     * For IPv6 addresses, the ranges are expressed in similar encoding as IPv4
     * addresses.
     */
    excludedIpRanges?: string[];
    /**
     * Contains the excluded URIs that apply to the host part of the name.
     * The value can be a hostname or a domain with a
     * leading period (like '.example.com')
     */
    excludedUris?: string[];
    /**
     * Contains permitted DNS names. Any DNS name that can be
     * constructed by simply adding zero or more labels to
     * the left-hand side of the name satisfies the name constraint.
     * For example, 'example.com', 'www.example.com', 'www.sub.example.com'
     * would satisfy 'example.com' while 'example1.com' does not.
     */
    permittedDnsNames?: string[];
    /**
     * Contains the permitted email addresses. The value can be a particular
     * email address, a hostname to indicate all email addresses on that host or
     * a domain with a leading period (e.g. '.example.com') to indicate
     * all email addresses in that domain.
     */
    permittedEmailAddresses?: string[];
    /**
     * Contains the permitted IP ranges. For IPv4 addresses, the ranges
     * are expressed using CIDR notation as specified in RFC 4632.
     * For IPv6 addresses, the ranges are expressed in similar encoding as IPv4
     * addresses.
     */
    permittedIpRanges?: string[];
    /**
     * Contains the permitted URIs that apply to the host part of the name.
     * The value can be a hostname or a domain with a
     * leading period (like '.example.com')
     */
    permittedUris?: string[];
}

export interface PrivatecaCertificateAuthorityConfigX509ConfigPolicyId {
    /**
     * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
     */
    objectIdPaths: number[];
}

export interface PrivatecaCertificateAuthorityKeySpec {
    /**
     * The algorithm to use for creating a managed Cloud KMS key for a for a simplified
     * experience. All managed keys will be have their ProtectionLevel as HSM. Possible values: ["SIGN_HASH_ALGORITHM_UNSPECIFIED", "RSA_PSS_2048_SHA256", "RSA_PSS_3072_SHA256", "RSA_PSS_4096_SHA256", "RSA_PKCS1_2048_SHA256", "RSA_PKCS1_3072_SHA256", "RSA_PKCS1_4096_SHA256", "EC_P256_SHA256", "EC_P384_SHA384"]
     */
    algorithm?: string;
    /**
     * The resource name for an existing Cloud KMS CryptoKeyVersion in the format
     * 'projects/*&#47;locations/*&#47;keyRings/*&#47;cryptoKeys/*&#47;cryptoKeyVersions/*'.
     */
    cloudKmsKeyVersion?: string;
}

export interface PrivatecaCertificateAuthoritySubordinateConfig {
    /**
     * This can refer to a CertificateAuthority that was used to create a
     * subordinate CertificateAuthority. This field is used for information
     * and usability purposes only. The resource name is in the format
     * 'projects/*&#47;locations/*&#47;caPools/*&#47;certificateAuthorities/*'.
     */
    certificateAuthority?: string;
    /**
     * Contains the PEM certificate chain for the issuers of this CertificateAuthority,
     * but not pem certificate for this CA itself.
     */
    pemIssuerChain?: outputs.PrivatecaCertificateAuthoritySubordinateConfigPemIssuerChain;
}

export interface PrivatecaCertificateAuthoritySubordinateConfigPemIssuerChain {
    /**
     * Expected to be in leaf-to-root order according to RFC 5246.
     */
    pemCertificates?: string[];
}

export interface PrivatecaCertificateAuthorityTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface PrivatecaCertificateCertificateDescription {
    aiaIssuingCertificateUrls: string[];
    authorityKeyIds: outputs.PrivatecaCertificateCertificateDescriptionAuthorityKeyId[];
    certFingerprints: outputs.PrivatecaCertificateCertificateDescriptionCertFingerprint[];
    crlDistributionPoints: string[];
    publicKeys: outputs.PrivatecaCertificateCertificateDescriptionPublicKey[];
    subjectDescriptions: outputs.PrivatecaCertificateCertificateDescriptionSubjectDescription[];
    subjectKeyIds: outputs.PrivatecaCertificateCertificateDescriptionSubjectKeyId[];
    x509Descriptions: outputs.PrivatecaCertificateCertificateDescriptionX509Description[];
}

export interface PrivatecaCertificateCertificateDescriptionAuthorityKeyId {
    keyId: string;
}

export interface PrivatecaCertificateCertificateDescriptionCertFingerprint {
    sha256Hash: string;
}

export interface PrivatecaCertificateCertificateDescriptionPublicKey {
    format: string;
    key: string;
}

export interface PrivatecaCertificateCertificateDescriptionSubjectDescription {
    hexSerialNumber: string;
    lifetime: string;
    notAfterTime: string;
    notBeforeTime: string;
    subjectAltNames: outputs.PrivatecaCertificateCertificateDescriptionSubjectDescriptionSubjectAltName[];
    subjects: outputs.PrivatecaCertificateCertificateDescriptionSubjectDescriptionSubject[];
}

export interface PrivatecaCertificateCertificateDescriptionSubjectDescriptionSubject {
    commonName: string;
    countryCode: string;
    locality: string;
    organization: string;
    organizationalUnit: string;
    postalCode: string;
    province: string;
    streetAddress: string;
}

export interface PrivatecaCertificateCertificateDescriptionSubjectDescriptionSubjectAltName {
    customSans: outputs.PrivatecaCertificateCertificateDescriptionSubjectDescriptionSubjectAltNameCustomSan[];
    dnsNames: string[];
    emailAddresses: string[];
    ipAddresses: string[];
    uris: string[];
}

export interface PrivatecaCertificateCertificateDescriptionSubjectDescriptionSubjectAltNameCustomSan {
    critical: boolean;
    obectIds: outputs.PrivatecaCertificateCertificateDescriptionSubjectDescriptionSubjectAltNameCustomSanObectId[];
    value: string;
}

export interface PrivatecaCertificateCertificateDescriptionSubjectDescriptionSubjectAltNameCustomSanObectId {
    objectIdPaths: number[];
}

export interface PrivatecaCertificateCertificateDescriptionSubjectKeyId {
    keyId: string;
}

export interface PrivatecaCertificateCertificateDescriptionX509Description {
    additionalExtensions: outputs.PrivatecaCertificateCertificateDescriptionX509DescriptionAdditionalExtension[];
    aiaOcspServers: string[];
    caOptions: outputs.PrivatecaCertificateCertificateDescriptionX509DescriptionCaOption[];
    keyUsages: outputs.PrivatecaCertificateCertificateDescriptionX509DescriptionKeyUsage[];
    nameConstraints: outputs.PrivatecaCertificateCertificateDescriptionX509DescriptionNameConstraint[];
    policyIds: outputs.PrivatecaCertificateCertificateDescriptionX509DescriptionPolicyId[];
}

export interface PrivatecaCertificateCertificateDescriptionX509DescriptionAdditionalExtension {
    critical: boolean;
    objectIds: outputs.PrivatecaCertificateCertificateDescriptionX509DescriptionAdditionalExtensionObjectId[];
    value: string;
}

export interface PrivatecaCertificateCertificateDescriptionX509DescriptionAdditionalExtensionObjectId {
    objectIdPaths: number[];
}

export interface PrivatecaCertificateCertificateDescriptionX509DescriptionCaOption {
    isCa: boolean;
    maxIssuerPathLength: number;
}

export interface PrivatecaCertificateCertificateDescriptionX509DescriptionKeyUsage {
    baseKeyUsages: outputs.PrivatecaCertificateCertificateDescriptionX509DescriptionKeyUsageBaseKeyUsage[];
    extendedKeyUsages: outputs.PrivatecaCertificateCertificateDescriptionX509DescriptionKeyUsageExtendedKeyUsage[];
    unknownExtendedKeyUsages: outputs.PrivatecaCertificateCertificateDescriptionX509DescriptionKeyUsageUnknownExtendedKeyUsage[];
}

export interface PrivatecaCertificateCertificateDescriptionX509DescriptionKeyUsageBaseKeyUsage {
    certSign: boolean;
    contentCommitment: boolean;
    crlSign: boolean;
    dataEncipherment: boolean;
    decipherOnly: boolean;
    digitalSignature: boolean;
    encipherOnly: boolean;
    keyAgreement: boolean;
    keyEncipherment: boolean;
}

export interface PrivatecaCertificateCertificateDescriptionX509DescriptionKeyUsageExtendedKeyUsage {
    clientAuth: boolean;
    codeSigning: boolean;
    emailProtection: boolean;
    ocspSigning: boolean;
    serverAuth: boolean;
    timeStamping: boolean;
}

export interface PrivatecaCertificateCertificateDescriptionX509DescriptionKeyUsageUnknownExtendedKeyUsage {
    objectIdPaths: number[];
}

export interface PrivatecaCertificateCertificateDescriptionX509DescriptionNameConstraint {
    critical: boolean;
    excludedDnsNames: string[];
    excludedEmailAddresses: string[];
    excludedIpRanges: string[];
    excludedUris: string[];
    permittedDnsNames: string[];
    permittedEmailAddresses: string[];
    permittedIpRanges: string[];
    permittedUris: string[];
}

export interface PrivatecaCertificateCertificateDescriptionX509DescriptionPolicyId {
    objectIdPaths: number[];
}

export interface PrivatecaCertificateConfig {
    /**
     * A PublicKey describes a public key.
     */
    publicKey: outputs.PrivatecaCertificateConfigPublicKey;
    /**
     * Specifies some of the values in a certificate that are related to the subject.
     */
    subjectConfig: outputs.PrivatecaCertificateConfigSubjectConfig;
    /**
     * When specified this provides a custom SKI to be used in the certificate. This should only be used to maintain a SKI of an existing CA originally created outside CA service, which was not generated using method (1) described in RFC 5280 section 4.2.1.2..
     */
    subjectKeyId?: outputs.PrivatecaCertificateConfigSubjectKeyId;
    /**
     * Describes how some of the technical X.509 fields in a certificate should be populated.
     */
    x509Config: outputs.PrivatecaCertificateConfigX509Config;
}

export interface PrivatecaCertificateConfigPublicKey {
    /**
     * The format of the public key. Currently, only PEM format is supported. Possible values: ["KEY_TYPE_UNSPECIFIED", "PEM"]
     */
    format: string;
    /**
     * Required. A public key. When this is specified in a request, the padding and encoding can be any of the options described by the respective 'KeyType' value. When this is generated by the service, it will always be an RFC 5280 SubjectPublicKeyInfo structure containing an algorithm identifier and a key. A base64-encoded string.
     */
    key?: string;
}

export interface PrivatecaCertificateConfigSubjectConfig {
    /**
     * Contains distinguished name fields such as the location and organization.
     */
    subject: outputs.PrivatecaCertificateConfigSubjectConfigSubject;
    /**
     * The subject alternative name fields.
     */
    subjectAltName?: outputs.PrivatecaCertificateConfigSubjectConfigSubjectAltName;
}

export interface PrivatecaCertificateConfigSubjectConfigSubject {
    /**
     * The common name of the distinguished name.
     */
    commonName: string;
    /**
     * The country code of the subject.
     */
    countryCode?: string;
    /**
     * The locality or city of the subject.
     */
    locality?: string;
    /**
     * The organization of the subject.
     */
    organization: string;
    /**
     * The organizational unit of the subject.
     */
    organizationalUnit?: string;
    /**
     * The postal code of the subject.
     */
    postalCode?: string;
    /**
     * The province, territory, or regional state of the subject.
     */
    province?: string;
    /**
     * The street address of the subject.
     */
    streetAddress?: string;
}

export interface PrivatecaCertificateConfigSubjectConfigSubjectAltName {
    /**
     * Contains only valid, fully-qualified host names.
     */
    dnsNames?: string[];
    /**
     * Contains only valid RFC 2822 E-mail addresses.
     */
    emailAddresses?: string[];
    /**
     * Contains only valid 32-bit IPv4 addresses or RFC 4291 IPv6 addresses.
     */
    ipAddresses?: string[];
    /**
     * Contains only valid RFC 3986 URIs.
     */
    uris?: string[];
}

export interface PrivatecaCertificateConfigSubjectKeyId {
    /**
     * The value of the KeyId in lowercase hexidecimal.
     */
    keyId?: string;
}

export interface PrivatecaCertificateConfigX509Config {
    /**
     * Specifies an X.509 extension, which may be used in different parts of X.509 objects like certificates, CSRs, and CRLs.
     */
    additionalExtensions?: outputs.PrivatecaCertificateConfigX509ConfigAdditionalExtension[];
    /**
     * Describes Online Certificate Status Protocol (OCSP) endpoint addresses that appear in the
     * "Authority Information Access" extension in the certificate.
     */
    aiaOcspServers?: string[];
    /**
     * Describes values that are relevant in a CA certificate.
     */
    caOptions?: outputs.PrivatecaCertificateConfigX509ConfigCaOptions;
    /**
     * Indicates the intended use for keys that correspond to a certificate.
     */
    keyUsage: outputs.PrivatecaCertificateConfigX509ConfigKeyUsage;
    /**
     * Describes the X.509 name constraints extension.
     */
    nameConstraints?: outputs.PrivatecaCertificateConfigX509ConfigNameConstraints;
    /**
     * Describes the X.509 certificate policy object identifiers, per https://tools.ietf.org/html/rfc5280#section-4.2.1.4.
     */
    policyIds?: outputs.PrivatecaCertificateConfigX509ConfigPolicyId[];
}

export interface PrivatecaCertificateConfigX509ConfigAdditionalExtension {
    /**
     * Indicates whether or not this extension is critical (i.e., if the client does not know how to
     * handle this extension, the client should consider this to be an error).
     */
    critical: boolean;
    /**
     * Describes values that are relevant in a CA certificate.
     */
    objectId: outputs.PrivatecaCertificateConfigX509ConfigAdditionalExtensionObjectId;
    /**
     * The value of this X.509 extension. A base64-encoded string.
     */
    value: string;
}

export interface PrivatecaCertificateConfigX509ConfigAdditionalExtensionObjectId {
    /**
     * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
     */
    objectIdPaths: number[];
}

export interface PrivatecaCertificateConfigX509ConfigCaOptions {
    /**
     * When true, the "CA" in Basic Constraints extension will be set to true.
     */
    isCa?: boolean;
    /**
     * Refers to the "path length constraint" in Basic Constraints extension. For a CA certificate, this value describes the depth of
     * subordinate CA certificates that are allowed. If this value is less than 0, the request will fail.
     */
    maxIssuerPathLength?: number;
    /**
     * When true, the "CA" in Basic Constraints extension will be set to false.
     * If both 'is_ca' and 'non_ca' are unset, the extension will be omitted from the CA certificate.
     */
    nonCa?: boolean;
    /**
     * When true, the "path length constraint" in Basic Constraints extension will be set to 0.
     * if both 'max_issuer_path_length' and 'zero_max_issuer_path_length' are unset,
     * the max path length will be omitted from the CA certificate.
     */
    zeroMaxIssuerPathLength?: boolean;
}

export interface PrivatecaCertificateConfigX509ConfigKeyUsage {
    /**
     * Describes high-level ways in which a key may be used.
     */
    baseKeyUsage: outputs.PrivatecaCertificateConfigX509ConfigKeyUsageBaseKeyUsage;
    /**
     * Describes high-level ways in which a key may be used.
     */
    extendedKeyUsage: outputs.PrivatecaCertificateConfigX509ConfigKeyUsageExtendedKeyUsage;
    /**
     * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
     */
    unknownExtendedKeyUsages?: outputs.PrivatecaCertificateConfigX509ConfigKeyUsageUnknownExtendedKeyUsage[];
}

export interface PrivatecaCertificateConfigX509ConfigKeyUsageBaseKeyUsage {
    /**
     * The key may be used to sign certificates.
     */
    certSign?: boolean;
    /**
     * The key may be used for cryptographic commitments. Note that this may also be referred to as "non-repudiation".
     */
    contentCommitment?: boolean;
    /**
     * The key may be used sign certificate revocation lists.
     */
    crlSign?: boolean;
    /**
     * The key may be used to encipher data.
     */
    dataEncipherment?: boolean;
    /**
     * The key may be used to decipher only.
     */
    decipherOnly?: boolean;
    /**
     * The key may be used for digital signatures.
     */
    digitalSignature?: boolean;
    /**
     * The key may be used to encipher only.
     */
    encipherOnly?: boolean;
    /**
     * The key may be used in a key agreement protocol.
     */
    keyAgreement?: boolean;
    /**
     * The key may be used to encipher other keys.
     */
    keyEncipherment?: boolean;
}

export interface PrivatecaCertificateConfigX509ConfigKeyUsageExtendedKeyUsage {
    /**
     * Corresponds to OID 1.3.6.1.5.5.7.3.2. Officially described as "TLS WWW client authentication", though regularly used for non-WWW TLS.
     */
    clientAuth?: boolean;
    /**
     * Corresponds to OID 1.3.6.1.5.5.7.3.3. Officially described as "Signing of downloadable executable code client authentication".
     */
    codeSigning?: boolean;
    /**
     * Corresponds to OID 1.3.6.1.5.5.7.3.4. Officially described as "Email protection".
     */
    emailProtection?: boolean;
    /**
     * Corresponds to OID 1.3.6.1.5.5.7.3.9. Officially described as "Signing OCSP responses".
     */
    ocspSigning?: boolean;
    /**
     * Corresponds to OID 1.3.6.1.5.5.7.3.1. Officially described as "TLS WWW server authentication", though regularly used for non-WWW TLS.
     */
    serverAuth?: boolean;
    /**
     * Corresponds to OID 1.3.6.1.5.5.7.3.8. Officially described as "Binding the hash of an object to a time".
     */
    timeStamping?: boolean;
}

export interface PrivatecaCertificateConfigX509ConfigKeyUsageUnknownExtendedKeyUsage {
    /**
     * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
     */
    objectIdPaths: number[];
}

export interface PrivatecaCertificateConfigX509ConfigNameConstraints {
    /**
     * Indicates whether or not the name constraints are marked critical.
     */
    critical: boolean;
    /**
     * Contains excluded DNS names. Any DNS name that can be
     * constructed by simply adding zero or more labels to
     * the left-hand side of the name satisfies the name constraint.
     * For example, 'example.com', 'www.example.com', 'www.sub.example.com'
     * would satisfy 'example.com' while 'example1.com' does not.
     */
    excludedDnsNames?: string[];
    /**
     * Contains the excluded email addresses. The value can be a particular
     * email address, a hostname to indicate all email addresses on that host or
     * a domain with a leading period (e.g. '.example.com') to indicate
     * all email addresses in that domain.
     */
    excludedEmailAddresses?: string[];
    /**
     * Contains the excluded IP ranges. For IPv4 addresses, the ranges
     * are expressed using CIDR notation as specified in RFC 4632.
     * For IPv6 addresses, the ranges are expressed in similar encoding as IPv4
     * addresses.
     */
    excludedIpRanges?: string[];
    /**
     * Contains the excluded URIs that apply to the host part of the name.
     * The value can be a hostname or a domain with a
     * leading period (like '.example.com')
     */
    excludedUris?: string[];
    /**
     * Contains permitted DNS names. Any DNS name that can be
     * constructed by simply adding zero or more labels to
     * the left-hand side of the name satisfies the name constraint.
     * For example, 'example.com', 'www.example.com', 'www.sub.example.com'
     * would satisfy 'example.com' while 'example1.com' does not.
     */
    permittedDnsNames?: string[];
    /**
     * Contains the permitted email addresses. The value can be a particular
     * email address, a hostname to indicate all email addresses on that host or
     * a domain with a leading period (e.g. '.example.com') to indicate
     * all email addresses in that domain.
     */
    permittedEmailAddresses?: string[];
    /**
     * Contains the permitted IP ranges. For IPv4 addresses, the ranges
     * are expressed using CIDR notation as specified in RFC 4632.
     * For IPv6 addresses, the ranges are expressed in similar encoding as IPv4
     * addresses.
     */
    permittedIpRanges?: string[];
    /**
     * Contains the permitted URIs that apply to the host part of the name.
     * The value can be a hostname or a domain with a
     * leading period (like '.example.com')
     */
    permittedUris?: string[];
}

export interface PrivatecaCertificateConfigX509ConfigPolicyId {
    /**
     * An ObjectId specifies an object identifier (OID). These provide context and describe types in ASN.1 messages.
     */
    objectIdPaths: number[];
}

export interface PrivatecaCertificateRevocationDetail {
    revocationState: string;
    revocationTime: string;
}

export interface PrivatecaCertificateTemplateIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface PrivatecaCertificateTemplateIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface PrivatecaCertificateTemplateIdentityConstraints {
    /**
     * Required. If this is true, the SubjectAltNames extension may be copied from a certificate request into the signed certificate. Otherwise, the requested SubjectAltNames will be discarded.
     */
    allowSubjectAltNamesPassthrough: boolean;
    /**
     * Required. If this is true, the Subject field may be copied from a certificate request into the signed certificate. Otherwise, the requested Subject will be discarded.
     */
    allowSubjectPassthrough: boolean;
    /**
     * Optional. A CEL expression that may be used to validate the resolved X.509 Subject and/or Subject Alternative Name before a certificate is signed. To see the full allowed syntax and some examples, see https://cloud.google.com/certificate-authority-service/docs/using-cel
     */
    celExpression?: outputs.PrivatecaCertificateTemplateIdentityConstraintsCelExpression;
}

export interface PrivatecaCertificateTemplateIdentityConstraintsCelExpression {
    /**
     * Optional. Description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression?: string;
    /**
     * Optional. String indicating the location of the expression for error reporting, e.g. a file name and a position in the file.
     */
    location?: string;
    /**
     * Optional. Title for the expression, i.e. a short string describing its purpose. This can be used e.g. in UIs which allow to enter the expression.
     */
    title?: string;
}

export interface PrivatecaCertificateTemplatePassthroughExtensions {
    /**
     * Optional. A set of ObjectIds identifying custom X.509 extensions. Will be combined with known_extensions to determine the full set of X.509 extensions.
     */
    additionalExtensions?: outputs.PrivatecaCertificateTemplatePassthroughExtensionsAdditionalExtension[];
    /**
     * Optional. A set of named X.509 extensions. Will be combined with additional_extensions to determine the full set of X.509 extensions.
     */
    knownExtensions?: string[];
}

export interface PrivatecaCertificateTemplatePassthroughExtensionsAdditionalExtension {
    /**
     * Required. The parts of an OID path. The most significant parts of the path come first.
     */
    objectIdPaths: number[];
}

export interface PrivatecaCertificateTemplatePredefinedValues {
    /**
     * Optional. Describes custom X.509 extensions.
     */
    additionalExtensions?: outputs.PrivatecaCertificateTemplatePredefinedValuesAdditionalExtension[];
    /**
     * Optional. Describes Online Certificate Status Protocol (OCSP) endpoint addresses that appear in the "Authority Information Access" extension in the certificate.
     */
    aiaOcspServers?: string[];
    /**
     * Optional. Describes options in this X509Parameters that are relevant in a CA certificate.
     */
    caOptions?: outputs.PrivatecaCertificateTemplatePredefinedValuesCaOptions;
    /**
     * Optional. Indicates the intended use for keys that correspond to a certificate.
     */
    keyUsage?: outputs.PrivatecaCertificateTemplatePredefinedValuesKeyUsage;
    /**
     * Optional. Describes the X.509 certificate policy object identifiers, per https://tools.ietf.org/html/rfc5280#section-4.2.1.4.
     */
    policyIds?: outputs.PrivatecaCertificateTemplatePredefinedValuesPolicyId[];
}

export interface PrivatecaCertificateTemplatePredefinedValuesAdditionalExtension {
    /**
     * Optional. Indicates whether or not this extension is critical (i.e., if the client does not know how to handle this extension, the client should consider this to be an error).
     */
    critical?: boolean;
    /**
     * Required. The OID for this X.509 extension.
     */
    objectId: outputs.PrivatecaCertificateTemplatePredefinedValuesAdditionalExtensionObjectId;
    /**
     * Required. The value of this X.509 extension.
     */
    value: string;
}

export interface PrivatecaCertificateTemplatePredefinedValuesAdditionalExtensionObjectId {
    /**
     * Required. The parts of an OID path. The most significant parts of the path come first.
     */
    objectIdPaths: number[];
}

export interface PrivatecaCertificateTemplatePredefinedValuesCaOptions {
    /**
     * Optional. Refers to the "CA" X.509 extension, which is a boolean value. When this value is missing, the extension will be omitted from the CA certificate.
     */
    isCa?: boolean;
    /**
     * Optional. Refers to the path length restriction X.509 extension. For a CA certificate, this value describes the depth of subordinate CA certificates that are allowed. If this value is less than 0, the request will fail. If this value is missing, the max path length will be omitted from the CA certificate.
     */
    maxIssuerPathLength?: number;
}

export interface PrivatecaCertificateTemplatePredefinedValuesKeyUsage {
    /**
     * Describes high-level ways in which a key may be used.
     */
    baseKeyUsage?: outputs.PrivatecaCertificateTemplatePredefinedValuesKeyUsageBaseKeyUsage;
    /**
     * Detailed scenarios in which a key may be used.
     */
    extendedKeyUsage?: outputs.PrivatecaCertificateTemplatePredefinedValuesKeyUsageExtendedKeyUsage;
    /**
     * Used to describe extended key usages that are not listed in the KeyUsage.ExtendedKeyUsageOptions message.
     */
    unknownExtendedKeyUsages?: outputs.PrivatecaCertificateTemplatePredefinedValuesKeyUsageUnknownExtendedKeyUsage[];
}

export interface PrivatecaCertificateTemplatePredefinedValuesKeyUsageBaseKeyUsage {
    /**
     * The key may be used to sign certificates.
     */
    certSign?: boolean;
    /**
     * The key may be used for cryptographic commitments. Note that this may also be referred to as "non-repudiation".
     */
    contentCommitment?: boolean;
    /**
     * The key may be used sign certificate revocation lists.
     */
    crlSign?: boolean;
    /**
     * The key may be used to encipher data.
     */
    dataEncipherment?: boolean;
    /**
     * The key may be used to decipher only.
     */
    decipherOnly?: boolean;
    /**
     * The key may be used for digital signatures.
     */
    digitalSignature?: boolean;
    /**
     * The key may be used to encipher only.
     */
    encipherOnly?: boolean;
    /**
     * The key may be used in a key agreement protocol.
     */
    keyAgreement?: boolean;
    /**
     * The key may be used to encipher other keys.
     */
    keyEncipherment?: boolean;
}

export interface PrivatecaCertificateTemplatePredefinedValuesKeyUsageExtendedKeyUsage {
    /**
     * Corresponds to OID 1.3.6.1.5.5.7.3.2. Officially described as "TLS WWW client authentication", though regularly used for non-WWW TLS.
     */
    clientAuth?: boolean;
    /**
     * Corresponds to OID 1.3.6.1.5.5.7.3.3. Officially described as "Signing of downloadable executable code client authentication".
     */
    codeSigning?: boolean;
    /**
     * Corresponds to OID 1.3.6.1.5.5.7.3.4. Officially described as "Email protection".
     */
    emailProtection?: boolean;
    /**
     * Corresponds to OID 1.3.6.1.5.5.7.3.9. Officially described as "Signing OCSP responses".
     */
    ocspSigning?: boolean;
    /**
     * Corresponds to OID 1.3.6.1.5.5.7.3.1. Officially described as "TLS WWW server authentication", though regularly used for non-WWW TLS.
     */
    serverAuth?: boolean;
    /**
     * Corresponds to OID 1.3.6.1.5.5.7.3.8. Officially described as "Binding the hash of an object to a time".
     */
    timeStamping?: boolean;
}

export interface PrivatecaCertificateTemplatePredefinedValuesKeyUsageUnknownExtendedKeyUsage {
    /**
     * Required. The parts of an OID path. The most significant parts of the path come first.
     */
    objectIdPaths: number[];
}

export interface PrivatecaCertificateTemplatePredefinedValuesPolicyId {
    /**
     * Required. The parts of an OID path. The most significant parts of the path come first.
     */
    objectIdPaths: number[];
}

export interface PrivatecaCertificateTemplateTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface PrivatecaCertificateTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface PrivilegedAccessManagerEntitlementAdditionalNotificationTargets {
    /**
     * Optional. Additional email addresses to be notified when a principal(requester) is granted access.
     */
    adminEmailRecipients?: string[];
    /**
     * Optional. Additional email address to be notified about an eligible entitlement.
     */
    requesterEmailRecipients?: string[];
}

export interface PrivilegedAccessManagerEntitlementApprovalWorkflow {
    /**
     * A manual approval workflow where users who are designated as approvers need to call the ApproveGrant/DenyGrant APIs for an Grant.
     * The workflow can consist of multiple serial steps where each step defines who can act as Approver in that step and how many of those users should approve before the workflow moves to the next step.
     * This can be used to create approval workflows such as
     * * Require an approval from any user in a group G.
     * * Require an approval from any k number of users from a Group G.
     * * Require an approval from any user in a group G and then from a user U. etc.
     * A single user might be part of 'approvers' ACL for multiple steps in this workflow but they can only approve once and that approval will only be considered to satisfy the approval step at which it was granted.
     */
    manualApprovals: outputs.PrivilegedAccessManagerEntitlementApprovalWorkflowManualApprovals;
}

export interface PrivilegedAccessManagerEntitlementApprovalWorkflowManualApprovals {
    /**
     * Optional. Do the approvers need to provide a justification for their actions?
     */
    requireApproverJustification?: boolean;
    /**
     * List of approval steps in this workflow. These steps would be followed in the specified order sequentially.  1 step is supported for now.
     */
    steps: outputs.PrivilegedAccessManagerEntitlementApprovalWorkflowManualApprovalsStep[];
}

export interface PrivilegedAccessManagerEntitlementApprovalWorkflowManualApprovalsStep {
    /**
     * How many users from the above list need to approve.
     * If there are not enough distinct users in the list above then the workflow
     * will indefinitely block. Should always be greater than 0. Currently 1 is the only
     * supported value.
     */
    approvalsNeeded?: number;
    /**
     * Optional. Additional email addresses to be notified when a grant is pending approval.
     */
    approverEmailRecipients?: string[];
    /**
     * The potential set of approvers in this step. This list should contain at only one entry.
     */
    approvers: outputs.PrivilegedAccessManagerEntitlementApprovalWorkflowManualApprovalsStepApprovers;
}

export interface PrivilegedAccessManagerEntitlementApprovalWorkflowManualApprovalsStepApprovers {
    /**
     * Users who are being allowed for the operation. Each entry should be a valid v1 IAM Principal Identifier. Format for these is documented at: https://cloud.google.com/iam/docs/principal-identifiers#v1
     */
    principals: string[];
}

export interface PrivilegedAccessManagerEntitlementEligibleUser {
    /**
     * Users who are being allowed for the operation. Each entry should be a valid v1 IAM Principal Identifier. Format for these is documented at "https://cloud.google.com/iam/docs/principal-identifiers#v1"
     */
    principals: string[];
}

export interface PrivilegedAccessManagerEntitlementPrivilegedAccess {
    /**
     * GcpIamAccess represents IAM based access control on a GCP resource. Refer to https://cloud.google.com/iam/docs to understand more about IAM.
     */
    gcpIamAccess: outputs.PrivilegedAccessManagerEntitlementPrivilegedAccessGcpIamAccess;
}

export interface PrivilegedAccessManagerEntitlementPrivilegedAccessGcpIamAccess {
    /**
     * Name of the resource.
     */
    resource: string;
    /**
     * The type of this resource.
     */
    resourceType: string;
    /**
     * Role bindings to be created on successful grant.
     */
    roleBindings: outputs.PrivilegedAccessManagerEntitlementPrivilegedAccessGcpIamAccessRoleBinding[];
}

export interface PrivilegedAccessManagerEntitlementPrivilegedAccessGcpIamAccessRoleBinding {
    /**
     * The expression field of the IAM condition to be associated with the role. If specified, a user with an active grant for this entitlement would be able to access the resource only if this condition evaluates to true for their request.
     * https://cloud.google.com/iam/docs/conditions-overview#attributes.
     */
    conditionExpression?: string;
    /**
     * IAM role to be granted. https://cloud.google.com/iam/docs/roles-overview.
     */
    role: string;
}

export interface PrivilegedAccessManagerEntitlementRequesterJustificationConfig {
    /**
     * The justification is not mandatory but can be provided in any of the supported formats.
     */
    notMandatory?: outputs.PrivilegedAccessManagerEntitlementRequesterJustificationConfigNotMandatory;
    /**
     * The requester has to provide a justification in the form of free flowing text.
     */
    unstructured?: outputs.PrivilegedAccessManagerEntitlementRequesterJustificationConfigUnstructured;
}

export interface PrivilegedAccessManagerEntitlementRequesterJustificationConfigNotMandatory {
}

export interface PrivilegedAccessManagerEntitlementRequesterJustificationConfigUnstructured {
}

export interface PrivilegedAccessManagerEntitlementTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ProjectAccessApprovalSettingsEnrolledService {
    /**
     * The product for which Access Approval will be enrolled. Allowed values are listed (case-sensitive):
     *   all
     *   appengine.googleapis.com
     *   bigquery.googleapis.com
     *   bigtable.googleapis.com
     *   cloudkms.googleapis.com
     *   compute.googleapis.com
     *   dataflow.googleapis.com
     *   iam.googleapis.com
     *   pubsub.googleapis.com
     *   storage.googleapis.com
     */
    cloudProduct: string;
    /**
     * The enrollment level of the service. Default value: "BLOCK_ALL" Possible values: ["BLOCK_ALL"]
     */
    enrollmentLevel?: string;
}

export interface ProjectAccessApprovalSettingsTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ProjectDefaultServiceAccountsTimeouts {
    create?: string;
    delete?: string;
    read?: string;
}

export interface ProjectIamAuditConfigAuditLogConfig {
    /**
     * Identities that do not cause logging for this type of permission. Each entry can have one of the following values:user:{emailid}: An email address that represents a specific Google account. For example, alice@gmail.com or joe@example.com. serviceAccount:{emailid}: An email address that represents a service account. For example, my-other-app@appspot.gserviceaccount.com. group:{emailid}: An email address that represents a Google group. For example, admins@example.com. domain:{domain}: A G Suite domain (primary, instead of alias) name that represents all the users of that domain. For example, google.com or example.com.
     */
    exemptedMembers?: string[];
    /**
     * Permission type for which logging is to be configured. Must be one of DATA_READ, DATA_WRITE, or ADMIN_READ.
     */
    logType: string;
}

export interface ProjectIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ProjectIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ProjectOrganizationPolicyBooleanPolicy {
    /**
     * If true, then the Policy is enforced. If false, then any configuration is acceptable.
     */
    enforced: boolean;
}

export interface ProjectOrganizationPolicyListPolicy {
    /**
     * One or the other must be set.
     */
    allow?: outputs.ProjectOrganizationPolicyListPolicyAllow;
    /**
     * One or the other must be set.
     */
    deny?: outputs.ProjectOrganizationPolicyListPolicyDeny;
    /**
     * If set to true, the values from the effective Policy of the parent resource are inherited, meaning the values set in this Policy are added to the values inherited up the hierarchy.
     */
    inheritFromParent?: boolean;
    /**
     * The Google Cloud Console will try to default to a configuration that matches the value specified in this field.
     */
    suggestedValue: string;
}

export interface ProjectOrganizationPolicyListPolicyAllow {
    /**
     * The policy allows or denies all values.
     */
    all?: boolean;
    /**
     * The policy can define specific values that are allowed or denied.
     */
    values?: string[];
}

export interface ProjectOrganizationPolicyListPolicyDeny {
    /**
     * The policy allows or denies all values.
     */
    all?: boolean;
    /**
     * The policy can define specific values that are allowed or denied.
     */
    values?: string[];
}

export interface ProjectOrganizationPolicyRestorePolicy {
    /**
     * May only be set to true. If set, then the default Policy is restored.
     */
    default: boolean;
}

export interface ProjectOrganizationPolicyTimeouts {
    create?: string;
    delete?: string;
    read?: string;
    update?: string;
}

export interface ProjectServiceTimeouts {
    create?: string;
    delete?: string;
    read?: string;
    update?: string;
}

export interface ProjectTimeouts {
    create?: string;
    delete?: string;
    read?: string;
    update?: string;
}

export interface ProjectUsageExportBucketTimeouts {
    create?: string;
    delete?: string;
}

export interface PublicCaExternalAccountKeyTimeouts {
    create?: string;
    delete?: string;
}

export interface PubsubLiteReservationTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface PubsubLiteSubscriptionDeliveryConfig {
    /**
     * When this subscription should send messages to subscribers relative to messages persistence in storage. Possible values: ["DELIVER_IMMEDIATELY", "DELIVER_AFTER_STORED", "DELIVERY_REQUIREMENT_UNSPECIFIED"]
     */
    deliveryRequirement: string;
}

export interface PubsubLiteSubscriptionTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface PubsubLiteTopicPartitionConfig {
    /**
     * The capacity configuration.
     */
    capacity?: outputs.PubsubLiteTopicPartitionConfigCapacity;
    /**
     * The number of partitions in the topic. Must be at least 1.
     */
    count: number;
}

export interface PubsubLiteTopicPartitionConfigCapacity {
    /**
     * Subscribe throughput capacity per partition in MiB/s. Must be >= 4 and <= 16.
     */
    publishMibPerSec: number;
    /**
     * Publish throughput capacity per partition in MiB/s. Must be >= 4 and <= 16.
     */
    subscribeMibPerSec: number;
}

export interface PubsubLiteTopicReservationConfig {
    /**
     * The Reservation to use for this topic's throughput capacity.
     */
    throughputReservation?: string;
}

export interface PubsubLiteTopicRetentionConfig {
    /**
     * The provisioned storage, in bytes, per partition. If the number of bytes stored
     * in any of the topic's partitions grows beyond this value, older messages will be
     * dropped to make room for newer ones, regardless of the value of period.
     */
    perPartitionBytes: string;
    /**
     * How long a published message is retained. If unset, messages will be retained as
     * long as the bytes retained for each partition is below perPartitionBytes. A
     * duration in seconds with up to nine fractional digits, terminated by 's'.
     * Example: "3.5s".
     */
    period?: string;
}

export interface PubsubLiteTopicTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface PubsubSchemaIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface PubsubSchemaIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface PubsubSchemaTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface PubsubSubscriptionBigqueryConfig {
    /**
     * When true and use_topic_schema or use_table_schema is true, any fields that are a part of the topic schema or message schema that
     * are not part of the BigQuery table schema are dropped when writing to BigQuery. Otherwise, the schemas must be kept in sync
     * and any messages with extra fields are not written and remain in the subscription's backlog.
     */
    dropUnknownFields?: boolean;
    /**
     * The service account to use to write to BigQuery. If not specified, the Pub/Sub
     * [service agent](https://cloud.google.com/iam/docs/service-agents),
     * service-{project_number}@gcp-sa-pubsub.iam.gserviceaccount.com, is used.
     */
    serviceAccountEmail?: string;
    /**
     * The name of the table to which to write data, of the form {projectId}.{datasetId}.{tableId}
     */
    table: string;
    /**
     * When true, use the BigQuery table's schema as the columns to write to in BigQuery. Messages
     * must be published in JSON format. Only one of use_topic_schema and use_table_schema can be set.
     */
    useTableSchema?: boolean;
    /**
     * When true, use the topic's schema as the columns to write to in BigQuery, if it exists.
     * Only one of use_topic_schema and use_table_schema can be set.
     */
    useTopicSchema?: boolean;
    /**
     * When true, write the subscription name, messageId, publishTime, attributes, and orderingKey to additional columns in the table.
     * The subscription name, messageId, and publishTime fields are put in their own columns while all other message properties (other than data) are written to a JSON object in the attributes column.
     */
    writeMetadata?: boolean;
}

export interface PubsubSubscriptionCloudStorageConfig {
    /**
     * If set, message data will be written to Cloud Storage in Avro format.
     */
    avroConfig?: outputs.PubsubSubscriptionCloudStorageConfigAvroConfig;
    /**
     * User-provided name for the Cloud Storage bucket. The bucket must be created by the user. The bucket name must be without any prefix like "gs://".
     */
    bucket: string;
    /**
     * User-provided format string specifying how to represent datetimes in Cloud Storage filenames.
     */
    filenameDatetimeFormat?: string;
    /**
     * User-provided prefix for Cloud Storage filename.
     */
    filenamePrefix?: string;
    /**
     * User-provided suffix for Cloud Storage filename. Must not end in "/".
     */
    filenameSuffix?: string;
    /**
     * The maximum bytes that can be written to a Cloud Storage file before a new file is created. Min 1 KB, max 10 GiB.
     * The maxBytes limit may be exceeded in cases where messages are larger than the limit.
     */
    maxBytes?: number;
    /**
     * The maximum duration that can elapse before a new Cloud Storage file is created. Min 1 minute, max 10 minutes, default 5 minutes.
     * May not exceed the subscription's acknowledgement deadline.
     * A duration in seconds with up to nine fractional digits, ending with 's'. Example: "3.5s".
     */
    maxDuration?: string;
    /**
     * The service account to use to write to Cloud Storage. If not specified, the Pub/Sub
     * [service agent](https://cloud.google.com/iam/docs/service-agents),
     * service-{project_number}@gcp-sa-pubsub.iam.gserviceaccount.com, is used.
     */
    serviceAccountEmail?: string;
    /**
     * An output-only field that indicates whether or not the subscription can receive messages.
     */
    state: string;
}

export interface PubsubSubscriptionCloudStorageConfigAvroConfig {
    /**
     * When true, write the subscription name, messageId, publishTime, attributes, and orderingKey as additional fields in the output.
     */
    writeMetadata?: boolean;
}

export interface PubsubSubscriptionDeadLetterPolicy {
    /**
     * The name of the topic to which dead letter messages should be published.
     * Format is 'projects/{project}/topics/{topic}'.
     *
     * The Cloud Pub/Sub service account associated with the enclosing subscription's
     * parent project (i.e.,
     * service-{project_number}@gcp-sa-pubsub.iam.gserviceaccount.com) must have
     * permission to Publish() to this topic.
     *
     * The operation will fail if the topic does not exist.
     * Users should ensure that there is a subscription attached to this topic
     * since messages published to a topic with no subscriptions are lost.
     */
    deadLetterTopic?: string;
    /**
     * The maximum number of delivery attempts for any message. The value must be
     * between 5 and 100.
     *
     * The number of delivery attempts is defined as 1 + (the sum of number of
     * NACKs and number of times the acknowledgement deadline has been exceeded for the message).
     *
     * A NACK is any call to ModifyAckDeadline with a 0 deadline. Note that
     * client libraries may automatically extend ack_deadlines.
     *
     * This field will be honored on a best effort basis.
     *
     * If this parameter is 0, a default value of 5 is used.
     */
    maxDeliveryAttempts?: number;
}

export interface PubsubSubscriptionExpirationPolicy {
    /**
     * Specifies the "time-to-live" duration for an associated resource. The
     * resource expires if it is not active for a period of ttl.
     * If ttl is set to "", the associated resource never expires.
     * A duration in seconds with up to nine fractional digits, terminated by 's'.
     * Example - "3.5s".
     */
    ttl: string;
}

export interface PubsubSubscriptionIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface PubsubSubscriptionIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface PubsubSubscriptionPushConfig {
    /**
     * Endpoint configuration attributes.
     *
     * Every endpoint has a set of API supported attributes that can
     * be used to control different aspects of the message delivery.
     *
     * The currently supported attribute is x-goog-version, which you
     * can use to change the format of the pushed message. This
     * attribute indicates the version of the data expected by
     * the endpoint. This controls the shape of the pushed message
     * (i.e., its fields and metadata). The endpoint version is
     * based on the version of the Pub/Sub API.
     *
     * If not present during the subscriptions.create call,
     * it will default to the version of the API used to make
     * such call. If not present during a subscriptions.modifyPushConfig
     * call, its value will not be changed. subscriptions.get
     * calls will always return a valid version, even if the
     * subscription was created without this attribute.
     *
     * The possible values for this attribute are:
     *
     * - v1beta1: uses the push format defined in the v1beta1 Pub/Sub API.
     * - v1 or v1beta2: uses the push format defined in the v1 Pub/Sub API.
     */
    attributes?: {[key: string]: string};
    /**
     * When set, the payload to the push endpoint is not wrapped.Sets the
     * 'data' field as the HTTP body for delivery.
     */
    noWrapper?: outputs.PubsubSubscriptionPushConfigNoWrapper;
    /**
     * If specified, Pub/Sub will generate and attach an OIDC JWT token as
     * an Authorization header in the HTTP request for every pushed message.
     */
    oidcToken?: outputs.PubsubSubscriptionPushConfigOidcToken;
    /**
     * A URL locating the endpoint to which messages should be pushed.
     * For example, a Webhook endpoint might use
     * "https://example.com/push".
     */
    pushEndpoint: string;
}

export interface PubsubSubscriptionPushConfigNoWrapper {
    /**
     * When true, writes the Pub/Sub message metadata to
     * 'x-goog-pubsub-<KEY>:<VAL>' headers of the HTTP request. Writes the
     * Pub/Sub message attributes to '<KEY>:<VAL>' headers of the HTTP request.
     */
    writeMetadata: boolean;
}

export interface PubsubSubscriptionPushConfigOidcToken {
    /**
     * Audience to be used when generating OIDC token. The audience claim
     * identifies the recipients that the JWT is intended for. The audience
     * value is a single case-sensitive string. Having multiple values (array)
     * for the audience field is not supported. More info about the OIDC JWT
     * token audience here: https://tools.ietf.org/html/rfc7519#section-4.1.3
     * Note: if not specified, the Push endpoint URL will be used.
     */
    audience?: string;
    /**
     * Service account email to be used for generating the OIDC token.
     * The caller (for subscriptions.create, subscriptions.patch, and
     * subscriptions.modifyPushConfig RPCs) must have the
     * iam.serviceAccounts.actAs permission for the service account.
     */
    serviceAccountEmail: string;
}

export interface PubsubSubscriptionRetryPolicy {
    /**
     * The maximum delay between consecutive deliveries of a given message. Value should be between 0 and 600 seconds. Defaults to 600 seconds.
     * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
     */
    maximumBackoff: string;
    /**
     * The minimum delay between consecutive deliveries of a given message. Value should be between 0 and 600 seconds. Defaults to 10 seconds.
     * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
     */
    minimumBackoff: string;
}

export interface PubsubSubscriptionTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface PubsubTopicIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface PubsubTopicIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface PubsubTopicIngestionDataSourceSettings {
    /**
     * Settings for ingestion from Amazon Kinesis Data Streams.
     */
    awsKinesis?: outputs.PubsubTopicIngestionDataSourceSettingsAwsKinesis;
}

export interface PubsubTopicIngestionDataSourceSettingsAwsKinesis {
    /**
     * AWS role ARN to be used for Federated Identity authentication with
     * Kinesis. Check the Pub/Sub docs for how to set up this role and the
     * required permissions that need to be attached to it.
     */
    awsRoleArn: string;
    /**
     * The Kinesis consumer ARN to used for ingestion in
     * Enhanced Fan-Out mode. The consumer must be already
     * created and ready to be used.
     */
    consumerArn: string;
    /**
     * The GCP service account to be used for Federated Identity authentication
     * with Kinesis (via a 'AssumeRoleWithWebIdentity' call for the provided
     * role). The 'awsRoleArn' must be set up with 'accounts.google.com:sub'
     * equals to this service account number.
     */
    gcpServiceAccount: string;
    /**
     * The Kinesis stream ARN to ingest data from.
     */
    streamArn: string;
}

export interface PubsubTopicMessageStoragePolicy {
    /**
     * A list of IDs of GCP regions where messages that are published to
     * the topic may be persisted in storage. Messages published by
     * publishers running in non-allowed GCP regions (or running outside
     * of GCP altogether) will be routed for storage in one of the
     * allowed regions. An empty list means that no regions are allowed,
     * and is not a valid configuration.
     */
    allowedPersistenceRegions: string[];
}

export interface PubsubTopicSchemaSettings {
    /**
     * The encoding of messages validated against schema. Default value: "ENCODING_UNSPECIFIED" Possible values: ["ENCODING_UNSPECIFIED", "JSON", "BINARY"]
     */
    encoding?: string;
    /**
     * The name of the schema that messages published should be
     * validated against. Format is projects/{project}/schemas/{schema}.
     * The value of this field will be _deleted-schema_
     * if the schema has been deleted.
     */
    schema: string;
}

export interface PubsubTopicTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface RecaptchaEnterpriseKeyAndroidSettings {
    /**
     * If set to true, it means allowed_package_names will not be enforced.
     */
    allowAllPackageNames?: boolean;
    /**
     * Android package names of apps allowed to use the key. Example: 'com.companyname.appname'
     */
    allowedPackageNames?: string[];
}

export interface RecaptchaEnterpriseKeyIosSettings {
    /**
     * If set to true, it means allowed_bundle_ids will not be enforced.
     */
    allowAllBundleIds?: boolean;
    /**
     * iOS bundle ids of apps allowed to use the key. Example: 'com.companyname.productname.appname'
     */
    allowedBundleIds?: string[];
}

export interface RecaptchaEnterpriseKeyTestingOptions {
    /**
     * For challenge-based keys only (CHECKBOX, INVISIBLE), all challenge requests for this site will return nocaptcha if NOCAPTCHA, or an unsolvable challenge if UNSOLVABLE_CHALLENGE. Possible values: TESTING_CHALLENGE_UNSPECIFIED, NOCAPTCHA, UNSOLVABLE_CHALLENGE
     */
    testingChallenge: string;
    /**
     * All assessments for this Key will return this score. Must be between 0 (likely not legitimate) and 1 (likely legitimate) inclusive.
     */
    testingScore?: number;
}

export interface RecaptchaEnterpriseKeyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface RecaptchaEnterpriseKeyWafSettings {
    /**
     * Supported WAF features. For more information, see https://cloud.google.com/recaptcha-enterprise/docs/usecase#comparison_of_features. Possible values: CHALLENGE_PAGE, SESSION_TOKEN, ACTION_TOKEN, EXPRESS
     */
    wafFeature: string;
    /**
     * The WAF service that uses this key. Possible values: CA, FASTLY
     */
    wafService: string;
}

export interface RecaptchaEnterpriseKeyWebSettings {
    /**
     * If set to true, it means allowed_domains will not be enforced.
     */
    allowAllDomains?: boolean;
    /**
     * If set to true, the key can be used on AMP (Accelerated Mobile Pages) websites. This is supported only for the SCORE integration type.
     */
    allowAmpTraffic?: boolean;
    /**
     * Domains or subdomains of websites allowed to use the key. All subdomains of an allowed domain are automatically allowed. A valid domain requires a host and must not include any path, port, query or fragment. Examples: 'example.com' or 'subdomain.example.com'
     */
    allowedDomains?: string[];
    /**
     * Settings for the frequency and difficulty at which this key triggers captcha challenges. This should only be specified for IntegrationTypes CHECKBOX and INVISIBLE. Possible values: CHALLENGE_SECURITY_PREFERENCE_UNSPECIFIED, USABILITY, BALANCE, SECURITY
     */
    challengeSecurityPreference: string;
    /**
     * Required. Describes how this key is integrated with the website. Possible values: SCORE, CHECKBOX, INVISIBLE
     */
    integrationType: string;
}

export interface RedisClusterDiscoveryEndpoint {
    address: string;
    port: number;
    pscConfigs: outputs.RedisClusterDiscoveryEndpointPscConfig[];
}

export interface RedisClusterDiscoveryEndpointPscConfig {
    network: string;
}

export interface RedisClusterPscConfig {
    /**
     * Required. The consumer network where the network address of
     * the discovery endpoint will be reserved, in the form of
     * projects/{network_project_id_or_number}/global/networks/{network_id}.
     */
    network: string;
}

export interface RedisClusterPscConnection {
    address: string;
    forwardingRule: string;
    network: string;
    projectId: string;
    pscConnectionId: string;
}

export interface RedisClusterStateInfo {
    updateInfos: outputs.RedisClusterStateInfoUpdateInfo[];
}

export interface RedisClusterStateInfoUpdateInfo {
    targetReplicaCount: number;
    targetShardCount: number;
}

export interface RedisClusterTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface RedisClusterZoneDistributionConfig {
    /**
     * Immutable. The mode for zone distribution for Memorystore Redis cluster.
     * If not provided, MULTI_ZONE will be used as default Possible values: ["MULTI_ZONE", "SINGLE_ZONE"]
     */
    mode: string;
    /**
     * Immutable. The zone for single zone Memorystore Redis cluster.
     */
    zone?: string;
}

export interface RedisInstanceMaintenancePolicy {
    /**
     * Output only. The time when the policy was created.
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond
     * resolution and up to nine fractional digits.
     */
    createTime: string;
    /**
     * Optional. Description of what this policy is for.
     * Create/Update methods return INVALID_ARGUMENT if the
     * length is greater than 512.
     */
    description?: string;
    /**
     * Output only. The time when the policy was last updated.
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond
     * resolution and up to nine fractional digits.
     */
    updateTime: string;
    /**
     * Optional. Maintenance window that is applied to resources covered by this policy.
     * Minimum 1. For the current version, the maximum number
     * of weekly_window is expected to be one.
     */
    weeklyMaintenanceWindows?: outputs.RedisInstanceMaintenancePolicyWeeklyMaintenanceWindow[];
}

export interface RedisInstanceMaintenancePolicyWeeklyMaintenanceWindow {
    /**
     * Required. The day of week that maintenance updates occur.
     *
     * - DAY_OF_WEEK_UNSPECIFIED: The day of the week is unspecified.
     * - MONDAY: Monday
     * - TUESDAY: Tuesday
     * - WEDNESDAY: Wednesday
     * - THURSDAY: Thursday
     * - FRIDAY: Friday
     * - SATURDAY: Saturday
     * - SUNDAY: Sunday Possible values: ["DAY_OF_WEEK_UNSPECIFIED", "MONDAY", "TUESDAY", "WEDNESDAY", "THURSDAY", "FRIDAY", "SATURDAY", "SUNDAY"]
     */
    day: string;
    /**
     * Output only. Duration of the maintenance window.
     * The current window is fixed at 1 hour.
     * A duration in seconds with up to nine fractional digits,
     * terminated by 's'. Example: "3.5s".
     */
    duration: string;
    /**
     * Required. Start time of the window in UTC time.
     */
    startTime: outputs.RedisInstanceMaintenancePolicyWeeklyMaintenanceWindowStartTime;
}

export interface RedisInstanceMaintenancePolicyWeeklyMaintenanceWindowStartTime {
    /**
     * Hours of day in 24 hour format. Should be from 0 to 23.
     * An API may choose to allow the value "24:00:00" for scenarios like business closing time.
     */
    hours?: number;
    /**
     * Minutes of hour of day. Must be from 0 to 59.
     */
    minutes?: number;
    /**
     * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
     */
    nanos?: number;
    /**
     * Seconds of minutes of the time. Must normally be from 0 to 59.
     * An API may allow the value 60 if it allows leap-seconds.
     */
    seconds?: number;
}

export interface RedisInstanceMaintenanceSchedule {
    endTime: string;
    scheduleDeadlineTime: string;
    startTime: string;
}

export interface RedisInstanceNode {
    id: string;
    zone: string;
}

export interface RedisInstancePersistenceConfig {
    /**
     * Optional. Controls whether Persistence features are enabled. If not provided, the existing value will be used.
     *
     * - DISABLED: 	Persistence is disabled for the instance, and any existing snapshots are deleted.
     * - RDB: RDB based Persistence is enabled. Possible values: ["DISABLED", "RDB"]
     */
    persistenceMode: string;
    /**
     * Output only. The next time that a snapshot attempt is scheduled to occur.
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up
     * to nine fractional digits.
     * Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
     */
    rdbNextSnapshotTime: string;
    /**
     * Optional. Available snapshot periods for scheduling.
     *
     * - ONE_HOUR:	Snapshot every 1 hour.
     * - SIX_HOURS:	Snapshot every 6 hours.
     * - TWELVE_HOURS:	Snapshot every 12 hours.
     * - TWENTY_FOUR_HOURS:	Snapshot every 24 hours. Possible values: ["ONE_HOUR", "SIX_HOURS", "TWELVE_HOURS", "TWENTY_FOUR_HOURS"]
     */
    rdbSnapshotPeriod?: string;
    /**
     * Optional. Date and time that the first snapshot was/will be attempted,
     * and to which future snapshots will be aligned. If not provided,
     * the current time will be used.
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution
     * and up to nine fractional digits.
     * Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
     */
    rdbSnapshotStartTime: string;
}

export interface RedisInstanceServerCaCert {
    cert: string;
    createTime: string;
    expireTime: string;
    serialNumber: string;
    sha1Fingerprint: string;
}

export interface RedisInstanceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ResourceManagerLienTimeouts {
    create?: string;
    delete?: string;
}

export interface SccEventThreatDetectionCustomModuleTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SccFolderCustomModuleCustomConfig {
    /**
     * Custom output properties.
     */
    customOutput?: outputs.SccFolderCustomModuleCustomConfigCustomOutput;
    /**
     * Text that describes the vulnerability or misconfiguration that the custom
     * module detects. This explanation is returned with each finding instance to
     * help investigators understand the detected issue. The text must be enclosed in quotation marks.
     */
    description?: string;
    /**
     * The CEL expression to evaluate to produce findings. When the expression evaluates
     * to true against a resource, a finding is generated.
     */
    predicate: outputs.SccFolderCustomModuleCustomConfigPredicate;
    /**
     * An explanation of the recommended steps that security teams can take to resolve
     * the detected issue. This explanation is returned with each finding generated by
     * this module in the nextSteps property of the finding JSON.
     */
    recommendation: string;
    /**
     * The resource types that the custom module operates on. Each custom module
     * can specify up to 5 resource types.
     */
    resourceSelector: outputs.SccFolderCustomModuleCustomConfigResourceSelector;
    /**
     * The severity to assign to findings generated by the module. Possible values: ["CRITICAL", "HIGH", "MEDIUM", "LOW"]
     */
    severity: string;
}

export interface SccFolderCustomModuleCustomConfigCustomOutput {
    /**
     * A list of custom output properties to add to the finding.
     */
    properties?: outputs.SccFolderCustomModuleCustomConfigCustomOutputProperty[];
}

export interface SccFolderCustomModuleCustomConfigCustomOutputProperty {
    /**
     * Name of the property for the custom output.
     */
    name?: string;
    /**
     * The CEL expression for the custom output. A resource property can be specified
     * to return the value of the property or a text string enclosed in quotation marks.
     */
    valueExpression?: outputs.SccFolderCustomModuleCustomConfigCustomOutputPropertyValueExpression;
}

export interface SccFolderCustomModuleCustomConfigCustomOutputPropertyValueExpression {
    /**
     * Description of the expression. This is a longer text which describes the
     * expression, e.g. when hovered over it in a UI.
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression: string;
    /**
     * String indicating the location of the expression for error reporting, e.g. a
     * file name and a position in the file.
     */
    location?: string;
    /**
     * Title for the expression, i.e. a short string describing its purpose. This can
     * be used e.g. in UIs which allow to enter the expression.
     */
    title?: string;
}

export interface SccFolderCustomModuleCustomConfigPredicate {
    /**
     * Description of the expression. This is a longer text which describes the
     * expression, e.g. when hovered over it in a UI.
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression: string;
    /**
     * String indicating the location of the expression for error reporting, e.g. a
     * file name and a position in the file.
     */
    location?: string;
    /**
     * Title for the expression, i.e. a short string describing its purpose. This can
     * be used e.g. in UIs which allow to enter the expression.
     */
    title?: string;
}

export interface SccFolderCustomModuleCustomConfigResourceSelector {
    /**
     * The resource types to run the detector on.
     */
    resourceTypes: string[];
}

export interface SccFolderCustomModuleTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SccFolderNotificationConfigStreamingConfig {
    /**
     * Expression that defines the filter to apply across create/update
     * events of assets or findings as specified by the event type. The
     * expression is a list of zero or more restrictions combined via
     * logical operators AND and OR. Parentheses are supported, and OR
     * has higher precedence than AND.
     *
     * Restrictions have the form <field> <operator> <value> and may have
     * a - character in front of them to indicate negation. The fields
     * map to those defined in the corresponding resource.
     *
     * The supported operators are:
     *
     * * = for all value types.
     * * >, <, >=, <= for integer values.
     * * :, meaning substring matching, for strings.
     *
     * The supported value types are:
     *
     * * string literals in quotes.
     * * integer literals without quotes.
     * * boolean literals true and false without quotes.
     *
     * See
     * [Filtering notifications](https://cloud.google.com/security-command-center/docs/how-to-api-filter-notifications)
     * for information on how to write a filter.
     */
    filter: string;
}

export interface SccFolderNotificationConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SccManagementFolderSecurityHealthAnalyticsCustomModuleCustomConfig {
    /**
     * Custom output properties.
     */
    customOutput?: outputs.SccManagementFolderSecurityHealthAnalyticsCustomModuleCustomConfigCustomOutput;
    /**
     * Text that describes the vulnerability or misconfiguration that the custom
     * module detects. This explanation is returned with each finding instance to
     * help investigators understand the detected issue. The text must be enclosed in quotation marks.
     */
    description?: string;
    /**
     * The CEL expression to evaluate to produce findings. When the expression evaluates
     * to true against a resource, a finding is generated.
     */
    predicate?: outputs.SccManagementFolderSecurityHealthAnalyticsCustomModuleCustomConfigPredicate;
    /**
     * An explanation of the recommended steps that security teams can take to resolve
     * the detected issue. This explanation is returned with each finding generated by
     * this module in the nextSteps property of the finding JSON.
     */
    recommendation?: string;
    /**
     * The resource types that the custom module operates on. Each custom module
     * can specify up to 5 resource types.
     */
    resourceSelector?: outputs.SccManagementFolderSecurityHealthAnalyticsCustomModuleCustomConfigResourceSelector;
    /**
     * The severity to assign to findings generated by the module. Possible values: ["CRITICAL", "HIGH", "MEDIUM", "LOW"]
     */
    severity?: string;
}

export interface SccManagementFolderSecurityHealthAnalyticsCustomModuleCustomConfigCustomOutput {
    /**
     * A list of custom output properties to add to the finding.
     */
    properties?: outputs.SccManagementFolderSecurityHealthAnalyticsCustomModuleCustomConfigCustomOutputProperty[];
}

export interface SccManagementFolderSecurityHealthAnalyticsCustomModuleCustomConfigCustomOutputProperty {
    /**
     * Name of the property for the custom output.
     */
    name?: string;
    /**
     * The CEL expression for the custom output. A resource property can be specified
     * to return the value of the property or a text string enclosed in quotation marks.
     */
    valueExpression?: outputs.SccManagementFolderSecurityHealthAnalyticsCustomModuleCustomConfigCustomOutputPropertyValueExpression;
}

export interface SccManagementFolderSecurityHealthAnalyticsCustomModuleCustomConfigCustomOutputPropertyValueExpression {
    /**
     * Description of the expression. This is a longer text which describes the
     * expression, e.g. when hovered over it in a UI.
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression: string;
    /**
     * String indicating the location of the expression for error reporting, e.g. a
     * file name and a position in the file.
     */
    location?: string;
    /**
     * Title for the expression, i.e. a short string describing its purpose. This can
     * be used e.g. in UIs which allow to enter the expression.
     */
    title?: string;
}

export interface SccManagementFolderSecurityHealthAnalyticsCustomModuleCustomConfigPredicate {
    /**
     * Description of the expression. This is a longer text which describes the
     * expression, e.g. when hovered over it in a UI.
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression: string;
    /**
     * String indicating the location of the expression for error reporting, e.g. a
     * file name and a position in the file.
     */
    location?: string;
    /**
     * Title for the expression, i.e. a short string describing its purpose. This can
     * be used e.g. in UIs which allow to enter the expression.
     */
    title?: string;
}

export interface SccManagementFolderSecurityHealthAnalyticsCustomModuleCustomConfigResourceSelector {
    /**
     * The resource types to run the detector on.
     */
    resourceTypes: string[];
}

export interface SccManagementFolderSecurityHealthAnalyticsCustomModuleTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SccManagementOrganizationEventThreatDetectionCustomModuleTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SccManagementOrganizationSecurityHealthAnalyticsCustomModuleCustomConfig {
    /**
     * Custom output properties.
     */
    customOutput?: outputs.SccManagementOrganizationSecurityHealthAnalyticsCustomModuleCustomConfigCustomOutput;
    /**
     * Text that describes the vulnerability or misconfiguration that the custom
     * module detects. This explanation is returned with each finding instance to
     * help investigators understand the detected issue. The text must be enclosed in quotation marks.
     */
    description?: string;
    /**
     * The CEL expression to evaluate to produce findings. When the expression evaluates
     * to true against a resource, a finding is generated.
     */
    predicate: outputs.SccManagementOrganizationSecurityHealthAnalyticsCustomModuleCustomConfigPredicate;
    /**
     * An explanation of the recommended steps that security teams can take to resolve
     * the detected issue. This explanation is returned with each finding generated by
     * this module in the nextSteps property of the finding JSON.
     */
    recommendation: string;
    /**
     * The resource types that the custom module operates on. Each custom module
     * can specify up to 5 resource types.
     */
    resourceSelector: outputs.SccManagementOrganizationSecurityHealthAnalyticsCustomModuleCustomConfigResourceSelector;
    /**
     * The severity to assign to findings generated by the module. Possible values: ["CRITICAL", "HIGH", "MEDIUM", "LOW"]
     */
    severity: string;
}

export interface SccManagementOrganizationSecurityHealthAnalyticsCustomModuleCustomConfigCustomOutput {
    /**
     * A list of custom output properties to add to the finding.
     */
    properties?: outputs.SccManagementOrganizationSecurityHealthAnalyticsCustomModuleCustomConfigCustomOutputProperty[];
}

export interface SccManagementOrganizationSecurityHealthAnalyticsCustomModuleCustomConfigCustomOutputProperty {
    /**
     * Name of the property for the custom output.
     */
    name?: string;
    /**
     * The CEL expression for the custom output. A resource property can be specified
     * to return the value of the property or a text string enclosed in quotation marks.
     */
    valueExpression?: outputs.SccManagementOrganizationSecurityHealthAnalyticsCustomModuleCustomConfigCustomOutputPropertyValueExpression;
}

export interface SccManagementOrganizationSecurityHealthAnalyticsCustomModuleCustomConfigCustomOutputPropertyValueExpression {
    /**
     * Description of the expression. This is a longer text which describes the
     * expression, e.g. when hovered over it in a UI.
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression: string;
    /**
     * String indicating the location of the expression for error reporting, e.g. a
     * file name and a position in the file.
     */
    location?: string;
    /**
     * Title for the expression, i.e. a short string describing its purpose. This can
     * be used e.g. in UIs which allow to enter the expression.
     */
    title?: string;
}

export interface SccManagementOrganizationSecurityHealthAnalyticsCustomModuleCustomConfigPredicate {
    /**
     * Description of the expression. This is a longer text which describes the
     * expression, e.g. when hovered over it in a UI.
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression: string;
    /**
     * String indicating the location of the expression for error reporting, e.g. a
     * file name and a position in the file.
     */
    location?: string;
    /**
     * Title for the expression, i.e. a short string describing its purpose. This can
     * be used e.g. in UIs which allow to enter the expression.
     */
    title?: string;
}

export interface SccManagementOrganizationSecurityHealthAnalyticsCustomModuleCustomConfigResourceSelector {
    /**
     * The resource types to run the detector on.
     */
    resourceTypes: string[];
}

export interface SccManagementOrganizationSecurityHealthAnalyticsCustomModuleTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SccManagementProjectSecurityHealthAnalyticsCustomModuleCustomConfig {
    /**
     * Custom output properties.
     */
    customOutput?: outputs.SccManagementProjectSecurityHealthAnalyticsCustomModuleCustomConfigCustomOutput;
    /**
     * Text that describes the vulnerability or misconfiguration that the custom
     * module detects. This explanation is returned with each finding instance to
     * help investigators understand the detected issue. The text must be enclosed in quotation marks.
     */
    description?: string;
    /**
     * The CEL expression to evaluate to produce findings. When the expression evaluates
     * to true against a resource, a finding is generated.
     */
    predicate: outputs.SccManagementProjectSecurityHealthAnalyticsCustomModuleCustomConfigPredicate;
    /**
     * An explanation of the recommended steps that security teams can take to resolve
     * the detected issue. This explanation is returned with each finding generated by
     * this module in the nextSteps property of the finding JSON.
     */
    recommendation: string;
    /**
     * The resource types that the custom module operates on. Each custom module
     * can specify up to 5 resource types.
     */
    resourceSelector: outputs.SccManagementProjectSecurityHealthAnalyticsCustomModuleCustomConfigResourceSelector;
    /**
     * The severity to assign to findings generated by the module. Possible values: ["CRITICAL", "HIGH", "MEDIUM", "LOW"]
     */
    severity: string;
}

export interface SccManagementProjectSecurityHealthAnalyticsCustomModuleCustomConfigCustomOutput {
    /**
     * A list of custom output properties to add to the finding.
     */
    properties?: outputs.SccManagementProjectSecurityHealthAnalyticsCustomModuleCustomConfigCustomOutputProperty[];
}

export interface SccManagementProjectSecurityHealthAnalyticsCustomModuleCustomConfigCustomOutputProperty {
    /**
     * Name of the property for the custom output.
     */
    name?: string;
    /**
     * The CEL expression for the custom output. A resource property can be specified
     * to return the value of the property or a text string enclosed in quotation marks.
     */
    valueExpression?: outputs.SccManagementProjectSecurityHealthAnalyticsCustomModuleCustomConfigCustomOutputPropertyValueExpression;
}

export interface SccManagementProjectSecurityHealthAnalyticsCustomModuleCustomConfigCustomOutputPropertyValueExpression {
    /**
     * Description of the expression. This is a longer text which describes the
     * expression, e.g. when hovered over it in a UI.
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression: string;
    /**
     * String indicating the location of the expression for error reporting, e.g. a
     * file name and a position in the file.
     */
    location?: string;
    /**
     * Title for the expression, i.e. a short string describing its purpose. This can
     * be used e.g. in UIs which allow to enter the expression.
     */
    title?: string;
}

export interface SccManagementProjectSecurityHealthAnalyticsCustomModuleCustomConfigPredicate {
    /**
     * Description of the expression. This is a longer text which describes the
     * expression, e.g. when hovered over it in a UI.
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression: string;
    /**
     * String indicating the location of the expression for error reporting, e.g. a
     * file name and a position in the file.
     */
    location?: string;
    /**
     * Title for the expression, i.e. a short string describing its purpose. This can
     * be used e.g. in UIs which allow to enter the expression.
     */
    title?: string;
}

export interface SccManagementProjectSecurityHealthAnalyticsCustomModuleCustomConfigResourceSelector {
    /**
     * The resource types to run the detector on.
     */
    resourceTypes: string[];
}

export interface SccManagementProjectSecurityHealthAnalyticsCustomModuleTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SccMuteConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SccNotificationConfigStreamingConfig {
    /**
     * Expression that defines the filter to apply across create/update
     * events of assets or findings as specified by the event type. The
     * expression is a list of zero or more restrictions combined via
     * logical operators AND and OR. Parentheses are supported, and OR
     * has higher precedence than AND.
     *
     * Restrictions have the form <field> <operator> <value> and may have
     * a - character in front of them to indicate negation. The fields
     * map to those defined in the corresponding resource.
     *
     * The supported operators are:
     *
     * * = for all value types.
     * * >, <, >=, <= for integer values.
     * * :, meaning substring matching, for strings.
     *
     * The supported value types are:
     *
     * * string literals in quotes.
     * * integer literals without quotes.
     * * boolean literals true and false without quotes.
     *
     * See
     * [Filtering notifications](https://cloud.google.com/security-command-center/docs/how-to-api-filter-notifications)
     * for information on how to write a filter.
     */
    filter: string;
}

export interface SccNotificationConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SccOrganizationCustomModuleCustomConfig {
    /**
     * Custom output properties.
     */
    customOutput?: outputs.SccOrganizationCustomModuleCustomConfigCustomOutput;
    /**
     * Text that describes the vulnerability or misconfiguration that the custom
     * module detects. This explanation is returned with each finding instance to
     * help investigators understand the detected issue. The text must be enclosed in quotation marks.
     */
    description?: string;
    /**
     * The CEL expression to evaluate to produce findings. When the expression evaluates
     * to true against a resource, a finding is generated.
     */
    predicate: outputs.SccOrganizationCustomModuleCustomConfigPredicate;
    /**
     * An explanation of the recommended steps that security teams can take to resolve
     * the detected issue. This explanation is returned with each finding generated by
     * this module in the nextSteps property of the finding JSON.
     */
    recommendation: string;
    /**
     * The resource types that the custom module operates on. Each custom module
     * can specify up to 5 resource types.
     */
    resourceSelector: outputs.SccOrganizationCustomModuleCustomConfigResourceSelector;
    /**
     * The severity to assign to findings generated by the module. Possible values: ["CRITICAL", "HIGH", "MEDIUM", "LOW"]
     */
    severity: string;
}

export interface SccOrganizationCustomModuleCustomConfigCustomOutput {
    /**
     * A list of custom output properties to add to the finding.
     */
    properties?: outputs.SccOrganizationCustomModuleCustomConfigCustomOutputProperty[];
}

export interface SccOrganizationCustomModuleCustomConfigCustomOutputProperty {
    /**
     * Name of the property for the custom output.
     */
    name?: string;
    /**
     * The CEL expression for the custom output. A resource property can be specified
     * to return the value of the property or a text string enclosed in quotation marks.
     */
    valueExpression?: outputs.SccOrganizationCustomModuleCustomConfigCustomOutputPropertyValueExpression;
}

export interface SccOrganizationCustomModuleCustomConfigCustomOutputPropertyValueExpression {
    /**
     * Description of the expression. This is a longer text which describes the
     * expression, e.g. when hovered over it in a UI.
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression: string;
    /**
     * String indicating the location of the expression for error reporting, e.g. a
     * file name and a position in the file.
     */
    location?: string;
    /**
     * Title for the expression, i.e. a short string describing its purpose. This can
     * be used e.g. in UIs which allow to enter the expression.
     */
    title?: string;
}

export interface SccOrganizationCustomModuleCustomConfigPredicate {
    /**
     * Description of the expression. This is a longer text which describes the
     * expression, e.g. when hovered over it in a UI.
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression: string;
    /**
     * String indicating the location of the expression for error reporting, e.g. a
     * file name and a position in the file.
     */
    location?: string;
    /**
     * Title for the expression, i.e. a short string describing its purpose. This can
     * be used e.g. in UIs which allow to enter the expression.
     */
    title?: string;
}

export interface SccOrganizationCustomModuleCustomConfigResourceSelector {
    /**
     * The resource types to run the detector on.
     */
    resourceTypes: string[];
}

export interface SccOrganizationCustomModuleTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SccProjectCustomModuleCustomConfig {
    /**
     * Custom output properties.
     */
    customOutput?: outputs.SccProjectCustomModuleCustomConfigCustomOutput;
    /**
     * Text that describes the vulnerability or misconfiguration that the custom
     * module detects. This explanation is returned with each finding instance to
     * help investigators understand the detected issue. The text must be enclosed in quotation marks.
     */
    description?: string;
    /**
     * The CEL expression to evaluate to produce findings. When the expression evaluates
     * to true against a resource, a finding is generated.
     */
    predicate: outputs.SccProjectCustomModuleCustomConfigPredicate;
    /**
     * An explanation of the recommended steps that security teams can take to resolve
     * the detected issue. This explanation is returned with each finding generated by
     * this module in the nextSteps property of the finding JSON.
     */
    recommendation: string;
    /**
     * The resource types that the custom module operates on. Each custom module
     * can specify up to 5 resource types.
     */
    resourceSelector: outputs.SccProjectCustomModuleCustomConfigResourceSelector;
    /**
     * The severity to assign to findings generated by the module. Possible values: ["CRITICAL", "HIGH", "MEDIUM", "LOW"]
     */
    severity: string;
}

export interface SccProjectCustomModuleCustomConfigCustomOutput {
    /**
     * A list of custom output properties to add to the finding.
     */
    properties?: outputs.SccProjectCustomModuleCustomConfigCustomOutputProperty[];
}

export interface SccProjectCustomModuleCustomConfigCustomOutputProperty {
    /**
     * Name of the property for the custom output.
     */
    name?: string;
    /**
     * The CEL expression for the custom output. A resource property can be specified
     * to return the value of the property or a text string enclosed in quotation marks.
     */
    valueExpression?: outputs.SccProjectCustomModuleCustomConfigCustomOutputPropertyValueExpression;
}

export interface SccProjectCustomModuleCustomConfigCustomOutputPropertyValueExpression {
    /**
     * Description of the expression. This is a longer text which describes the
     * expression, e.g. when hovered over it in a UI.
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression: string;
    /**
     * String indicating the location of the expression for error reporting, e.g. a
     * file name and a position in the file.
     */
    location?: string;
    /**
     * Title for the expression, i.e. a short string describing its purpose. This can
     * be used e.g. in UIs which allow to enter the expression.
     */
    title?: string;
}

export interface SccProjectCustomModuleCustomConfigPredicate {
    /**
     * Description of the expression. This is a longer text which describes the
     * expression, e.g. when hovered over it in a UI.
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression: string;
    /**
     * String indicating the location of the expression for error reporting, e.g. a
     * file name and a position in the file.
     */
    location?: string;
    /**
     * Title for the expression, i.e. a short string describing its purpose. This can
     * be used e.g. in UIs which allow to enter the expression.
     */
    title?: string;
}

export interface SccProjectCustomModuleCustomConfigResourceSelector {
    /**
     * The resource types to run the detector on.
     */
    resourceTypes: string[];
}

export interface SccProjectCustomModuleTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SccProjectNotificationConfigStreamingConfig {
    /**
     * Expression that defines the filter to apply across create/update
     * events of assets or findings as specified by the event type. The
     * expression is a list of zero or more restrictions combined via
     * logical operators AND and OR. Parentheses are supported, and OR
     * has higher precedence than AND.
     *
     * Restrictions have the form <field> <operator> <value> and may have
     * a - character in front of them to indicate negation. The fields
     * map to those defined in the corresponding resource.
     *
     * The supported operators are:
     *
     * * = for all value types.
     * * >, <, >=, <= for integer values.
     * * :, meaning substring matching, for strings.
     *
     * The supported value types are:
     *
     * * string literals in quotes.
     * * integer literals without quotes.
     * * boolean literals true and false without quotes.
     *
     * See
     * [Filtering notifications](https://cloud.google.com/security-command-center/docs/how-to-api-filter-notifications)
     * for information on how to write a filter.
     */
    filter: string;
}

export interface SccProjectNotificationConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SccSourceIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface SccSourceIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface SccSourceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SccV2FolderMuteConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SccV2FolderNotificationConfigStreamingConfig {
    /**
     * Expression that defines the filter to apply across create/update
     * events of assets or findings as specified by the event type. The
     * expression is a list of zero or more restrictions combined via
     * logical operators AND and OR. Parentheses are supported, and OR
     * has higher precedence than AND.
     *
     * Restrictions have the form <field> <operator> <value> and may have
     * a - character in front of them to indicate negation. The fields
     * map to those defined in the corresponding resource.
     *
     * The supported operators are:
     *
     * * = for all value types.
     * * >, <, >=, <= for integer values.
     * * :, meaning substring matching, for strings.
     *
     * The supported value types are:
     *
     * * string literals in quotes.
     * * integer literals without quotes.
     * * boolean literals true and false without quotes.
     *
     * See
     * [Filtering notifications](https://cloud.google.com/security-command-center/docs/how-to-api-filter-notifications)
     * for information on how to write a filter.
     */
    filter: string;
}

export interface SccV2FolderNotificationConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SccV2OrganizationMuteConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SccV2OrganizationNotificationConfigStreamingConfig {
    /**
     * Expression that defines the filter to apply across create/update
     * events of assets or findings as specified by the event type. The
     * expression is a list of zero or more restrictions combined via
     * logical operators AND and OR. Parentheses are supported, and OR
     * has higher precedence than AND.
     *
     * Restrictions have the form <field> <operator> <value> and may have
     * a - character in front of them to indicate negation. The fields
     * map to those defined in the corresponding resource.
     *
     * The supported operators are:
     *
     * * = for all value types.
     * * >, <, >=, <= for integer values.
     * * :, meaning substring matching, for strings.
     *
     * The supported value types are:
     *
     * * string literals in quotes.
     * * integer literals without quotes.
     * * boolean literals true and false without quotes.
     *
     * See
     * [Filtering notifications](https://cloud.google.com/security-command-center/docs/how-to-api-filter-notifications)
     * for information on how to write a filter.
     */
    filter: string;
}

export interface SccV2OrganizationNotificationConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SccV2OrganizationSccBigQueryExportsTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SccV2OrganizationSourceIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface SccV2OrganizationSourceIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface SccV2OrganizationSourceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SccV2ProjectMuteConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SccV2ProjectNotificationConfigStreamingConfig {
    /**
     * Expression that defines the filter to apply across create/update
     * events of assets or findings as specified by the event type. The
     * expression is a list of zero or more restrictions combined via
     * logical operators AND and OR. Parentheses are supported, and OR
     * has higher precedence than AND.
     *
     * Restrictions have the form <field> <operator> <value> and may have
     * a - character in front of them to indicate negation. The fields
     * map to those defined in the corresponding resource.
     *
     * The supported operators are:
     *
     * * = for all value types.
     * * >, <, >=, <= for integer values.
     * * :, meaning substring matching, for strings.
     *
     * The supported value types are:
     *
     * * string literals in quotes.
     * * integer literals without quotes.
     * * boolean literals true and false without quotes.
     *
     * See
     * [Filtering notifications](https://cloud.google.com/security-command-center/docs/how-to-api-filter-notifications)
     * for information on how to write a filter.
     */
    filter: string;
}

export interface SccV2ProjectNotificationConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SecretManagerSecretIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface SecretManagerSecretIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface SecretManagerSecretReplication {
    /**
     * The Secret will automatically be replicated without any restrictions.
     */
    auto?: outputs.SecretManagerSecretReplicationAuto;
    /**
     * The Secret will be replicated to the regions specified by the user.
     */
    userManaged?: outputs.SecretManagerSecretReplicationUserManaged;
}

export interface SecretManagerSecretReplicationAuto {
    /**
     * The customer-managed encryption configuration of the Secret.
     * If no configuration is provided, Google-managed default
     * encryption is used.
     */
    customerManagedEncryption?: outputs.SecretManagerSecretReplicationAutoCustomerManagedEncryption;
}

export interface SecretManagerSecretReplicationAutoCustomerManagedEncryption {
    /**
     * The resource name of the Cloud KMS CryptoKey used to encrypt secret payloads.
     */
    kmsKeyName: string;
}

export interface SecretManagerSecretReplicationUserManaged {
    /**
     * The list of Replicas for this Secret. Cannot be empty.
     */
    replicas: outputs.SecretManagerSecretReplicationUserManagedReplica[];
}

export interface SecretManagerSecretReplicationUserManagedReplica {
    /**
     * Customer Managed Encryption for the secret.
     */
    customerManagedEncryption?: outputs.SecretManagerSecretReplicationUserManagedReplicaCustomerManagedEncryption;
    /**
     * The canonical IDs of the location to replicate data. For example: "us-east1".
     */
    location: string;
}

export interface SecretManagerSecretReplicationUserManagedReplicaCustomerManagedEncryption {
    /**
     * Describes the Cloud KMS encryption key that will be used to protect destination secret.
     */
    kmsKeyName: string;
}

export interface SecretManagerSecretRotation {
    /**
     * Timestamp in UTC at which the Secret is scheduled to rotate.
     * A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
     */
    nextRotationTime?: string;
    /**
     * The Duration between rotation notifications. Must be in seconds and at least 3600s (1h) and at most 3153600000s (100 years).
     * If rotationPeriod is set, 'next_rotation_time' must be set. 'next_rotation_time' will be advanced by this period when the service automatically sends rotation notifications.
     */
    rotationPeriod?: string;
}

export interface SecretManagerSecretTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SecretManagerSecretTopic {
    /**
     * The resource name of the Pub/Sub topic that will be published to, in the following format: projects/*&#47;topics/*.
     * For publication to succeed, the Secret Manager Service Agent service account must have pubsub.publisher permissions on the topic.
     */
    name: string;
}

export interface SecretManagerSecretVersionTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SecureSourceManagerInstanceHostConfig {
    api: string;
    gitHttp: string;
    gitSsh: string;
    html: string;
}

export interface SecureSourceManagerInstanceIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface SecureSourceManagerInstanceIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface SecureSourceManagerInstancePrivateConfig {
    /**
     * CA pool resource, resource must in the format of 'projects/{project}/locations/{location}/caPools/{ca_pool}'.
     */
    caPool: string;
    /**
     * Service Attachment for HTTP, resource is in the format of 'projects/{project}/regions/{region}/serviceAttachments/{service_attachment}'.
     */
    httpServiceAttachment: string;
    /**
     * 'Indicate if it's private instance.'
     */
    isPrivate: boolean;
    /**
     * Service Attachment for SSH, resource is in the format of 'projects/{project}/regions/{region}/serviceAttachments/{service_attachment}'.
     */
    sshServiceAttachment: string;
}

export interface SecureSourceManagerInstanceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SecureSourceManagerRepositoryIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface SecureSourceManagerRepositoryIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface SecureSourceManagerRepositoryInitialConfig {
    /**
     * Default branch name of the repository.
     */
    defaultBranch?: string;
    /**
     * List of gitignore template names user can choose from.
     * Valid values can be viewed at https://cloud.google.com/secure-source-manager/docs/reference/rest/v1/projects.locations.repositories#initialconfig.
     */
    gitignores?: string[];
    /**
     * License template name user can choose from.
     * Valid values can be viewed at https://cloud.google.com/secure-source-manager/docs/reference/rest/v1/projects.locations.repositories#initialconfig.
     */
    license?: string;
    /**
     * README template name.
     * Valid values can be viewed at https://cloud.google.com/secure-source-manager/docs/reference/rest/v1/projects.locations.repositories#initialconfig.
     */
    readme?: string;
}

export interface SecureSourceManagerRepositoryTimeouts {
    create?: string;
    delete?: string;
}

export interface SecureSourceManagerRepositoryUri {
    api: string;
    gitHttps: string;
    html: string;
}

export interface SecurityposturePostureDeploymentTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SecurityposturePosturePolicySet {
    /**
     * Description of the policy set.
     */
    description?: string;
    /**
     * List of security policy
     */
    policies: outputs.SecurityposturePosturePolicySetPolicy[];
    /**
     * ID of the policy set.
     */
    policySetId: string;
}

export interface SecurityposturePosturePolicySetPolicy {
    /**
     * Mapping for policy to security standards and controls.
     */
    complianceStandards?: outputs.SecurityposturePosturePolicySetPolicyComplianceStandard[];
    /**
     * Policy constraint definition.It can have the definition of one of following constraints: orgPolicyConstraint orgPolicyConstraintCustom securityHealthAnalyticsModule securityHealthAnalyticsCustomModule
     */
    constraint: outputs.SecurityposturePosturePolicySetPolicyConstraint;
    /**
     * Description of the policy.
     */
    description?: string;
    /**
     * ID of the policy.
     */
    policyId: string;
}

export interface SecurityposturePosturePolicySetPolicyComplianceStandard {
    /**
     * Mapping of security controls for the policy.
     */
    control?: string;
    /**
     * Mapping of compliance standards for the policy.
     */
    standard?: string;
}

export interface SecurityposturePosturePolicySetPolicyConstraint {
    /**
     * Organization policy canned constraint definition.
     */
    orgPolicyConstraint?: outputs.SecurityposturePosturePolicySetPolicyConstraintOrgPolicyConstraint;
    /**
     * Organization policy custom constraint policy definition.
     */
    orgPolicyConstraintCustom?: outputs.SecurityposturePosturePolicySetPolicyConstraintOrgPolicyConstraintCustom;
    /**
     * Definition of Security Health Analytics Custom Module.
     */
    securityHealthAnalyticsCustomModule?: outputs.SecurityposturePosturePolicySetPolicyConstraintSecurityHealthAnalyticsCustomModule;
    /**
     * Security Health Analytics built-in detector definition.
     */
    securityHealthAnalyticsModule?: outputs.SecurityposturePosturePolicySetPolicyConstraintSecurityHealthAnalyticsModule;
}

export interface SecurityposturePosturePolicySetPolicyConstraintOrgPolicyConstraint {
    /**
     * Organization policy canned constraint Id
     */
    cannedConstraintId: string;
    /**
     * Definition of policy rules
     */
    policyRules: outputs.SecurityposturePosturePolicySetPolicyConstraintOrgPolicyConstraintPolicyRule[];
}

export interface SecurityposturePosturePolicySetPolicyConstraintOrgPolicyConstraintCustom {
    /**
     * Organization policy custom constraint definition.
     */
    customConstraint?: outputs.SecurityposturePosturePolicySetPolicyConstraintOrgPolicyConstraintCustomCustomConstraint;
    /**
     * Definition of policy rules
     */
    policyRules: outputs.SecurityposturePosturePolicySetPolicyConstraintOrgPolicyConstraintCustomPolicyRule[];
}

export interface SecurityposturePosturePolicySetPolicyConstraintOrgPolicyConstraintCustomCustomConstraint {
    /**
     * The action to take if the condition is met. Possible values: ["ALLOW", "DENY"]
     */
    actionType: string;
    /**
     * A CEL condition that refers to a supported service resource, for example 'resource.management.autoUpgrade == false'. For details about CEL usage, see [Common Expression Language](https://cloud.google.com/resource-manager/docs/organization-policy/creating-managing-custom-constraints#common_expression_language).
     */
    condition: string;
    /**
     * A human-friendly description of the constraint to display as an error message when the policy is violated.
     */
    description?: string;
    /**
     * A human-friendly name for the constraint.
     */
    displayName?: string;
    /**
     * A list of RESTful methods for which to enforce the constraint. Can be 'CREATE', 'UPDATE', or both. Not all Google Cloud services support both methods. To see supported methods for each service, find the service in [Supported services](https://cloud.google.com/resource-manager/docs/organization-policy/custom-constraint-supported-services).
     */
    methodTypes: string[];
    /**
     * Immutable. The name of the custom constraint. This is unique within the organization.
     */
    name: string;
    /**
     * Immutable. The fully qualified name of the Google Cloud REST resource containing the object and field you want to restrict. For example, 'container.googleapis.com/NodePool'.
     */
    resourceTypes: string[];
}

export interface SecurityposturePosturePolicySetPolicyConstraintOrgPolicyConstraintCustomPolicyRule {
    /**
     * Setting this to true means that all values are allowed. This field can be set only in policies for list constraints.
     */
    allowAll?: boolean;
    /**
     * Represents a textual expression in the Common Expression Language (CEL) syntax. CEL is a C-like expression language.
     * This page details the objects and attributes that are used to the build the CEL expressions for
     * custom access levels - https://cloud.google.com/access-context-manager/docs/custom-access-level-spec.
     */
    condition?: outputs.SecurityposturePosturePolicySetPolicyConstraintOrgPolicyConstraintCustomPolicyRuleCondition;
    /**
     * Setting this to true means that all values are denied. This field can be set only in policies for list constraints.
     */
    denyAll?: boolean;
    /**
     * If 'true', then the policy is enforced. If 'false', then any configuration is acceptable.
     * This field can be set only in policies for boolean constraints.
     */
    enforce?: boolean;
    /**
     * List of values to be used for this policy rule. This field can be set only in policies for list constraints.
     */
    values?: outputs.SecurityposturePosturePolicySetPolicyConstraintOrgPolicyConstraintCustomPolicyRuleValues;
}

export interface SecurityposturePosturePolicySetPolicyConstraintOrgPolicyConstraintCustomPolicyRuleCondition {
    /**
     * Description of the expression
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression: string;
    /**
     * String indicating the location of the expression for error reporting, e.g. a file name and a position in the file
     */
    location?: string;
    /**
     * Title for the expression, i.e. a short string describing its purpose.
     */
    title?: string;
}

export interface SecurityposturePosturePolicySetPolicyConstraintOrgPolicyConstraintCustomPolicyRuleValues {
    /**
     * List of values allowed at this resource.
     */
    allowedValues?: string[];
    /**
     * List of values denied at this resource.
     */
    deniedValues?: string[];
}

export interface SecurityposturePosturePolicySetPolicyConstraintOrgPolicyConstraintPolicyRule {
    /**
     * Setting this to true means that all values are allowed. This field can be set only in policies for list constraints.
     */
    allowAll?: boolean;
    /**
     * Represents a textual expression in the Common Expression Language (CEL) syntax. CEL is a C-like expression language.
     * This page details the objects and attributes that are used to the build the CEL expressions for
     * custom access levels - https://cloud.google.com/access-context-manager/docs/custom-access-level-spec.
     */
    condition?: outputs.SecurityposturePosturePolicySetPolicyConstraintOrgPolicyConstraintPolicyRuleCondition;
    /**
     * Setting this to true means that all values are denied. This field can be set only in policies for list constraints.
     */
    denyAll?: boolean;
    /**
     * If 'true', then the policy is enforced. If 'false', then any configuration is acceptable.
     * This field can be set only in policies for boolean constraints.
     */
    enforce?: boolean;
    /**
     * List of values to be used for this policy rule. This field can be set only in policies for list constraints.
     */
    values?: outputs.SecurityposturePosturePolicySetPolicyConstraintOrgPolicyConstraintPolicyRuleValues;
}

export interface SecurityposturePosturePolicySetPolicyConstraintOrgPolicyConstraintPolicyRuleCondition {
    /**
     * Description of the expression
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression: string;
    /**
     * String indicating the location of the expression for error reporting, e.g. a file name and a position in the file
     */
    location?: string;
    /**
     * Title for the expression, i.e. a short string describing its purpose.
     */
    title?: string;
}

export interface SecurityposturePosturePolicySetPolicyConstraintOrgPolicyConstraintPolicyRuleValues {
    /**
     * List of values allowed at this resource.
     */
    allowedValues?: string[];
    /**
     * List of values denied at this resource.
     */
    deniedValues?: string[];
}

export interface SecurityposturePosturePolicySetPolicyConstraintSecurityHealthAnalyticsCustomModule {
    /**
     * Custom module details.
     */
    config: outputs.SecurityposturePosturePolicySetPolicyConstraintSecurityHealthAnalyticsCustomModuleConfig;
    /**
     * The display name of the Security Health Analytics custom module. This
     * display name becomes the finding category for all findings that are
     * returned by this custom module.
     */
    displayName?: string;
    /**
     * A server generated id of custom module.
     */
    id: string;
    /**
     * The state of enablement for the module at its level of the resource hierarchy. Possible values: ["ENABLEMENT_STATE_UNSPECIFIED", "ENABLED", "DISABLED"]
     */
    moduleEnablementState?: string;
}

export interface SecurityposturePosturePolicySetPolicyConstraintSecurityHealthAnalyticsCustomModuleConfig {
    /**
     * Custom output properties. A set of optional name-value pairs that define custom source properties to
     * return with each finding that is generated by the custom module. The custom
     * source properties that are defined here are included in the finding JSON
     * under 'sourceProperties'.
     */
    customOutput?: outputs.SecurityposturePosturePolicySetPolicyConstraintSecurityHealthAnalyticsCustomModuleConfigCustomOutput;
    /**
     * Text that describes the vulnerability or misconfiguration that the custom
     * module detects.
     */
    description?: string;
    /**
     * The CEL expression to evaluate to produce findings.When the expression
     * evaluates to true against a resource, a finding is generated.
     */
    predicate: outputs.SecurityposturePosturePolicySetPolicyConstraintSecurityHealthAnalyticsCustomModuleConfigPredicate;
    /**
     * An explanation of the recommended steps that security teams can take to
     * resolve the detected issue
     */
    recommendation?: string;
    /**
     * The resource types that the custom module operates on. Each custom module
     * can specify up to 5 resource types.
     */
    resourceSelector: outputs.SecurityposturePosturePolicySetPolicyConstraintSecurityHealthAnalyticsCustomModuleConfigResourceSelector;
    /**
     * The severity to assign to findings generated by the module. Possible values: ["SEVERITY_UNSPECIFIED", "CRITICAL", "HIGH", "MEDIUM", "LOW"]
     */
    severity: string;
}

export interface SecurityposturePosturePolicySetPolicyConstraintSecurityHealthAnalyticsCustomModuleConfigCustomOutput {
    /**
     * A list of custom output properties to add to the finding.
     */
    properties?: outputs.SecurityposturePosturePolicySetPolicyConstraintSecurityHealthAnalyticsCustomModuleConfigCustomOutputProperty[];
}

export interface SecurityposturePosturePolicySetPolicyConstraintSecurityHealthAnalyticsCustomModuleConfigCustomOutputProperty {
    /**
     * Name of the property for the custom output.
     */
    name: string;
    /**
     * The CEL expression for the custom output. A resource property can be
     * specified to return the value of the property or a text string enclosed
     * in quotation marks.
     */
    valueExpression?: outputs.SecurityposturePosturePolicySetPolicyConstraintSecurityHealthAnalyticsCustomModuleConfigCustomOutputPropertyValueExpression;
}

export interface SecurityposturePosturePolicySetPolicyConstraintSecurityHealthAnalyticsCustomModuleConfigCustomOutputPropertyValueExpression {
    /**
     * Description of the expression
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression: string;
    /**
     * String indicating the location of the expression for error reporting, e.g. a file name and a position in the file
     */
    location?: string;
    /**
     * Title for the expression, i.e. a short string describing its purpose.
     */
    title?: string;
}

export interface SecurityposturePosturePolicySetPolicyConstraintSecurityHealthAnalyticsCustomModuleConfigPredicate {
    /**
     * Description of the expression
     */
    description?: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression: string;
    /**
     * String indicating the location of the expression for error reporting, e.g. a file name and a position in the file
     */
    location?: string;
    /**
     * Title for the expression, i.e. a short string describing its purpose.
     */
    title?: string;
}

export interface SecurityposturePosturePolicySetPolicyConstraintSecurityHealthAnalyticsCustomModuleConfigResourceSelector {
    /**
     * The resource types to run the detector on.
     */
    resourceTypes: string[];
}

export interface SecurityposturePosturePolicySetPolicyConstraintSecurityHealthAnalyticsModule {
    /**
     * The state of enablement for the module at its level of the resource hierarchy. Possible values: ["ENABLEMENT_STATE_UNSPECIFIED", "ENABLED", "DISABLED"]
     */
    moduleEnablementState?: string;
    /**
     * The name of the module eg: BIGQUERY_TABLE_CMEK_DISABLED.
     */
    moduleName: string;
}

export interface SecurityposturePostureTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ServiceAccountIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ServiceAccountIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface ServiceAccountTimeouts {
    create?: string;
}

export interface ServiceNetworkingConnectionTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface ServiceNetworkingPeeredDnsDomainTimeouts {
    create?: string;
    delete?: string;
    read?: string;
}

export interface ServiceNetworkingVpcServiceControlsTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SourcerepoRepositoryIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface SourcerepoRepositoryIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface SourcerepoRepositoryPubsubConfig {
    /**
     * The format of the Cloud Pub/Sub messages.
     * - PROTOBUF: The message payload is a serialized protocol buffer of SourceRepoEvent.
     * - JSON: The message payload is a JSON string of SourceRepoEvent. Possible values: ["PROTOBUF", "JSON"]
     */
    messageFormat: string;
    /**
     * Email address of the service account used for publishing Cloud Pub/Sub messages.
     * This service account needs to be in the same project as the PubsubConfig. When added,
     * the caller needs to have iam.serviceAccounts.actAs permission on this service account.
     * If unspecified, it defaults to the compute engine default service account.
     */
    serviceAccountEmail: string;
    topic: string;
}

export interface SourcerepoRepositoryTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SpannerDatabaseEncryptionConfig {
    /**
     * Fully qualified name of the KMS key to use to encrypt this database. This key must exist
     * in the same location as the Spanner Database.
     */
    kmsKeyName: string;
}

export interface SpannerDatabaseIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface SpannerDatabaseIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface SpannerDatabaseTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SpannerInstanceAutoscalingConfig {
    /**
     * Defines scale in controls to reduce the risk of response latency
     * and outages due to abrupt scale-in events. Users can define the minimum and
     * maximum compute capacity allocated to the instance, and the autoscaler will
     * only scale within that range. Users can either use nodes or processing
     * units to specify the limits, but should use the same unit to set both the
     * min_limit and max_limit.
     */
    autoscalingLimits?: outputs.SpannerInstanceAutoscalingConfigAutoscalingLimits;
    /**
     * Defines scale in controls to reduce the risk of response latency
     * and outages due to abrupt scale-in events
     */
    autoscalingTargets?: outputs.SpannerInstanceAutoscalingConfigAutoscalingTargets;
}

export interface SpannerInstanceAutoscalingConfigAutoscalingLimits {
    /**
     * Specifies maximum number of nodes allocated to the instance. If set, this number
     * should be greater than or equal to min_nodes.
     */
    maxNodes?: number;
    /**
     * Specifies maximum number of processing units allocated to the instance.
     * If set, this number should be multiples of 1000 and be greater than or equal to
     * min_processing_units.
     */
    maxProcessingUnits?: number;
    /**
     * Specifies number of nodes allocated to the instance. If set, this number
     * should be greater than or equal to 1.
     */
    minNodes?: number;
    /**
     * Specifies minimum number of processing units allocated to the instance.
     * If set, this number should be multiples of 1000.
     */
    minProcessingUnits?: number;
}

export interface SpannerInstanceAutoscalingConfigAutoscalingTargets {
    /**
     * Specifies the target high priority cpu utilization percentage that the autoscaler
     * should be trying to achieve for the instance.
     * This number is on a scale from 0 (no utilization) to 100 (full utilization)..
     */
    highPriorityCpuUtilizationPercent?: number;
    /**
     * Specifies the target storage utilization percentage that the autoscaler
     * should be trying to achieve for the instance.
     * This number is on a scale from 0 (no utilization) to 100 (full utilization).
     */
    storageUtilizationPercent?: number;
}

export interface SpannerInstanceConfigReplica {
    /**
     * If true, this location is designated as the default leader location where
     * leader replicas are placed.
     */
    defaultLeaderLocation?: boolean;
    /**
     * The location of the serving resources, e.g. "us-central1".
     */
    location?: string;
    /**
     * Indicates the type of replica.  See the [replica types
     * documentation](https://cloud.google.com/spanner/docs/replication#replica_types)
     * for more details. Possible values: ["READ_WRITE", "READ_ONLY", "WITNESS"]
     */
    type?: string;
}

export interface SpannerInstanceConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SpannerInstanceIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface SpannerInstanceIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface SpannerInstanceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SqlDatabaseInstanceClone {
    /**
     * The name of the allocated ip range for the private ip CloudSQL instance. For example: "google-managed-services-default". If set, the cloned instance ip will be created in the allocated range. The range name must comply with [RFC 1035](https://tools.ietf.org/html/rfc1035). Specifically, the name must be 1-63 characters long and match the regular expression a-z?.
     */
    allocatedIpRange?: string;
    /**
     * (SQL Server only, use with point_in_time) clone only the specified databases from the source instance. Clone all databases if empty.
     */
    databaseNames?: string[];
    /**
     * The timestamp of the point in time that should be restored.
     */
    pointInTime?: string;
    /**
     * (Point-in-time recovery for PostgreSQL only) Clone to an instance in the specified zone. If no zone is specified, clone to the same zone as the source instance.
     */
    preferredZone?: string;
    /**
     * The name of the instance from which the point in time should be restored.
     */
    sourceInstanceName: string;
}

export interface SqlDatabaseInstanceIpAddress {
    ipAddress: string;
    timeToRetire: string;
    type: string;
}

export interface SqlDatabaseInstanceReplicaConfiguration {
    /**
     * PEM representation of the trusted CA's x509 certificate.
     */
    caCertificate?: string;
    /**
     * PEM representation of the replica's x509 certificate.
     */
    clientCertificate?: string;
    /**
     * PEM representation of the replica's private key. The corresponding public key in encoded in the client_certificate.
     */
    clientKey?: string;
    /**
     * The number of seconds between connect retries. MySQL's default is 60 seconds.
     */
    connectRetryInterval?: number;
    /**
     * Path to a SQL file in Google Cloud Storage from which replica instances are created. Format is gs://bucket/filename.
     */
    dumpFilePath?: string;
    /**
     * Specifies if the replica is the failover target. If the field is set to true the replica will be designated as a failover replica. If the master instance fails, the replica instance will be promoted as the new master instance. Not supported for Postgres
     */
    failoverTarget?: boolean;
    /**
     * Time in ms between replication heartbeats.
     */
    masterHeartbeatPeriod?: number;
    /**
     * Password for the replication connection.
     */
    password?: string;
    /**
     * Permissible ciphers for use in SSL encryption.
     */
    sslCipher?: string;
    /**
     * Username for replication connection.
     */
    username?: string;
    /**
     * True if the master's common name value is checked during the SSL handshake.
     */
    verifyServerCertificate?: boolean;
}

export interface SqlDatabaseInstanceRestoreBackupContext {
    /**
     * The ID of the backup run to restore from.
     */
    backupRunId: number;
    /**
     * The ID of the instance that the backup was taken from.
     */
    instanceId?: string;
    /**
     * The full project ID of the source instance.
     */
    project?: string;
}

export interface SqlDatabaseInstanceServerCaCert {
    cert: string;
    commonName: string;
    createTime: string;
    expirationTime: string;
    sha1Fingerprint: string;
}

export interface SqlDatabaseInstanceSettings {
    /**
     * This specifies when the instance should be active. Can be either ALWAYS, NEVER or ON_DEMAND.
     */
    activationPolicy?: string;
    activeDirectoryConfig?: outputs.SqlDatabaseInstanceSettingsActiveDirectoryConfig;
    advancedMachineFeatures?: outputs.SqlDatabaseInstanceSettingsAdvancedMachineFeatures;
    /**
     * The availability type of the Cloud SQL instance, high availability
     * (REGIONAL) or single zone (ZONAL). For all instances, ensure that
     * settings.backup_configuration.enabled is set to true.
     * For MySQL instances, ensure that settings.backup_configuration.binary_log_enabled is set to true.
     * For Postgres instances, ensure that settings.backup_configuration.point_in_time_recovery_enabled
     * is set to true. Defaults to ZONAL.
     */
    availabilityType?: string;
    backupConfiguration?: outputs.SqlDatabaseInstanceSettingsBackupConfiguration;
    /**
     * The name of server instance collation.
     */
    collation?: string;
    /**
     * Enables the enforcement of Cloud SQL Auth Proxy or Cloud SQL connectors for all the connections. If enabled, all the direct connections are rejected.
     */
    connectorEnforcement: string;
    /**
     * Data cache configurations.
     */
    dataCacheConfig?: outputs.SqlDatabaseInstanceSettingsDataCacheConfig;
    databaseFlags?: outputs.SqlDatabaseInstanceSettingsDatabaseFlag[];
    /**
     * Configuration to protect against accidental instance deletion.
     */
    deletionProtectionEnabled?: boolean;
    denyMaintenancePeriod?: outputs.SqlDatabaseInstanceSettingsDenyMaintenancePeriod;
    /**
     * Enables auto-resizing of the storage size. Defaults to true.
     */
    diskAutoresize?: boolean;
    /**
     * The maximum size, in GB, to which storage capacity can be automatically increased. The default value is 0, which specifies that there is no limit.
     */
    diskAutoresizeLimit?: number;
    /**
     * The size of data disk, in GB. Size of a running instance cannot be reduced but can be increased. The minimum value is 10GB.
     */
    diskSize: number;
    /**
     * The type of data disk: PD_SSD or PD_HDD. Defaults to PD_SSD.
     */
    diskType?: string;
    /**
     * The edition of the instance, can be ENTERPRISE or ENTERPRISE_PLUS.
     */
    edition?: string;
    /**
     * Enables Dataplex Integration.
     */
    enableDataplexIntegration?: boolean;
    /**
     * Enables Vertex AI Integration.
     */
    enableGoogleMlIntegration?: boolean;
    /**
     * Configuration of Query Insights.
     */
    insightsConfig?: outputs.SqlDatabaseInstanceSettingsInsightsConfig;
    ipConfiguration?: outputs.SqlDatabaseInstanceSettingsIpConfiguration;
    locationPreference?: outputs.SqlDatabaseInstanceSettingsLocationPreference;
    /**
     * Declares a one-hour maintenance window when an Instance can automatically restart to apply updates. The maintenance window is specified in UTC time.
     */
    maintenanceWindow?: outputs.SqlDatabaseInstanceSettingsMaintenanceWindow;
    passwordValidationPolicy?: outputs.SqlDatabaseInstanceSettingsPasswordValidationPolicy;
    /**
     * Pricing plan for this instance, can only be PER_USE.
     */
    pricingPlan?: string;
    sqlServerAuditConfig?: outputs.SqlDatabaseInstanceSettingsSqlServerAuditConfig;
    /**
     * The machine type to use. See tiers for more details and supported versions. Postgres supports only shared-core machine types, and custom machine types such as db-custom-2-13312. See the Custom Machine Type Documentation to learn about specifying custom machine types.
     */
    tier: string;
    /**
     * The time_zone to be used by the database engine (supported only for SQL Server), in SQL Server timezone format.
     */
    timeZone?: string;
    /**
     * A set of key/value user label pairs to assign to the instance.
     */
    userLabels: {[key: string]: string};
    /**
     * Used to make sure changes to the settings block are atomic.
     */
    version: number;
}

export interface SqlDatabaseInstanceSettingsActiveDirectoryConfig {
    /**
     * Domain name of the Active Directory for SQL Server (e.g., mydomain.com).
     */
    domain: string;
}

export interface SqlDatabaseInstanceSettingsAdvancedMachineFeatures {
    /**
     * The number of threads per physical core. Can be 1 or 2.
     */
    threadsPerCore?: number;
}

export interface SqlDatabaseInstanceSettingsBackupConfiguration {
    backupRetentionSettings?: outputs.SqlDatabaseInstanceSettingsBackupConfigurationBackupRetentionSettings;
    /**
     * True if binary logging is enabled. If settings.backup_configuration.enabled is false, this must be as well. Can only be used with MySQL.
     */
    binaryLogEnabled?: boolean;
    /**
     * True if backup configuration is enabled.
     */
    enabled?: boolean;
    /**
     * Location of the backup configuration.
     */
    location?: string;
    /**
     * True if Point-in-time recovery is enabled.
     */
    pointInTimeRecoveryEnabled?: boolean;
    /**
     * HH:MM format time indicating when backup configuration starts.
     */
    startTime: string;
    /**
     * The number of days of transaction logs we retain for point in time restore, from 1-7. (For PostgreSQL Enterprise Plus instances, from 1 to 35.)
     */
    transactionLogRetentionDays: number;
}

export interface SqlDatabaseInstanceSettingsBackupConfigurationBackupRetentionSettings {
    /**
     * Number of backups to retain.
     */
    retainedBackups: number;
    /**
     * The unit that 'retainedBackups' represents. Defaults to COUNT
     */
    retentionUnit?: string;
}

export interface SqlDatabaseInstanceSettingsDataCacheConfig {
    /**
     * Whether data cache is enabled for the instance.
     */
    dataCacheEnabled?: boolean;
}

export interface SqlDatabaseInstanceSettingsDatabaseFlag {
    /**
     * Name of the flag.
     */
    name: string;
    /**
     * Value of the flag.
     */
    value: string;
}

export interface SqlDatabaseInstanceSettingsDenyMaintenancePeriod {
    /**
     * End date before which maintenance will not take place. The date is in format yyyy-mm-dd i.e., 2020-11-01, or mm-dd, i.e., 11-01
     */
    endDate: string;
    /**
     * Start date after which maintenance will not take place. The date is in format yyyy-mm-dd i.e., 2020-11-01, or mm-dd, i.e., 11-01
     */
    startDate: string;
    /**
     * Time in UTC when the "deny maintenance period" starts on start_date and ends on end_date. The time is in format: HH:mm:SS, i.e., 00:00:00
     */
    time: string;
}

export interface SqlDatabaseInstanceSettingsInsightsConfig {
    /**
     * True if Query Insights feature is enabled.
     */
    queryInsightsEnabled?: boolean;
    /**
     * Number of query execution plans captured by Insights per minute for all queries combined. Between 0 and 20. Default to 5.
     */
    queryPlansPerMinute: number;
    /**
     * Maximum query length stored in bytes. Between 256 and 4500. Default to 1024.
     */
    queryStringLength?: number;
    /**
     * True if Query Insights will record application tags from query when enabled.
     */
    recordApplicationTags?: boolean;
    /**
     * True if Query Insights will record client address when enabled.
     */
    recordClientAddress?: boolean;
}

export interface SqlDatabaseInstanceSettingsIpConfiguration {
    /**
     * The name of the allocated ip range for the private ip CloudSQL instance. For example: "google-managed-services-default". If set, the instance ip will be created in the allocated range. The range name must comply with RFC 1035. Specifically, the name must be 1-63 characters long and match the regular expression a-z?.
     */
    allocatedIpRange?: string;
    authorizedNetworks?: outputs.SqlDatabaseInstanceSettingsIpConfigurationAuthorizedNetwork[];
    /**
     * Whether Google Cloud services such as BigQuery are allowed to access data in this Cloud SQL instance over a private IP connection. SQLSERVER database type is not supported.
     */
    enablePrivatePathForGoogleCloudServices?: boolean;
    /**
     * Whether this Cloud SQL instance should be assigned a public IPV4 address. At least ipv4_enabled must be enabled or a private_network must be configured.
     */
    ipv4Enabled?: boolean;
    /**
     * The VPC network from which the Cloud SQL instance is accessible for private IP. For example, projects/myProject/global/networks/default. Specifying a network enables private IP. At least ipv4_enabled must be enabled or a private_network must be configured. This setting can be updated, but it cannot be removed after it is set.
     */
    privateNetwork?: string;
    /**
     * PSC settings for a Cloud SQL instance.
     */
    pscConfigs?: outputs.SqlDatabaseInstanceSettingsIpConfigurationPscConfig[];
    /**
     * Specify how the server certificate's Certificate Authority is hosted.
     */
    serverCaMode: string;
    /**
     * Specify how SSL connection should be enforced in DB connections.
     */
    sslMode: string;
}

export interface SqlDatabaseInstanceSettingsIpConfigurationAuthorizedNetwork {
    expirationTime?: string;
    name?: string;
    value: string;
}

export interface SqlDatabaseInstanceSettingsIpConfigurationPscConfig {
    /**
     * List of consumer projects that are allow-listed for PSC connections to this instance. This instance can be connected to with PSC from any network in these projects. Each consumer project in this list may be represented by a project number (numeric) or by a project id (alphanumeric).
     */
    allowedConsumerProjects?: string[];
    /**
     * Whether PSC connectivity is enabled for this instance.
     */
    pscEnabled?: boolean;
}

export interface SqlDatabaseInstanceSettingsLocationPreference {
    /**
     * A Google App Engine application whose zone to remain in. Must be in the same region as this instance.
     */
    followGaeApplication?: string;
    /**
     * The preferred Compute Engine zone for the secondary/failover
     */
    secondaryZone?: string;
    /**
     * The preferred compute engine zone.
     */
    zone?: string;
}

export interface SqlDatabaseInstanceSettingsMaintenanceWindow {
    /**
     * Day of week (1-7), starting on Monday
     */
    day?: number;
    /**
     * Hour of day (0-23), ignored if day not set
     */
    hour?: number;
    /**
     * Receive updates after one week (canary) or after two weeks (stable) or after five weeks (week5) of notification.
     */
    updateTrack?: string;
}

export interface SqlDatabaseInstanceSettingsPasswordValidationPolicy {
    /**
     * Password complexity.
     */
    complexity?: string;
    /**
     * Disallow username as a part of the password.
     */
    disallowUsernameSubstring?: boolean;
    /**
     * Whether the password policy is enabled or not.
     */
    enablePasswordPolicy: boolean;
    /**
     * Minimum number of characters allowed.
     */
    minLength?: number;
    /**
     * Minimum interval after which the password can be changed. This flag is only supported for PostgresSQL.
     */
    passwordChangeInterval?: string;
    /**
     * Number of previous passwords that cannot be reused.
     */
    reuseInterval?: number;
}

export interface SqlDatabaseInstanceSettingsSqlServerAuditConfig {
    /**
     * The name of the destination bucket (e.g., gs://mybucket).
     */
    bucket?: string;
    /**
     * How long to keep generated audit files. A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s"..
     */
    retentionInterval?: string;
    /**
     * How often to upload generated audit files. A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
     */
    uploadInterval?: string;
}

export interface SqlDatabaseInstanceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SqlDatabaseTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface SqlSourceRepresentationInstanceTimeouts {
    create?: string;
    delete?: string;
}

export interface SqlSslCertTimeouts {
    create?: string;
    delete?: string;
}

export interface SqlUserPasswordPolicy {
    /**
     * Number of failed attempts allowed before the user get locked.
     */
    allowedFailedAttempts?: number;
    /**
     * If true, the check that will lock user after too many failed login attempts will be enabled.
     */
    enableFailedAttemptsCheck?: boolean;
    /**
     * If true, the user must specify the current password before changing the password. This flag is supported only for MySQL.
     */
    enablePasswordVerification?: boolean;
    /**
     * Password expiration duration with one week grace period.
     */
    passwordExpirationDuration?: string;
    statuses: outputs.SqlUserPasswordPolicyStatus[];
}

export interface SqlUserPasswordPolicyStatus {
    locked: boolean;
    passwordExpirationTime: string;
}

export interface SqlUserSqlServerUserDetail {
    disabled: boolean;
    serverRoles: string[];
}

export interface SqlUserTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface StorageBucketAccessControlTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface StorageBucketAutoclass {
    /**
     * While set to true, autoclass automatically transitions objects in your bucket to appropriate storage classes based on each object's access pattern.
     */
    enabled: boolean;
    /**
     * The storage class that objects in the bucket eventually transition to if they are not read for a certain length of time. Supported values include: NEARLINE, ARCHIVE.
     */
    terminalStorageClass: string;
}

export interface StorageBucketCor {
    /**
     * The value, in seconds, to return in the Access-Control-Max-Age header used in preflight responses.
     */
    maxAgeSeconds?: number;
    /**
     * The list of HTTP methods on which to include CORS response headers, (GET, OPTIONS, POST, etc) Note: "*" is permitted in the list of methods, and means "any method".
     */
    methods?: string[];
    /**
     * The list of Origins eligible to receive CORS response headers. Note: "*" is permitted in the list of origins, and means "any Origin".
     */
    origins?: string[];
    /**
     * The list of HTTP headers other than the simple response headers to give permission for the user-agent to share across domains.
     */
    responseHeaders?: string[];
}

export interface StorageBucketCustomPlacementConfig {
    /**
     * The list of individual regions that comprise a dual-region bucket. See the docs for a list of acceptable regions. Note: If any of the data_locations changes, it will recreate the bucket.
     */
    dataLocations: string[];
}

export interface StorageBucketEncryption {
    /**
     * A Cloud KMS key that will be used to encrypt objects inserted into this bucket, if no encryption method is specified. You must pay attention to whether the crypto key is available in the location that this bucket is created in. See the docs for more details.
     */
    defaultKmsKeyName: string;
}

export interface StorageBucketIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface StorageBucketIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface StorageBucketLifecycleRule {
    /**
     * The Lifecycle Rule's action configuration. A single block of this type is supported.
     */
    action: outputs.StorageBucketLifecycleRuleAction;
    /**
     * The Lifecycle Rule's condition configuration.
     */
    condition: outputs.StorageBucketLifecycleRuleCondition;
}

export interface StorageBucketLifecycleRuleAction {
    /**
     * The target Storage Class of objects affected by this Lifecycle Rule. Supported values include: MULTI_REGIONAL, REGIONAL, NEARLINE, COLDLINE, ARCHIVE.
     */
    storageClass?: string;
    /**
     * The type of the action of this Lifecycle Rule. Supported values include: Delete, SetStorageClass and AbortIncompleteMultipartUpload.
     */
    type: string;
}

export interface StorageBucketLifecycleRuleCondition {
    /**
     * Minimum age of an object in days to satisfy this condition.
     */
    age?: number;
    /**
     * Creation date of an object in RFC 3339 (e.g. 2017-06-13) to satisfy this condition.
     */
    createdBefore?: string;
    /**
     * Creation date of an object in RFC 3339 (e.g. 2017-06-13) to satisfy this condition.
     */
    customTimeBefore?: string;
    /**
     * Number of days elapsed since the user-specified timestamp set on an object.
     */
    daysSinceCustomTime?: number;
    /**
     * Number of days elapsed since the noncurrent timestamp of an object. This
     * 										condition is relevant only for versioned objects.
     */
    daysSinceNoncurrentTime?: number;
    /**
     * One or more matching name prefixes to satisfy this condition.
     */
    matchesPrefixes?: string[];
    /**
     * Storage Class of objects to satisfy this condition. Supported values include: MULTI_REGIONAL, REGIONAL, NEARLINE, COLDLINE, ARCHIVE, STANDARD, DURABLE_REDUCED_AVAILABILITY.
     */
    matchesStorageClasses?: string[];
    /**
     * One or more matching name suffixes to satisfy this condition.
     */
    matchesSuffixes?: string[];
    /**
     * Creation date of an object in RFC 3339 (e.g. 2017-06-13) to satisfy this condition.
     */
    noncurrentTimeBefore?: string;
    /**
     * Relevant only for versioned objects. The number of newer versions of an object to satisfy this condition.
     */
    numNewerVersions?: number;
    /**
     * While set true, age value will be sent in the request even for zero value of the field. This field is only useful for setting 0 value to the age field. It can be used alone or together with age.
     */
    sendAgeIfZero?: boolean;
    /**
     * While set true, days_since_custom_time value will be sent in the request even for zero value of the field. This field is only useful for setting 0 value to the days_since_custom_time field. It can be used alone or together with days_since_custom_time.
     */
    sendDaysSinceCustomTimeIfZero?: boolean;
    /**
     * While set true, days_since_noncurrent_time value will be sent in the request even for zero value of the field. This field is only useful for setting 0 value to the days_since_noncurrent_time field. It can be used alone or together with days_since_noncurrent_time.
     */
    sendDaysSinceNoncurrentTimeIfZero?: boolean;
    /**
     * While set true, num_newer_versions value will be sent in the request even for zero value of the field. This field is only useful for setting 0 value to the num_newer_versions field. It can be used alone or together with num_newer_versions.
     */
    sendNumNewerVersionsIfZero?: boolean;
    /**
     * Match to live and/or archived objects. Unversioned buckets have only live objects. Supported values include: "LIVE", "ARCHIVED", "ANY".
     */
    withState: string;
}

export interface StorageBucketLogging {
    /**
     * The bucket that will receive log objects.
     */
    logBucket: string;
    /**
     * The object prefix for log objects. If it's not provided, by default Google Cloud Storage sets this to this bucket's name.
     */
    logObjectPrefix: string;
}

export interface StorageBucketObjectCustomerEncryption {
    /**
     * The encryption algorithm. Default: AES256
     */
    encryptionAlgorithm?: string;
    /**
     * Base64 encoded customer supplied encryption key.
     */
    encryptionKey: string;
}

export interface StorageBucketObjectRetention {
    /**
     * The object retention mode. Supported values include: "Unlocked", "Locked".
     */
    mode: string;
    /**
     * Time in RFC 3339 (e.g. 2030-01-01T02:03:04Z) until which object retention protects this object.
     */
    retainUntilTime: string;
}

export interface StorageBucketObjectTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface StorageBucketRetentionPolicy {
    /**
     * If set to true, the bucket will be locked and permanently restrict edits to the bucket's retention policy.  Caution: Locking a bucket is an irreversible action.
     */
    isLocked?: boolean;
    /**
     * The period of time, in seconds, that objects in the bucket must be retained and cannot be deleted, overwritten, or archived. The value must be less than 3,155,760,000 seconds.
     */
    retentionPeriod: number;
}

export interface StorageBucketSoftDeletePolicy {
    /**
     * Server-determined value that indicates the time from which the policy, or one with a greater retention, was effective. This value is in RFC 3339 format.
     */
    effectiveTime: string;
    /**
     * The duration in seconds that soft-deleted objects in the bucket will be retained and cannot be permanently deleted. Default value is 604800.
     */
    retentionDurationSeconds?: number;
}

export interface StorageBucketTimeouts {
    create?: string;
    read?: string;
    update?: string;
}

export interface StorageBucketVersioning {
    /**
     * While set to true, versioning is fully enabled for this bucket.
     */
    enabled: boolean;
}

export interface StorageBucketWebsite {
    /**
     * Behaves as the bucket's directory index where missing objects are treated as potential directories.
     */
    mainPageSuffix?: string;
    /**
     * The custom object to return when a requested resource is not found.
     */
    notFoundPage?: string;
}

export interface StorageDefaultObjectAccessControlProjectTeam {
    projectNumber: string;
    team: string;
}

export interface StorageDefaultObjectAccessControlTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface StorageHmacKeyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface StorageInsightsReportConfigCsvOptions {
    /**
     * The delimiter used to separate the fields in the inventory report CSV file.
     */
    delimiter?: string;
    /**
     * The boolean that indicates whether or not headers are included in the inventory report CSV file.
     */
    headerRequired?: boolean;
    /**
     * The character used to separate the records in the inventory report CSV file.
     */
    recordSeparator?: string;
}

export interface StorageInsightsReportConfigFrequencyOptions {
    /**
     * The date to stop generating inventory reports. For example, {"day": 15, "month": 9, "year": 2022}.
     */
    endDate: outputs.StorageInsightsReportConfigFrequencyOptionsEndDate;
    /**
     * The frequency in which inventory reports are generated. Values are DAILY or WEEKLY. Possible values: ["DAILY", "WEEKLY"]
     */
    frequency: string;
    /**
     * The date to start generating inventory reports. For example, {"day": 15, "month": 8, "year": 2022}.
     */
    startDate: outputs.StorageInsightsReportConfigFrequencyOptionsStartDate;
}

export interface StorageInsightsReportConfigFrequencyOptionsEndDate {
    /**
     * The day of the month to stop generating inventory reports.
     */
    day: number;
    /**
     * The month to stop generating inventory reports.
     */
    month: number;
    /**
     * The year to stop generating inventory reports
     */
    year: number;
}

export interface StorageInsightsReportConfigFrequencyOptionsStartDate {
    /**
     * The day of the month to start generating inventory reports.
     */
    day: number;
    /**
     * The month to start generating inventory reports.
     */
    month: number;
    /**
     * The year to start generating inventory reports
     */
    year: number;
}

export interface StorageInsightsReportConfigObjectMetadataReportOptions {
    /**
     * The metadata fields included in an inventory report.
     */
    metadataFields: string[];
    /**
     * Options for where the inventory reports are stored.
     */
    storageDestinationOptions: outputs.StorageInsightsReportConfigObjectMetadataReportOptionsStorageDestinationOptions;
    /**
     * A nested object resource
     */
    storageFilters?: outputs.StorageInsightsReportConfigObjectMetadataReportOptionsStorageFilters;
}

export interface StorageInsightsReportConfigObjectMetadataReportOptionsStorageDestinationOptions {
    /**
     * The destination bucket that stores the generated inventory reports.
     */
    bucket: string;
    /**
     * The path within the destination bucket to store generated inventory reports.
     */
    destinationPath?: string;
}

export interface StorageInsightsReportConfigObjectMetadataReportOptionsStorageFilters {
    /**
     * The filter to use when specifying which bucket to generate inventory reports for.
     */
    bucket?: string;
}

export interface StorageInsightsReportConfigTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface StorageManagedFolderIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface StorageManagedFolderIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface StorageManagedFolderTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface StorageObjectAccessControlProjectTeam {
    projectNumber: string;
    team: string;
}

export interface StorageObjectAccessControlTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface StorageTransferAgentPoolBandwidthLimit {
    /**
     * Bandwidth rate in megabytes per second, distributed across all the agents in the pool.
     */
    limitMbps: string;
}

export interface StorageTransferAgentPoolTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface StorageTransferJobEventStream {
    /**
     * Specifies the data and time at which Storage Transfer Service stops listening for events from this stream. After this time, any transfers in progress will complete, but no new transfers are initiated
     */
    eventStreamExpirationTime?: string;
    /**
     * Specifies the date and time that Storage Transfer Service starts listening for events from this stream. If no start time is specified or start time is in the past, Storage Transfer Service starts listening immediately
     */
    eventStreamStartTime?: string;
    /**
     * Specifies a unique name of the resource such as AWS SQS ARN in the form 'arn:aws:sqs:region:account_id:queue_name', or Pub/Sub subscription resource name in the form 'projects/{project}/subscriptions/{sub}'
     */
    name: string;
}

export interface StorageTransferJobNotificationConfig {
    /**
     * Event types for which a notification is desired. If empty, send notifications for all event types. The valid types are "TRANSFER_OPERATION_SUCCESS", "TRANSFER_OPERATION_FAILED", "TRANSFER_OPERATION_ABORTED".
     */
    eventTypes?: string[];
    /**
     * The desired format of the notification message payloads. One of "NONE" or "JSON".
     */
    payloadFormat: string;
    /**
     * The Topic.name of the Pub/Sub topic to which to publish notifications.
     */
    pubsubTopic: string;
}

export interface StorageTransferJobSchedule {
    /**
     * Interval between the start of each scheduled transfer. If unspecified, the default value is 24 hours. This value may not be less than 1 hour. A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
     */
    repeatInterval?: string;
    /**
     * The last day the recurring transfer will be run. If schedule_end_date is the same as schedule_start_date, the transfer will be executed only once.
     */
    scheduleEndDate?: outputs.StorageTransferJobScheduleScheduleEndDate;
    /**
     * The first day the recurring transfer is scheduled to run. If schedule_start_date is in the past, the transfer will run for the first time on the following day.
     */
    scheduleStartDate: outputs.StorageTransferJobScheduleScheduleStartDate;
    /**
     * The time in UTC at which the transfer will be scheduled to start in a day. Transfers may start later than this time. If not specified, recurring and one-time transfers that are scheduled to run today will run immediately; recurring transfers that are scheduled to run on a future date will start at approximately midnight UTC on that date. Note that when configuring a transfer with the Cloud Platform Console, the transfer's start time in a day is specified in your local timezone.
     */
    startTimeOfDay?: outputs.StorageTransferJobScheduleStartTimeOfDay;
}

export interface StorageTransferJobScheduleScheduleEndDate {
    /**
     * Day of month. Must be from 1 to 31 and valid for the year and month.
     */
    day: number;
    /**
     * Month of year. Must be from 1 to 12.
     */
    month: number;
    /**
     * Year of date. Must be from 1 to 9999.
     */
    year: number;
}

export interface StorageTransferJobScheduleScheduleStartDate {
    /**
     * Day of month. Must be from 1 to 31 and valid for the year and month.
     */
    day: number;
    /**
     * Month of year. Must be from 1 to 12.
     */
    month: number;
    /**
     * Year of date. Must be from 1 to 9999.
     */
    year: number;
}

export interface StorageTransferJobScheduleStartTimeOfDay {
    /**
     * Hours of day in 24 hour format. Should be from 0 to 23.
     */
    hours: number;
    /**
     * Minutes of hour of day. Must be from 0 to 59.
     */
    minutes: number;
    /**
     * Fractions of seconds in nanoseconds. Must be from 0 to 999,999,999.
     */
    nanos: number;
    /**
     * Seconds of minutes of the time. Must normally be from 0 to 59.
     */
    seconds: number;
}

export interface StorageTransferJobTransferSpec {
    /**
     * An AWS S3 data source.
     */
    awsS3DataSource?: outputs.StorageTransferJobTransferSpecAwsS3DataSource;
    /**
     * An Azure Blob Storage data source.
     */
    azureBlobStorageDataSource?: outputs.StorageTransferJobTransferSpecAzureBlobStorageDataSource;
    /**
     * A Google Cloud Storage data sink.
     */
    gcsDataSink?: outputs.StorageTransferJobTransferSpecGcsDataSink;
    /**
     * A Google Cloud Storage data source.
     */
    gcsDataSource?: outputs.StorageTransferJobTransferSpecGcsDataSource;
    /**
     * A HTTP URL data source.
     */
    httpDataSource?: outputs.StorageTransferJobTransferSpecHttpDataSource;
    /**
     * Only objects that satisfy these object conditions are included in the set of data source and data sink objects. Object conditions based on objects' last_modification_time do not exclude objects in a data sink.
     */
    objectConditions?: outputs.StorageTransferJobTransferSpecObjectConditions;
    /**
     * A POSIX filesystem data sink.
     */
    posixDataSink?: outputs.StorageTransferJobTransferSpecPosixDataSink;
    /**
     * A POSIX filesystem data source.
     */
    posixDataSource?: outputs.StorageTransferJobTransferSpecPosixDataSource;
    /**
     * Specifies the agent pool name associated with the posix data source. When unspecified, the default name is used.
     */
    sinkAgentPoolName: string;
    /**
     * Specifies the agent pool name associated with the posix data source. When unspecified, the default name is used.
     */
    sourceAgentPoolName: string;
    /**
     * Characteristics of how to treat files from datasource and sink during job. If the option delete_objects_unique_in_sink is true, object conditions based on objects' last_modification_time are ignored and do not exclude objects in a data source or a data sink.
     */
    transferOptions?: outputs.StorageTransferJobTransferSpecTransferOptions;
}

export interface StorageTransferJobTransferSpecAwsS3DataSource {
    /**
     * AWS credentials block.
     */
    awsAccessKey?: outputs.StorageTransferJobTransferSpecAwsS3DataSourceAwsAccessKey;
    /**
     * S3 Bucket name.
     */
    bucketName: string;
    /**
     * S3 Bucket path in bucket to transfer.
     */
    path?: string;
    /**
     * The Amazon Resource Name (ARN) of the role to support temporary credentials via 'AssumeRoleWithWebIdentity'. For more information about ARNs, see [IAM ARNs](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_identifiers.html#identifiers-arns). When a role ARN is provided, Transfer Service fetches temporary credentials for the session using a 'AssumeRoleWithWebIdentity' call for the provided role using the [GoogleServiceAccount][] for this project.
     */
    roleArn?: string;
}

export interface StorageTransferJobTransferSpecAwsS3DataSourceAwsAccessKey {
    /**
     * AWS Key ID.
     */
    accessKeyId: string;
    /**
     * AWS Secret Access Key.
     */
    secretAccessKey: string;
}

export interface StorageTransferJobTransferSpecAzureBlobStorageDataSource {
    /**
     * Credentials used to authenticate API requests to Azure.
     */
    azureCredentials: outputs.StorageTransferJobTransferSpecAzureBlobStorageDataSourceAzureCredentials;
    /**
     * The container to transfer from the Azure Storage account.
     */
    container: string;
    /**
     * Root path to transfer objects. Must be an empty string or full path name that ends with a '/'. This field is treated as an object prefix. As such, it should generally not begin with a '/'.
     */
    path: string;
    /**
     * The name of the Azure Storage account.
     */
    storageAccount: string;
}

export interface StorageTransferJobTransferSpecAzureBlobStorageDataSourceAzureCredentials {
    /**
     * Azure shared access signature.
     */
    sasToken: string;
}

export interface StorageTransferJobTransferSpecGcsDataSink {
    /**
     * Google Cloud Storage bucket name.
     */
    bucketName: string;
    /**
     * Google Cloud Storage path in bucket to transfer
     */
    path: string;
}

export interface StorageTransferJobTransferSpecGcsDataSource {
    /**
     * Google Cloud Storage bucket name.
     */
    bucketName: string;
    /**
     * Google Cloud Storage path in bucket to transfer
     */
    path: string;
}

export interface StorageTransferJobTransferSpecHttpDataSource {
    /**
     * The URL that points to the file that stores the object list entries. This file must allow public access. Currently, only URLs with HTTP and HTTPS schemes are supported.
     */
    listUrl: string;
}

export interface StorageTransferJobTransferSpecObjectConditions {
    /**
     * exclude_prefixes must follow the requirements described for include_prefixes.
     */
    excludePrefixes?: string[];
    /**
     * If include_refixes is specified, objects that satisfy the object conditions must have names that start with one of the include_prefixes and that do not start with any of the exclude_prefixes. If include_prefixes is not specified, all objects except those that have names starting with one of the exclude_prefixes must satisfy the object conditions.
     */
    includePrefixes?: string[];
    /**
     * If specified, only objects with a "last modification time" before this timestamp and objects that don't have a "last modification time" are transferred. A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
     */
    lastModifiedBefore?: string;
    /**
     * If specified, only objects with a "last modification time" on or after this timestamp and objects that don't have a "last modification time" are transferred. A timestamp in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits. Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".
     */
    lastModifiedSince?: string;
    /**
     * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
     */
    maxTimeElapsedSinceLastModification?: string;
    /**
     * A duration in seconds with up to nine fractional digits, terminated by 's'. Example: "3.5s".
     */
    minTimeElapsedSinceLastModification?: string;
}

export interface StorageTransferJobTransferSpecPosixDataSink {
    /**
     * Root directory path to the filesystem.
     */
    rootDirectory: string;
}

export interface StorageTransferJobTransferSpecPosixDataSource {
    /**
     * Root directory path to the filesystem.
     */
    rootDirectory: string;
}

export interface StorageTransferJobTransferSpecTransferOptions {
    /**
     * Whether objects should be deleted from the source after they are transferred to the sink. Note that this option and delete_objects_unique_in_sink are mutually exclusive.
     */
    deleteObjectsFromSourceAfterTransfer?: boolean;
    /**
     * Whether objects that exist only in the sink should be deleted. Note that this option and delete_objects_from_source_after_transfer are mutually exclusive.
     */
    deleteObjectsUniqueInSink?: boolean;
    /**
     * Whether overwriting objects that already exist in the sink is allowed.
     */
    overwriteObjectsAlreadyExistingInSink?: boolean;
    /**
     * When to overwrite objects that already exist in the sink. If not set, overwrite behavior is determined by overwriteObjectsAlreadyExistingInSink.
     */
    overwriteWhen?: string;
}

export interface TagsLocationTagBindingTimeouts {
    create?: string;
    delete?: string;
}

export interface TagsTagBindingTimeouts {
    create?: string;
    delete?: string;
}

export interface TagsTagKeyIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface TagsTagKeyIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface TagsTagKeyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface TagsTagValueIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface TagsTagValueIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface TagsTagValueTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface TpuNodeNetworkEndpoint {
    ipAddress: string;
    port: number;
}

export interface TpuNodeSchedulingConfig {
    /**
     * Defines whether the TPU instance is preemptible.
     */
    preemptible: boolean;
}

export interface TpuNodeTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface VertexAiDatasetEncryptionSpec {
    /**
     * Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource.
     * Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the resource is created.
     */
    kmsKeyName?: string;
}

export interface VertexAiDatasetTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface VertexAiDeploymentResourcePoolDedicatedResources {
    /**
     * A list of the metric specifications that overrides a resource utilization metric.
     */
    autoscalingMetricSpecs?: outputs.VertexAiDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpec[];
    /**
     * The specification of a single machine used by the prediction
     */
    machineSpec: outputs.VertexAiDeploymentResourcePoolDedicatedResourcesMachineSpec;
    /**
     * The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, will use min_replica_count as the default value. The value of this field impacts the charge against Vertex CPU and GPU quotas. Specifically, you will be charged for max_replica_count * number of cores in the selected machine type) and (max_replica_count * number of GPUs per replica in the selected machine type).
     */
    maxReplicaCount?: number;
    /**
     * The minimum number of machine replicas this DeployedModel will be always deployed on. This value must be greater than or equal to 1. If traffic against the DeployedModel increases, it may dynamically be deployed onto more replicas, and as traffic decreases, some of these extra replicas may be freed.
     */
    minReplicaCount: number;
}

export interface VertexAiDeploymentResourcePoolDedicatedResourcesAutoscalingMetricSpec {
    /**
     * The resource metric name. Supported metrics: For Online Prediction: * 'aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle' * 'aiplatform.googleapis.com/prediction/online/cpu/utilization'
     */
    metricName: string;
    /**
     * The target resource utilization in percentage (1% - 100%) for the given metric; once the real usage deviates from the target by a certain percentage, the machine replicas change. The default value is 60 (representing 60%) if not provided.
     */
    target?: number;
}

export interface VertexAiDeploymentResourcePoolDedicatedResourcesMachineSpec {
    /**
     * The number of accelerators to attach to the machine.
     */
    acceleratorCount?: number;
    /**
     * The type of accelerator(s) that may be attached to the machine as per accelerator_count. See possible values [here](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec#AcceleratorType).
     */
    acceleratorType?: string;
    /**
     * The type of the machine. See the [list of machine types supported for prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types).
     */
    machineType?: string;
}

export interface VertexAiDeploymentResourcePoolTimeouts {
    create?: string;
    delete?: string;
}

export interface VertexAiEndpointDeployedModel {
    automaticResources: outputs.VertexAiEndpointDeployedModelAutomaticResource[];
    createTime: string;
    dedicatedResources: outputs.VertexAiEndpointDeployedModelDedicatedResource[];
    displayName: string;
    enableAccessLogging: boolean;
    enableContainerLogging: boolean;
    id: string;
    model: string;
    modelVersionId: string;
    privateEndpoints: outputs.VertexAiEndpointDeployedModelPrivateEndpoint[];
    serviceAccount: string;
    sharedResources: string;
}

export interface VertexAiEndpointDeployedModelAutomaticResource {
    maxReplicaCount: number;
    minReplicaCount: number;
}

export interface VertexAiEndpointDeployedModelDedicatedResource {
    autoscalingMetricSpecs: outputs.VertexAiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpec[];
    machineSpecs: outputs.VertexAiEndpointDeployedModelDedicatedResourceMachineSpec[];
    maxReplicaCount: number;
    minReplicaCount: number;
}

export interface VertexAiEndpointDeployedModelDedicatedResourceAutoscalingMetricSpec {
    metricName: string;
    target: number;
}

export interface VertexAiEndpointDeployedModelDedicatedResourceMachineSpec {
    acceleratorCount: number;
    acceleratorType: string;
    machineType: string;
}

export interface VertexAiEndpointDeployedModelPrivateEndpoint {
    explainHttpUri: string;
    healthHttpUri: string;
    predictHttpUri: string;
    serviceAttachment: string;
}

export interface VertexAiEndpointEncryptionSpec {
    /**
     * Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: 'projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key'. The key needs to be in the same region as where the compute resource is created.
     */
    kmsKeyName: string;
}

export interface VertexAiEndpointTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface VertexAiFeatureGroupBigQuery {
    /**
     * The BigQuery source URI that points to either a BigQuery Table or View.
     */
    bigQuerySource: outputs.VertexAiFeatureGroupBigQueryBigQuerySource;
    /**
     * Columns to construct entityId / row keys. If not provided defaults to entityId.
     */
    entityIdColumns?: string[];
}

export interface VertexAiFeatureGroupBigQueryBigQuerySource {
    /**
     * BigQuery URI to a table, up to 2000 characters long. For example: 'bq://projectId.bqDatasetId.bqTableId.'
     */
    inputUri: string;
}

export interface VertexAiFeatureGroupFeatureTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface VertexAiFeatureGroupTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface VertexAiFeatureOnlineStoreBigtable {
    /**
     * Autoscaling config applied to Bigtable Instance.
     */
    autoScaling: outputs.VertexAiFeatureOnlineStoreBigtableAutoScaling;
}

export interface VertexAiFeatureOnlineStoreBigtableAutoScaling {
    /**
     * A percentage of the cluster's CPU capacity. Can be from 10% to 80%. When a cluster's CPU utilization exceeds the target that you have set, Bigtable immediately adds nodes to the cluster. When CPU utilization is substantially lower than the target, Bigtable removes nodes. If not set will default to 50%.
     */
    cpuUtilizationTarget: number;
    /**
     * The maximum number of nodes to scale up to. Must be greater than or equal to minNodeCount, and less than or equal to 10 times of 'minNodeCount'.
     */
    maxNodeCount: number;
    /**
     * The minimum number of nodes to scale down to. Must be greater than or equal to 1.
     */
    minNodeCount: number;
}

export interface VertexAiFeatureOnlineStoreDedicatedServingEndpoint {
    /**
     * Private service connect config.
     */
    privateServiceConnectConfig?: outputs.VertexAiFeatureOnlineStoreDedicatedServingEndpointPrivateServiceConnectConfig;
    /**
     * Domain name to use for this FeatureOnlineStore
     */
    publicEndpointDomainName: string;
    /**
     * Name of the service attachment resource. Applicable only if private service connect is enabled and after FeatureViewSync is created.
     */
    serviceAttachment: string;
}

export interface VertexAiFeatureOnlineStoreDedicatedServingEndpointPrivateServiceConnectConfig {
    /**
     * If set to true, customers will use private service connection to send request. Otherwise, the connection will set to public endpoint.
     */
    enablePrivateServiceConnect: boolean;
    /**
     * A list of Projects from which the forwarding rule will target the service attachment.
     */
    projectAllowlists?: string[];
}

export interface VertexAiFeatureOnlineStoreFeatureviewBigQuerySource {
    /**
     * Columns to construct entityId / row keys. Start by supporting 1 only.
     */
    entityIdColumns: string[];
    /**
     * The BigQuery view URI that will be materialized on each sync trigger based on FeatureView.SyncConfig.
     */
    uri: string;
}

export interface VertexAiFeatureOnlineStoreFeatureviewFeatureRegistrySource {
    /**
     * List of features that need to be synced to Online Store.
     */
    featureGroups: outputs.VertexAiFeatureOnlineStoreFeatureviewFeatureRegistrySourceFeatureGroup[];
    /**
     * The project number of the parent project of the feature Groups.
     */
    projectNumber?: string;
}

export interface VertexAiFeatureOnlineStoreFeatureviewFeatureRegistrySourceFeatureGroup {
    /**
     * Identifier of the feature group.
     */
    featureGroupId: string;
    /**
     * Identifiers of features under the feature group.
     */
    featureIds: string[];
}

export interface VertexAiFeatureOnlineStoreFeatureviewSyncConfig {
    /**
     * Cron schedule (https://en.wikipedia.org/wiki/Cron) to launch scheduled runs.
     * To explicitly set a timezone to the cron tab, apply a prefix in the cron tab: "CRON_TZ=${IANA_TIME_ZONE}" or "TZ=${IANA_TIME_ZONE}".
     */
    cron: string;
}

export interface VertexAiFeatureOnlineStoreFeatureviewTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface VertexAiFeatureOnlineStoreOptimized {
}

export interface VertexAiFeatureOnlineStoreTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface VertexAiFeaturestoreEncryptionSpec {
    /**
     * The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the compute resource is created.
     */
    kmsKeyName: string;
}

export interface VertexAiFeaturestoreEntitytypeFeatureTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface VertexAiFeaturestoreEntitytypeMonitoringConfig {
    /**
     * Threshold for categorical features of anomaly detection. This is shared by all types of Featurestore Monitoring for categorical features (i.e. Features with type (Feature.ValueType) BOOL or STRING).
     */
    categoricalThresholdConfig?: outputs.VertexAiFeaturestoreEntitytypeMonitoringConfigCategoricalThresholdConfig;
    /**
     * The config for ImportFeatures Analysis Based Feature Monitoring.
     */
    importFeaturesAnalysis?: outputs.VertexAiFeaturestoreEntitytypeMonitoringConfigImportFeaturesAnalysis;
    /**
     * Threshold for numerical features of anomaly detection. This is shared by all objectives of Featurestore Monitoring for numerical features (i.e. Features with type (Feature.ValueType) DOUBLE or INT64).
     */
    numericalThresholdConfig?: outputs.VertexAiFeaturestoreEntitytypeMonitoringConfigNumericalThresholdConfig;
    /**
     * The config for Snapshot Analysis Based Feature Monitoring.
     */
    snapshotAnalysis?: outputs.VertexAiFeaturestoreEntitytypeMonitoringConfigSnapshotAnalysis;
}

export interface VertexAiFeaturestoreEntitytypeMonitoringConfigCategoricalThresholdConfig {
    /**
     * Specify a threshold value that can trigger the alert. For categorical feature, the distribution distance is calculated by L-inifinity norm. Each feature must have a non-zero threshold if they need to be monitored. Otherwise no alert will be triggered for that feature. The default value is 0.3.
     */
    value: number;
}

export interface VertexAiFeaturestoreEntitytypeMonitoringConfigImportFeaturesAnalysis {
    /**
     * Defines the baseline to do anomaly detection for feature values imported by each [entityTypes.importFeatureValues][] operation. The value must be one of the values below:
     * * LATEST_STATS: Choose the later one statistics generated by either most recent snapshot analysis or previous import features analysis. If non of them exists, skip anomaly detection and only generate a statistics.
     * * MOST_RECENT_SNAPSHOT_STATS: Use the statistics generated by the most recent snapshot analysis if exists.
     * * PREVIOUS_IMPORT_FEATURES_STATS: Use the statistics generated by the previous import features analysis if exists.
     */
    anomalyDetectionBaseline?: string;
    /**
     * Whether to enable / disable / inherite default hebavior for import features analysis. The value must be one of the values below:
     * * DEFAULT: The default behavior of whether to enable the monitoring. EntityType-level config: disabled.
     * * ENABLED: Explicitly enables import features analysis. EntityType-level config: by default enables import features analysis for all Features under it.
     * * DISABLED: Explicitly disables import features analysis. EntityType-level config: by default disables import features analysis for all Features under it.
     */
    state?: string;
}

export interface VertexAiFeaturestoreEntitytypeMonitoringConfigNumericalThresholdConfig {
    /**
     * Specify a threshold value that can trigger the alert. For numerical feature, the distribution distance is calculated by Jensen–Shannon divergence. Each feature must have a non-zero threshold if they need to be monitored. Otherwise no alert will be triggered for that feature. The default value is 0.3.
     */
    value: number;
}

export interface VertexAiFeaturestoreEntitytypeMonitoringConfigSnapshotAnalysis {
    /**
     * The monitoring schedule for snapshot analysis. For EntityType-level config: unset / disabled = true indicates disabled by default for Features under it; otherwise by default enable snapshot analysis monitoring with monitoringInterval for Features under it.
     */
    disabled?: boolean;
    /**
     * Configuration of the snapshot analysis based monitoring pipeline running interval. The value indicates number of days. The default value is 1.
     * If both FeaturestoreMonitoringConfig.SnapshotAnalysis.monitoring_interval_days and [FeaturestoreMonitoringConfig.SnapshotAnalysis.monitoring_interval][] are set when creating/updating EntityTypes/Features, FeaturestoreMonitoringConfig.SnapshotAnalysis.monitoring_interval_days will be used.
     */
    monitoringIntervalDays?: number;
    /**
     * Customized export features time window for snapshot analysis. Unit is one day. The default value is 21 days. Minimum value is 1 day. Maximum value is 4000 days.
     */
    stalenessDays?: number;
}

export interface VertexAiFeaturestoreEntitytypeTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface VertexAiFeaturestoreOnlineServingConfig {
    /**
     * The number of nodes for each cluster. The number of nodes will not scale automatically but can be scaled manually by providing different values when updating.
     */
    fixedNodeCount?: number;
    /**
     * Online serving scaling configuration. Only one of fixedNodeCount and scaling can be set. Setting one will reset the other.
     */
    scaling?: outputs.VertexAiFeaturestoreOnlineServingConfigScaling;
}

export interface VertexAiFeaturestoreOnlineServingConfigScaling {
    /**
     * The maximum number of nodes to scale up to. Must be greater than minNodeCount, and less than or equal to 10 times of 'minNodeCount'.
     */
    maxNodeCount: number;
    /**
     * The minimum number of nodes to scale down to. Must be greater than or equal to 1.
     */
    minNodeCount: number;
}

export interface VertexAiFeaturestoreTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface VertexAiIndexDeployedIndex {
    deployedIndexId: string;
    indexEndpoint: string;
}

export interface VertexAiIndexEndpointDeployedIndexAutomaticResources {
    /**
     * The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If maxReplicaCount is not set, the default value is minReplicaCount. The max allowed replica count is 1000.
     *
     * The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, a no upper bound for scaling under heavy traffic will be assume, though Vertex AI may be unable to scale beyond certain replica number.
     */
    maxReplicaCount: number;
    /**
     * The minimum number of replicas this DeployedModel will be always deployed on. If minReplicaCount is not set, the default value is 2 (we don't provide SLA when minReplicaCount=1).
     *
     * If traffic against it increases, it may dynamically be deployed onto more replicas up to [maxReplicaCount](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/AutomaticResources#FIELDS.max_replica_count), and as traffic decreases, some of these extra replicas may be freed. If the requested value is too large, the deployment will error.
     */
    minReplicaCount: number;
}

export interface VertexAiIndexEndpointDeployedIndexDedicatedResources {
    /**
     * The minimum number of replicas this DeployedModel will be always deployed on.
     */
    machineSpec: outputs.VertexAiIndexEndpointDeployedIndexDedicatedResourcesMachineSpec;
    /**
     * The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If maxReplicaCount is not set, the default value is minReplicaCount
     */
    maxReplicaCount: number;
    /**
     * The minimum number of machine replicas this DeployedModel will be always deployed on. This value must be greater than or equal to 1.
     */
    minReplicaCount: number;
}

export interface VertexAiIndexEndpointDeployedIndexDedicatedResourcesMachineSpec {
    /**
     * The type of the machine.
     *
     * See the [list of machine types supported for prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types)
     *
     * See the [list of machine types supported for custom training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types).
     *
     * For [DeployedModel](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints#DeployedModel) this field is optional, and the default value is n1-standard-2. For [BatchPredictionJob](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#BatchPredictionJob) or as part of [WorkerPoolSpec](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/CustomJobSpec#WorkerPoolSpec) this field is required.
     */
    machineType?: string;
}

export interface VertexAiIndexEndpointDeployedIndexDeployedIndexAuthConfig {
    /**
     * Defines the authentication provider that the DeployedIndex uses.
     */
    authProvider?: outputs.VertexAiIndexEndpointDeployedIndexDeployedIndexAuthConfigAuthProvider;
}

export interface VertexAiIndexEndpointDeployedIndexDeployedIndexAuthConfigAuthProvider {
    /**
     * A list of allowed JWT issuers. Each entry must be a valid Google service account, in the following format: service-account-name@project-id.iam.gserviceaccount.com
     */
    allowedIssuers?: string[];
    /**
     * The list of JWT audiences. that are allowed to access. A JWT containing any of these audiences will be accepted.
     */
    audiences?: string[];
}

export interface VertexAiIndexEndpointDeployedIndexPrivateEndpoint {
    matchGrpcAddress: string;
    pscAutomatedEndpoints: outputs.VertexAiIndexEndpointDeployedIndexPrivateEndpointPscAutomatedEndpoint[];
    serviceAttachment: string;
}

export interface VertexAiIndexEndpointDeployedIndexPrivateEndpointPscAutomatedEndpoint {
    matchAddress: string;
    network: string;
    projectId: string;
}

export interface VertexAiIndexEndpointDeployedIndexTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface VertexAiIndexEndpointPrivateServiceConnectConfig {
    /**
     * If set to true, the IndexEndpoint is created without private service access.
     */
    enablePrivateServiceConnect: boolean;
    /**
     * A list of Projects from which the forwarding rule will target the service attachment.
     */
    projectAllowlists?: string[];
}

export interface VertexAiIndexEndpointTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface VertexAiIndexIndexStat {
    shardsCount: number;
    vectorsCount: string;
}

export interface VertexAiIndexMetadata {
    /**
     * The configuration of the Matching Engine Index.
     */
    config?: outputs.VertexAiIndexMetadataConfig;
    /**
     * Allows inserting, updating  or deleting the contents of the Matching Engine Index.
     * The string must be a valid Cloud Storage directory path. If this
     * field is set when calling IndexService.UpdateIndex, then no other
     * Index field can be also updated as part of the same call.
     * The expected structure and format of the files this URI points to is
     * described at https://cloud.google.com/vertex-ai/docs/matching-engine/using-matching-engine#input-data-format
     */
    contentsDeltaUri: string;
    /**
     * If this field is set together with contentsDeltaUri when calling IndexService.UpdateIndex,
     * then existing content of the Index will be replaced by the data from the contentsDeltaUri.
     */
    isCompleteOverwrite?: boolean;
}

export interface VertexAiIndexMetadataConfig {
    /**
     * The configuration with regard to the algorithms used for efficient search.
     */
    algorithmConfig?: outputs.VertexAiIndexMetadataConfigAlgorithmConfig;
    /**
     * The default number of neighbors to find via approximate search before exact reordering is
     * performed. Exact reordering is a procedure where results returned by an
     * approximate search algorithm are reordered via a more expensive distance computation.
     * Required if tree-AH algorithm is used.
     */
    approximateNeighborsCount?: number;
    /**
     * The number of dimensions of the input vectors.
     */
    dimensions: number;
    /**
     * The distance measure used in nearest neighbor search. The value must be one of the followings:
     * * SQUARED_L2_DISTANCE: Euclidean (L_2) Distance
     * * L1_DISTANCE: Manhattan (L_1) Distance
     * * COSINE_DISTANCE: Cosine Distance. Defined as 1 - cosine similarity.
     * * DOT_PRODUCT_DISTANCE: Dot Product Distance. Defined as a negative of the dot product
     */
    distanceMeasureType?: string;
    /**
     * Type of normalization to be carried out on each vector. The value must be one of the followings:
     * * UNIT_L2_NORM: Unit L2 normalization type
     * * NONE: No normalization type is specified.
     */
    featureNormType?: string;
    /**
     * Index data is split into equal parts to be processed. These are called "shards".
     * The shard size must be specified when creating an index. The value must be one of the followings:
     * * SHARD_SIZE_SMALL: Small (2GB)
     * * SHARD_SIZE_MEDIUM: Medium (20GB)
     * * SHARD_SIZE_LARGE: Large (50GB)
     */
    shardSize: string;
}

export interface VertexAiIndexMetadataConfigAlgorithmConfig {
    /**
     * Configuration options for using brute force search, which simply implements the
     * standard linear search in the database for each query.
     */
    bruteForceConfig?: outputs.VertexAiIndexMetadataConfigAlgorithmConfigBruteForceConfig;
    /**
     * Configuration options for using the tree-AH algorithm (Shallow tree + Asymmetric Hashing).
     * Please refer to this paper for more details: https://arxiv.org/abs/1908.10396
     */
    treeAhConfig?: outputs.VertexAiIndexMetadataConfigAlgorithmConfigTreeAhConfig;
}

export interface VertexAiIndexMetadataConfigAlgorithmConfigBruteForceConfig {
}

export interface VertexAiIndexMetadataConfigAlgorithmConfigTreeAhConfig {
    /**
     * Number of embeddings on each leaf node. The default value is 1000 if not set.
     */
    leafNodeEmbeddingCount?: number;
    /**
     * The default percentage of leaf nodes that any query may be searched. Must be in
     * range 1-100, inclusive. The default value is 10 (means 10%) if not set.
     */
    leafNodesToSearchPercent?: number;
}

export interface VertexAiIndexTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface VertexAiTensorboardEncryptionSpec {
    /**
     * The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource.
     * Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the resource is created.
     */
    kmsKeyName: string;
}

export interface VertexAiTensorboardTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface VmwareengineClusterNodeTypeConfig {
    /**
     * Customized number of cores available to each node of the type.
     * This number must always be one of 'nodeType.availableCustomCoreCounts'.
     * If zero is provided max value from 'nodeType.availableCustomCoreCounts' will be used.
     * Once the customer is created then corecount cannot be changed.
     */
    customCoreCount?: number;
    /**
     * The number of nodes of this type in the cluster.
     */
    nodeCount: number;
    nodeTypeId: string;
}

export interface VmwareengineClusterTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface VmwareengineExternalAccessRuleDestinationIpRange {
    /**
     * The name of an 'ExternalAddress' resource.
     */
    externalAddress?: string;
    /**
     * An IP address range in the CIDR format.
     */
    ipAddressRange?: string;
}

export interface VmwareengineExternalAccessRuleSourceIpRange {
    /**
     * A single IP address.
     */
    ipAddress?: string;
    /**
     * An IP address range in the CIDR format.
     */
    ipAddressRange?: string;
}

export interface VmwareengineExternalAccessRuleTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface VmwareengineExternalAddressTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface VmwareengineNetworkPeeringTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface VmwareengineNetworkPolicyExternalIp {
    /**
     * True if the service is enabled; false otherwise.
     */
    enabled?: boolean;
    /**
     * State of the service. New values may be added to this enum when appropriate.
     */
    state: string;
}

export interface VmwareengineNetworkPolicyInternetAccess {
    /**
     * True if the service is enabled; false otherwise.
     */
    enabled?: boolean;
    /**
     * State of the service. New values may be added to this enum when appropriate.
     */
    state: string;
}

export interface VmwareengineNetworkPolicyTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface VmwareengineNetworkTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface VmwareengineNetworkVpcNetwork {
    network: string;
    type: string;
}

export interface VmwareenginePrivateCloudHcx {
    fqdn: string;
    internalIp: string;
    state: string;
    version: string;
}

export interface VmwareenginePrivateCloudManagementCluster {
    /**
     * The user-provided identifier of the new Cluster. The identifier must meet the following requirements:
     *   * Only contains 1-63 alphanumeric characters and hyphens
     *   * Begins with an alphabetical character
     *   * Ends with a non-hyphen character
     *   * Not formatted as a UUID
     *   * Complies with RFC 1034 (https://datatracker.ietf.org/doc/html/rfc1034) (section 3.5)
     */
    clusterId: string;
    /**
     * The map of cluster node types in this cluster,
     * where the key is canonical identifier of the node type (corresponds to the NodeType).
     */
    nodeTypeConfigs?: outputs.VmwareenginePrivateCloudManagementClusterNodeTypeConfig[];
    /**
     * The stretched cluster configuration for the private cloud.
     */
    stretchedClusterConfig?: outputs.VmwareenginePrivateCloudManagementClusterStretchedClusterConfig;
}

export interface VmwareenginePrivateCloudManagementClusterNodeTypeConfig {
    /**
     * Customized number of cores available to each node of the type.
     * This number must always be one of 'nodeType.availableCustomCoreCounts'.
     * If zero is provided max value from 'nodeType.availableCustomCoreCounts' will be used.
     * This cannot be changed once the PrivateCloud is created.
     */
    customCoreCount?: number;
    /**
     * The number of nodes of this type in the cluster.
     */
    nodeCount: number;
    nodeTypeId: string;
}

export interface VmwareenginePrivateCloudManagementClusterStretchedClusterConfig {
    /**
     * Zone that will remain operational when connection between the two zones is lost.
     */
    preferredLocation?: string;
    /**
     * Additional zone for a higher level of availability and load balancing.
     */
    secondaryLocation?: string;
}

export interface VmwareenginePrivateCloudNetworkConfig {
    /**
     * DNS Server IP of the Private Cloud.
     */
    dnsServerIp: string;
    /**
     * Management CIDR used by VMware management appliances.
     */
    managementCidr: string;
    /**
     * The IP address layout version of the management IP address range.
     * Possible versions include:
     * * managementIpAddressLayoutVersion=1: Indicates the legacy IP address layout used by some existing private clouds. This is no longer supported for new private clouds
     * as it does not support all features.
     * * managementIpAddressLayoutVersion=2: Indicates the latest IP address layout
     * used by all newly created private clouds. This version supports all current features.
     */
    managementIpAddressLayoutVersion: number;
    /**
     * The relative resource name of the VMware Engine network attached to the private cloud.
     * Specify the name in the following form: projects/{project}/locations/{location}/vmwareEngineNetworks/{vmwareEngineNetworkId}
     * where {project} can either be a project number or a project ID.
     */
    vmwareEngineNetwork?: string;
    /**
     * The canonical name of the VMware Engine network in
     * the form: projects/{project_number}/locations/{location}/vmwareEngineNetworks/{vmwareEngineNetworkId}
     */
    vmwareEngineNetworkCanonical: string;
}

export interface VmwareenginePrivateCloudNsx {
    fqdn: string;
    internalIp: string;
    state: string;
    version: string;
}

export interface VmwareenginePrivateCloudTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface VmwareenginePrivateCloudVcenter {
    fqdn: string;
    internalIp: string;
    state: string;
    version: string;
}

export interface VmwareengineSubnetDhcpAddressRange {
    firstAddress: string;
    lastAddress: string;
}

export interface VmwareengineSubnetTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface VpcAccessConnectorSubnet {
    /**
     * Subnet name (relative, not fully qualified). E.g. if the full subnet selfLink is
     * https://compute.googleapis.com/compute/v1/projects/{project}/regions/{region}/subnetworks/{subnetName} the correct input for this field would be {subnetName}"
     */
    name?: string;
    /**
     * Project in which the subnet exists. If not set, this project is assumed to be the project for which the connector create request was issued.
     */
    projectId: string;
}

export interface VpcAccessConnectorTimeouts {
    create?: string;
    delete?: string;
}

export interface WorkbenchInstanceGceSetup {
    /**
     * The hardware accelerators used on this instance. If you use accelerators, make sure that your configuration has
     * [enough vCPUs and memory to support the 'machine_type' you have selected](https://cloud.google.com/compute/docs/gpus/#gpus-list).
     * Currently supports only one accelerator configuration.
     */
    acceleratorConfigs?: outputs.WorkbenchInstanceGceSetupAcceleratorConfig[];
    /**
     * The definition of a boot disk.
     */
    bootDisk?: outputs.WorkbenchInstanceGceSetupBootDisk;
    /**
     * Use a container image to start the workbench instance.
     */
    containerImage?: outputs.WorkbenchInstanceGceSetupContainerImage;
    /**
     * Data disks attached to the VM instance. Currently supports only one data disk.
     */
    dataDisks?: outputs.WorkbenchInstanceGceSetupDataDisks;
    /**
     * Optional. If true, no external IP will be assigned to this VM instance.
     */
    disablePublicIp: boolean;
    /**
     * Optional. Flag to enable ip forwarding or not, default false/off.
     * https://cloud.google.com/vpc/docs/using-routes#canipforward
     */
    enableIpForwarding?: boolean;
    /**
     * Optional. The machine type of the VM instance. https://cloud.google.com/compute/docs/machine-resource
     */
    machineType: string;
    /**
     * Optional. Custom metadata to apply to this instance.
     */
    metadata: {[key: string]: string};
    /**
     * The network interfaces for the VM. Supports only one interface.
     */
    networkInterfaces?: outputs.WorkbenchInstanceGceSetupNetworkInterface[];
    /**
     * The service account that serves as an identity for the VM instance. Currently supports only one service account.
     */
    serviceAccounts?: outputs.WorkbenchInstanceGceSetupServiceAccount[];
    /**
     * A set of Shielded Instance options. See [Images using supported Shielded
     * VM features](https://cloud.google.com/compute/docs/instances/modifying-shielded-vm).
     * Not all combinations are valid.
     */
    shieldedInstanceConfig?: outputs.WorkbenchInstanceGceSetupShieldedInstanceConfig;
    /**
     * Optional. The Compute Engine tags to add to instance (see [Tagging
     * instances](https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
     */
    tags: string[];
    /**
     * Definition of a custom Compute Engine virtual machine image for starting
     * a workbench instance with the environment installed directly on the VM.
     */
    vmImage?: outputs.WorkbenchInstanceGceSetupVmImage;
}

export interface WorkbenchInstanceGceSetupAcceleratorConfig {
    /**
     * Optional. Count of cores of this accelerator.
     */
    coreCount?: string;
    /**
     * Optional. Type of this accelerator. Possible values: ["NVIDIA_TESLA_P100", "NVIDIA_TESLA_V100", "NVIDIA_TESLA_P4", "NVIDIA_TESLA_T4", "NVIDIA_TESLA_A100", "NVIDIA_A100_80GB", "NVIDIA_L4", "NVIDIA_TESLA_T4_VWS", "NVIDIA_TESLA_P100_VWS", "NVIDIA_TESLA_P4_VWS"]
     */
    type?: string;
}

export interface WorkbenchInstanceGceSetupBootDisk {
    /**
     * Optional. Input only. Disk encryption method used on the boot and
     * data disks, defaults to GMEK. Possible values: ["GMEK", "CMEK"]
     */
    diskEncryption: string;
    /**
     * Optional. The size of the boot disk in GB attached to this instance,
     * up to a maximum of 64000 GB (64 TB). If not specified, this defaults to the
     * recommended value of 150GB.
     */
    diskSizeGb: string;
    /**
     * Optional. Indicates the type of the disk. Possible values: ["PD_STANDARD", "PD_SSD", "PD_BALANCED", "PD_EXTREME"]
     */
    diskType: string;
    /**
     * 'Optional. The KMS key used to encrypt the disks, only
     * applicable if disk_encryption is CMEK. Format: 'projects/{project_id}/locations/{location}/keyRings/{key_ring_id}/cryptoKeys/{key_id}'
     * Learn more about using your own encryption keys.'
     */
    kmsKey?: string;
}

export interface WorkbenchInstanceGceSetupContainerImage {
    /**
     * The path to the container image repository.
     * For example: gcr.io/{project_id}/{imageName}
     */
    repository: string;
    /**
     * The tag of the container image. If not specified, this defaults to the latest tag.
     */
    tag?: string;
}

export interface WorkbenchInstanceGceSetupDataDisks {
    /**
     * Optional. Input only. Disk encryption method used on the boot
     * and data disks, defaults to GMEK. Possible values: ["GMEK", "CMEK"]
     */
    diskEncryption: string;
    /**
     * Optional. The size of the disk in GB attached to this VM instance,
     * up to a maximum of 64000 GB (64 TB). If not specified, this defaults to
     * 100.
     */
    diskSizeGb: string;
    /**
     * Optional. Input only. Indicates the type of the disk. Possible values: ["PD_STANDARD", "PD_SSD", "PD_BALANCED", "PD_EXTREME"]
     */
    diskType?: string;
    /**
     * 'Optional. The KMS key used to encrypt the disks,
     * only applicable if disk_encryption is CMEK. Format: 'projects/{project_id}/locations/{location}/keyRings/{key_ring_id}/cryptoKeys/{key_id}'
     * Learn more about using your own encryption keys.'
     */
    kmsKey?: string;
}

export interface WorkbenchInstanceGceSetupNetworkInterface {
    /**
     * Optional. An array of configurations for this interface. Currently, only one access
     * config, ONE_TO_ONE_NAT, is supported. If no accessConfigs specified, the
     * instance will have an external internet access through an ephemeral
     * external IP address.
     */
    accessConfigs?: outputs.WorkbenchInstanceGceSetupNetworkInterfaceAccessConfig[];
    /**
     * Optional. The name of the VPC that this VM instance is in.
     */
    network: string;
    /**
     * Optional. The type of vNIC to be used on this interface. This
     * may be gVNIC or VirtioNet. Possible values: ["VIRTIO_NET", "GVNIC"]
     */
    nicType?: string;
    /**
     * Optional. The name of the subnet that this VM instance is in.
     */
    subnet: string;
}

export interface WorkbenchInstanceGceSetupNetworkInterfaceAccessConfig {
    /**
     * An external IP address associated with this instance. Specify an unused
     * static external IP address available to the project or leave this field
     * undefined to use an IP from a shared ephemeral IP address pool. If you
     * specify a static external IP address, it must live in the same region as
     * the zone of the instance.
     */
    externalIp: string;
}

export interface WorkbenchInstanceGceSetupServiceAccount {
    /**
     * Optional. Email address of the service account.
     */
    email: string;
    /**
     * Output only. The list of scopes to be made available for this
     * service account. Set by the CLH to https://www.googleapis.com/auth/cloud-platform
     */
    scopes: string[];
}

export interface WorkbenchInstanceGceSetupShieldedInstanceConfig {
    /**
     * Optional. Defines whether the VM instance has integrity monitoring
     * enabled. Enables monitoring and attestation of the boot integrity of the VM
     * instance. The attestation is performed against the integrity policy baseline.
     * This baseline is initially derived from the implicitly trusted boot image
     * when the VM instance is created. Enabled by default.
     */
    enableIntegrityMonitoring?: boolean;
    /**
     * Optional. Defines whether the VM instance has Secure Boot enabled.
     * Secure Boot helps ensure that the system only runs authentic software by verifying
     * the digital signature of all boot components, and halting the boot process
     * if signature verification fails. Disabled by default.
     */
    enableSecureBoot?: boolean;
    /**
     * Optional. Defines whether the VM instance has the vTPM enabled.
     * Enabled by default.
     */
    enableVtpm?: boolean;
}

export interface WorkbenchInstanceGceSetupVmImage {
    /**
     * Optional. Use this VM image family to find the image; the newest
     * image in this family will be used.
     */
    family?: string;
    /**
     * Optional. Use VM image name to find the image.
     */
    name?: string;
    /**
     * The name of the Google Cloud project that this VM image belongs to.
     * Format: {project_id}
     */
    project?: string;
}

export interface WorkbenchInstanceHealthInfo {
}

export interface WorkbenchInstanceIamBindingCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface WorkbenchInstanceIamMemberCondition {
    description?: string;
    expression: string;
    title: string;
}

export interface WorkbenchInstanceTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export interface WorkbenchInstanceUpgradeHistory {
    action: string;
    containerImage: string;
    createTime: string;
    framework: string;
    snapshot: string;
    state: string;
    targetVersion: string;
    version: string;
    vmImage: string;
}

export interface WorkflowsWorkflowTimeouts {
    create?: string;
    delete?: string;
    update?: string;
}

export namespace config {
    export interface Batching {
        enableBatching?: boolean;
        sendAfter?: string;
    }

}
