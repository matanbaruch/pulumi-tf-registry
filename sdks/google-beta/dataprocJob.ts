// *** WARNING: this file was generated by pulumi-language-nodejs. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "./types/input";
import * as outputs from "./types/output";
import * as utilities from "./utilities";

export class DataprocJob extends pulumi.CustomResource {
    /**
     * Get an existing DataprocJob resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state Any extra arguments used during the lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    public static get(name: string, id: pulumi.Input<pulumi.ID>, state?: DataprocJobState, opts?: pulumi.CustomResourceOptions): DataprocJob {
        return new DataprocJob(name, <any>state, { ...opts, id: id });
    }

    /** @internal */
    public static readonly __pulumiType = 'google-beta:index/dataprocJob:DataprocJob';

    /**
     * Returns true if the given object is an instance of DataprocJob.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    public static isInstance(obj: any): obj is DataprocJob {
        if (obj === undefined || obj === null) {
            return false;
        }
        return obj['__pulumiType'] === DataprocJob.__pulumiType;
    }

    /**
     * Output-only. If present, the location of miscellaneous control files which may be used as part of job setup and
     * handling. If not present, control files may be placed in the same location as driver_output_uri.
     */
    public /*out*/ readonly driverControlsFilesUri!: pulumi.Output<string>;
    /**
     * Output-only. A URI pointing to the location of the stdout of the job's driver program
     */
    public /*out*/ readonly driverOutputResourceUri!: pulumi.Output<string>;
    public /*out*/ readonly effectiveLabels!: pulumi.Output<{[key: string]: string}>;
    /**
     * By default, you can only delete inactive jobs within Dataproc. Setting this to true, and calling destroy, will ensure
     * that the job is first cancelled before issuing the delete.
     */
    public readonly forceDelete!: pulumi.Output<boolean | undefined>;
    /**
     * The config of Hadoop job
     */
    public readonly hadoopConfig!: pulumi.Output<outputs.DataprocJobHadoopConfig | undefined>;
    /**
     * The config of hive job
     */
    public readonly hiveConfig!: pulumi.Output<outputs.DataprocJobHiveConfig | undefined>;
    /**
     * Optional. The labels to associate with this job. **Note**: This field is non-authoritative, and will only manage the
     * labels present in your configuration. Please refer to the field 'effective_labels' for all of the labels present on the
     * resource.
     */
    public readonly labels!: pulumi.Output<{[key: string]: string} | undefined>;
    /**
     * The config of pag job.
     */
    public readonly pigConfig!: pulumi.Output<outputs.DataprocJobPigConfig | undefined>;
    /**
     * The config of job placement.
     */
    public readonly placement!: pulumi.Output<outputs.DataprocJobPlacement>;
    /**
     * The config of presto job
     */
    public readonly prestoConfig!: pulumi.Output<outputs.DataprocJobPrestoConfig | undefined>;
    /**
     * The project in which the cluster can be found and jobs subsequently run against. If it is not provided, the provider
     * project is used.
     */
    public readonly project!: pulumi.Output<string>;
    /**
     * The config of pySpark job.
     */
    public readonly pysparkConfig!: pulumi.Output<outputs.DataprocJobPysparkConfig | undefined>;
    /**
     * The reference of the job
     */
    public readonly reference!: pulumi.Output<outputs.DataprocJobReference | undefined>;
    /**
     * The Cloud Dataproc region. This essentially determines which clusters are available for this job to be submitted to. If
     * not specified, defaults to global.
     */
    public readonly region!: pulumi.Output<string | undefined>;
    /**
     * Optional. Job scheduling configuration.
     */
    public readonly scheduling!: pulumi.Output<outputs.DataprocJobScheduling | undefined>;
    /**
     * The config of the Spark job.
     */
    public readonly sparkConfig!: pulumi.Output<outputs.DataprocJobSparkConfig | undefined>;
    /**
     * The config of SparkSql job
     */
    public readonly sparksqlConfig!: pulumi.Output<outputs.DataprocJobSparksqlConfig | undefined>;
    /**
     * The status of the job.
     */
    public /*out*/ readonly statuses!: pulumi.Output<outputs.DataprocJobStatus[]>;
    /**
     * The combination of labels configured directly on the resource and default labels configured on the provider.
     */
    public /*out*/ readonly terraformLabels!: pulumi.Output<{[key: string]: string}>;
    public readonly timeouts!: pulumi.Output<outputs.DataprocJobTimeouts | undefined>;

    /**
     * Create a DataprocJob resource with the given unique name, arguments, and options.
     *
     * @param name The _unique_ name of the resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param opts A bag of options that control this resource's behavior.
     */
    constructor(name: string, args: DataprocJobArgs, opts?: pulumi.CustomResourceOptions)
    constructor(name: string, argsOrState?: DataprocJobArgs | DataprocJobState, opts?: pulumi.CustomResourceOptions) {
        let resourceInputs: pulumi.Inputs = {};
        opts = opts || {};
        if (opts.id) {
            const state = argsOrState as DataprocJobState | undefined;
            resourceInputs["driverControlsFilesUri"] = state ? state.driverControlsFilesUri : undefined;
            resourceInputs["driverOutputResourceUri"] = state ? state.driverOutputResourceUri : undefined;
            resourceInputs["effectiveLabels"] = state ? state.effectiveLabels : undefined;
            resourceInputs["forceDelete"] = state ? state.forceDelete : undefined;
            resourceInputs["hadoopConfig"] = state ? state.hadoopConfig : undefined;
            resourceInputs["hiveConfig"] = state ? state.hiveConfig : undefined;
            resourceInputs["labels"] = state ? state.labels : undefined;
            resourceInputs["pigConfig"] = state ? state.pigConfig : undefined;
            resourceInputs["placement"] = state ? state.placement : undefined;
            resourceInputs["prestoConfig"] = state ? state.prestoConfig : undefined;
            resourceInputs["project"] = state ? state.project : undefined;
            resourceInputs["pysparkConfig"] = state ? state.pysparkConfig : undefined;
            resourceInputs["reference"] = state ? state.reference : undefined;
            resourceInputs["region"] = state ? state.region : undefined;
            resourceInputs["scheduling"] = state ? state.scheduling : undefined;
            resourceInputs["sparkConfig"] = state ? state.sparkConfig : undefined;
            resourceInputs["sparksqlConfig"] = state ? state.sparksqlConfig : undefined;
            resourceInputs["statuses"] = state ? state.statuses : undefined;
            resourceInputs["terraformLabels"] = state ? state.terraformLabels : undefined;
            resourceInputs["timeouts"] = state ? state.timeouts : undefined;
        } else {
            const args = argsOrState as DataprocJobArgs | undefined;
            if ((!args || args.placement === undefined) && !opts.urn) {
                throw new Error("Missing required property 'placement'");
            }
            resourceInputs["forceDelete"] = args ? args.forceDelete : undefined;
            resourceInputs["hadoopConfig"] = args ? args.hadoopConfig : undefined;
            resourceInputs["hiveConfig"] = args ? args.hiveConfig : undefined;
            resourceInputs["labels"] = args ? args.labels : undefined;
            resourceInputs["pigConfig"] = args ? args.pigConfig : undefined;
            resourceInputs["placement"] = args ? args.placement : undefined;
            resourceInputs["prestoConfig"] = args ? args.prestoConfig : undefined;
            resourceInputs["project"] = args ? args.project : undefined;
            resourceInputs["pysparkConfig"] = args ? args.pysparkConfig : undefined;
            resourceInputs["reference"] = args ? args.reference : undefined;
            resourceInputs["region"] = args ? args.region : undefined;
            resourceInputs["scheduling"] = args ? args.scheduling : undefined;
            resourceInputs["sparkConfig"] = args ? args.sparkConfig : undefined;
            resourceInputs["sparksqlConfig"] = args ? args.sparksqlConfig : undefined;
            resourceInputs["timeouts"] = args ? args.timeouts : undefined;
            resourceInputs["driverControlsFilesUri"] = undefined /*out*/;
            resourceInputs["driverOutputResourceUri"] = undefined /*out*/;
            resourceInputs["effectiveLabels"] = undefined /*out*/;
            resourceInputs["statuses"] = undefined /*out*/;
            resourceInputs["terraformLabels"] = undefined /*out*/;
        }
        opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts);
        super(DataprocJob.__pulumiType, name, resourceInputs, opts, false /*dependency*/, utilities.getPackage());
    }
}

/**
 * Input properties used for looking up and filtering DataprocJob resources.
 */
export interface DataprocJobState {
    /**
     * Output-only. If present, the location of miscellaneous control files which may be used as part of job setup and
     * handling. If not present, control files may be placed in the same location as driver_output_uri.
     */
    driverControlsFilesUri?: pulumi.Input<string>;
    /**
     * Output-only. A URI pointing to the location of the stdout of the job's driver program
     */
    driverOutputResourceUri?: pulumi.Input<string>;
    effectiveLabels?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    /**
     * By default, you can only delete inactive jobs within Dataproc. Setting this to true, and calling destroy, will ensure
     * that the job is first cancelled before issuing the delete.
     */
    forceDelete?: pulumi.Input<boolean>;
    /**
     * The config of Hadoop job
     */
    hadoopConfig?: pulumi.Input<inputs.DataprocJobHadoopConfig>;
    /**
     * The config of hive job
     */
    hiveConfig?: pulumi.Input<inputs.DataprocJobHiveConfig>;
    /**
     * Optional. The labels to associate with this job. **Note**: This field is non-authoritative, and will only manage the
     * labels present in your configuration. Please refer to the field 'effective_labels' for all of the labels present on the
     * resource.
     */
    labels?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    /**
     * The config of pag job.
     */
    pigConfig?: pulumi.Input<inputs.DataprocJobPigConfig>;
    /**
     * The config of job placement.
     */
    placement?: pulumi.Input<inputs.DataprocJobPlacement>;
    /**
     * The config of presto job
     */
    prestoConfig?: pulumi.Input<inputs.DataprocJobPrestoConfig>;
    /**
     * The project in which the cluster can be found and jobs subsequently run against. If it is not provided, the provider
     * project is used.
     */
    project?: pulumi.Input<string>;
    /**
     * The config of pySpark job.
     */
    pysparkConfig?: pulumi.Input<inputs.DataprocJobPysparkConfig>;
    /**
     * The reference of the job
     */
    reference?: pulumi.Input<inputs.DataprocJobReference>;
    /**
     * The Cloud Dataproc region. This essentially determines which clusters are available for this job to be submitted to. If
     * not specified, defaults to global.
     */
    region?: pulumi.Input<string>;
    /**
     * Optional. Job scheduling configuration.
     */
    scheduling?: pulumi.Input<inputs.DataprocJobScheduling>;
    /**
     * The config of the Spark job.
     */
    sparkConfig?: pulumi.Input<inputs.DataprocJobSparkConfig>;
    /**
     * The config of SparkSql job
     */
    sparksqlConfig?: pulumi.Input<inputs.DataprocJobSparksqlConfig>;
    /**
     * The status of the job.
     */
    statuses?: pulumi.Input<pulumi.Input<inputs.DataprocJobStatus>[]>;
    /**
     * The combination of labels configured directly on the resource and default labels configured on the provider.
     */
    terraformLabels?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    timeouts?: pulumi.Input<inputs.DataprocJobTimeouts>;
}

/**
 * The set of arguments for constructing a DataprocJob resource.
 */
export interface DataprocJobArgs {
    /**
     * By default, you can only delete inactive jobs within Dataproc. Setting this to true, and calling destroy, will ensure
     * that the job is first cancelled before issuing the delete.
     */
    forceDelete?: pulumi.Input<boolean>;
    /**
     * The config of Hadoop job
     */
    hadoopConfig?: pulumi.Input<inputs.DataprocJobHadoopConfig>;
    /**
     * The config of hive job
     */
    hiveConfig?: pulumi.Input<inputs.DataprocJobHiveConfig>;
    /**
     * Optional. The labels to associate with this job. **Note**: This field is non-authoritative, and will only manage the
     * labels present in your configuration. Please refer to the field 'effective_labels' for all of the labels present on the
     * resource.
     */
    labels?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    /**
     * The config of pag job.
     */
    pigConfig?: pulumi.Input<inputs.DataprocJobPigConfig>;
    /**
     * The config of job placement.
     */
    placement: pulumi.Input<inputs.DataprocJobPlacement>;
    /**
     * The config of presto job
     */
    prestoConfig?: pulumi.Input<inputs.DataprocJobPrestoConfig>;
    /**
     * The project in which the cluster can be found and jobs subsequently run against. If it is not provided, the provider
     * project is used.
     */
    project?: pulumi.Input<string>;
    /**
     * The config of pySpark job.
     */
    pysparkConfig?: pulumi.Input<inputs.DataprocJobPysparkConfig>;
    /**
     * The reference of the job
     */
    reference?: pulumi.Input<inputs.DataprocJobReference>;
    /**
     * The Cloud Dataproc region. This essentially determines which clusters are available for this job to be submitted to. If
     * not specified, defaults to global.
     */
    region?: pulumi.Input<string>;
    /**
     * Optional. Job scheduling configuration.
     */
    scheduling?: pulumi.Input<inputs.DataprocJobScheduling>;
    /**
     * The config of the Spark job.
     */
    sparkConfig?: pulumi.Input<inputs.DataprocJobSparkConfig>;
    /**
     * The config of SparkSql job
     */
    sparksqlConfig?: pulumi.Input<inputs.DataprocJobSparksqlConfig>;
    timeouts?: pulumi.Input<inputs.DataprocJobTimeouts>;
}
